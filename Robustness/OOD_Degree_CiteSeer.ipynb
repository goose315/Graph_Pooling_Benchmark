{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/boot/anaconda3/envs/ 1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport sys\nimport torch\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom ogb.graphproppred import PygGraphPropPredDataset, Evaluator\nfrom torch_geometric.loader import DataLoader\nimport os\nimport random\nimport pandas as pd\nimport torch\nimport torch_geometric.transforms as T\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data\nfrom torch_geometric.data.datapipes import functional_transform\nfrom torch_geometric.transforms import BaseTransform\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import Actor\nfrom torch_geometric.datasets import GNNBenchmarkDataset\nfrom torch_geometric.datasets import TUDataset\nfrom sklearn.metrics import r2_score\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import MoleculeNet\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.utils import to_networkx\nfrom torch.nn import Linear\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport time\nimport psutil\nimport torch\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom typing import List, Optional, Tuple, Union\nimport math\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.nn.models.mlp import Linear\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import BatchNorm\nimport os.path as osp\nimport time\nfrom math import ceil\nfrom torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nfrom torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nimport random\nfrom torch_geometric.nn import GCNConv\nimport os.path as osp\nimport time\nfrom math import ceil\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, dense_diff_pool\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, TopKPooling, SAGPooling\nfrom torch_geometric.utils import to_dense_batch\nfrom sklearn.metrics import f1_score"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train Mask: 1663 nodes\n",
                        "Val Mask: 831 nodes\n",
                        "Test Mask: 833 nodes\n",
                        "Train Mask Average Degree: 4.3313\n",
                        "Val Mask Average Degree: 1.3430\n",
                        "Test Mask Average Degree: 0.9424\n"
                    ]
                }
            ],
            "source": "import torch\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.utils import degree\ndata_path = \"/data/ /Pooling/\"\ndataset_CiteSeer = Planetoid(root=data_path, name=\"CiteSeer\")\ndata = dataset_CiteSeer[0]\ndeg = degree(data.edge_index[0], data.num_nodes)\nsorted_indices = torch.argsort(deg, descending=True)\nnum_nodes = data.num_nodes\ntrain_size = int(0.5 * num_nodes)\nval_size = int(0.25 * num_nodes)\ntest_size = num_nodes - train_size - val_size\ntrain_indices = sorted_indices[:train_size]\nval_indices = sorted_indices[train_size:train_size + val_size]\ntest_indices = sorted_indices[train_size + val_size:]\ntrain_mask = torch.zeros(num_nodes, dtype=torch.bool)\nval_mask = torch.zeros(num_nodes, dtype=torch.bool)\ntest_mask = torch.zeros(num_nodes, dtype=torch.bool)\ntrain_mask[train_indices] = True\nval_mask[val_indices] = True\ntest_mask[test_indices] = True\ndata.train_mask = train_mask\ndata.val_mask = val_mask\ndata.test_mask = test_mask\nprint(f\"Train Mask: {train_mask.sum().item()} nodes\")\nprint(f\"Val Mask: {val_mask.sum().item()} nodes\")\nprint(f\"Test Mask: {test_mask.sum().item()} nodes\")\ntrain_deg_avg = deg[train_mask].float().mean().item()\nval_deg_avg = deg[val_mask].float().mean().item()\ntest_deg_avg = deg[test_mask].float().mean().item()\nprint(f\"Train Mask Average Degree: {train_deg_avg:.4f}\")\nprint(f\"Val Mask Average Degree: {val_deg_avg:.4f}\")\nprint(f\"Test Mask Average Degree: {test_deg_avg:.4f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TopKPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 67.27$\\pm$0.35\n",
                        "Macro F1: 63.60$\\pm$0.26\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_CiteSeer.num_classes\nin_channels = dataset_CiteSeer.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_TOPK(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_TOPK, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(TopKPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_TOPK(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SAGPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 65.46$\\pm$0.61\n",
                        "Macro F1: 62.22$\\pm$0.49\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import SAGPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_CiteSeer.num_classes\nin_channels = dataset_CiteSeer.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_SAG(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_SAG, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(SAGPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_SAG(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ASAPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 67.47$\\pm$0.23\n",
                        "Macro F1: 63.94$\\pm$0.31\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_CiteSeer.num_classes\nin_channels = dataset_CiteSeer.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_ASA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_ASA, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(ASAPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_ASA(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PANPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nclass PANPooling(torch.nn.Module):\n    r\"\"\" General Graph pooling layer based on PAN, which can work with all layers.\n    \"\"\"\n    def __init__(self, in_channels, ratio=0.5, pan_pool_weight=None, min_score=None, multiplier=1,\n                 nonlinearity=torch.tanh, filter_size=3, panpool_filter_weight=None):\n        super(PANPooling, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.nonlinearity = nonlinearity\n        self.filter_size = filter_size\n        if panpool_filter_weight is None:\n            self.panpool_filter_weight = torch.nn.Parameter(0.5 * torch.ones(filter_size), requires_grad=True)\n        self.transform = Parameter(torch.ones(in_channels), requires_grad=True)\n        if pan_pool_weight is None:\n            self.pan_pool_weight = torch.nn.Parameter(0.5 * torch.ones(2), requires_grad=True)\n        else:\n            self.pan_pool_weight = pan_pool_weight\n    def forward(self, x, edge_index, M=None, batch=None, num_nodes=None):\n        \"\"\"\"\"\"\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        edge_index, edge_weight = self.panentropy_sparse(edge_index, num_nodes)\n        num_nodes = x.size(0)\n        degree = torch.zeros(num_nodes, device=edge_index.device)\n        degree = scatter_add(edge_weight, edge_index[0], out=degree)\n        xtransform = torch.matmul(x, self.transform)\n        x_transform_norm = xtransform \n        degree_norm = degree \n        score = self.pan_pool_weight[0] * x_transform_norm + self.pan_pool_weight[1] * degree_norm\n        if self.min_score is None:\n            score = self.nonlinearity(score)\n        else:\n            score = softmax(score, batch)\n        perm = self.topk(score, self.ratio, batch, self.min_score)\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n        batch = batch[perm]\n        edge_index, edge_weight = self.filter_adj(edge_index, edge_weight, perm, num_nodes=score.size(0))\n        return x, edge_index, edge_weight, batch, perm, score[perm]\n    def topk(self, x, ratio, batch, min_score=None, tol=1e-7):\n        if min_score is not None:\n            scores_max = scatter_max(x, batch)[0][batch] - tol\n            scores_min = scores_max.clamp(max=min_score)\n            perm = torch.nonzero(x > scores_min).view(-1)\n        else:\n            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n            batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n            cum_num_nodes = torch.cat(\n                [num_nodes.new_zeros(1),\n                 num_nodes.cumsum(dim=0)[:-1]], dim=0)\n            index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n            index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n            dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n            dense_x[index] = x\n            dense_x = dense_x.view(batch_size, max_num_nodes)\n            _, perm = dense_x.sort(dim=-1, descending=True)\n            perm = perm + cum_num_nodes.view(-1, 1)\n            perm = perm.view(-1)\n            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n            mask = [\n                torch.arange(k[i], dtype=torch.long, device=x.device) +\n                i * max_num_nodes for i in range(batch_size)\n            ]\n            mask = torch.cat(mask, dim=0)\n            perm = perm[mask]\n        return perm\n    def filter_adj(self, edge_index, edge_weight, perm, num_nodes=None):\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        mask = perm.new_full((num_nodes, ), -1)\n        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n        mask[perm] = i\n        row, col = edge_index\n        row, col = mask[row], mask[col]\n        mask = (row >= 0) & (col >= 0)\n        row, col = row[mask], col[mask]\n        if edge_weight is not None:\n            edge_weight = edge_weight[mask]\n        return torch.stack([row, col], dim=0), edge_weight\n    def panentropy_sparse(self, edge_index, num_nodes):\n        edge_value = torch.ones(edge_index.size(1), device=edge_index.device)\n        edge_index, edge_value = coalesce(edge_index, edge_value, num_nodes, num_nodes)\n        pan_index, pan_value = eye(num_nodes, device=edge_index.device)\n        indextmp = pan_index.clone().to(edge_index.device)\n        valuetmp = pan_value.clone().to(edge_index.device)\n        pan_value = self.panpool_filter_weight[0] * pan_value\n        for i in range(self.filter_size - 1):\n            indextmp, valuetmp = spspmm(indextmp, valuetmp, edge_index, edge_value, num_nodes, num_nodes, num_nodes)\n            valuetmp = valuetmp * self.panpool_filter_weight[i+1]\n            indextmp, valuetmp = coalesce(indextmp, valuetmp, num_nodes, num_nodes)\n            pan_index = torch.cat((pan_index, indextmp), 1)\n            pan_value = torch.cat((pan_value, valuetmp))\n        return coalesce(pan_index, pan_value, num_nodes, num_nodes, op='add')"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 67.63$\\pm$0.26\n",
                        "Macro F1: 63.85$\\pm$0.28\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_CiteSeer.num_classes\nin_channels = dataset_CiteSeer.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_PAN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_PAN, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(PANPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, score_perm = self.pools[i - 1](x, edge_index, batch=batch, M=None)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_PAN(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CoPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\ndef cumsum(x: Tensor, dim: int = 0) -> Tensor:\n    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n    Args:\n        x (torch.Tensor): The input tensor.\n        dim (int, optional): The dimension to do the operation over.\n            (default: :obj:`0`)\n    Example:\n        >>> x = torch.tensor([2, 4, 1])\n        >>> cumsum(x)\n        tensor([0, 2, 6, 7])\n    \"\"\"\n    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n    out = x.new_empty(size)\n    out.narrow(dim, 0, 1).zero_()\n    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n    return out\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\ndef topk(\n    x: Tensor,\n    ratio: Optional[Union[float, int]],\n    batch: Tensor,\n    min_score: Optional[float] = None,\n    tol: float = 1e-7,\n) -> Tensor:\n    if min_score is not None:\n        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero().view(-1)\n        return perm\n    if ratio is not None:\n        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n        if ratio >= 1:\n            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n        else:\n            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n        x, x_perm = torch.sort(x.view(-1), descending=True)\n        batch = batch[x_perm]\n        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n        ptr = cumsum(num_nodes)\n        batched_arange = arange - ptr[batch]\n        mask = batched_arange < k[batch]\n        return x_perm[batch_perm[mask]]\n    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n                     \"must be specified\")\nclass GPR_prop(MessagePassing):\n    '''\n    propagation class for GPR_GNN\n    '''\n    def __init__(self, K, alpha, Init, Gamma=None, bias=True, **kwargs):\n        super(GPR_prop, self).__init__(aggr='add', **kwargs)\n        self.K = K\n        self.Init = Init\n        self.alpha = alpha\n        assert Init in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']\n        if Init == 'SGC':\n            TEMP = 0.0*np.ones(K+1)\n            TEMP[alpha] = 1.0\n        elif Init == 'PPR':\n            TEMP = alpha*(1-alpha)**np.arange(K+1)\n            TEMP[-1] = (1-alpha)**K\n        elif Init == 'NPPR':\n            TEMP = (alpha)**np.arange(K+1)\n            TEMP = TEMP/np.sum(np.abs(TEMP))\n        elif Init == 'Random':\n            bound = np.sqrt(3/(K+1))\n            TEMP = np.random.uniform(-bound, bound, K+1)\n            TEMP = TEMP/np.sum(np.abs(TEMP))\n        elif Init == 'WS':\n            TEMP = Gamma\n        self.temp = Parameter(torch.tensor(TEMP))\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.temp)\n        for k in range(self.K+1):\n            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n        self.temp.data[-1] = (1-self.alpha)**self.K\n    def forward(self, x, edge_index, edge_weight=None):\n        edge_index, norm = gcn_norm(\n            edge_index, edge_weight, num_nodes=x.size(0), dtype=x.dtype)\n        hidden = x*(self.temp[0])\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=norm)\n            gamma = self.temp[k+1]\n            hidden = hidden + gamma*x\n        return hidden\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def __repr__(self):\n        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n                                           self.temp)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) \n        edge_index = edge_index.type(torch.long)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass graph_attention(torch.nn.Module):\n    src_nodes_dim = 0  \n    trg_nodes_dim = 1  \n    nodes_dim = 0      \n    head_dim = 1       \n    def __init__(self, num_in_features, num_out_features, num_of_heads, dropout_prob=0.6, log_attention_weights=False):\n        super().__init__()\n        self.num_of_heads = num_of_heads\n        self.num_out_features = num_out_features\n        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.init_params()\n    def init_params(self):\n        \"\"\"\n        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n        Feel free to experiment - there may be better initializations depending on your problem.\n        \"\"\"\n        nn.init.xavier_uniform_(self.linear_proj.weight)\n        nn.init.xavier_uniform_(self.scoring_fn_target)\n        nn.init.xavier_uniform_(self.scoring_fn_source)\n    def forward(self, x, edge_index):\n        in_nodes_features = x  \n        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n        scores_per_edge = scores_source_lifted + scores_target_lifted\n        return torch.sigmoid(scores_per_edge)\n    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n        \"\"\"\n        Lifts i.e. duplicates certain vectors depending on the edge index.\n        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n        \"\"\"\n        src_nodes_index = edge_index[self.src_nodes_dim]\n        trg_nodes_index = edge_index[self.trg_nodes_dim]\n        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n        return scores_source, scores_target, nodes_features_matrix_proj_lifted\nclass CoPooling(torch.nn.Module):\n    def __init__(self, ratio=0.5, K=0.05, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=None):\n        super(CoPooling, self).__init__()\n        self.ratio = ratio\n        self.calc_information_score = NodeInformationScore()\n        self.edge_ratio = edge_ratio\n        self.prop1 = GPR_prop(K, alpha, Init, Gamma)\n        score_dim = 32\n        self.G_att = graph_attention(num_in_features=nhid, num_out_features=score_dim, num_of_heads=1)\n        self.weight = Parameter(torch.Tensor(2*nhid, nhid))\n        nn.init.xavier_uniform_(self.weight.data)\n        self.bias = Parameter(torch.Tensor(nhid))\n        nn.init.zeros_(self.bias.data)\n        self.reset_parameters()\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight.data)\n        nn.init.zeros_(self.bias.data)\n        self.prop1.reset_parameters()\n        self.G_att.init_params()\n    def forward(self, x, edge_index, edge_attr, batch=None, nodes_index=None, node_attr=None):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        ori_batch = batch.clone()\n        device = x.device\n        num_nodes = x.shape[0]\n        x_cut = self.prop1(x, edge_index) \n        attention = self.G_att(x_cut, edge_index) \n        attention = attention.sum(dim=1) \n        edge_index, attention = add_self_loops(edge_index, attention, 1.0, num_nodes) \n        edge_index_t, attention_t = transpose(edge_index, attention, num_nodes, num_nodes)\n        edge_tmp = torch.cat((edge_index, edge_index_t), 1)\n        att_tmp = torch.cat((attention, attention_t),0)\n        edge_index, attention = coalesce(edge_tmp, att_tmp, num_nodes, num_nodes, 'mean')\n        attention_np = attention.cpu().data.numpy()\n        cut_val = np.percentile(attention_np, int(100*(1-self.edge_ratio))) \n        attention = attention * (attention >= cut_val) \n        kep_idx = attention > 0.0\n        cut_edge_index, cut_edge_attr = edge_index[:, kep_idx], attention[kep_idx]\n        x_information_score = self.calc_information_score(x, cut_edge_index, cut_edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        perm = topk(score, self.ratio, batch)\n        x_topk = x[perm]\n        batch = batch[perm]\n        if nodes_index is not None:\n            nodes_index = nodes_index[perm]\n        if node_attr is not None:\n            node_attr = node_attr[perm]\n        if cut_edge_index is not None or cut_edge_index.nelement() != 0:\n            induced_edge_index, induced_edge_attr = filter_adj(cut_edge_index, cut_edge_attr, perm, num_nodes=num_nodes)\n        else:\n            print('All edges are cut!')\n            induced_edge_index, induced_edge_attr = cut_edge_index, cut_edge_attr\n        attention_dense = (to_dense_adj(cut_edge_index, edge_attr=cut_edge_attr, max_num_nodes=num_nodes)).squeeze()\n        x = F.relu(torch.matmul(torch.cat((x_topk, torch.matmul(attention_dense[perm],x)), 1), self.weight) + self.bias)\n        return x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 66.43$\\pm$0.59\n",
                        "Macro F1: 62.93$\\pm$0.42\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_CiteSeer.num_classes\nin_channels = dataset_CiteSeer.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_CO(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_CO, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(CoPooling(ratio=pool_ratios[i], K=1, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=1.0))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, perm, _, batch, _, _, _ = self.pools[i - 1](x, edge_index, edge_attr=None, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_CO(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CGIPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "from torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\n@dataclass(init=False)\nclass SelectOutput:\n    r\"\"\"The output of the :class:`Select` method, which holds an assignment\n    from selected nodes to their respective cluster(s).\n    Args:\n        node_index (torch.Tensor): The indices of the selected nodes.\n        num_nodes (int): The number of nodes.\n        cluster_index (torch.Tensor): The indices of the clusters each node in\n            :obj:`node_index` is assigned to.\n        num_clusters (int): The number of clusters.\n        weight (torch.Tensor, optional): A weight vector, denoting the strength\n            of the assignment of a node to its cluster. (default: :obj:`None`)\n    \"\"\"\n    node_index: Tensor\n    num_nodes: int\n    cluster_index: Tensor\n    num_clusters: int\n    weight: Optional[Tensor] = None\n    def __init__(\n        self,\n        node_index: Tensor,\n        num_nodes: int,\n        cluster_index: Tensor,\n        num_clusters: int,\n        weight: Optional[Tensor] = None,\n    ):\n        if node_index.dim() != 1:\n            raise ValueError(f\"Expected 'node_index' to be one-dimensional \"\n                             f\"(got {node_index.dim()} dimensions)\")\n        if cluster_index.dim() != 1:\n            raise ValueError(f\"Expected 'cluster_index' to be one-dimensional \"\n                             f\"(got {cluster_index.dim()} dimensions)\")\n        if node_index.numel() != cluster_index.numel():\n            raise ValueError(f\"Expected 'node_index' and 'cluster_index' to \"\n                             f\"hold the same number of values (got \"\n                             f\"{node_index.numel()} and \"\n                             f\"{cluster_index.numel()} values)\")\n        if weight is not None and weight.dim() != 1:\n            raise ValueError(f\"Expected 'weight' vector to be one-dimensional \"\n                             f\"(got {weight.dim()} dimensions)\")\n        if weight is not None and weight.numel() != node_index.numel():\n            raise ValueError(f\"Expected 'weight' to hold {node_index.numel()} \"\n                             f\"values (got {weight.numel()} values)\")\n        self.node_index = node_index\n        self.num_nodes = num_nodes\n        self.cluster_index = cluster_index\n        self.num_clusters = num_clusters\n        self.weight = weight\nclass Select(torch.nn.Module):\n    r\"\"\"An abstract base class for implementing custom node selections as\n    described in the `\"Understanding Pooling in Graph Neural Networks\"\n    <https://arxiv.org/abs/1905.05178>`_ paper, which maps the nodes of an\n    input graph to supernodes in the coarsened graph.\n    Specifically, :class:`Select` returns a :class:`SelectOutput` output, which\n    holds a (sparse) mapping :math:`\\mathbf{C} \\in {[0, 1]}^{N \\times C}` that\n    assigns selected nodes to one or more of :math:`C` super nodes.\n    \"\"\"\n    def reset_parameters(self):\n        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n        pass\n    def forward(self, *args, **kwargs) -> SelectOutput:\n        raise NotImplementedError\n    def __repr__(self) -> str:\n        return f'{self.__class__.__name__}()'\ndef cumsum(x: Tensor, dim: int = 0) -> Tensor:\n    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n    Args:\n        x (torch.Tensor): The input tensor.\n        dim (int, optional): The dimension to do the operation over.\n            (default: :obj:`0`)\n    Example:\n        >>> x = torch.tensor([2, 4, 1])\n        >>> cumsum(x)\n        tensor([0, 2, 6, 7])\n    \"\"\"\n    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n    out = x.new_empty(size)\n    out.narrow(dim, 0, 1).zero_()\n    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n    return out\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\ndef topk(\n    x: Tensor,\n    ratio: Optional[Union[float, int]],\n    batch: Tensor,\n    min_score: Optional[float] = None,\n    tol: float = 1e-7,\n) -> Tensor:\n    if min_score is not None:\n        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero().view(-1)\n        return perm\n    if ratio is not None:\n        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n        if ratio >= 1:\n            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n        else:\n            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n        x, x_perm = torch.sort(x.view(-1), descending=True)\n        batch = batch[x_perm]\n        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n        ptr = cumsum(num_nodes)\n        batched_arange = arange - ptr[batch]\n        mask = batched_arange < k[batch]\n        return x_perm[batch_perm[mask]]\n    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n                     \"must be specified\")\nclass Discriminator(torch.nn.Module):\n    def __init__(self, in_channels):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(in_channels * 2, in_channels)\n        self.fc2 = nn.Linear(in_channels, 1)\n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = F.sigmoid(self.fc2(x))\n        return x\nclass CGIPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, non_lin=torch.tanh):\n        super(CGIPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.non_lin = non_lin\n        self.hidden_dim = in_channels\n        self.transform = GraphConv(in_channels, self.hidden_dim)\n        self.pp_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n        self.np_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n        self.positive_pooling = GraphConv(self.hidden_dim, 1)\n        self.negative_pooling = GraphConv(self.hidden_dim, 1)\n        self.discriminator = Discriminator(self.hidden_dim)\n        self.loss_fn = torch.nn.BCELoss()\n    def forward(self, x, edge_index, edge_attr=None, batch=None):\n        device = x.device  \n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_transform = F.leaky_relu(self.transform(x, edge_index), 0.2)\n        x_tp = F.leaky_relu(self.pp_conv(x, edge_index), 0.2)\n        x_tn = F.leaky_relu(self.np_conv(x, edge_index), 0.2)\n        s_pp = self.positive_pooling(x_tp, edge_index).squeeze()\n        s_np = self.negative_pooling(x_tn, edge_index).squeeze()\n        perm_positive = topk(s_pp, 1, batch)\n        perm_negative = topk(s_np, 1, batch)\n        x_pp = x_transform[perm_positive] * self.non_lin(s_pp[perm_positive]).view(-1, 1)\n        x_np = x_transform[perm_negative] * self.non_lin(s_np[perm_negative]).view(-1, 1)\n        x_pp_readout = gap(x_pp, batch[perm_positive])\n        x_np_readout = gap(x_np, batch[perm_negative])\n        x_readout = gap(x_transform, batch)\n        positive_pair = torch.cat([x_pp_readout, x_readout], dim=1)\n        negative_pair = torch.cat([x_np_readout, x_readout], dim=1)\n        real = torch.ones(positive_pair.shape[0], device=device)  \n        fake = torch.zeros(negative_pair.shape[0], device=device)  \n        score = (s_pp - s_np)\n        perm = topk(score, self.ratio, batch)\n        x = x_transform[perm] * self.non_lin(score[perm]).view(-1, 1)\n        batch = batch[perm]\n        filter_edge_index, filter_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        return x, filter_edge_index, filter_edge_attr, batch, perm"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 65.66$\\pm$1.34\n",
                        "Macro F1: 62.14$\\pm$1.46\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_CiteSeer.num_classes\nin_channels = dataset_CiteSeer.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_CGI(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_CGI, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(CGIPool(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_CGI(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### KMISPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "from typing import Callable, Optional, Tuple, Union\nfrom torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\nScorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\nfrom torch_sparse import SparseTensor, remove_diag\nfrom torch_geometric.nn.aggr import Aggregation\nfrom torch_geometric.nn.dense import Linear\nfrom torch.nn import Module\nfrom torch_scatter import scatter_max, scatter_min\ndef maximal_independent_set(edge_index: Adj, k: int = 1,\n                            perm: OptTensor = None) -> Tensor:\n    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n    returned set.\n    The algorithm greedily selects the nodes in their canonical order. If a\n    permutation :obj:`perm` is provided, the nodes are extracted following\n    that permutation instead.\n    This method follows `Blelloch's Alogirithm\n    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n    higher values of :math:`k`.\n    Args:\n        edge_index (Tensor or SparseTensor): The graph connectivity.\n        k (int): The :math:`k` value (defaults to 1).\n        perm (LongTensor, optional): Permutation vector. Must be of size\n            :obj:`(n,)` (defaults to :obj:`None`).\n    :rtype: :class:`ByteTensor`\n    \"\"\"\n    if isinstance(edge_index, SparseTensor):\n        row, col, _ = edge_index.coo()\n        device = edge_index.device()\n        n = edge_index.size(0)\n    else:\n        row, col = edge_index[0], edge_index[1]\n        device = row.device\n        n = edge_index.max().item() + 1\n    if perm is None:\n        rank = torch.arange(n, dtype=torch.long, device=device)\n    else:\n        rank = torch.zeros_like(perm)\n        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n    mis = torch.zeros(n, dtype=torch.bool, device=device)\n    mask = mis.clone()\n    min_rank = rank.clone()\n    while not mask.all():\n        for _ in range(k):\n            min_neigh = torch.full_like(min_rank, fill_value=n)\n            scatter_min(min_rank[row], col, out=min_neigh)\n            torch.minimum(min_neigh, min_rank, out=min_rank)  \n        mis = mis | torch.eq(rank, min_rank)\n        mask = mis.clone().byte()\n        for _ in range(k):\n            max_neigh = torch.full_like(mask, fill_value=0)\n            scatter_max(mask[row], col, out=max_neigh)\n            torch.maximum(max_neigh, mask, out=mask)  \n        mask = mask.to(dtype=torch.bool)\n        min_rank = rank.clone()\n        min_rank[mask] = n\n    return mis\ndef maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n                                    perm: OptTensor = None) -> PairTensor:\n    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n    clustering of a graph, as defined in `\"Generalizing Downsampling from\n    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n    The algorithm greedily selects the nodes in their canonical order. If a\n    permutation :obj:`perm` is provided, the nodes are extracted following\n    that permutation instead.\n    This method returns both the :math:`k`-MIS and the clustering, where the\n    :math:`c`-th cluster refers to the :math:`c`-th element of the\n    :math:`k`-MIS.\n    Args:\n        edge_index (Tensor or SparseTensor): The graph connectivity.\n        k (int): The :math:`k` value (defaults to 1).\n        perm (LongTensor, optional): Permutation vector. Must be of size\n            :obj:`(n,)` (defaults to :obj:`None`).\n    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n    \"\"\"\n    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n    n, device = mis.size(0), mis.device\n    if isinstance(edge_index, SparseTensor):\n        row, col, _ = edge_index.coo()\n    else:\n        row, col = edge_index[0], edge_index[1]\n    if perm is None:\n        rank = torch.arange(n, dtype=torch.long, device=device)\n    else:\n        rank = torch.zeros_like(perm)\n        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n    rank_mis = rank[mis]\n    min_rank[mis] = rank_mis\n    for _ in range(k):\n        min_neigh = torch.full_like(min_rank, fill_value=n)\n        scatter_min(min_rank[row], col, out=min_neigh)\n        torch.minimum(min_neigh, min_rank, out=min_rank)\n    _, clusters = torch.unique(min_rank, return_inverse=True)\n    perm = torch.argsort(rank_mis)\n    return mis, perm[clusters]\nclass KMISPooling(Module):\n    _heuristics = {None, 'greedy', 'w-greedy'}\n    _passthroughs = {None, 'before', 'after'}\n    _scorers = {\n        'linear',\n        'random',\n        'constant',\n        'canonical',\n        'first',\n        'last',\n    }\n    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n                 scorer: Union[Scorer, str] = 'linear',\n                 score_heuristic: Optional[str] = 'greedy',\n                 score_passthrough: Optional[str] = 'before',\n                 aggr_x: Optional[Union[str, Aggregation]] = None,\n                 aggr_edge: str = 'sum',\n                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n                 remove_self_loops: bool = True) -> None:\n        super(KMISPooling, self).__init__()\n        assert score_heuristic in self._heuristics, \\\n            \"Unrecognized `score_heuristic` value.\"\n        assert score_passthrough in self._passthroughs, \\\n            \"Unrecognized `score_passthrough` value.\"\n        if not callable(scorer):\n            assert scorer in self._scorers, \\\n                \"Unrecognized `scorer` value.\"\n        self.k = k\n        self.scorer = scorer\n        self.score_heuristic = score_heuristic\n        self.score_passthrough = score_passthrough\n        self.aggr_x = aggr_x\n        self.aggr_edge = aggr_edge\n        self.aggr_score = aggr_score\n        self.remove_self_loops = remove_self_loops\n        if scorer == 'linear':\n            assert self.score_passthrough is not None, \\\n                \"`'score_passthrough'` must not be `None`\" \\\n                \" when using `'linear'` scorer\"\n            self.lin = nn.Linear(in_channels, 1)\n    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n        if self.score_heuristic is None:\n            return x\n        row, col, _ = adj.coo()\n        x = x.view(-1)\n        if self.score_heuristic == 'greedy':\n            k_sums = torch.ones_like(x)\n        else:\n            k_sums = x.clone()\n        for _ in range(self.k):\n            scatter_add(k_sums[row], col, out=k_sums)\n        return x / k_sums\n    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n                batch: OptTensor = None) -> Tensor:\n        if self.scorer == 'linear':\n            return self.lin(x).sigmoid()\n        if self.scorer == 'random':\n            return torch.rand((x.size(0), 1), device=x.device)\n        if self.scorer == 'constant':\n            return torch.ones((x.size(0), 1), device=x.device)\n        if self.scorer == 'canonical':\n            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n        if self.scorer == 'first':\n            return x[..., [0]]\n        if self.scorer == 'last':\n            return x[..., [-1]]\n        return self.scorer(x, edge_index, edge_attr, batch)\n    def forward(self, x: Tensor, edge_index: Adj,\n                edge_attr: OptTensor = None,\n                batch: OptTensor = None) \\\n            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n        \"\"\"\"\"\"\n        edge_index = edge_index.long()\n        adj, n = edge_index, x.size(0)\n        if not isinstance(edge_index, SparseTensor):\n            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n        score = self._scorer(x, edge_index, edge_attr, batch)\n        updated_score = self._apply_heuristic(score, adj)\n        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n        row, col, val = adj.coo()\n        c = mis.sum()\n        if val is None:\n            val = torch.ones_like(row, dtype=torch.float)\n        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n                           is_sorted=False,\n                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n        if self.remove_self_loops:\n            adj = remove_diag(adj)\n        if self.score_passthrough == 'before':\n            x = self.aggr_score(x, score)\n        if self.aggr_x is None:\n            x = x[mis]\n        elif isinstance(self.aggr_x, str):\n            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n                        reduce=self.aggr_x)\n        else:\n            x = self.aggr_x(x, cluster, dim_size=c)\n        if self.score_passthrough == 'after':\n            x = self.aggr_score(x, score[mis])\n        if isinstance(edge_index, SparseTensor):\n            edge_index, edge_attr = adj, None\n        else:\n            row, col, edge_attr = adj.coo()\n            edge_index = torch.stack([row, col])\n        if batch is not None:\n            batch = batch[mis]\n        perm = perm[mis]\n        return x, edge_index, edge_attr, batch, mis, cluster, perm\n    def __repr__(self):\n        if self.scorer == 'linear':\n            channels = f\"in_channels={self.lin.in_channels}, \"\n        else:\n            channels = \"\"\n        return f'{self.__class__.__name__}({channels}k={self.k})'"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 67.35$\\pm$0.15\n",
                        "Macro F1: 63.69$\\pm$0.14\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_CiteSeer.num_classes\nin_channels = dataset_CiteSeer.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\nclass HierarchicalGCN_KMIS(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_KMIS, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(KMISPooling(64, k=1, aggr_x='sum'))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_KMIS(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### GSAPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "import math\nfrom typing import Union, Optional, Callable\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax\nfrom torch_geometric.nn import GCNConv, SAGEConv, GATConv, ChebConv, GraphConv\ndef uniform(size, tensor):\n    if tensor is not None:\n        bound = 1.0 / math.sqrt(size)\n        tensor.data.uniform_(-bound, bound)\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ),\n                             torch.finfo(x.dtype).min)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        if isinstance(ratio, int):\n            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n            k = torch.min(k, num_nodes)\n        else:\n            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\nclass GSAPool(torch.nn.Module):\n    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n                        min_score=None, multiplier=1,\n                        non_linearity=torch.tanh,\n                        cus_drop_ratio =0):\n        super(GSAPool,self).__init__()\n        self.in_channels = in_channels\n        self.ratio = pooling_ratio\n        self.alpha = alpha\n        self.sbtl_layer = GCNConv(in_channels,1)\n        self.fbtl_layer = nn.Linear(in_channels, 1)\n        self.fusion = GCNConv(in_channels,in_channels)\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.fusion_flag = 0\n        self.non_linearity = non_linearity\n        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n    def conv_selection(self, conv, in_channels, conv_type=0):\n        if(conv_type == 0):\n            out_channels = 1\n        elif(conv_type == 1):\n            out_channels = in_channels\n        if(conv == \"GCNConv\"):\n            return GCNConv(in_channels,out_channels)\n        elif(conv == \"ChebConv\"):\n            return ChebConv(in_channels,out_channels,1)\n        elif(conv == \"SAGEConv\"):\n            return SAGEConv(in_channels,out_channels)\n        elif(conv == \"GATConv\"):\n            return GATConv(in_channels,out_channels, heads=1, concat=True)\n        elif(conv == \"GraphConv\"):\n            return GraphConv(in_channels,out_channels)\n        else:\n            raise ValueError\n    def forward(self, x, edge_index, edge_attr=None, batch=None):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x = x.unsqueeze(-1) if x.dim() == 1 else x\n        score_s = self.sbtl_layer(x,edge_index).squeeze()\n        score_f = self.fbtl_layer(x).squeeze()\n        score = score_s*self.alpha + score_f*(1-self.alpha)\n        score = score.unsqueeze(-1) if score.dim()==0 else score\n        if self.min_score is None:\n            score = self.non_linearity(score)\n        else:\n            score = softmax(score, batch)\n        sc = self.dropout(score)\n        perm = topk(sc, self.ratio, batch)\n        if(self.fusion_flag == 1):\n            x = self.fusion(x, edge_index)\n        x_ae = x[perm]\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n        batch = batch[perm]\n        edge_index, edge_attr = filter_adj(\n            edge_index, edge_attr, perm, num_nodes=score.size(0))\n        return x, edge_index, edge_attr, batch, perm, x_ae"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 67.07$\\pm$0.49\n",
                        "Macro F1: 63.56$\\pm$0.42\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_CiteSeer.num_classes\nin_channels = dataset_CiteSeer.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_GSA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_GSA, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(GSAPool(64, pooling_ratio=pool_ratios[i], alpha = 0.6, cus_drop_ratio = 0))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, edge_attr, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_GSA(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### HGPSLPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([0.5344, 0.0000, 0.0000, 0.4656, 0.0613, 0.0000, 0.0000, 0.0000, 0.5640,\n",
                        "        0.3748])\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = torch.nonzero(x > scores_min).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n            num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_weight, perm, num_nodes=None):\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        mask = perm.new_full((num_nodes, ), -1)\n        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n        mask[perm] = i\n        row, col = edge_index\n        row, col = mask[row], mask[col]\n        mask = (row >= 0) & (col >= 0)\n        row, col = row[mask], col[mask]\n        if edge_weight is not None:\n            edge_weight = edge_weight[mask]\n        return torch.stack([row, col], dim=0), edge_weight\ndef scatter_sort(x, batch, fill_value=-1e16):\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n    dense_x[index] = x\n    dense_x = dense_x.view(batch_size, max_num_nodes)\n    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n    sorted_x = sorted_x.view(-1)\n    filled_index = sorted_x != fill_value\n    sorted_x = sorted_x[filled_index]\n    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n    return sorted_x, cumsum_sorted_x\ndef _make_ix_like(batch):\n    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n    idx = torch.cat(idx, dim=0)\n    return idx\ndef _threshold_and_support(x, batch):\n    \"\"\"Sparsemax building block: compute the threshold\n    Args:\n        x: input tensor to apply the sparsemax\n        batch: group indicators\n    Returns:\n        the threshold value\n    \"\"\"\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    sorted_input, input_cumsum = scatter_sort(x, batch)\n    input_cumsum = input_cumsum - 1.0\n    rhos = _make_ix_like(batch).to(x.dtype)\n    support = rhos * sorted_input > input_cumsum\n    support_size = scatter_add(support.to(batch.dtype), batch)\n    idx = support_size + cum_num_nodes - 1\n    mask = idx < 0\n    idx[mask] = 0\n    tau = input_cumsum.gather(0, idx)\n    tau /= support_size.to(x.dtype)\n    return tau, support_size\nclass SparsemaxFunction(Function):\n    @staticmethod\n    def forward(ctx, x, batch):\n        \"\"\"sparsemax: normalizing sparse transform\n        Parameters:\n            ctx: context object\n            x (Tensor): shape (N, )\n            batch: group indicator\n        Returns:\n            output (Tensor): same shape as input\n        \"\"\"\n        max_val, _ = scatter_max(x, batch)\n        x -= max_val[batch]\n        tau, supp_size = _threshold_and_support(x, batch)\n        output = torch.clamp(x - tau[batch], min=0)\n        ctx.save_for_backward(supp_size, output, batch)\n        return output\n    @staticmethod\n    def backward(ctx, grad_output):\n        supp_size, output, batch = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n        return grad_input, None\nsparsemax = SparsemaxFunction.apply\nclass Sparsemax(nn.Module):\n    def __init__(self):\n        super(Sparsemax, self).__init__()\n    def forward(self, x, batch):\n        return sparsemax(x, batch)\nif __name__ == '__main__':\n    sparse_attention = Sparsemax()\n    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n    res = sparse_attention(input_x, input_batch)\n    print(res)\nclass TwoHopNeighborhood(object):\n    def __call__(self, data):\n        edge_index, edge_attr = data.edge_index, data.edge_attr\n        n = data.num_nodes\n        fill = 1e16\n        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n        edge_index = torch.cat([edge_index, index], dim=1)\n        if edge_attr is None:\n            data.edge_index, _ = coalesce(edge_index, None, n, n)\n        else:\n            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n            value = value.expand(-1, *list(edge_attr.size())[1:])\n            edge_attr = torch.cat([edge_attr, value], dim=0)\n            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min', fill_value=fill)\n            edge_attr[edge_attr >= fill] = 0\n            data.edge_attr = edge_attr\n        return data\n    def __repr__(self):\n        return '{}()'.format(self.__class__.__name__)\nclass GCN(MessagePassing):\n    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n        super(GCN, self).__init__(aggr='add', **kwargs)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n        nn.init.xavier_uniform_(self.weight.data)\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n            nn.init.zeros_(self.bias.data)\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n    def reset_parameters(self):\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight=None):\n        x = torch.matmul(x, self.weight)\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        if self.bias is not None:\n            aggr_out = aggr_out + self.bias\n        return aggr_out\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n        row, col = edge_index\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass HGPSLPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n        super(HGPSLPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.sample = sample\n        self.sparse = sparse\n        self.sl = sl\n        self.negative_slop = negative_slop\n        self.lamb = lamb\n        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n        nn.init.xavier_uniform_(self.att.data)\n        self.sparse_attention = Sparsemax()\n        self.neighbor_augment = TwoHopNeighborhood()\n        self.calc_information_score = NodeInformationScore()\n    def forward(self, x, edge_index, edge_attr, batch):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        original_x = x\n        perm = topk(score, self.ratio, batch)\n        x = x[perm]\n        batch = batch[perm]\n        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        if self.sl is False:\n            return x, induced_edge_index, induced_edge_attr, batch\n        if self.sample:\n            k_hop = 3\n            if edge_attr is None:\n                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n            for _ in range(k_hop - 1):\n                hop_data = self.neighbor_augment(hop_data)\n            hop_edge_index = hop_data.edge_index\n            hop_edge_attr = hop_data.edge_attr\n            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            adj[row, col] = weights\n            new_edge_index, weights = dense_to_sparse(adj)\n            row, col = new_edge_index\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        else:\n            if edge_attr is None:\n                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n                                               device=induced_edge_index.device)\n            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n            cum_num_nodes = num_nodes.cumsum(dim=0)\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n            new_edge_index, _ = dense_to_sparse(adj)\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop)\n            adj[row, col] = weights\n            induced_row, induced_col = induced_edge_index\n            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n            weights = adj[row, col]\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, num_nodes=x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        return x, new_edge_index, new_edge_attr, batch, perm"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 001: Loss = 1.7847, Val Acc = 0.2840, Micro F1 = 0.2840, Macro F1 = 0.2101\n",
                        "Epoch 002: Loss = 1.7582, Val Acc = 0.3574, Micro F1 = 0.3574, Macro F1 = 0.2679\n",
                        "Epoch 003: Loss = 1.7341, Val Acc = 0.4043, Micro F1 = 0.4043, Macro F1 = 0.3061\n",
                        "Epoch 004: Loss = 1.7081, Val Acc = 0.4200, Micro F1 = 0.4200, Macro F1 = 0.3196\n",
                        "Epoch 005: Loss = 1.6807, Val Acc = 0.4308, Micro F1 = 0.4308, Macro F1 = 0.3299\n",
                        "Epoch 006: Loss = 1.6465, Val Acc = 0.4465, Micro F1 = 0.4465, Macro F1 = 0.3454\n",
                        "Epoch 007: Loss = 1.6115, Val Acc = 0.4609, Micro F1 = 0.4609, Macro F1 = 0.3590\n",
                        "Epoch 008: Loss = 1.5708, Val Acc = 0.4741, Micro F1 = 0.4741, Macro F1 = 0.3774\n",
                        "Epoch 009: Loss = 1.5270, Val Acc = 0.4886, Micro F1 = 0.4886, Macro F1 = 0.3897\n",
                        "Epoch 010: Loss = 1.4786, Val Acc = 0.4970, Micro F1 = 0.4970, Macro F1 = 0.3999\n",
                        "Epoch 011: Loss = 1.4332, Val Acc = 0.5174, Micro F1 = 0.5174, Macro F1 = 0.4280\n",
                        "Epoch 012: Loss = 1.3811, Val Acc = 0.5307, Micro F1 = 0.5307, Macro F1 = 0.4478\n",
                        "Epoch 013: Loss = 1.3265, Val Acc = 0.5499, Micro F1 = 0.5499, Macro F1 = 0.4709\n",
                        "Epoch 014: Loss = 1.2801, Val Acc = 0.5788, Micro F1 = 0.5788, Macro F1 = 0.5026\n",
                        "Epoch 015: Loss = 1.2280, Val Acc = 0.6005, Micro F1 = 0.6005, Macro F1 = 0.5248\n",
                        "Epoch 016: Loss = 1.1724, Val Acc = 0.6294, Micro F1 = 0.6294, Macro F1 = 0.5513\n",
                        "Epoch 017: Loss = 1.1267, Val Acc = 0.6498, Micro F1 = 0.6498, Macro F1 = 0.5700\n",
                        "Epoch 018: Loss = 1.0653, Val Acc = 0.6582, Micro F1 = 0.6582, Macro F1 = 0.5769\n",
                        "Epoch 019: Loss = 1.0170, Val Acc = 0.6703, Micro F1 = 0.6703, Macro F1 = 0.5873\n",
                        "Epoch 020: Loss = 0.9820, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.5965\n",
                        "Epoch 021: Loss = 0.9331, Val Acc = 0.6835, Micro F1 = 0.6835, Macro F1 = 0.5989\n",
                        "Epoch 022: Loss = 0.8899, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6069\n",
                        "Epoch 023: Loss = 0.8492, Val Acc = 0.6931, Micro F1 = 0.6931, Macro F1 = 0.6101\n",
                        "Epoch 024: Loss = 0.8170, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6167\n",
                        "Epoch 025: Loss = 0.7810, Val Acc = 0.6943, Micro F1 = 0.6943, Macro F1 = 0.6144\n",
                        "Epoch 026: Loss = 0.7450, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6165\n",
                        "Epoch 027: Loss = 0.7159, Val Acc = 0.6992, Micro F1 = 0.6992, Macro F1 = 0.6223\n",
                        "Epoch 028: Loss = 0.6888, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6253\n",
                        "Epoch 029: Loss = 0.6577, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6335\n",
                        "Epoch 030: Loss = 0.6374, Val Acc = 0.7076, Micro F1 = 0.7076, Macro F1 = 0.6414\n",
                        "Epoch 031: Loss = 0.6176, Val Acc = 0.7088, Micro F1 = 0.7088, Macro F1 = 0.6424\n",
                        "Epoch 032: Loss = 0.5977, Val Acc = 0.7112, Micro F1 = 0.7112, Macro F1 = 0.6504\n",
                        "Epoch 033: Loss = 0.5785, Val Acc = 0.7148, Micro F1 = 0.7148, Macro F1 = 0.6569\n",
                        "Epoch 034: Loss = 0.5686, Val Acc = 0.7172, Micro F1 = 0.7172, Macro F1 = 0.6660\n",
                        "Epoch 035: Loss = 0.5531, Val Acc = 0.7184, Micro F1 = 0.7184, Macro F1 = 0.6710\n",
                        "Epoch 036: Loss = 0.5335, Val Acc = 0.7208, Micro F1 = 0.7208, Macro F1 = 0.6773\n",
                        "Epoch 037: Loss = 0.5234, Val Acc = 0.7148, Micro F1 = 0.7148, Macro F1 = 0.6717\n",
                        "Epoch 038: Loss = 0.5101, Val Acc = 0.7160, Micro F1 = 0.7160, Macro F1 = 0.6747\n",
                        "Epoch 039: Loss = 0.4945, Val Acc = 0.7148, Micro F1 = 0.7148, Macro F1 = 0.6732\n",
                        "Epoch 040: Loss = 0.4906, Val Acc = 0.7148, Micro F1 = 0.7148, Macro F1 = 0.6730\n",
                        "Epoch 041: Loss = 0.4790, Val Acc = 0.7124, Micro F1 = 0.7124, Macro F1 = 0.6708\n",
                        "Epoch 042: Loss = 0.4744, Val Acc = 0.7100, Micro F1 = 0.7100, Macro F1 = 0.6684\n",
                        "Epoch 043: Loss = 0.4637, Val Acc = 0.7076, Micro F1 = 0.7076, Macro F1 = 0.6663\n",
                        "Epoch 044: Loss = 0.4564, Val Acc = 0.7076, Micro F1 = 0.7076, Macro F1 = 0.6665\n",
                        "Epoch 045: Loss = 0.4504, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6641\n",
                        "Epoch 046: Loss = 0.4368, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6644\n",
                        "Epoch 047: Loss = 0.4303, Val Acc = 0.7040, Micro F1 = 0.7040, Macro F1 = 0.6635\n",
                        "Epoch 048: Loss = 0.4311, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6624\n",
                        "Epoch 049: Loss = 0.4231, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6646\n",
                        "Epoch 050: Loss = 0.4166, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6642\n",
                        "Epoch 051: Loss = 0.4111, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6634\n",
                        "Epoch 052: Loss = 0.4054, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6634\n",
                        "Epoch 053: Loss = 0.4016, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6666\n",
                        "Epoch 054: Loss = 0.3969, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6664\n",
                        "Epoch 055: Loss = 0.3913, Val Acc = 0.7076, Micro F1 = 0.7076, Macro F1 = 0.6686\n",
                        "Epoch 056: Loss = 0.3884, Val Acc = 0.7076, Micro F1 = 0.7076, Macro F1 = 0.6690\n",
                        "Epoch 057: Loss = 0.3818, Val Acc = 0.7064, Micro F1 = 0.7064, Macro F1 = 0.6682\n",
                        "Epoch 058: Loss = 0.3758, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6649\n",
                        "Epoch 059: Loss = 0.3731, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6638\n",
                        "Epoch 060: Loss = 0.3678, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6647\n",
                        "Epoch 061: Loss = 0.3677, Val Acc = 0.6992, Micro F1 = 0.6992, Macro F1 = 0.6612\n",
                        "Epoch 062: Loss = 0.3557, Val Acc = 0.6992, Micro F1 = 0.6992, Macro F1 = 0.6595\n",
                        "Epoch 063: Loss = 0.3600, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6618\n",
                        "Epoch 064: Loss = 0.3597, Val Acc = 0.6980, Micro F1 = 0.6980, Macro F1 = 0.6585\n",
                        "Epoch 065: Loss = 0.3514, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6574\n",
                        "Epoch 066: Loss = 0.3466, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6517\n",
                        "Epoch 067: Loss = 0.3485, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6513\n",
                        "Epoch 068: Loss = 0.3443, Val Acc = 0.6931, Micro F1 = 0.6931, Macro F1 = 0.6552\n",
                        "Epoch 069: Loss = 0.3399, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6516\n",
                        "Epoch 070: Loss = 0.3375, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6528\n",
                        "Epoch 071: Loss = 0.3348, Val Acc = 0.6955, Micro F1 = 0.6955, Macro F1 = 0.6560\n",
                        "Epoch 072: Loss = 0.3348, Val Acc = 0.6943, Micro F1 = 0.6943, Macro F1 = 0.6550\n",
                        "Epoch 073: Loss = 0.3254, Val Acc = 0.6931, Micro F1 = 0.6931, Macro F1 = 0.6540\n",
                        "Epoch 074: Loss = 0.3265, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6530\n",
                        "Epoch 075: Loss = 0.3214, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6517\n",
                        "Epoch 076: Loss = 0.3238, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6518\n",
                        "Epoch 077: Loss = 0.3168, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6518\n",
                        "Epoch 078: Loss = 0.3176, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6531\n",
                        "Epoch 079: Loss = 0.3130, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6533\n",
                        "Epoch 080: Loss = 0.3172, Val Acc = 0.6943, Micro F1 = 0.6943, Macro F1 = 0.6553\n",
                        "Epoch 081: Loss = 0.3110, Val Acc = 0.6955, Micro F1 = 0.6955, Macro F1 = 0.6565\n",
                        "Epoch 082: Loss = 0.3095, Val Acc = 0.6955, Micro F1 = 0.6955, Macro F1 = 0.6562\n",
                        "Epoch 083: Loss = 0.3053, Val Acc = 0.6955, Micro F1 = 0.6955, Macro F1 = 0.6563\n",
                        "Epoch 084: Loss = 0.3048, Val Acc = 0.6931, Micro F1 = 0.6931, Macro F1 = 0.6539\n",
                        "Epoch 085: Loss = 0.3051, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6518\n",
                        "Epoch 086: Loss = 0.3021, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6521\n",
                        "Epoch 087: Loss = 0.2976, Val Acc = 0.6931, Micro F1 = 0.6931, Macro F1 = 0.6556\n",
                        "Epoch 088: Loss = 0.2977, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6531\n",
                        "Epoch 089: Loss = 0.2943, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6520\n",
                        "Epoch 090: Loss = 0.2934, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6512\n",
                        "Epoch 091: Loss = 0.2970, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6520\n",
                        "Epoch 092: Loss = 0.2940, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6509\n",
                        "Epoch 093: Loss = 0.2909, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6511\n",
                        "Epoch 094: Loss = 0.2884, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6520\n",
                        "Epoch 095: Loss = 0.2920, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6500\n",
                        "Epoch 096: Loss = 0.2846, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6515\n",
                        "Epoch 097: Loss = 0.2842, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6528\n",
                        "Epoch 098: Loss = 0.2863, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6537\n",
                        "Epoch 099: Loss = 0.2801, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6537\n",
                        "Epoch 100: Loss = 0.2822, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6529\n",
                        "Epoch 101: Loss = 0.2829, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6503\n",
                        "Epoch 102: Loss = 0.2802, Val Acc = 0.6871, Micro F1 = 0.6871, Macro F1 = 0.6495\n",
                        "Epoch 103: Loss = 0.2747, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6533\n",
                        "Epoch 104: Loss = 0.2711, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6531\n",
                        "Epoch 105: Loss = 0.2669, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6518\n",
                        "Epoch 106: Loss = 0.2775, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6505\n",
                        "Epoch 107: Loss = 0.2757, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6505\n",
                        "Epoch 108: Loss = 0.2650, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6519\n",
                        "Epoch 109: Loss = 0.2728, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6519\n",
                        "Epoch 110: Loss = 0.2624, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6507\n",
                        "Epoch 111: Loss = 0.2745, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6518\n",
                        "Epoch 112: Loss = 0.2786, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6530\n",
                        "Epoch 113: Loss = 0.2716, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6517\n",
                        "Epoch 114: Loss = 0.2707, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6515\n",
                        "Epoch 115: Loss = 0.2648, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6501\n",
                        "Epoch 116: Loss = 0.2690, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6502\n",
                        "Epoch 117: Loss = 0.2620, Val Acc = 0.6835, Micro F1 = 0.6835, Macro F1 = 0.6459\n",
                        "Epoch 118: Loss = 0.2613, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.6482\n",
                        "Epoch 119: Loss = 0.2573, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.6485\n",
                        "Epoch 120: Loss = 0.2621, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.6483\n",
                        "Epoch 121: Loss = 0.2631, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6522\n",
                        "Epoch 122: Loss = 0.2599, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6533\n",
                        "Epoch 123: Loss = 0.2542, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6523\n",
                        "Epoch 124: Loss = 0.2567, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6515\n",
                        "Epoch 125: Loss = 0.2556, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.6494\n",
                        "Epoch 126: Loss = 0.2542, Val Acc = 0.6835, Micro F1 = 0.6835, Macro F1 = 0.6483\n",
                        "Epoch 127: Loss = 0.2609, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6471\n",
                        "Epoch 128: Loss = 0.2535, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6492\n",
                        "Epoch 129: Loss = 0.2562, Val Acc = 0.6871, Micro F1 = 0.6871, Macro F1 = 0.6480\n",
                        "Epoch 130: Loss = 0.2533, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6491\n",
                        "Epoch 131: Loss = 0.2536, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.6471\n",
                        "Epoch 132: Loss = 0.2550, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6416\n",
                        "Epoch 133: Loss = 0.2530, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6391\n",
                        "Epoch 134: Loss = 0.2474, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6420\n",
                        "Epoch 135: Loss = 0.2511, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6421\n",
                        "Epoch 136: Loss = 0.2506, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6410\n",
                        "Epoch 137: Loss = 0.2458, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6461\n",
                        "Epoch 138: Loss = 0.2452, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6411\n",
                        "Epoch 139: Loss = 0.2526, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6416\n",
                        "Epoch 140: Loss = 0.2484, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6440\n",
                        "Epoch 141: Loss = 0.2487, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6505\n",
                        "Epoch 142: Loss = 0.2466, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6503\n",
                        "Epoch 143: Loss = 0.2428, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6531\n",
                        "Epoch 144: Loss = 0.2518, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6489\n",
                        "Epoch 145: Loss = 0.2503, Val Acc = 0.6835, Micro F1 = 0.6835, Macro F1 = 0.6465\n",
                        "Epoch 146: Loss = 0.2528, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6413\n",
                        "Epoch 147: Loss = 0.2468, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6402\n",
                        "Epoch 148: Loss = 0.2464, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6423\n",
                        "Epoch 149: Loss = 0.2432, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6449\n",
                        "Epoch 150: Loss = 0.2458, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6463\n",
                        "Epoch 151: Loss = 0.2536, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6442\n",
                        "Epoch 152: Loss = 0.2431, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6412\n",
                        "Epoch 153: Loss = 0.2407, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6401\n",
                        "Epoch 154: Loss = 0.2430, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6472\n",
                        "Epoch 155: Loss = 0.2437, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6420\n",
                        "Epoch 156: Loss = 0.2428, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6392\n",
                        "Epoch 157: Loss = 0.2380, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6426\n",
                        "Epoch 158: Loss = 0.2413, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6405\n",
                        "Epoch 159: Loss = 0.2444, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6382\n",
                        "Epoch 160: Loss = 0.2368, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6407\n",
                        "Epoch 161: Loss = 0.2404, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6420\n",
                        "Epoch 162: Loss = 0.2380, Val Acc = 0.6703, Micro F1 = 0.6703, Macro F1 = 0.6332\n",
                        "Epoch 163: Loss = 0.2550, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6417\n",
                        "Epoch 164: Loss = 0.2414, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6440\n",
                        "Epoch 165: Loss = 0.2367, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6446\n",
                        "Epoch 166: Loss = 0.2365, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6389\n",
                        "Epoch 167: Loss = 0.2355, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6386\n",
                        "Epoch 168: Loss = 0.2352, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6376\n",
                        "Epoch 169: Loss = 0.2375, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6356\n",
                        "Epoch 170: Loss = 0.2331, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6365\n",
                        "Epoch 171: Loss = 0.2353, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6371\n",
                        "Epoch 172: Loss = 0.2344, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6371\n",
                        "Epoch 173: Loss = 0.2359, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6386\n",
                        "Epoch 174: Loss = 0.2309, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6369\n",
                        "Epoch 175: Loss = 0.2356, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6383\n",
                        "Epoch 176: Loss = 0.2315, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6409\n",
                        "Epoch 177: Loss = 0.2410, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6395\n",
                        "Epoch 178: Loss = 0.2307, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6394\n",
                        "Epoch 179: Loss = 0.2274, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6363\n",
                        "Epoch 180: Loss = 0.2332, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6389\n",
                        "Epoch 181: Loss = 0.2284, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6389\n",
                        "Epoch 182: Loss = 0.2346, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6370\n",
                        "Epoch 183: Loss = 0.2302, Val Acc = 0.6691, Micro F1 = 0.6691, Macro F1 = 0.6335\n",
                        "Epoch 184: Loss = 0.2347, Val Acc = 0.6691, Micro F1 = 0.6691, Macro F1 = 0.6336\n",
                        "Epoch 185: Loss = 0.2403, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6371\n",
                        "Early stopping at epoch 186\n",
                        "Epoch 001: Loss = 1.8043, Val Acc = 0.1853, Micro F1 = 0.1853, Macro F1 = 0.1338\n",
                        "Epoch 002: Loss = 1.7782, Val Acc = 0.2298, Micro F1 = 0.2298, Macro F1 = 0.1494\n",
                        "Epoch 003: Loss = 1.7554, Val Acc = 0.2671, Micro F1 = 0.2671, Macro F1 = 0.1781\n",
                        "Epoch 004: Loss = 1.7313, Val Acc = 0.2972, Micro F1 = 0.2972, Macro F1 = 0.2004\n",
                        "Epoch 005: Loss = 1.7052, Val Acc = 0.3249, Micro F1 = 0.3249, Macro F1 = 0.2235\n",
                        "Epoch 006: Loss = 1.6772, Val Acc = 0.3381, Micro F1 = 0.3381, Macro F1 = 0.2341\n",
                        "Epoch 007: Loss = 1.6430, Val Acc = 0.3622, Micro F1 = 0.3622, Macro F1 = 0.2522\n",
                        "Epoch 008: Loss = 1.6100, Val Acc = 0.3803, Micro F1 = 0.3803, Macro F1 = 0.2652\n",
                        "Epoch 009: Loss = 1.5718, Val Acc = 0.3995, Micro F1 = 0.3995, Macro F1 = 0.2828\n",
                        "Epoch 010: Loss = 1.5309, Val Acc = 0.4284, Micro F1 = 0.4284, Macro F1 = 0.3088\n",
                        "Epoch 011: Loss = 1.4921, Val Acc = 0.4561, Micro F1 = 0.4561, Macro F1 = 0.3374\n",
                        "Epoch 012: Loss = 1.4469, Val Acc = 0.4741, Micro F1 = 0.4741, Macro F1 = 0.3594\n",
                        "Epoch 013: Loss = 1.3999, Val Acc = 0.5138, Micro F1 = 0.5138, Macro F1 = 0.4132\n",
                        "Epoch 014: Loss = 1.3527, Val Acc = 0.5499, Micro F1 = 0.5499, Macro F1 = 0.4554\n",
                        "Epoch 015: Loss = 1.3054, Val Acc = 0.5776, Micro F1 = 0.5776, Macro F1 = 0.4866\n",
                        "Epoch 016: Loss = 1.2582, Val Acc = 0.6101, Micro F1 = 0.6101, Macro F1 = 0.5232\n",
                        "Epoch 017: Loss = 1.2069, Val Acc = 0.6498, Micro F1 = 0.6498, Macro F1 = 0.5618\n",
                        "Epoch 018: Loss = 1.1551, Val Acc = 0.6606, Micro F1 = 0.6606, Macro F1 = 0.5725\n",
                        "Epoch 019: Loss = 1.1072, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.5883\n",
                        "Epoch 020: Loss = 1.0599, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6051\n",
                        "Epoch 021: Loss = 1.0129, Val Acc = 0.6871, Micro F1 = 0.6871, Macro F1 = 0.6033\n",
                        "Epoch 022: Loss = 0.9705, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6076\n",
                        "Epoch 023: Loss = 0.9244, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6050\n",
                        "Epoch 024: Loss = 0.8788, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6084\n",
                        "Epoch 025: Loss = 0.8428, Val Acc = 0.6931, Micro F1 = 0.6931, Macro F1 = 0.6157\n",
                        "Epoch 026: Loss = 0.8014, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6222\n",
                        "Epoch 027: Loss = 0.7670, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6272\n",
                        "Epoch 028: Loss = 0.7359, Val Acc = 0.7088, Micro F1 = 0.7088, Macro F1 = 0.6452\n",
                        "Epoch 029: Loss = 0.6987, Val Acc = 0.7088, Micro F1 = 0.7088, Macro F1 = 0.6451\n",
                        "Epoch 030: Loss = 0.6691, Val Acc = 0.7040, Micro F1 = 0.7040, Macro F1 = 0.6404\n",
                        "Epoch 031: Loss = 0.6418, Val Acc = 0.7040, Micro F1 = 0.7040, Macro F1 = 0.6484\n",
                        "Epoch 032: Loss = 0.6195, Val Acc = 0.7040, Micro F1 = 0.7040, Macro F1 = 0.6509\n",
                        "Epoch 033: Loss = 0.5985, Val Acc = 0.7040, Micro F1 = 0.7040, Macro F1 = 0.6511\n",
                        "Epoch 034: Loss = 0.5779, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6546\n",
                        "Epoch 035: Loss = 0.5567, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6544\n",
                        "Epoch 036: Loss = 0.5446, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6565\n",
                        "Epoch 037: Loss = 0.5332, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6546\n",
                        "Epoch 038: Loss = 0.5128, Val Acc = 0.7004, Micro F1 = 0.7004, Macro F1 = 0.6525\n",
                        "Epoch 039: Loss = 0.5103, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6586\n",
                        "Epoch 040: Loss = 0.4927, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6585\n",
                        "Epoch 041: Loss = 0.4772, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6604\n",
                        "Epoch 042: Loss = 0.4737, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6622\n",
                        "Epoch 043: Loss = 0.4660, Val Acc = 0.7004, Micro F1 = 0.7004, Macro F1 = 0.6612\n",
                        "Epoch 044: Loss = 0.4639, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6626\n",
                        "Epoch 045: Loss = 0.4494, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6611\n",
                        "Epoch 046: Loss = 0.4482, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6568\n",
                        "Epoch 047: Loss = 0.4340, Val Acc = 0.6955, Micro F1 = 0.6955, Macro F1 = 0.6559\n",
                        "Epoch 048: Loss = 0.4280, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6572\n",
                        "Epoch 049: Loss = 0.4269, Val Acc = 0.7004, Micro F1 = 0.7004, Macro F1 = 0.6622\n",
                        "Epoch 050: Loss = 0.4158, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6636\n",
                        "Epoch 051: Loss = 0.4061, Val Acc = 0.7004, Micro F1 = 0.7004, Macro F1 = 0.6609\n",
                        "Epoch 052: Loss = 0.4101, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6635\n",
                        "Epoch 053: Loss = 0.3987, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6622\n",
                        "Epoch 054: Loss = 0.3924, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6622\n",
                        "Epoch 055: Loss = 0.3935, Val Acc = 0.6943, Micro F1 = 0.6943, Macro F1 = 0.6558\n",
                        "Epoch 056: Loss = 0.3864, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6539\n",
                        "Epoch 057: Loss = 0.3820, Val Acc = 0.6955, Micro F1 = 0.6955, Macro F1 = 0.6570\n",
                        "Epoch 058: Loss = 0.3735, Val Acc = 0.6955, Micro F1 = 0.6955, Macro F1 = 0.6585\n",
                        "Epoch 059: Loss = 0.3685, Val Acc = 0.6980, Micro F1 = 0.6980, Macro F1 = 0.6602\n",
                        "Epoch 060: Loss = 0.3652, Val Acc = 0.6943, Micro F1 = 0.6943, Macro F1 = 0.6569\n",
                        "Epoch 061: Loss = 0.3653, Val Acc = 0.6871, Micro F1 = 0.6871, Macro F1 = 0.6503\n",
                        "Epoch 062: Loss = 0.3550, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6518\n",
                        "Epoch 063: Loss = 0.3591, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6538\n",
                        "Epoch 064: Loss = 0.3523, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6541\n",
                        "Epoch 065: Loss = 0.3491, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6551\n",
                        "Epoch 066: Loss = 0.3465, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6520\n",
                        "Epoch 067: Loss = 0.3432, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6539\n",
                        "Epoch 068: Loss = 0.3409, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6527\n",
                        "Epoch 069: Loss = 0.3344, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6526\n",
                        "Epoch 070: Loss = 0.3338, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6518\n",
                        "Epoch 071: Loss = 0.3303, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6521\n",
                        "Epoch 072: Loss = 0.3271, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6521\n",
                        "Epoch 073: Loss = 0.3321, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6520\n",
                        "Epoch 074: Loss = 0.3244, Val Acc = 0.6931, Micro F1 = 0.6931, Macro F1 = 0.6559\n",
                        "Epoch 075: Loss = 0.3221, Val Acc = 0.6943, Micro F1 = 0.6943, Macro F1 = 0.6568\n",
                        "Epoch 076: Loss = 0.3268, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6518\n",
                        "Epoch 077: Loss = 0.3153, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6518\n",
                        "Epoch 078: Loss = 0.3124, Val Acc = 0.6871, Micro F1 = 0.6871, Macro F1 = 0.6509\n",
                        "Epoch 079: Loss = 0.3129, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6536\n",
                        "Epoch 080: Loss = 0.3100, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6543\n",
                        "Epoch 081: Loss = 0.3047, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6521\n",
                        "Epoch 082: Loss = 0.3097, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6533\n",
                        "Epoch 083: Loss = 0.3055, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6521\n",
                        "Epoch 084: Loss = 0.3009, Val Acc = 0.6907, Micro F1 = 0.6907, Macro F1 = 0.6535\n",
                        "Epoch 085: Loss = 0.3050, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.6487\n",
                        "Epoch 086: Loss = 0.3017, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6457\n",
                        "Epoch 087: Loss = 0.2952, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6473\n",
                        "Epoch 088: Loss = 0.2972, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6514\n",
                        "Epoch 089: Loss = 0.2928, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6550\n",
                        "Epoch 090: Loss = 0.2962, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6529\n",
                        "Epoch 091: Loss = 0.2914, Val Acc = 0.6871, Micro F1 = 0.6871, Macro F1 = 0.6508\n",
                        "Epoch 092: Loss = 0.2940, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6446\n",
                        "Epoch 093: Loss = 0.2867, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6418\n",
                        "Epoch 094: Loss = 0.2848, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6444\n",
                        "Epoch 095: Loss = 0.2835, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6439\n",
                        "Epoch 096: Loss = 0.2865, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6477\n",
                        "Epoch 097: Loss = 0.2805, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.6492\n",
                        "Epoch 098: Loss = 0.2824, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6469\n",
                        "Epoch 099: Loss = 0.2778, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6449\n",
                        "Epoch 100: Loss = 0.2794, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6437\n",
                        "Epoch 101: Loss = 0.2764, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6436\n",
                        "Epoch 102: Loss = 0.2745, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6442\n",
                        "Epoch 103: Loss = 0.2739, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6422\n",
                        "Epoch 104: Loss = 0.2718, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.6480\n",
                        "Epoch 105: Loss = 0.2761, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6470\n",
                        "Epoch 106: Loss = 0.2760, Val Acc = 0.6835, Micro F1 = 0.6835, Macro F1 = 0.6461\n",
                        "Epoch 107: Loss = 0.2744, Val Acc = 0.6871, Micro F1 = 0.6871, Macro F1 = 0.6491\n",
                        "Epoch 108: Loss = 0.2679, Val Acc = 0.6835, Micro F1 = 0.6835, Macro F1 = 0.6460\n",
                        "Epoch 109: Loss = 0.2726, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6469\n",
                        "Epoch 110: Loss = 0.2692, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6450\n",
                        "Epoch 111: Loss = 0.2692, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6431\n",
                        "Epoch 112: Loss = 0.2679, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6422\n",
                        "Epoch 113: Loss = 0.2699, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6415\n",
                        "Epoch 114: Loss = 0.2624, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6486\n",
                        "Epoch 115: Loss = 0.2654, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6489\n",
                        "Epoch 116: Loss = 0.2605, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6456\n",
                        "Epoch 117: Loss = 0.2630, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6456\n",
                        "Epoch 118: Loss = 0.2586, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6414\n",
                        "Epoch 119: Loss = 0.2543, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6421\n",
                        "Epoch 120: Loss = 0.2612, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6396\n",
                        "Epoch 121: Loss = 0.2596, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6399\n",
                        "Epoch 122: Loss = 0.2560, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6467\n",
                        "Epoch 123: Loss = 0.2546, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6428\n",
                        "Epoch 124: Loss = 0.2581, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6406\n",
                        "Epoch 125: Loss = 0.2595, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6435\n",
                        "Epoch 126: Loss = 0.2557, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6463\n",
                        "Epoch 127: Loss = 0.2548, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6463\n",
                        "Epoch 128: Loss = 0.2538, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6438\n",
                        "Epoch 129: Loss = 0.2515, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6464\n",
                        "Epoch 130: Loss = 0.2513, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6408\n",
                        "Epoch 131: Loss = 0.2528, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6410\n",
                        "Epoch 132: Loss = 0.2513, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6409\n",
                        "Epoch 133: Loss = 0.2542, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6383\n",
                        "Epoch 134: Loss = 0.2487, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6386\n",
                        "Epoch 135: Loss = 0.2465, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6387\n",
                        "Epoch 136: Loss = 0.2469, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6375\n",
                        "Epoch 137: Loss = 0.2508, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6396\n",
                        "Epoch 138: Loss = 0.2441, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6394\n",
                        "Epoch 139: Loss = 0.2462, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6363\n",
                        "Epoch 140: Loss = 0.2516, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6371\n",
                        "Epoch 141: Loss = 0.2448, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6385\n",
                        "Epoch 142: Loss = 0.2448, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6395\n",
                        "Epoch 143: Loss = 0.2451, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6410\n",
                        "Epoch 144: Loss = 0.2479, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6399\n",
                        "Epoch 145: Loss = 0.2511, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6399\n",
                        "Epoch 146: Loss = 0.2490, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6400\n",
                        "Epoch 147: Loss = 0.2438, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6413\n",
                        "Epoch 148: Loss = 0.2411, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6398\n",
                        "Epoch 149: Loss = 0.2471, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6363\n",
                        "Epoch 150: Loss = 0.2427, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6376\n",
                        "Epoch 151: Loss = 0.2380, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6366\n",
                        "Epoch 152: Loss = 0.2430, Val Acc = 0.6691, Micro F1 = 0.6691, Macro F1 = 0.6353\n",
                        "Epoch 153: Loss = 0.2405, Val Acc = 0.6679, Micro F1 = 0.6679, Macro F1 = 0.6329\n",
                        "Epoch 154: Loss = 0.2415, Val Acc = 0.6679, Micro F1 = 0.6679, Macro F1 = 0.6340\n",
                        "Epoch 155: Loss = 0.2368, Val Acc = 0.6679, Micro F1 = 0.6679, Macro F1 = 0.6328\n",
                        "Epoch 156: Loss = 0.2419, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6360\n",
                        "Epoch 157: Loss = 0.2402, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6360\n",
                        "Epoch 158: Loss = 0.2376, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6360\n",
                        "Epoch 159: Loss = 0.2297, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6360\n",
                        "Epoch 160: Loss = 0.2369, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6359\n",
                        "Epoch 161: Loss = 0.2354, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6385\n",
                        "Epoch 162: Loss = 0.2360, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6374\n",
                        "Epoch 163: Loss = 0.2354, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6397\n",
                        "Epoch 164: Loss = 0.2332, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6414\n",
                        "Epoch 165: Loss = 0.2390, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6435\n",
                        "Epoch 166: Loss = 0.2341, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6423\n",
                        "Epoch 167: Loss = 0.2352, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6413\n",
                        "Epoch 168: Loss = 0.2311, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6406\n",
                        "Epoch 169: Loss = 0.2349, Val Acc = 0.6691, Micro F1 = 0.6691, Macro F1 = 0.6340\n",
                        "Epoch 170: Loss = 0.2411, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6361\n",
                        "Epoch 171: Loss = 0.2339, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6395\n",
                        "Epoch 172: Loss = 0.2322, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6418\n",
                        "Epoch 173: Loss = 0.2316, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6434\n",
                        "Epoch 174: Loss = 0.2305, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6427\n",
                        "Epoch 175: Loss = 0.2296, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6416\n",
                        "Epoch 176: Loss = 0.2345, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6445\n",
                        "Epoch 177: Loss = 0.2299, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6462\n",
                        "Early stopping at epoch 178\n",
                        "Epoch 001: Loss = 1.7928, Val Acc = 0.2756, Micro F1 = 0.2756, Macro F1 = 0.2224\n",
                        "Epoch 002: Loss = 1.7728, Val Acc = 0.3682, Micro F1 = 0.3682, Macro F1 = 0.2887\n",
                        "Epoch 003: Loss = 1.7525, Val Acc = 0.4248, Micro F1 = 0.4248, Macro F1 = 0.3247\n",
                        "Epoch 004: Loss = 1.7307, Val Acc = 0.4621, Micro F1 = 0.4621, Macro F1 = 0.3567\n",
                        "Epoch 005: Loss = 1.7068, Val Acc = 0.4789, Micro F1 = 0.4789, Macro F1 = 0.3690\n",
                        "Epoch 006: Loss = 1.6810, Val Acc = 0.5054, Micro F1 = 0.5054, Macro F1 = 0.3913\n",
                        "Epoch 007: Loss = 1.6478, Val Acc = 0.5223, Micro F1 = 0.5223, Macro F1 = 0.4065\n",
                        "Epoch 008: Loss = 1.6111, Val Acc = 0.5235, Micro F1 = 0.5235, Macro F1 = 0.4077\n",
                        "Epoch 009: Loss = 1.5755, Val Acc = 0.5319, Micro F1 = 0.5319, Macro F1 = 0.4209\n",
                        "Epoch 010: Loss = 1.5316, Val Acc = 0.5487, Micro F1 = 0.5487, Macro F1 = 0.4407\n",
                        "Epoch 011: Loss = 1.4899, Val Acc = 0.5535, Micro F1 = 0.5535, Macro F1 = 0.4479\n",
                        "Epoch 012: Loss = 1.4385, Val Acc = 0.5692, Micro F1 = 0.5692, Macro F1 = 0.4694\n",
                        "Epoch 013: Loss = 1.3964, Val Acc = 0.5776, Micro F1 = 0.5776, Macro F1 = 0.4838\n",
                        "Epoch 014: Loss = 1.3394, Val Acc = 0.5836, Micro F1 = 0.5836, Macro F1 = 0.4897\n",
                        "Epoch 015: Loss = 1.2882, Val Acc = 0.6005, Micro F1 = 0.6005, Macro F1 = 0.5130\n",
                        "Epoch 016: Loss = 1.2351, Val Acc = 0.6209, Micro F1 = 0.6209, Macro F1 = 0.5397\n",
                        "Epoch 017: Loss = 1.1858, Val Acc = 0.6354, Micro F1 = 0.6354, Macro F1 = 0.5517\n",
                        "Epoch 018: Loss = 1.1316, Val Acc = 0.6474, Micro F1 = 0.6474, Macro F1 = 0.5635\n",
                        "Epoch 019: Loss = 1.0810, Val Acc = 0.6522, Micro F1 = 0.6522, Macro F1 = 0.5687\n",
                        "Epoch 020: Loss = 1.0274, Val Acc = 0.6619, Micro F1 = 0.6619, Macro F1 = 0.5770\n",
                        "Epoch 021: Loss = 0.9799, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.5893\n",
                        "Epoch 022: Loss = 0.9354, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.5949\n",
                        "Epoch 023: Loss = 0.8938, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.5988\n",
                        "Epoch 024: Loss = 0.8574, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.5990\n",
                        "Epoch 025: Loss = 0.8107, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.5990\n",
                        "Epoch 026: Loss = 0.7762, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.5994\n",
                        "Epoch 027: Loss = 0.7506, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6026\n",
                        "Epoch 028: Loss = 0.7209, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6028\n",
                        "Epoch 029: Loss = 0.6855, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6197\n",
                        "Epoch 030: Loss = 0.6626, Val Acc = 0.6955, Micro F1 = 0.6955, Macro F1 = 0.6184\n",
                        "Epoch 031: Loss = 0.6366, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6245\n",
                        "Epoch 032: Loss = 0.6176, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6285\n",
                        "Epoch 033: Loss = 0.5999, Val Acc = 0.7040, Micro F1 = 0.7040, Macro F1 = 0.6353\n",
                        "Epoch 034: Loss = 0.5821, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6372\n",
                        "Epoch 035: Loss = 0.5632, Val Acc = 0.7040, Micro F1 = 0.7040, Macro F1 = 0.6435\n",
                        "Epoch 036: Loss = 0.5605, Val Acc = 0.7076, Micro F1 = 0.7076, Macro F1 = 0.6522\n",
                        "Epoch 037: Loss = 0.5398, Val Acc = 0.7088, Micro F1 = 0.7088, Macro F1 = 0.6555\n",
                        "Epoch 038: Loss = 0.5326, Val Acc = 0.7076, Micro F1 = 0.7076, Macro F1 = 0.6546\n",
                        "Epoch 039: Loss = 0.5264, Val Acc = 0.7100, Micro F1 = 0.7100, Macro F1 = 0.6561\n",
                        "Epoch 040: Loss = 0.4990, Val Acc = 0.7076, Micro F1 = 0.7076, Macro F1 = 0.6538\n",
                        "Epoch 041: Loss = 0.4986, Val Acc = 0.7064, Micro F1 = 0.7064, Macro F1 = 0.6527\n",
                        "Epoch 042: Loss = 0.4905, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6518\n",
                        "Epoch 043: Loss = 0.4784, Val Acc = 0.7064, Micro F1 = 0.7064, Macro F1 = 0.6574\n",
                        "Epoch 044: Loss = 0.4804, Val Acc = 0.7064, Micro F1 = 0.7064, Macro F1 = 0.6574\n",
                        "Epoch 045: Loss = 0.4669, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6584\n",
                        "Epoch 046: Loss = 0.4592, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6584\n",
                        "Epoch 047: Loss = 0.4516, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6552\n",
                        "Epoch 048: Loss = 0.4407, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6586\n",
                        "Epoch 049: Loss = 0.4333, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6589\n",
                        "Epoch 050: Loss = 0.4351, Val Acc = 0.7040, Micro F1 = 0.7040, Macro F1 = 0.6574\n",
                        "Epoch 051: Loss = 0.4299, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6565\n",
                        "Epoch 052: Loss = 0.4203, Val Acc = 0.7028, Micro F1 = 0.7028, Macro F1 = 0.6570\n",
                        "Epoch 053: Loss = 0.4108, Val Acc = 0.7076, Micro F1 = 0.7076, Macro F1 = 0.6654\n",
                        "Epoch 054: Loss = 0.4088, Val Acc = 0.7052, Micro F1 = 0.7052, Macro F1 = 0.6628\n",
                        "Epoch 055: Loss = 0.4044, Val Acc = 0.7040, Micro F1 = 0.7040, Macro F1 = 0.6617\n",
                        "Epoch 056: Loss = 0.3963, Val Acc = 0.7004, Micro F1 = 0.7004, Macro F1 = 0.6583\n",
                        "Epoch 057: Loss = 0.3966, Val Acc = 0.7004, Micro F1 = 0.7004, Macro F1 = 0.6582\n",
                        "Epoch 058: Loss = 0.3841, Val Acc = 0.6980, Micro F1 = 0.6980, Macro F1 = 0.6564\n",
                        "Epoch 059: Loss = 0.3850, Val Acc = 0.6992, Micro F1 = 0.6992, Macro F1 = 0.6594\n",
                        "Epoch 060: Loss = 0.3768, Val Acc = 0.7004, Micro F1 = 0.7004, Macro F1 = 0.6602\n",
                        "Epoch 061: Loss = 0.3761, Val Acc = 0.6980, Micro F1 = 0.6980, Macro F1 = 0.6581\n",
                        "Epoch 062: Loss = 0.3746, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6549\n",
                        "Epoch 063: Loss = 0.3670, Val Acc = 0.6980, Micro F1 = 0.6980, Macro F1 = 0.6562\n",
                        "Epoch 064: Loss = 0.3702, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6554\n",
                        "Epoch 065: Loss = 0.3633, Val Acc = 0.6931, Micro F1 = 0.6931, Macro F1 = 0.6519\n",
                        "Epoch 066: Loss = 0.3567, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6551\n",
                        "Epoch 067: Loss = 0.3520, Val Acc = 0.6955, Micro F1 = 0.6955, Macro F1 = 0.6539\n",
                        "Epoch 068: Loss = 0.3465, Val Acc = 0.6968, Micro F1 = 0.6968, Macro F1 = 0.6558\n",
                        "Epoch 069: Loss = 0.3514, Val Acc = 0.6943, Micro F1 = 0.6943, Macro F1 = 0.6537\n",
                        "Epoch 070: Loss = 0.3449, Val Acc = 0.6931, Micro F1 = 0.6931, Macro F1 = 0.6524\n",
                        "Epoch 071: Loss = 0.3387, Val Acc = 0.6919, Micro F1 = 0.6919, Macro F1 = 0.6517\n",
                        "Epoch 072: Loss = 0.3447, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6486\n",
                        "Epoch 073: Loss = 0.3401, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6498\n",
                        "Epoch 074: Loss = 0.3410, Val Acc = 0.6895, Micro F1 = 0.6895, Macro F1 = 0.6494\n",
                        "Epoch 075: Loss = 0.3321, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.6464\n",
                        "Epoch 076: Loss = 0.3300, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6458\n",
                        "Epoch 077: Loss = 0.3284, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6434\n",
                        "Epoch 078: Loss = 0.3275, Val Acc = 0.6835, Micro F1 = 0.6835, Macro F1 = 0.6446\n",
                        "Epoch 079: Loss = 0.3304, Val Acc = 0.6835, Micro F1 = 0.6835, Macro F1 = 0.6443\n",
                        "Epoch 080: Loss = 0.3163, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6431\n",
                        "Epoch 081: Loss = 0.3189, Val Acc = 0.6871, Micro F1 = 0.6871, Macro F1 = 0.6476\n",
                        "Epoch 082: Loss = 0.3194, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6410\n",
                        "Epoch 083: Loss = 0.3179, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6423\n",
                        "Epoch 084: Loss = 0.3119, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6429\n",
                        "Epoch 085: Loss = 0.3113, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6430\n",
                        "Epoch 086: Loss = 0.3094, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6458\n",
                        "Epoch 087: Loss = 0.2997, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6445\n",
                        "Epoch 088: Loss = 0.3036, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6462\n",
                        "Epoch 089: Loss = 0.3040, Val Acc = 0.6835, Micro F1 = 0.6835, Macro F1 = 0.6488\n",
                        "Epoch 090: Loss = 0.3065, Val Acc = 0.6859, Micro F1 = 0.6859, Macro F1 = 0.6496\n",
                        "Epoch 091: Loss = 0.2993, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6466\n",
                        "Epoch 092: Loss = 0.3032, Val Acc = 0.6835, Micro F1 = 0.6835, Macro F1 = 0.6479\n",
                        "Epoch 093: Loss = 0.2940, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6471\n",
                        "Epoch 094: Loss = 0.2921, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6461\n",
                        "Epoch 095: Loss = 0.2964, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6472\n",
                        "Epoch 096: Loss = 0.2922, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6448\n",
                        "Epoch 097: Loss = 0.2960, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6447\n",
                        "Epoch 098: Loss = 0.2916, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6447\n",
                        "Epoch 099: Loss = 0.2875, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6437\n",
                        "Epoch 100: Loss = 0.2808, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6448\n",
                        "Epoch 101: Loss = 0.2878, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6437\n",
                        "Epoch 102: Loss = 0.2847, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6448\n",
                        "Epoch 103: Loss = 0.2844, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6449\n",
                        "Epoch 104: Loss = 0.2825, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6416\n",
                        "Epoch 105: Loss = 0.2851, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6428\n",
                        "Epoch 106: Loss = 0.2799, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6413\n",
                        "Epoch 107: Loss = 0.2773, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6436\n",
                        "Epoch 108: Loss = 0.2780, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6467\n",
                        "Epoch 109: Loss = 0.2785, Val Acc = 0.6847, Micro F1 = 0.6847, Macro F1 = 0.6491\n",
                        "Epoch 110: Loss = 0.2655, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6459\n",
                        "Epoch 111: Loss = 0.2724, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6472\n",
                        "Epoch 112: Loss = 0.2780, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6469\n",
                        "Epoch 113: Loss = 0.2806, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6447\n",
                        "Epoch 114: Loss = 0.2709, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6425\n",
                        "Epoch 115: Loss = 0.2690, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6413\n",
                        "Epoch 116: Loss = 0.2654, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6434\n",
                        "Epoch 117: Loss = 0.2696, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6419\n",
                        "Epoch 118: Loss = 0.2677, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6421\n",
                        "Epoch 119: Loss = 0.2693, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6447\n",
                        "Epoch 120: Loss = 0.2635, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6430\n",
                        "Epoch 121: Loss = 0.2646, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6417\n",
                        "Epoch 122: Loss = 0.2612, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6427\n",
                        "Epoch 123: Loss = 0.2622, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6410\n",
                        "Epoch 124: Loss = 0.2637, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6398\n",
                        "Epoch 125: Loss = 0.2580, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6400\n",
                        "Epoch 126: Loss = 0.2581, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6460\n",
                        "Epoch 127: Loss = 0.2653, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6440\n",
                        "Epoch 128: Loss = 0.2587, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6429\n",
                        "Epoch 129: Loss = 0.2577, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6406\n",
                        "Epoch 130: Loss = 0.2632, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6428\n",
                        "Epoch 131: Loss = 0.2533, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6413\n",
                        "Epoch 132: Loss = 0.2526, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6439\n",
                        "Epoch 133: Loss = 0.2520, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6439\n",
                        "Epoch 134: Loss = 0.2590, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6463\n",
                        "Epoch 135: Loss = 0.2593, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6431\n",
                        "Epoch 136: Loss = 0.2514, Val Acc = 0.6715, Micro F1 = 0.6715, Macro F1 = 0.6372\n",
                        "Epoch 137: Loss = 0.2627, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6367\n",
                        "Epoch 138: Loss = 0.2492, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6410\n",
                        "Epoch 139: Loss = 0.2489, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6454\n",
                        "Epoch 140: Loss = 0.2472, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6458\n",
                        "Epoch 141: Loss = 0.2515, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6447\n",
                        "Epoch 142: Loss = 0.2475, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6426\n",
                        "Epoch 143: Loss = 0.2475, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6441\n",
                        "Epoch 144: Loss = 0.2564, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6441\n",
                        "Epoch 145: Loss = 0.2521, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6452\n",
                        "Epoch 146: Loss = 0.2445, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6473\n",
                        "Epoch 147: Loss = 0.2491, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6470\n",
                        "Epoch 148: Loss = 0.2493, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6460\n",
                        "Epoch 149: Loss = 0.2496, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6406\n",
                        "Epoch 150: Loss = 0.2463, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6406\n",
                        "Epoch 151: Loss = 0.2425, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6421\n",
                        "Epoch 152: Loss = 0.2486, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6420\n",
                        "Epoch 153: Loss = 0.2433, Val Acc = 0.6799, Micro F1 = 0.6799, Macro F1 = 0.6452\n",
                        "Epoch 154: Loss = 0.2442, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6459\n",
                        "Epoch 155: Loss = 0.2436, Val Acc = 0.6823, Micro F1 = 0.6823, Macro F1 = 0.6471\n",
                        "Epoch 156: Loss = 0.2432, Val Acc = 0.6811, Micro F1 = 0.6811, Macro F1 = 0.6461\n",
                        "Epoch 157: Loss = 0.2406, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6408\n",
                        "Epoch 158: Loss = 0.2418, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6407\n",
                        "Epoch 159: Loss = 0.2446, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6400\n",
                        "Epoch 160: Loss = 0.2508, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6435\n",
                        "Epoch 161: Loss = 0.2369, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6408\n",
                        "Epoch 162: Loss = 0.2436, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6445\n",
                        "Epoch 163: Loss = 0.2396, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6421\n",
                        "Epoch 164: Loss = 0.2400, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6413\n",
                        "Epoch 165: Loss = 0.2366, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6434\n",
                        "Epoch 166: Loss = 0.2427, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6432\n",
                        "Epoch 167: Loss = 0.2384, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6409\n",
                        "Epoch 168: Loss = 0.2378, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6409\n",
                        "Epoch 169: Loss = 0.2378, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6441\n",
                        "Epoch 170: Loss = 0.2329, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6411\n",
                        "Epoch 171: Loss = 0.2360, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6420\n",
                        "Epoch 172: Loss = 0.2329, Val Acc = 0.6751, Micro F1 = 0.6751, Macro F1 = 0.6408\n",
                        "Epoch 173: Loss = 0.2387, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6399\n",
                        "Epoch 174: Loss = 0.2379, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6389\n",
                        "Epoch 175: Loss = 0.2286, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6391\n",
                        "Epoch 176: Loss = 0.2339, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6427\n",
                        "Epoch 177: Loss = 0.2339, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6424\n",
                        "Epoch 178: Loss = 0.2390, Val Acc = 0.6763, Micro F1 = 0.6763, Macro F1 = 0.6425\n",
                        "Epoch 179: Loss = 0.2365, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6382\n",
                        "Epoch 180: Loss = 0.2386, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6369\n",
                        "Epoch 181: Loss = 0.2384, Val Acc = 0.6703, Micro F1 = 0.6703, Macro F1 = 0.6348\n",
                        "Epoch 182: Loss = 0.2410, Val Acc = 0.6727, Micro F1 = 0.6727, Macro F1 = 0.6372\n",
                        "Epoch 183: Loss = 0.2311, Val Acc = 0.6787, Micro F1 = 0.6787, Macro F1 = 0.6442\n",
                        "Epoch 184: Loss = 0.2311, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6433\n",
                        "Epoch 185: Loss = 0.2330, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6433\n",
                        "Epoch 186: Loss = 0.2405, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6419\n",
                        "Epoch 187: Loss = 0.2292, Val Acc = 0.6775, Micro F1 = 0.6775, Macro F1 = 0.6419\n",
                        "Epoch 188: Loss = 0.2308, Val Acc = 0.6739, Micro F1 = 0.6739, Macro F1 = 0.6389\n",
                        "Early stopping at epoch 189\n",
                        "Micro F1: 67.67$\\pm$0.15\n",
                        "Macro F1: 64.13$\\pm$0.16\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_CiteSeer.num_classes\nin_channels = dataset_CiteSeer.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_HGPSL(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_HGPSL, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(HGPSLPool(hidden_channels, ratio=pool_ratios[i], sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        edge_attr = None\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, edge_attr, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        micro_f1, macro_f1 = compute_f1_scores(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            print(f'Early stopping at epoch {epoch}')\n            break\n        print(f'Epoch {epoch:03d}: Loss = {loss.item():.4f}, Val Acc = {val_acc:.4f}, Micro F1 = {micro_f1:.4f}, Macro F1 = {macro_f1:.4f}')\n    return model, best_val_acc\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_HGPSL(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "CG-ODE",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}