{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boot/anaconda3/envs/ 1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn.models.mlp import Linear\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, TopKPooling, SAGPooling\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Mask: 1354 nodes\n",
      "Val Mask: 677 nodes\n",
      "Test Mask: 677 nodes\n",
      "Train Mask Average Closeness: 0.1076\n",
      "Val Mask Average Closeness: 0.1560\n",
      "Test Mask Average Closeness: 0.1786\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "data_path = \"/data/ /Pooling/\"\n",
    "dataset_Cora = Planetoid(root=data_path, name=\"Cora\")\n",
    "data = dataset_Cora[0]\n",
    "\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "closeness_centrality_tensor = torch.tensor([closeness_centrality[i] for i in range(data.num_nodes)])\n",
    "\n",
    "sorted_indices = torch.argsort(closeness_centrality_tensor)\n",
    "\n",
    "num_nodes = data.num_nodes\n",
    "train_size = int(0.5 * num_nodes)\n",
    "val_size = int(0.25 * num_nodes)\n",
    "test_size = num_nodes - train_size - val_size\n",
    "\n",
    "train_indices = sorted_indices[:train_size]\n",
    "val_indices = sorted_indices[train_size:train_size + val_size]\n",
    "test_indices = sorted_indices[train_size + val_size:]\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "print(f\"Train Mask: {train_mask.sum().item()} nodes\")\n",
    "print(f\"Val Mask: {val_mask.sum().item()} nodes\")\n",
    "print(f\"Test Mask: {test_mask.sum().item()} nodes\")\n",
    "\n",
    "train_closeness_avg = closeness_centrality_tensor[train_mask].float().mean().item()\n",
    "val_closeness_avg = closeness_centrality_tensor[val_mask].float().mean().item()\n",
    "test_closeness_avg = closeness_centrality_tensor[test_mask].float().mean().item()\n",
    "\n",
    "print(f\"Train Mask Average Closeness: {train_closeness_avg:.4f}\")\n",
    "print(f\"Val Mask Average Closeness: {val_closeness_avg:.4f}\")\n",
    "print(f\"Test Mask Average Closeness: {test_closeness_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 83.21$\\pm$0.18\n",
      "Macro F1: 81.98$\\pm$0.31\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_Cora.num_classes\n",
    "in_channels = dataset_Cora.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_TOPK(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_TOPK, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(TopKPooling(channels, ratio=pool_ratios[i]))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "#  \n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "#  \n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_TOPK(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    #  \n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    #  \n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "#  \n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 81.68$\\pm$1.54\n",
      "Macro F1: 80.34$\\pm$1.55\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGPooling\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_Cora.num_classes\n",
    "in_channels = dataset_Cora.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_SAG(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_SAG, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(SAGPooling(channels, ratio=pool_ratios[i]))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "#  \n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_SAG(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASAPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 84.10$\\pm$0.37\n",
      "Macro F1: 82.97$\\pm$0.32\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import ASAPooling\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_Cora.num_classes\n",
    "in_channels = dataset_Cora.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_ASA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_ASA, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(ASAPooling(channels, ratio=pool_ratios[i]))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "#  \n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_ASA(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PANPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "class PANPooling(torch.nn.Module):\n",
    "    r\"\"\" General Graph pooling layer based on PAN, which can work with all layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, ratio=0.5, pan_pool_weight=None, min_score=None, multiplier=1,\n",
    "                 nonlinearity=torch.tanh, filter_size=3, panpool_filter_weight=None):\n",
    "        super(PANPooling, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.min_score = min_score\n",
    "        self.multiplier = multiplier\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        self.filter_size = filter_size\n",
    "        if panpool_filter_weight is None:\n",
    "            self.panpool_filter_weight = torch.nn.Parameter(0.5 * torch.ones(filter_size), requires_grad=True)\n",
    "\n",
    "        self.transform = Parameter(torch.ones(in_channels), requires_grad=True)\n",
    "\n",
    "        if pan_pool_weight is None:\n",
    "            #self.weight = torch.tensor([0.7, 0.3], device=self.transform.device)\n",
    "            self.pan_pool_weight = torch.nn.Parameter(0.5 * torch.ones(2), requires_grad=True)\n",
    "        else:\n",
    "            self.pan_pool_weight = pan_pool_weight\n",
    "\n",
    "    def forward(self, x, edge_index, M=None, batch=None, num_nodes=None):\n",
    "\n",
    "        \"\"\"\"\"\"\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        # Path integral\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "        edge_index, edge_weight = self.panentropy_sparse(edge_index, num_nodes)\n",
    "\n",
    "        # weighted degree\n",
    "        num_nodes = x.size(0)\n",
    "        degree = torch.zeros(num_nodes, device=edge_index.device)\n",
    "        degree = scatter_add(edge_weight, edge_index[0], out=degree)\n",
    "\n",
    "        # linear transform\n",
    "        xtransform = torch.matmul(x, self.transform)\n",
    "\n",
    "        # aggregate score\n",
    "        x_transform_norm = xtransform #/ xtransform.norm(p=2, dim=-1)\n",
    "        degree_norm = degree #/ degree.norm(p=2, dim=-1)\n",
    "        score = self.pan_pool_weight[0] * x_transform_norm + self.pan_pool_weight[1] * degree_norm\n",
    "\n",
    "        if self.min_score is None:\n",
    "            score = self.nonlinearity(score)\n",
    "        else:\n",
    "            score = softmax(score, batch)\n",
    "\n",
    "        perm = self.topk(score, self.ratio, batch, self.min_score)\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        x = self.multiplier * x if self.multiplier != 1 else x\n",
    "\n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_weight = self.filter_adj(edge_index, edge_weight, perm, num_nodes=score.size(0))\n",
    "\n",
    "        return x, edge_index, edge_weight, batch, perm, score[perm]\n",
    "\n",
    "    def topk(self, x, ratio, batch, min_score=None, tol=1e-7):\n",
    "\n",
    "        if min_score is not None:\n",
    "            # Make sure that we do not drop all nodes in a graph.\n",
    "            scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "            scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "            perm = torch.nonzero(x > scores_min).view(-1)\n",
    "        else:\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "            cum_num_nodes = torch.cat(\n",
    "                [num_nodes.new_zeros(1),\n",
    "                 num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "            index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "            index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "            dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n",
    "            dense_x[index] = x\n",
    "            dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "            _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "            perm = perm + cum_num_nodes.view(-1, 1)\n",
    "            perm = perm.view(-1)\n",
    "\n",
    "            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "            mask = [\n",
    "                torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "                i * max_num_nodes for i in range(batch_size)\n",
    "            ]\n",
    "            mask = torch.cat(mask, dim=0)\n",
    "\n",
    "            perm = perm[mask]\n",
    "\n",
    "        return perm\n",
    "\n",
    "    def filter_adj(self, edge_index, edge_weight, perm, num_nodes=None):\n",
    "\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "        mask = perm.new_full((num_nodes, ), -1)\n",
    "        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "        mask[perm] = i\n",
    "\n",
    "        row, col = edge_index\n",
    "        row, col = mask[row], mask[col]\n",
    "        mask = (row >= 0) & (col >= 0)\n",
    "        row, col = row[mask], col[mask]\n",
    "\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight[mask]\n",
    "\n",
    "        return torch.stack([row, col], dim=0), edge_weight\n",
    "\n",
    "    def panentropy_sparse(self, edge_index, num_nodes):\n",
    "\n",
    "        edge_value = torch.ones(edge_index.size(1), device=edge_index.device)\n",
    "        edge_index, edge_value = coalesce(edge_index, edge_value, num_nodes, num_nodes)\n",
    "\n",
    "        # iteratively add weighted matrix power\n",
    "        pan_index, pan_value = eye(num_nodes, device=edge_index.device)\n",
    "        indextmp = pan_index.clone().to(edge_index.device)\n",
    "        valuetmp = pan_value.clone().to(edge_index.device)\n",
    "\n",
    "        pan_value = self.panpool_filter_weight[0] * pan_value\n",
    "\n",
    "        for i in range(self.filter_size - 1):\n",
    "            #indextmp, valuetmp = coalesce(indextmp, valuetmp, num_nodes, num_nodes)\n",
    "            indextmp, valuetmp = spspmm(indextmp, valuetmp, edge_index, edge_value, num_nodes, num_nodes, num_nodes)\n",
    "            valuetmp = valuetmp * self.panpool_filter_weight[i+1]\n",
    "            indextmp, valuetmp = coalesce(indextmp, valuetmp, num_nodes, num_nodes)\n",
    "            pan_index = torch.cat((pan_index, indextmp), 1)\n",
    "            pan_value = torch.cat((pan_value, valuetmp))\n",
    "\n",
    "        return coalesce(pan_index, pan_value, num_nodes, num_nodes, op='add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 84.44$\\pm$0.30\n",
      "Macro F1: 83.44$\\pm$0.36\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import ASAPooling\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_Cora.num_classes\n",
    "in_channels = dataset_Cora.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_PAN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_PAN, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(PANPooling(channels, ratio=pool_ratios[i]))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, score_perm = self.pools[i - 1](x, edge_index, batch=batch, M=None)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "#  \n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_PAN(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "class GPR_prop(MessagePassing):\n",
    "    '''\n",
    "    propagation class for GPR_GNN\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, alpha, Init, Gamma=None, bias=True, **kwargs):\n",
    "        super(GPR_prop, self).__init__(aggr='add', **kwargs)\n",
    "        self.K = K\n",
    "        self.Init = Init\n",
    "        self.alpha = alpha\n",
    "\n",
    "        assert Init in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']\n",
    "        if Init == 'SGC':\n",
    "            # SGC-like\n",
    "            TEMP = 0.0*np.ones(K+1)\n",
    "            TEMP[alpha] = 1.0\n",
    "        elif Init == 'PPR':\n",
    "            # PPR-like\n",
    "            TEMP = alpha*(1-alpha)**np.arange(K+1)\n",
    "            TEMP[-1] = (1-alpha)**K\n",
    "        elif Init == 'NPPR':\n",
    "            # Negative PPR\n",
    "            TEMP = (alpha)**np.arange(K+1)\n",
    "            TEMP = TEMP/np.sum(np.abs(TEMP))\n",
    "        elif Init == 'Random':\n",
    "            # Random\n",
    "            bound = np.sqrt(3/(K+1))\n",
    "            TEMP = np.random.uniform(-bound, bound, K+1)\n",
    "            TEMP = TEMP/np.sum(np.abs(TEMP))\n",
    "        elif Init == 'WS':\n",
    "            # Specify Gamma\n",
    "            TEMP = Gamma\n",
    "\n",
    "        self.temp = Parameter(torch.tensor(TEMP))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.zeros_(self.temp)\n",
    "        for k in range(self.K+1):\n",
    "            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n",
    "        self.temp.data[-1] = (1-self.alpha)**self.K\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        edge_index, norm = gcn_norm(\n",
    "            edge_index, edge_weight, num_nodes=x.size(0), dtype=x.dtype)\n",
    "\n",
    "        hidden = x*(self.temp[0])\n",
    "        for k in range(self.K):\n",
    "            x = self.propagate(edge_index, x=x, norm=norm)\n",
    "            gamma = self.temp[k+1]\n",
    "            hidden = hidden + gamma*x\n",
    "        return hidden\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n",
    "                                           self.temp)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) # in case all the edges are removed\n",
    "\n",
    "        edge_index = edge_index.type(torch.long)\n",
    "        row, col = edge_index\n",
    "        # print(row, col)\n",
    "        # print(edge_weight.shape, row.shape, num_nodes)\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        # row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "class graph_attention(torch.nn.Module):\n",
    "    # reference: https://github.com/gordicaleksa/pytorch-GAT/blob/39c8f0ee634477033e8b1a6e9a6da3c7ed71bbd1/models/definitions/GAT.py#L324\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    nodes_dim = 0      # node dimension/axis\n",
    "    head_dim = 1       # attention head dimension/axis\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, dropout_prob=0.6, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Saving these as we'll need them in forward propagation in children layers (imp1/2/3)\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        # You can treat this one matrix as num_of_heads independent W matrices\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the additive scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
    "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
    "        Feel free to experiment - there may be better initializations depending on your problem.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features = x  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = scores_source_lifted + scores_target_lifted\n",
    "\n",
    "        return torch.sigmoid(scores_per_edge)\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "\n",
    "\n",
    "class CoPooling(torch.nn.Module):\n",
    "    # reference for GAT code: https://github.com/PetarV-/GAT\n",
    "    # reference for generalized pagerank code: https://github.com/jianhao2016/GPRGNN\n",
    "    def __init__(self, ratio=0.5, K=0.05, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=None):\n",
    "        super(CoPooling, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "        self.edge_ratio = edge_ratio\n",
    "\n",
    "        self.prop1 = GPR_prop(K, alpha, Init, Gamma)\n",
    "\n",
    "        score_dim = 32\n",
    "        self.G_att = graph_attention(num_in_features=nhid, num_out_features=score_dim, num_of_heads=1)\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(2*nhid, nhid))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        self.bias = Parameter(torch.Tensor(nhid))\n",
    "        nn.init.zeros_(self.bias.data)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        nn.init.zeros_(self.bias.data)\n",
    "        self.prop1.reset_parameters()\n",
    "        self.G_att.init_params()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None, nodes_index=None, node_attr=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        ori_batch = batch.clone()\n",
    "        device = x.device\n",
    "        num_nodes = x.shape[0]\n",
    "\n",
    "        # cut edges based on scores\n",
    "        x_cut = self.prop1(x, edge_index) # run generalized pagerank to update features\n",
    "\n",
    "        attention = self.G_att(x_cut, edge_index) # get the attention weights after sigmoid\n",
    "        attention = attention.sum(dim=1) #sum the weights on head dim\n",
    "        edge_index, attention = add_self_loops(edge_index, attention, 1.0, num_nodes) # add self loops in case no edges\n",
    "\n",
    "        # to get a systemitic adj matrix\n",
    "        edge_index_t, attention_t = transpose(edge_index, attention, num_nodes, num_nodes)\n",
    "        edge_tmp = torch.cat((edge_index, edge_index_t), 1)\n",
    "        att_tmp = torch.cat((attention, attention_t),0)\n",
    "        edge_index, attention = coalesce(edge_tmp, att_tmp, num_nodes, num_nodes, 'mean')\n",
    "\n",
    "        attention_np = attention.cpu().data.numpy()\n",
    "        cut_val = np.percentile(attention_np, int(100*(1-self.edge_ratio))) # this is for keep the top edge_ratio edges\n",
    "        attention = attention * (attention >= cut_val) # keep the edge_ratio higher weights of edges\n",
    "\n",
    "        kep_idx = attention > 0.0\n",
    "        cut_edge_index, cut_edge_attr = edge_index[:, kep_idx], attention[kep_idx]\n",
    "\n",
    "        # Graph Pooling based on nodes\n",
    "        x_information_score = self.calc_information_score(x, cut_edge_index, cut_edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x_topk = x[perm]\n",
    "        batch = batch[perm]\n",
    "        if nodes_index is not None:\n",
    "            nodes_index = nodes_index[perm]\n",
    "\n",
    "        if node_attr is not None:\n",
    "            node_attr = node_attr[perm]\n",
    "        if cut_edge_index is not None or cut_edge_index.nelement() != 0:\n",
    "            induced_edge_index, induced_edge_attr = filter_adj(cut_edge_index, cut_edge_attr, perm, num_nodes=num_nodes)\n",
    "        else:\n",
    "            print('All edges are cut!')\n",
    "            induced_edge_index, induced_edge_attr = cut_edge_index, cut_edge_attr\n",
    "\n",
    "        # update node features\n",
    "        attention_dense = (to_dense_adj(cut_edge_index, edge_attr=cut_edge_attr, max_num_nodes=num_nodes)).squeeze()\n",
    "        x = F.relu(torch.matmul(torch.cat((x_topk, torch.matmul(attention_dense[perm],x)), 1), self.weight) + self.bias)\n",
    "\n",
    "        return x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 81.14$\\pm$1.72\n",
      "Macro F1: 80.00$\\pm$1.01\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import ASAPooling\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_Cora.num_classes\n",
    "in_channels = dataset_Cora.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_CO(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_CO, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(CoPooling(ratio=pool_ratios[i], K=1, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=1.0))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, perm, _, batch, _, _, _ = self.pools[i - 1](x, edge_index, edge_attr=None, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "#  \n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_CO(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGIPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "@dataclass(init=False)\n",
    "class SelectOutput:\n",
    "    r\"\"\"The output of the :class:`Select` method, which holds an assignment\n",
    "    from selected nodes to their respective cluster(s).\n",
    "\n",
    "    Args:\n",
    "        node_index (torch.Tensor): The indices of the selected nodes.\n",
    "        num_nodes (int): The number of nodes.\n",
    "        cluster_index (torch.Tensor): The indices of the clusters each node in\n",
    "            :obj:`node_index` is assigned to.\n",
    "        num_clusters (int): The number of clusters.\n",
    "        weight (torch.Tensor, optional): A weight vector, denoting the strength\n",
    "            of the assignment of a node to its cluster. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    node_index: Tensor\n",
    "    num_nodes: int\n",
    "    cluster_index: Tensor\n",
    "    num_clusters: int\n",
    "    weight: Optional[Tensor] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_index: Tensor,\n",
    "        num_nodes: int,\n",
    "        cluster_index: Tensor,\n",
    "        num_clusters: int,\n",
    "        weight: Optional[Tensor] = None,\n",
    "    ):\n",
    "        if node_index.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'node_index' to be one-dimensional \"\n",
    "                             f\"(got {node_index.dim()} dimensions)\")\n",
    "\n",
    "        if cluster_index.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'cluster_index' to be one-dimensional \"\n",
    "                             f\"(got {cluster_index.dim()} dimensions)\")\n",
    "\n",
    "        if node_index.numel() != cluster_index.numel():\n",
    "            raise ValueError(f\"Expected 'node_index' and 'cluster_index' to \"\n",
    "                             f\"hold the same number of values (got \"\n",
    "                             f\"{node_index.numel()} and \"\n",
    "                             f\"{cluster_index.numel()} values)\")\n",
    "\n",
    "        if weight is not None and weight.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'weight' vector to be one-dimensional \"\n",
    "                             f\"(got {weight.dim()} dimensions)\")\n",
    "\n",
    "        if weight is not None and weight.numel() != node_index.numel():\n",
    "            raise ValueError(f\"Expected 'weight' to hold {node_index.numel()} \"\n",
    "                             f\"values (got {weight.numel()} values)\")\n",
    "\n",
    "        self.node_index = node_index\n",
    "        self.num_nodes = num_nodes\n",
    "        self.cluster_index = cluster_index\n",
    "        self.num_clusters = num_clusters\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Select(torch.nn.Module):\n",
    "    r\"\"\"An abstract base class for implementing custom node selections as\n",
    "    described in the `\"Understanding Pooling in Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1905.05178>`_ paper, which maps the nodes of an\n",
    "    input graph to supernodes in the coarsened graph.\n",
    "\n",
    "    Specifically, :class:`Select` returns a :class:`SelectOutput` output, which\n",
    "    holds a (sparse) mapping :math:`\\mathbf{C} \\in {[0, 1]}^{N \\times C}` that\n",
    "    assigns selected nodes to one or more of :math:`C` super nodes.\n",
    "    \"\"\"\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> SelectOutput:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'\n",
    "\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels * 2, in_channels)\n",
    "        self.fc2 = nn.Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CGIPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.5, non_lin=torch.tanh):\n",
    "        super(CGIPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.non_lin = non_lin\n",
    "        self.hidden_dim = in_channels\n",
    "        self.transform = GraphConv(in_channels, self.hidden_dim)\n",
    "        self.pp_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n",
    "        self.np_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        self.positive_pooling = GraphConv(self.hidden_dim, 1)\n",
    "        self.negative_pooling = GraphConv(self.hidden_dim, 1)\n",
    "\n",
    "        self.discriminator = Discriminator(self.hidden_dim)\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        device = x.device  # \n",
    "\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_transform = F.leaky_relu(self.transform(x, edge_index), 0.2)\n",
    "        x_tp = F.leaky_relu(self.pp_conv(x, edge_index), 0.2)\n",
    "        x_tn = F.leaky_relu(self.np_conv(x, edge_index), 0.2)\n",
    "        s_pp = self.positive_pooling(x_tp, edge_index).squeeze()\n",
    "        s_np = self.negative_pooling(x_tn, edge_index).squeeze()\n",
    "\n",
    "        perm_positive = topk(s_pp, 1, batch)\n",
    "        perm_negative = topk(s_np, 1, batch)\n",
    "        x_pp = x_transform[perm_positive] * self.non_lin(s_pp[perm_positive]).view(-1, 1)\n",
    "        x_np = x_transform[perm_negative] * self.non_lin(s_np[perm_negative]).view(-1, 1)\n",
    "\n",
    "        x_pp_readout = gap(x_pp, batch[perm_positive])\n",
    "        x_np_readout = gap(x_np, batch[perm_negative])\n",
    "        x_readout = gap(x_transform, batch)\n",
    "\n",
    "        positive_pair = torch.cat([x_pp_readout, x_readout], dim=1)\n",
    "        negative_pair = torch.cat([x_np_readout, x_readout], dim=1)\n",
    "\n",
    "        real = torch.ones(positive_pair.shape[0], device=device)  # \n",
    "        fake = torch.zeros(negative_pair.shape[0], device=device)  # \n",
    "        #real_loss = self.loss_fn(self.discriminator(positive_pair), real)\n",
    "        #fake_loss = self.loss_fn(self.discriminator(negative_pair), fake)\n",
    "        #discrimination_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        score = (s_pp - s_np)\n",
    "\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x_transform[perm] * self.non_lin(score[perm]).view(-1, 1)\n",
    "        batch = batch[perm]\n",
    "\n",
    "        filter_edge_index, filter_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "        return x, filter_edge_index, filter_edge_attr, batch, perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 82.27$\\pm$0.55\n",
      "Macro F1: 80.91$\\pm$0.80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_Cora.num_classes\n",
    "in_channels = dataset_Cora.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_CGI(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_CGI, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(CGIPool(channels, ratio=pool_ratios[i]))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "#  \n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_CGI(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMISPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Tuple, Union\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\n",
    "Scorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\n",
    "from torch_sparse import SparseTensor, remove_diag\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.nn.dense import Linear\n",
    "from torch.nn import Module\n",
    "from torch_scatter import scatter_max, scatter_min\n",
    "\n",
    "def maximal_independent_set(edge_index: Adj, k: int = 1,\n",
    "                            perm: OptTensor = None) -> Tensor:\n",
    "    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n",
    "    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n",
    "    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n",
    "    returned set.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method follows `Blelloch's Alogirithm\n",
    "    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n",
    "    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n",
    "    higher values of :math:`k`.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: :class:`ByteTensor`\n",
    "    \"\"\"\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "        device = edge_index.device()\n",
    "        n = edge_index.size(0)\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        device = row.device\n",
    "        n = edge_index.max().item() + 1\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    mis = torch.zeros(n, dtype=torch.bool, device=device)\n",
    "    mask = mis.clone()\n",
    "    min_rank = rank.clone()\n",
    "\n",
    "    while not mask.all():\n",
    "        for _ in range(k):\n",
    "            min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "            scatter_min(min_rank[row], col, out=min_neigh)\n",
    "            torch.minimum(min_neigh, min_rank, out=min_rank)  # self-loops\n",
    "\n",
    "        mis = mis | torch.eq(rank, min_rank)\n",
    "        mask = mis.clone().byte()\n",
    "\n",
    "        for _ in range(k):\n",
    "            max_neigh = torch.full_like(mask, fill_value=0)\n",
    "            scatter_max(mask[row], col, out=max_neigh)\n",
    "            torch.maximum(max_neigh, mask, out=mask)  # self-loops\n",
    "\n",
    "        mask = mask.to(dtype=torch.bool)\n",
    "        min_rank = rank.clone()\n",
    "        min_rank[mask] = n\n",
    "\n",
    "    return mis\n",
    "\n",
    "def maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n",
    "                                    perm: OptTensor = None) -> PairTensor:\n",
    "    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n",
    "    clustering of a graph, as defined in `\"Generalizing Downsampling from\n",
    "    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method returns both the :math:`k`-MIS and the clustering, where the\n",
    "    :math:`c`-th cluster refers to the :math:`c`-th element of the\n",
    "    :math:`k`-MIS.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n",
    "    \"\"\"\n",
    "    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n",
    "    n, device = mis.size(0), mis.device\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n",
    "    rank_mis = rank[mis]\n",
    "    min_rank[mis] = rank_mis\n",
    "\n",
    "    for _ in range(k):\n",
    "        min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "        scatter_min(min_rank[row], col, out=min_neigh)\n",
    "        torch.minimum(min_neigh, min_rank, out=min_rank)\n",
    "\n",
    "    _, clusters = torch.unique(min_rank, return_inverse=True)\n",
    "    perm = torch.argsort(rank_mis)\n",
    "    return mis, perm[clusters]\n",
    "\n",
    "\n",
    "class KMISPooling(Module):\n",
    "\n",
    "    _heuristics = {None, 'greedy', 'w-greedy'}\n",
    "    _passthroughs = {None, 'before', 'after'}\n",
    "    _scorers = {\n",
    "        'linear',\n",
    "        'random',\n",
    "        'constant',\n",
    "        'canonical',\n",
    "        'first',\n",
    "        'last',\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n",
    "                 scorer: Union[Scorer, str] = 'linear',\n",
    "                 score_heuristic: Optional[str] = 'greedy',\n",
    "                 score_passthrough: Optional[str] = 'before',\n",
    "                 aggr_x: Optional[Union[str, Aggregation]] = None,\n",
    "                 aggr_edge: str = 'sum',\n",
    "                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n",
    "                 remove_self_loops: bool = True) -> None:\n",
    "        super(KMISPooling, self).__init__()\n",
    "        assert score_heuristic in self._heuristics, \\\n",
    "            \"Unrecognized `score_heuristic` value.\"\n",
    "        assert score_passthrough in self._passthroughs, \\\n",
    "            \"Unrecognized `score_passthrough` value.\"\n",
    "\n",
    "        if not callable(scorer):\n",
    "            assert scorer in self._scorers, \\\n",
    "                \"Unrecognized `scorer` value.\"\n",
    "\n",
    "        self.k = k\n",
    "        self.scorer = scorer\n",
    "        self.score_heuristic = score_heuristic\n",
    "        self.score_passthrough = score_passthrough\n",
    "\n",
    "        self.aggr_x = aggr_x\n",
    "        self.aggr_edge = aggr_edge\n",
    "        self.aggr_score = aggr_score\n",
    "        self.remove_self_loops = remove_self_loops\n",
    "\n",
    "        if scorer == 'linear':\n",
    "            assert self.score_passthrough is not None, \\\n",
    "                \"`'score_passthrough'` must not be `None`\" \\\n",
    "                \" when using `'linear'` scorer\"\n",
    "\n",
    "            self.lin = nn.Linear(in_channels, 1)\n",
    "\n",
    "    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n",
    "        if self.score_heuristic is None:\n",
    "            return x\n",
    "\n",
    "        row, col, _ = adj.coo()\n",
    "        x = x.view(-1)\n",
    "\n",
    "        if self.score_heuristic == 'greedy':\n",
    "            k_sums = torch.ones_like(x)\n",
    "        else:\n",
    "            k_sums = x.clone()\n",
    "\n",
    "        for _ in range(self.k):\n",
    "            scatter_add(k_sums[row], col, out=k_sums)\n",
    "\n",
    "        return x / k_sums\n",
    "\n",
    "    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) -> Tensor:\n",
    "        if self.scorer == 'linear':\n",
    "            return self.lin(x).sigmoid()\n",
    "\n",
    "        if self.scorer == 'random':\n",
    "            return torch.rand((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'constant':\n",
    "            return torch.ones((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'canonical':\n",
    "            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n",
    "\n",
    "        if self.scorer == 'first':\n",
    "            return x[..., [0]]\n",
    "\n",
    "        if self.scorer == 'last':\n",
    "            return x[..., [-1]]\n",
    "\n",
    "        return self.scorer(x, edge_index, edge_attr, batch)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) \\\n",
    "            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n",
    "        \"\"\"\"\"\"\n",
    "        edge_index = edge_index.long()\n",
    "        adj, n = edge_index, x.size(0)\n",
    "\n",
    "        if not isinstance(edge_index, SparseTensor):\n",
    "            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n",
    "\n",
    "        score = self._scorer(x, edge_index, edge_attr, batch)\n",
    "        updated_score = self._apply_heuristic(score, adj)\n",
    "        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n",
    "\n",
    "        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n",
    "\n",
    "        row, col, val = adj.coo()\n",
    "        c = mis.sum()\n",
    "\n",
    "        if val is None:\n",
    "            val = torch.ones_like(row, dtype=torch.float)\n",
    "\n",
    "        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n",
    "                           is_sorted=False,\n",
    "                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n",
    "\n",
    "        if self.remove_self_loops:\n",
    "            adj = remove_diag(adj)\n",
    "\n",
    "        if self.score_passthrough == 'before':\n",
    "            x = self.aggr_score(x, score)\n",
    "\n",
    "        if self.aggr_x is None:\n",
    "            x = x[mis]\n",
    "        elif isinstance(self.aggr_x, str):\n",
    "            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n",
    "                        reduce=self.aggr_x)\n",
    "        else:\n",
    "            x = self.aggr_x(x, cluster, dim_size=c)\n",
    "\n",
    "        if self.score_passthrough == 'after':\n",
    "            x = self.aggr_score(x, score[mis])\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_index, edge_attr = adj, None\n",
    "\n",
    "        else:\n",
    "            row, col, edge_attr = adj.coo()\n",
    "            edge_index = torch.stack([row, col])\n",
    "\n",
    "        if batch is not None:\n",
    "            batch = batch[mis]\n",
    "\n",
    "        #attn = x\n",
    "        #select_out = topk(attn, batch)\n",
    "        perm = perm[mis]\n",
    "\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, mis, cluster, perm\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.scorer == 'linear':\n",
    "            channels = f\"in_channels={self.lin.in_channels}, \"\n",
    "        else:\n",
    "            channels = \"\"\n",
    "\n",
    "        return f'{self.__class__.__name__}({channels}k={self.k})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 83.55$\\pm$0.39\n",
      "Macro F1: 82.36$\\pm$0.41\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_Cora.num_classes\n",
    "in_channels = dataset_Cora.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(64, k=1, aggr_x='sum'))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "#  \n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_KMIS(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSAPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Union, Optional, Callable\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, ChebConv, GraphConv\n",
    "\n",
    "def uniform(size, tensor):\n",
    "    if tensor is not None:\n",
    "        bound = 1.0 / math.sqrt(size)\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "\n",
    "def topk(x, ratio, batch, min_score=None, tol=1e-7):\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n",
    "    else:\n",
    "        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "        cum_num_nodes = torch.cat(\n",
    "            [num_nodes.new_zeros(1),\n",
    "             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "        dense_x = x.new_full((batch_size * max_num_nodes, ),\n",
    "                             torch.finfo(x.dtype).min)\n",
    "        dense_x[index] = x\n",
    "        dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "        _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "        perm = perm + cum_num_nodes.view(-1, 1)\n",
    "        perm = perm.view(-1)\n",
    "\n",
    "        if isinstance(ratio, int):\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n",
    "            k = torch.min(k, num_nodes)\n",
    "        else:\n",
    "            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "\n",
    "        mask = [\n",
    "            torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "            i * max_num_nodes for i in range(batch_size)\n",
    "        ]\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "\n",
    "        perm = perm[mask]\n",
    "\n",
    "    return perm\n",
    "\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "\n",
    "class GSAPool(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n",
    "                        min_score=None, multiplier=1,\n",
    "                        non_linearity=torch.tanh,\n",
    "                        cus_drop_ratio =0):\n",
    "        super(GSAPool,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.ratio = pooling_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.sbtl_layer = GCNConv(in_channels,1)\n",
    "        self.fbtl_layer = nn.Linear(in_channels, 1)\n",
    "        self.fusion = GCNConv(in_channels,in_channels)\n",
    "\n",
    "        self.min_score = min_score\n",
    "        self.multiplier = multiplier\n",
    "        self.fusion_flag = 0\n",
    "        self.non_linearity = non_linearity\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n",
    "\n",
    "    def conv_selection(self, conv, in_channels, conv_type=0):\n",
    "        if(conv_type == 0):\n",
    "            out_channels = 1\n",
    "        elif(conv_type == 1):\n",
    "            out_channels = in_channels\n",
    "        if(conv == \"GCNConv\"):\n",
    "            return GCNConv(in_channels,out_channels)\n",
    "        elif(conv == \"ChebConv\"):\n",
    "            return ChebConv(in_channels,out_channels,1)\n",
    "        elif(conv == \"SAGEConv\"):\n",
    "            return SAGEConv(in_channels,out_channels)\n",
    "        elif(conv == \"GATConv\"):\n",
    "            return GATConv(in_channels,out_channels, heads=1, concat=True)\n",
    "        elif(conv == \"GraphConv\"):\n",
    "            return GraphConv(in_channels,out_channels)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "\n",
    "        #SBTL\n",
    "        score_s = self.sbtl_layer(x,edge_index).squeeze()\n",
    "        #FBTL\n",
    "        score_f = self.fbtl_layer(x).squeeze()\n",
    "        #hyperparametr alpha\n",
    "        score = score_s*self.alpha + score_f*(1-self.alpha)\n",
    "\n",
    "        score = score.unsqueeze(-1) if score.dim()==0 else score\n",
    "\n",
    "        if self.min_score is None:\n",
    "            score = self.non_linearity(score)\n",
    "        else:\n",
    "            score = softmax(score, batch)\n",
    "\n",
    "        sc = self.dropout(score)\n",
    "        perm = topk(sc, self.ratio, batch)\n",
    "\n",
    "        #fusion\n",
    "        if(self.fusion_flag == 1):\n",
    "            x = self.fusion(x, edge_index)\n",
    "        x_ae = x[perm]\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        x = self.multiplier * x if self.multiplier != 1 else x\n",
    "\n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_attr = filter_adj(\n",
    "            edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, perm, x_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 83.16$\\pm$0.32\n",
      "Macro F1: 81.89$\\pm$0.37\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_Cora.num_classes\n",
    "in_channels = dataset_Cora.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(GSAPool(64, pooling_ratio=pool_ratios[i], alpha = 0.6, cus_drop_ratio = 0))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, edge_attr, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "#  \n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_GSA(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HGPSLPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5344, 0.0000, 0.0000, 0.4656, 0.0613, 0.0000, 0.0000, 0.0000, 0.5640,\n",
      "        0.3748])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
    "def topk(x, ratio, batch, min_score=None, tol=1e-7):\n",
    "\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = torch.nonzero(x > scores_min).view(-1)\n",
    "    else:\n",
    "        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "        cum_num_nodes = torch.cat(\n",
    "            [num_nodes.new_zeros(1),\n",
    "            num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n",
    "        dense_x[index] = x\n",
    "        dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "        _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "        perm = perm + cum_num_nodes.view(-1, 1)\n",
    "        perm = perm.view(-1)\n",
    "\n",
    "        k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "        mask = [\n",
    "            torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "            i * max_num_nodes for i in range(batch_size)\n",
    "        ]\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "\n",
    "        perm = perm[mask]\n",
    "\n",
    "    return perm\n",
    "\n",
    "def filter_adj(edge_index, edge_weight, perm, num_nodes=None):\n",
    "\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "        mask = perm.new_full((num_nodes, ), -1)\n",
    "        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "        mask[perm] = i\n",
    "\n",
    "        row, col = edge_index\n",
    "        row, col = mask[row], mask[col]\n",
    "        mask = (row >= 0) & (col >= 0)\n",
    "        row, col = row[mask], col[mask]\n",
    "\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight[mask]\n",
    "\n",
    "        return torch.stack([row, col], dim=0), edge_weight\n",
    "\n",
    "def scatter_sort(x, batch, fill_value=-1e16):\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n",
    "    dense_x[index] = x\n",
    "    dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n",
    "    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n",
    "    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n",
    "\n",
    "    sorted_x = sorted_x.view(-1)\n",
    "    filled_index = sorted_x != fill_value\n",
    "\n",
    "    sorted_x = sorted_x[filled_index]\n",
    "    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n",
    "\n",
    "    return sorted_x, cumsum_sorted_x\n",
    "\n",
    "\n",
    "def _make_ix_like(batch):\n",
    "    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n",
    "    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n",
    "    idx = torch.cat(idx, dim=0)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _threshold_and_support(x, batch):\n",
    "    \"\"\"Sparsemax building block: compute the threshold\n",
    "    Args:\n",
    "        x: input tensor to apply the sparsemax\n",
    "        batch: group indicators\n",
    "    Returns:\n",
    "        the threshold value\n",
    "    \"\"\"\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    sorted_input, input_cumsum = scatter_sort(x, batch)\n",
    "    input_cumsum = input_cumsum - 1.0\n",
    "    rhos = _make_ix_like(batch).to(x.dtype)\n",
    "    support = rhos * sorted_input > input_cumsum\n",
    "\n",
    "    support_size = scatter_add(support.to(batch.dtype), batch)\n",
    "    # mask invalid index, for example, if batch is not start from 0 or not continuous, it may result in negative index\n",
    "    idx = support_size + cum_num_nodes - 1\n",
    "    mask = idx < 0\n",
    "    idx[mask] = 0\n",
    "    tau = input_cumsum.gather(0, idx)\n",
    "    tau /= support_size.to(x.dtype)\n",
    "\n",
    "    return tau, support_size\n",
    "\n",
    "\n",
    "class SparsemaxFunction(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, batch):\n",
    "        \"\"\"sparsemax: normalizing sparse transform\n",
    "        Parameters:\n",
    "            ctx: context object\n",
    "            x (Tensor): shape (N, )\n",
    "            batch: group indicator\n",
    "        Returns:\n",
    "            output (Tensor): same shape as input\n",
    "        \"\"\"\n",
    "        max_val, _ = scatter_max(x, batch)\n",
    "        x -= max_val[batch]\n",
    "        tau, supp_size = _threshold_and_support(x, batch)\n",
    "        output = torch.clamp(x - tau[batch], min=0)\n",
    "        ctx.save_for_backward(supp_size, output, batch)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output, batch = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n",
    "\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        return sparsemax(x, batch)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sparse_attention = Sparsemax()\n",
    "    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n",
    "    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n",
    "    res = sparse_attention(input_x, input_batch)\n",
    "    print(res)\n",
    "\n",
    "class TwoHopNeighborhood(object):\n",
    "    def __call__(self, data):\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "        n = data.num_nodes\n",
    "\n",
    "        fill = 1e16\n",
    "        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n",
    "\n",
    "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n",
    "\n",
    "        edge_index = torch.cat([edge_index, index], dim=1)\n",
    "        if edge_attr is None:\n",
    "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
    "        else:\n",
    "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
    "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
    "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
    "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min', fill_value=fill)\n",
    "            edge_attr[edge_attr >= fill] = 0\n",
    "            data.edge_attr = edge_attr\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)\n",
    "\n",
    "\n",
    "class GCN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
    "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            nn.init.zeros_(self.bias.data)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
    "\n",
    "        row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "class HGPSLPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n",
    "        super(HGPSLPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.sample = sample\n",
    "        self.sparse = sparse\n",
    "        self.sl = sl\n",
    "        self.negative_slop = negative_slop\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att.data)\n",
    "        self.sparse_attention = Sparsemax()\n",
    "        self.neighbor_augment = TwoHopNeighborhood()\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "\n",
    "        # Graph Pooling\n",
    "        original_x = x\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x[perm]\n",
    "        batch = batch[perm]\n",
    "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        # Discard structure learning layer, directly return\n",
    "        if self.sl is False:\n",
    "            return x, induced_edge_index, induced_edge_attr, batch\n",
    "\n",
    "        # Structure Learning\n",
    "        if self.sample:\n",
    "            # A fast mode for large graphs.\n",
    "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
    "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
    "            # edge weights between them.\n",
    "            k_hop = 3\n",
    "            if edge_attr is None:\n",
    "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
    "\n",
    "            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            for _ in range(k_hop - 1):\n",
    "                hop_data = self.neighbor_augment(hop_data)\n",
    "            hop_edge_index = hop_data.edge_index\n",
    "            hop_edge_attr = hop_data.edge_attr\n",
    "            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
    "            row, col = new_edge_index\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            adj[row, col] = weights\n",
    "            new_edge_index, weights = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
    "            if edge_attr is None:\n",
    "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
    "                                               device=induced_edge_index.device)\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            # Construct batch fully connected graph in block diagonal matirx format\n",
    "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
    "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
    "            new_edge_index, _ = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop)\n",
    "            adj[row, col] = weights\n",
    "            induced_row, induced_col = induced_edge_index\n",
    "\n",
    "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
    "            weights = adj[row, col]\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, num_nodes=x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x, new_edge_index, new_edge_attr, batch, perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss = 1.9392, Val Acc = 0.2895, Micro F1 = 0.2895, Macro F1 = 0.1406\n",
      "Epoch 002: Loss = 1.9174, Val Acc = 0.2452, Micro F1 = 0.2452, Macro F1 = 0.0587\n",
      "Epoch 003: Loss = 1.8991, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0561\n",
      "Epoch 004: Loss = 1.8793, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 005: Loss = 1.8589, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 006: Loss = 1.8368, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 007: Loss = 1.8105, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 008: Loss = 1.7871, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 009: Loss = 1.7637, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 010: Loss = 1.7314, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 011: Loss = 1.7097, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 012: Loss = 1.6784, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 013: Loss = 1.6548, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 014: Loss = 1.6159, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 015: Loss = 1.5907, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 016: Loss = 1.5737, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 017: Loss = 1.5432, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 018: Loss = 1.5249, Val Acc = 0.2452, Micro F1 = 0.2452, Macro F1 = 0.0599\n",
      "Epoch 019: Loss = 1.4894, Val Acc = 0.2496, Micro F1 = 0.2496, Macro F1 = 0.0709\n",
      "Epoch 020: Loss = 1.4642, Val Acc = 0.2570, Micro F1 = 0.2570, Macro F1 = 0.0876\n",
      "Epoch 021: Loss = 1.4343, Val Acc = 0.2674, Micro F1 = 0.2674, Macro F1 = 0.1079\n",
      "Epoch 022: Loss = 1.4082, Val Acc = 0.2836, Micro F1 = 0.2836, Macro F1 = 0.1316\n",
      "Epoch 023: Loss = 1.3714, Val Acc = 0.2939, Micro F1 = 0.2939, Macro F1 = 0.1476\n",
      "Epoch 024: Loss = 1.3351, Val Acc = 0.3412, Micro F1 = 0.3412, Macro F1 = 0.2094\n",
      "Epoch 025: Loss = 1.3094, Val Acc = 0.3693, Micro F1 = 0.3693, Macro F1 = 0.2394\n",
      "Epoch 026: Loss = 1.2856, Val Acc = 0.3900, Micro F1 = 0.3900, Macro F1 = 0.2628\n",
      "Epoch 027: Loss = 1.2371, Val Acc = 0.4092, Micro F1 = 0.4092, Macro F1 = 0.2794\n",
      "Epoch 028: Loss = 1.2126, Val Acc = 0.4431, Micro F1 = 0.4431, Macro F1 = 0.3119\n",
      "Epoch 029: Loss = 1.1616, Val Acc = 0.4609, Micro F1 = 0.4609, Macro F1 = 0.3232\n",
      "Epoch 030: Loss = 1.1346, Val Acc = 0.4727, Micro F1 = 0.4727, Macro F1 = 0.3307\n",
      "Epoch 031: Loss = 1.1028, Val Acc = 0.4697, Micro F1 = 0.4697, Macro F1 = 0.3273\n",
      "Epoch 032: Loss = 1.0692, Val Acc = 0.4742, Micro F1 = 0.4742, Macro F1 = 0.3308\n",
      "Epoch 033: Loss = 1.0500, Val Acc = 0.4845, Micro F1 = 0.4845, Macro F1 = 0.3389\n",
      "Epoch 034: Loss = 1.0092, Val Acc = 0.4874, Micro F1 = 0.4874, Macro F1 = 0.3385\n",
      "Epoch 035: Loss = 0.9810, Val Acc = 0.4978, Micro F1 = 0.4978, Macro F1 = 0.3438\n",
      "Epoch 036: Loss = 0.9349, Val Acc = 0.5037, Micro F1 = 0.5037, Macro F1 = 0.3473\n",
      "Epoch 037: Loss = 0.9045, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.3613\n",
      "Epoch 038: Loss = 0.8888, Val Acc = 0.5229, Micro F1 = 0.5229, Macro F1 = 0.3802\n",
      "Epoch 039: Loss = 0.8444, Val Acc = 0.5421, Micro F1 = 0.5421, Macro F1 = 0.4219\n",
      "Epoch 040: Loss = 0.8345, Val Acc = 0.5583, Micro F1 = 0.5583, Macro F1 = 0.4508\n",
      "Epoch 041: Loss = 0.7863, Val Acc = 0.5731, Micro F1 = 0.5731, Macro F1 = 0.4731\n",
      "Epoch 042: Loss = 0.7742, Val Acc = 0.5894, Micro F1 = 0.5894, Macro F1 = 0.4968\n",
      "Epoch 043: Loss = 0.7360, Val Acc = 0.6027, Micro F1 = 0.6027, Macro F1 = 0.5167\n",
      "Epoch 044: Loss = 0.7014, Val Acc = 0.6263, Micro F1 = 0.6263, Macro F1 = 0.5479\n",
      "Epoch 045: Loss = 0.6935, Val Acc = 0.6396, Micro F1 = 0.6396, Macro F1 = 0.5638\n",
      "Epoch 046: Loss = 0.6534, Val Acc = 0.6588, Micro F1 = 0.6588, Macro F1 = 0.5830\n",
      "Epoch 047: Loss = 0.6323, Val Acc = 0.6677, Micro F1 = 0.6677, Macro F1 = 0.5928\n",
      "Epoch 048: Loss = 0.6191, Val Acc = 0.6809, Micro F1 = 0.6809, Macro F1 = 0.6042\n",
      "Epoch 049: Loss = 0.5977, Val Acc = 0.6942, Micro F1 = 0.6942, Macro F1 = 0.6167\n",
      "Epoch 050: Loss = 0.5765, Val Acc = 0.7016, Micro F1 = 0.7016, Macro F1 = 0.6249\n",
      "Epoch 051: Loss = 0.5570, Val Acc = 0.7090, Micro F1 = 0.7090, Macro F1 = 0.6306\n",
      "Epoch 052: Loss = 0.5290, Val Acc = 0.7223, Micro F1 = 0.7223, Macro F1 = 0.6414\n",
      "Epoch 053: Loss = 0.5056, Val Acc = 0.7326, Micro F1 = 0.7326, Macro F1 = 0.6499\n",
      "Epoch 054: Loss = 0.5010, Val Acc = 0.7341, Micro F1 = 0.7341, Macro F1 = 0.6504\n",
      "Epoch 055: Loss = 0.4812, Val Acc = 0.7430, Micro F1 = 0.7430, Macro F1 = 0.6652\n",
      "Epoch 056: Loss = 0.4497, Val Acc = 0.7504, Micro F1 = 0.7504, Macro F1 = 0.6778\n",
      "Epoch 057: Loss = 0.4543, Val Acc = 0.7548, Micro F1 = 0.7548, Macro F1 = 0.6874\n",
      "Epoch 058: Loss = 0.4270, Val Acc = 0.7533, Micro F1 = 0.7533, Macro F1 = 0.6921\n",
      "Epoch 059: Loss = 0.4170, Val Acc = 0.7592, Micro F1 = 0.7592, Macro F1 = 0.6996\n",
      "Epoch 060: Loss = 0.4166, Val Acc = 0.7651, Micro F1 = 0.7651, Macro F1 = 0.7122\n",
      "Epoch 061: Loss = 0.3957, Val Acc = 0.7710, Micro F1 = 0.7710, Macro F1 = 0.7256\n",
      "Epoch 062: Loss = 0.3859, Val Acc = 0.7784, Micro F1 = 0.7784, Macro F1 = 0.7369\n",
      "Epoch 063: Loss = 0.3815, Val Acc = 0.7888, Micro F1 = 0.7888, Macro F1 = 0.7550\n",
      "Epoch 064: Loss = 0.3603, Val Acc = 0.7976, Micro F1 = 0.7976, Macro F1 = 0.7697\n",
      "Epoch 065: Loss = 0.3504, Val Acc = 0.8006, Micro F1 = 0.8006, Macro F1 = 0.7721\n",
      "Epoch 066: Loss = 0.3402, Val Acc = 0.8035, Micro F1 = 0.8035, Macro F1 = 0.7711\n",
      "Epoch 067: Loss = 0.3243, Val Acc = 0.8080, Micro F1 = 0.8080, Macro F1 = 0.7764\n",
      "Epoch 068: Loss = 0.3260, Val Acc = 0.8139, Micro F1 = 0.8139, Macro F1 = 0.7867\n",
      "Epoch 069: Loss = 0.3224, Val Acc = 0.8168, Micro F1 = 0.8168, Macro F1 = 0.7909\n",
      "Epoch 070: Loss = 0.2956, Val Acc = 0.8109, Micro F1 = 0.8109, Macro F1 = 0.7816\n",
      "Epoch 071: Loss = 0.2999, Val Acc = 0.8109, Micro F1 = 0.8109, Macro F1 = 0.7816\n",
      "Epoch 072: Loss = 0.2826, Val Acc = 0.8080, Micro F1 = 0.8080, Macro F1 = 0.7777\n",
      "Epoch 073: Loss = 0.2765, Val Acc = 0.8095, Micro F1 = 0.8095, Macro F1 = 0.7804\n",
      "Epoch 074: Loss = 0.2737, Val Acc = 0.8154, Micro F1 = 0.8154, Macro F1 = 0.7868\n",
      "Epoch 075: Loss = 0.2717, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8000\n",
      "Epoch 076: Loss = 0.2602, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8089\n",
      "Epoch 077: Loss = 0.2557, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8070\n",
      "Epoch 078: Loss = 0.2618, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8095\n",
      "Epoch 079: Loss = 0.2477, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8086\n",
      "Epoch 080: Loss = 0.2434, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8085\n",
      "Epoch 081: Loss = 0.2304, Val Acc = 0.8227, Micro F1 = 0.8227, Macro F1 = 0.8022\n",
      "Epoch 082: Loss = 0.2367, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8018\n",
      "Epoch 083: Loss = 0.2328, Val Acc = 0.8154, Micro F1 = 0.8154, Macro F1 = 0.7958\n",
      "Epoch 084: Loss = 0.2237, Val Acc = 0.8139, Micro F1 = 0.8139, Macro F1 = 0.7966\n",
      "Epoch 085: Loss = 0.2212, Val Acc = 0.8183, Micro F1 = 0.8183, Macro F1 = 0.8002\n",
      "Epoch 086: Loss = 0.2191, Val Acc = 0.8198, Micro F1 = 0.8198, Macro F1 = 0.8024\n",
      "Epoch 087: Loss = 0.2184, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8136\n",
      "Epoch 088: Loss = 0.2123, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8132\n",
      "Epoch 089: Loss = 0.1966, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8140\n",
      "Epoch 090: Loss = 0.2041, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8205\n",
      "Epoch 091: Loss = 0.2059, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8160\n",
      "Epoch 092: Loss = 0.2012, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8182\n",
      "Epoch 093: Loss = 0.1915, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8127\n",
      "Epoch 094: Loss = 0.1972, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8148\n",
      "Epoch 095: Loss = 0.2030, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8180\n",
      "Epoch 096: Loss = 0.1874, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8179\n",
      "Epoch 097: Loss = 0.1926, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8244\n",
      "Epoch 098: Loss = 0.1862, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8214\n",
      "Epoch 099: Loss = 0.1812, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8148\n",
      "Epoch 100: Loss = 0.1760, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8195\n",
      "Epoch 101: Loss = 0.1893, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8270\n",
      "Epoch 102: Loss = 0.1755, Val Acc = 0.8449, Micro F1 = 0.8449, Macro F1 = 0.8293\n",
      "Epoch 103: Loss = 0.1702, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8238\n",
      "Epoch 104: Loss = 0.1712, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8228\n",
      "Epoch 105: Loss = 0.1696, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8268\n",
      "Epoch 106: Loss = 0.1762, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8225\n",
      "Epoch 107: Loss = 0.1676, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8209\n",
      "Epoch 108: Loss = 0.1729, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8250\n",
      "Epoch 109: Loss = 0.1663, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8231\n",
      "Epoch 110: Loss = 0.1564, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8216\n",
      "Epoch 111: Loss = 0.1648, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8198\n",
      "Epoch 112: Loss = 0.1587, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8217\n",
      "Epoch 113: Loss = 0.1670, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8246\n",
      "Epoch 114: Loss = 0.1569, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8219\n",
      "Epoch 115: Loss = 0.1548, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8249\n",
      "Epoch 116: Loss = 0.1541, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8229\n",
      "Epoch 117: Loss = 0.1546, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8241\n",
      "Epoch 118: Loss = 0.1608, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8283\n",
      "Epoch 119: Loss = 0.1665, Val Acc = 0.8449, Micro F1 = 0.8449, Macro F1 = 0.8314\n",
      "Epoch 120: Loss = 0.1585, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8287\n",
      "Epoch 121: Loss = 0.1527, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8278\n",
      "Epoch 122: Loss = 0.1445, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8282\n",
      "Epoch 123: Loss = 0.1488, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8256\n",
      "Epoch 124: Loss = 0.1529, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8256\n",
      "Epoch 125: Loss = 0.1469, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8235\n",
      "Epoch 126: Loss = 0.1537, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8208\n",
      "Epoch 127: Loss = 0.1467, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8218\n",
      "Epoch 128: Loss = 0.1508, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8214\n",
      "Epoch 129: Loss = 0.1480, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8229\n",
      "Epoch 130: Loss = 0.1383, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8248\n",
      "Epoch 131: Loss = 0.1414, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8250\n",
      "Epoch 132: Loss = 0.1402, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8280\n",
      "Epoch 133: Loss = 0.1347, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8280\n",
      "Epoch 134: Loss = 0.1279, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8290\n",
      "Epoch 135: Loss = 0.1413, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8300\n",
      "Epoch 136: Loss = 0.1441, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8231\n",
      "Epoch 137: Loss = 0.1414, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8254\n",
      "Epoch 138: Loss = 0.1437, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8177\n",
      "Epoch 139: Loss = 0.1484, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8233\n",
      "Epoch 140: Loss = 0.1460, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8219\n",
      "Epoch 141: Loss = 0.1359, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8186\n",
      "Epoch 142: Loss = 0.1434, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8170\n",
      "Epoch 143: Loss = 0.1359, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8209\n",
      "Epoch 144: Loss = 0.1400, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8210\n",
      "Epoch 145: Loss = 0.1332, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8206\n",
      "Epoch 146: Loss = 0.1294, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8209\n",
      "Epoch 147: Loss = 0.1314, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8241\n",
      "Epoch 148: Loss = 0.1388, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8231\n",
      "Epoch 149: Loss = 0.1360, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8199\n",
      "Epoch 150: Loss = 0.1281, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8230\n",
      "Epoch 151: Loss = 0.1308, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8268\n",
      "Epoch 152: Loss = 0.1273, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8231\n",
      "Epoch 153: Loss = 0.1367, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8195\n",
      "Epoch 154: Loss = 0.1288, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8195\n",
      "Epoch 155: Loss = 0.1268, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8215\n",
      "Epoch 156: Loss = 0.1313, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8226\n",
      "Epoch 157: Loss = 0.1265, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8286\n",
      "Epoch 158: Loss = 0.1277, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8280\n",
      "Epoch 159: Loss = 0.1260, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8269\n",
      "Epoch 160: Loss = 0.1358, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8259\n",
      "Epoch 161: Loss = 0.1335, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8202\n",
      "Epoch 162: Loss = 0.1355, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8176\n",
      "Epoch 163: Loss = 0.1333, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8218\n",
      "Epoch 164: Loss = 0.1248, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8226\n",
      "Epoch 165: Loss = 0.1298, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8204\n",
      "Epoch 166: Loss = 0.1283, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8263\n",
      "Epoch 167: Loss = 0.1256, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8246\n",
      "Epoch 168: Loss = 0.1315, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8228\n",
      "Epoch 169: Loss = 0.1312, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8245\n",
      "Epoch 170: Loss = 0.1297, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8203\n",
      "Epoch 171: Loss = 0.1266, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8316\n",
      "Epoch 172: Loss = 0.1238, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8306\n",
      "Epoch 173: Loss = 0.1313, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8303\n",
      "Epoch 174: Loss = 0.1254, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8270\n",
      "Epoch 175: Loss = 0.1246, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8203\n",
      "Epoch 176: Loss = 0.1338, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8223\n",
      "Epoch 177: Loss = 0.1303, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8245\n",
      "Epoch 178: Loss = 0.1287, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8244\n",
      "Epoch 179: Loss = 0.1198, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8215\n",
      "Epoch 180: Loss = 0.1287, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8154\n",
      "Epoch 181: Loss = 0.1245, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8152\n",
      "Epoch 182: Loss = 0.1182, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8176\n",
      "Epoch 183: Loss = 0.1232, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8228\n",
      "Epoch 184: Loss = 0.1158, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8229\n",
      "Epoch 185: Loss = 0.1190, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8209\n",
      "Epoch 186: Loss = 0.1258, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8209\n",
      "Epoch 187: Loss = 0.1165, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8276\n",
      "Epoch 188: Loss = 0.1304, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8335\n",
      "Epoch 189: Loss = 0.1169, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8305\n",
      "Epoch 190: Loss = 0.1219, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8273\n",
      "Epoch 191: Loss = 0.1065, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8244\n",
      "Epoch 192: Loss = 0.1216, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8227\n",
      "Epoch 193: Loss = 0.1266, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8258\n",
      "Epoch 194: Loss = 0.1139, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8195\n",
      "Epoch 195: Loss = 0.1206, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8203\n",
      "Epoch 196: Loss = 0.1172, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8186\n",
      "Epoch 197: Loss = 0.1168, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8205\n",
      "Epoch 198: Loss = 0.1276, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8194\n",
      "Epoch 199: Loss = 0.1092, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8231\n",
      "Epoch 200: Loss = 0.1160, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8249\n",
      "Epoch 001: Loss = 1.9549, Val Acc = 0.2363, Micro F1 = 0.2363, Macro F1 = 0.0806\n",
      "Epoch 002: Loss = 1.9391, Val Acc = 0.3205, Micro F1 = 0.3205, Macro F1 = 0.1462\n",
      "Epoch 003: Loss = 1.9138, Val Acc = 0.2792, Micro F1 = 0.2792, Macro F1 = 0.1111\n",
      "Epoch 004: Loss = 1.8953, Val Acc = 0.2555, Micro F1 = 0.2555, Macro F1 = 0.0771\n",
      "Epoch 005: Loss = 1.8765, Val Acc = 0.2452, Micro F1 = 0.2452, Macro F1 = 0.0599\n",
      "Epoch 006: Loss = 1.8574, Val Acc = 0.2452, Micro F1 = 0.2452, Macro F1 = 0.0599\n",
      "Epoch 007: Loss = 1.8331, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 008: Loss = 1.8097, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 009: Loss = 1.7874, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 010: Loss = 1.7618, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 011: Loss = 1.7363, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 012: Loss = 1.7173, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 013: Loss = 1.6864, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 014: Loss = 1.6557, Val Acc = 0.2437, Micro F1 = 0.2437, Macro F1 = 0.0560\n",
      "Epoch 015: Loss = 1.6317, Val Acc = 0.2452, Micro F1 = 0.2452, Macro F1 = 0.0599\n",
      "Epoch 016: Loss = 1.5974, Val Acc = 0.2452, Micro F1 = 0.2452, Macro F1 = 0.0599\n",
      "Epoch 017: Loss = 1.5812, Val Acc = 0.2452, Micro F1 = 0.2452, Macro F1 = 0.0599\n",
      "Epoch 018: Loss = 1.5490, Val Acc = 0.2511, Micro F1 = 0.2511, Macro F1 = 0.0744\n",
      "Epoch 019: Loss = 1.5295, Val Acc = 0.2600, Micro F1 = 0.2600, Macro F1 = 0.0930\n",
      "Epoch 020: Loss = 1.4931, Val Acc = 0.2733, Micro F1 = 0.2733, Macro F1 = 0.1165\n",
      "Epoch 021: Loss = 1.4649, Val Acc = 0.2792, Micro F1 = 0.2792, Macro F1 = 0.1251\n",
      "Epoch 022: Loss = 1.4334, Val Acc = 0.2895, Micro F1 = 0.2895, Macro F1 = 0.1415\n",
      "Epoch 023: Loss = 1.4060, Val Acc = 0.3087, Micro F1 = 0.3087, Macro F1 = 0.1716\n",
      "Epoch 024: Loss = 1.3828, Val Acc = 0.3176, Micro F1 = 0.3176, Macro F1 = 0.1894\n",
      "Epoch 025: Loss = 1.3351, Val Acc = 0.3323, Micro F1 = 0.3323, Macro F1 = 0.2176\n",
      "Epoch 026: Loss = 1.3039, Val Acc = 0.3456, Micro F1 = 0.3456, Macro F1 = 0.2364\n",
      "Epoch 027: Loss = 1.2656, Val Acc = 0.3516, Micro F1 = 0.3516, Macro F1 = 0.2449\n",
      "Epoch 028: Loss = 1.2354, Val Acc = 0.3589, Micro F1 = 0.3589, Macro F1 = 0.2560\n",
      "Epoch 029: Loss = 1.2073, Val Acc = 0.3575, Micro F1 = 0.3575, Macro F1 = 0.2573\n",
      "Epoch 030: Loss = 1.1680, Val Acc = 0.3634, Micro F1 = 0.3634, Macro F1 = 0.2670\n",
      "Epoch 031: Loss = 1.1279, Val Acc = 0.3722, Micro F1 = 0.3722, Macro F1 = 0.2797\n",
      "Epoch 032: Loss = 1.0856, Val Acc = 0.3781, Micro F1 = 0.3781, Macro F1 = 0.2875\n",
      "Epoch 033: Loss = 1.0521, Val Acc = 0.3885, Micro F1 = 0.3885, Macro F1 = 0.2991\n",
      "Epoch 034: Loss = 1.0201, Val Acc = 0.4106, Micro F1 = 0.4106, Macro F1 = 0.3389\n",
      "Epoch 035: Loss = 0.9769, Val Acc = 0.4269, Micro F1 = 0.4269, Macro F1 = 0.3602\n",
      "Epoch 036: Loss = 0.9407, Val Acc = 0.4609, Micro F1 = 0.4609, Macro F1 = 0.3991\n",
      "Epoch 037: Loss = 0.9124, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.4294\n",
      "Epoch 038: Loss = 0.8752, Val Acc = 0.5155, Micro F1 = 0.5155, Macro F1 = 0.4484\n",
      "Epoch 039: Loss = 0.8501, Val Acc = 0.5288, Micro F1 = 0.5288, Macro F1 = 0.4585\n",
      "Epoch 040: Loss = 0.8118, Val Acc = 0.5406, Micro F1 = 0.5406, Macro F1 = 0.4683\n",
      "Epoch 041: Loss = 0.7953, Val Acc = 0.5480, Micro F1 = 0.5480, Macro F1 = 0.4722\n",
      "Epoch 042: Loss = 0.7620, Val Acc = 0.5524, Micro F1 = 0.5524, Macro F1 = 0.4761\n",
      "Epoch 043: Loss = 0.7348, Val Acc = 0.5524, Micro F1 = 0.5524, Macro F1 = 0.4783\n",
      "Epoch 044: Loss = 0.7067, Val Acc = 0.5554, Micro F1 = 0.5554, Macro F1 = 0.4864\n",
      "Epoch 045: Loss = 0.6781, Val Acc = 0.5702, Micro F1 = 0.5702, Macro F1 = 0.5034\n",
      "Epoch 046: Loss = 0.6553, Val Acc = 0.5864, Micro F1 = 0.5864, Macro F1 = 0.5241\n",
      "Epoch 047: Loss = 0.6306, Val Acc = 0.6130, Micro F1 = 0.6130, Macro F1 = 0.5545\n",
      "Epoch 048: Loss = 0.6200, Val Acc = 0.6455, Micro F1 = 0.6455, Macro F1 = 0.5889\n",
      "Epoch 049: Loss = 0.5953, Val Acc = 0.6691, Micro F1 = 0.6691, Macro F1 = 0.6071\n",
      "Epoch 050: Loss = 0.5883, Val Acc = 0.6839, Micro F1 = 0.6839, Macro F1 = 0.6200\n",
      "Epoch 051: Loss = 0.5587, Val Acc = 0.6972, Micro F1 = 0.6972, Macro F1 = 0.6326\n",
      "Epoch 052: Loss = 0.5362, Val Acc = 0.7075, Micro F1 = 0.7075, Macro F1 = 0.6417\n",
      "Epoch 053: Loss = 0.5176, Val Acc = 0.7179, Micro F1 = 0.7179, Macro F1 = 0.6485\n",
      "Epoch 054: Loss = 0.4995, Val Acc = 0.7253, Micro F1 = 0.7253, Macro F1 = 0.6521\n",
      "Epoch 055: Loss = 0.4852, Val Acc = 0.7326, Micro F1 = 0.7326, Macro F1 = 0.6568\n",
      "Epoch 056: Loss = 0.4695, Val Acc = 0.7386, Micro F1 = 0.7386, Macro F1 = 0.6596\n",
      "Epoch 057: Loss = 0.4559, Val Acc = 0.7430, Micro F1 = 0.7430, Macro F1 = 0.6621\n",
      "Epoch 058: Loss = 0.4409, Val Acc = 0.7474, Micro F1 = 0.7474, Macro F1 = 0.6698\n",
      "Epoch 059: Loss = 0.4359, Val Acc = 0.7563, Micro F1 = 0.7563, Macro F1 = 0.6847\n",
      "Epoch 060: Loss = 0.4244, Val Acc = 0.7637, Micro F1 = 0.7637, Macro F1 = 0.7076\n",
      "Epoch 061: Loss = 0.4048, Val Acc = 0.7725, Micro F1 = 0.7725, Macro F1 = 0.7263\n",
      "Epoch 062: Loss = 0.3919, Val Acc = 0.7770, Micro F1 = 0.7770, Macro F1 = 0.7363\n",
      "Epoch 063: Loss = 0.3804, Val Acc = 0.7799, Micro F1 = 0.7799, Macro F1 = 0.7424\n",
      "Epoch 064: Loss = 0.3849, Val Acc = 0.7843, Micro F1 = 0.7843, Macro F1 = 0.7503\n",
      "Epoch 065: Loss = 0.3592, Val Acc = 0.7829, Micro F1 = 0.7829, Macro F1 = 0.7445\n",
      "Epoch 066: Loss = 0.3663, Val Acc = 0.7829, Micro F1 = 0.7829, Macro F1 = 0.7434\n",
      "Epoch 067: Loss = 0.3322, Val Acc = 0.7873, Micro F1 = 0.7873, Macro F1 = 0.7498\n",
      "Epoch 068: Loss = 0.3309, Val Acc = 0.7903, Micro F1 = 0.7903, Macro F1 = 0.7541\n",
      "Epoch 069: Loss = 0.3207, Val Acc = 0.7976, Micro F1 = 0.7976, Macro F1 = 0.7645\n",
      "Epoch 070: Loss = 0.3200, Val Acc = 0.8035, Micro F1 = 0.8035, Macro F1 = 0.7711\n",
      "Epoch 071: Loss = 0.3161, Val Acc = 0.8035, Micro F1 = 0.8035, Macro F1 = 0.7713\n",
      "Epoch 072: Loss = 0.3084, Val Acc = 0.8050, Micro F1 = 0.8050, Macro F1 = 0.7723\n",
      "Epoch 073: Loss = 0.2904, Val Acc = 0.8124, Micro F1 = 0.8124, Macro F1 = 0.7867\n",
      "Epoch 074: Loss = 0.2895, Val Acc = 0.8154, Micro F1 = 0.8154, Macro F1 = 0.7931\n",
      "Epoch 075: Loss = 0.2823, Val Acc = 0.8168, Micro F1 = 0.8168, Macro F1 = 0.7953\n",
      "Epoch 076: Loss = 0.2840, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8020\n",
      "Epoch 077: Loss = 0.2769, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8024\n",
      "Epoch 078: Loss = 0.2694, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8046\n",
      "Epoch 079: Loss = 0.2596, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8034\n",
      "Epoch 080: Loss = 0.2639, Val Acc = 0.8227, Micro F1 = 0.8227, Macro F1 = 0.8026\n",
      "Epoch 081: Loss = 0.2625, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8116\n",
      "Epoch 082: Loss = 0.2466, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8098\n",
      "Epoch 083: Loss = 0.2347, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8021\n",
      "Epoch 084: Loss = 0.2331, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8004\n",
      "Epoch 085: Loss = 0.2368, Val Acc = 0.8227, Micro F1 = 0.8227, Macro F1 = 0.7988\n",
      "Epoch 086: Loss = 0.2283, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8102\n",
      "Epoch 087: Loss = 0.2230, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8109\n",
      "Epoch 088: Loss = 0.2262, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8158\n",
      "Epoch 089: Loss = 0.2136, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8161\n",
      "Epoch 090: Loss = 0.2110, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8202\n",
      "Epoch 091: Loss = 0.2032, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8221\n",
      "Epoch 092: Loss = 0.2123, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8224\n",
      "Epoch 093: Loss = 0.2020, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8280\n",
      "Epoch 094: Loss = 0.1989, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8284\n",
      "Epoch 095: Loss = 0.1983, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8264\n",
      "Epoch 096: Loss = 0.1989, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8210\n",
      "Epoch 097: Loss = 0.1954, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8274\n",
      "Epoch 098: Loss = 0.1836, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8198\n",
      "Epoch 099: Loss = 0.1828, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8181\n",
      "Epoch 100: Loss = 0.1791, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8166\n",
      "Epoch 101: Loss = 0.1856, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8206\n",
      "Epoch 102: Loss = 0.1839, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8184\n",
      "Epoch 103: Loss = 0.1814, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8199\n",
      "Epoch 104: Loss = 0.1817, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8224\n",
      "Epoch 105: Loss = 0.1790, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8219\n",
      "Epoch 106: Loss = 0.1724, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8277\n",
      "Epoch 107: Loss = 0.1788, Val Acc = 0.8479, Micro F1 = 0.8479, Macro F1 = 0.8332\n",
      "Epoch 108: Loss = 0.1757, Val Acc = 0.8449, Micro F1 = 0.8449, Macro F1 = 0.8296\n",
      "Epoch 109: Loss = 0.1749, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8239\n",
      "Epoch 110: Loss = 0.1676, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8232\n",
      "Epoch 111: Loss = 0.1644, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8227\n",
      "Epoch 112: Loss = 0.1680, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8267\n",
      "Epoch 113: Loss = 0.1662, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8282\n",
      "Epoch 114: Loss = 0.1595, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8293\n",
      "Epoch 115: Loss = 0.1601, Val Acc = 0.8479, Micro F1 = 0.8479, Macro F1 = 0.8347\n",
      "Epoch 116: Loss = 0.1712, Val Acc = 0.8464, Micro F1 = 0.8464, Macro F1 = 0.8341\n",
      "Epoch 117: Loss = 0.1538, Val Acc = 0.8464, Micro F1 = 0.8464, Macro F1 = 0.8340\n",
      "Epoch 118: Loss = 0.1608, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8298\n",
      "Epoch 119: Loss = 0.1501, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8269\n",
      "Epoch 120: Loss = 0.1629, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8261\n",
      "Epoch 121: Loss = 0.1583, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8188\n",
      "Epoch 122: Loss = 0.1559, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8188\n",
      "Epoch 123: Loss = 0.1589, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8200\n",
      "Epoch 124: Loss = 0.1490, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8268\n",
      "Epoch 125: Loss = 0.1524, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8278\n",
      "Epoch 126: Loss = 0.1516, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8289\n",
      "Epoch 127: Loss = 0.1505, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8285\n",
      "Epoch 128: Loss = 0.1565, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8309\n",
      "Epoch 129: Loss = 0.1443, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8296\n",
      "Epoch 130: Loss = 0.1444, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8259\n",
      "Epoch 131: Loss = 0.1473, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8256\n",
      "Epoch 132: Loss = 0.1481, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8246\n",
      "Epoch 133: Loss = 0.1493, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8260\n",
      "Epoch 134: Loss = 0.1530, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8267\n",
      "Epoch 135: Loss = 0.1465, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8253\n",
      "Epoch 136: Loss = 0.1349, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8264\n",
      "Epoch 137: Loss = 0.1429, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8284\n",
      "Epoch 138: Loss = 0.1410, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8241\n",
      "Epoch 139: Loss = 0.1389, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8262\n",
      "Epoch 140: Loss = 0.1430, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8267\n",
      "Epoch 141: Loss = 0.1484, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8291\n",
      "Epoch 142: Loss = 0.1303, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8295\n",
      "Epoch 143: Loss = 0.1446, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8261\n",
      "Epoch 144: Loss = 0.1350, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8251\n",
      "Epoch 145: Loss = 0.1349, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8264\n",
      "Epoch 146: Loss = 0.1383, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8310\n",
      "Epoch 147: Loss = 0.1396, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8330\n",
      "Epoch 148: Loss = 0.1324, Val Acc = 0.8449, Micro F1 = 0.8449, Macro F1 = 0.8342\n",
      "Epoch 149: Loss = 0.1380, Val Acc = 0.8464, Micro F1 = 0.8464, Macro F1 = 0.8357\n",
      "Epoch 150: Loss = 0.1375, Val Acc = 0.8449, Micro F1 = 0.8449, Macro F1 = 0.8332\n",
      "Epoch 151: Loss = 0.1443, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8268\n",
      "Epoch 152: Loss = 0.1262, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8236\n",
      "Epoch 153: Loss = 0.1348, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8210\n",
      "Epoch 154: Loss = 0.1339, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8242\n",
      "Epoch 155: Loss = 0.1288, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8254\n",
      "Epoch 156: Loss = 0.1368, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8282\n",
      "Epoch 157: Loss = 0.1367, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8271\n",
      "Epoch 158: Loss = 0.1244, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8317\n",
      "Epoch 159: Loss = 0.1340, Val Acc = 0.8479, Micro F1 = 0.8479, Macro F1 = 0.8370\n",
      "Epoch 160: Loss = 0.1236, Val Acc = 0.8479, Micro F1 = 0.8479, Macro F1 = 0.8384\n",
      "Epoch 161: Loss = 0.1294, Val Acc = 0.8479, Micro F1 = 0.8479, Macro F1 = 0.8390\n",
      "Epoch 162: Loss = 0.1273, Val Acc = 0.8479, Micro F1 = 0.8479, Macro F1 = 0.8386\n",
      "Epoch 163: Loss = 0.1306, Val Acc = 0.8449, Micro F1 = 0.8449, Macro F1 = 0.8359\n",
      "Epoch 164: Loss = 0.1293, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8341\n",
      "Epoch 165: Loss = 0.1286, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8335\n",
      "Epoch 166: Loss = 0.1136, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8313\n",
      "Epoch 167: Loss = 0.1308, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8305\n",
      "Epoch 168: Loss = 0.1255, Val Acc = 0.8449, Micro F1 = 0.8449, Macro F1 = 0.8346\n",
      "Epoch 169: Loss = 0.1233, Val Acc = 0.8479, Micro F1 = 0.8479, Macro F1 = 0.8376\n",
      "Epoch 170: Loss = 0.1299, Val Acc = 0.8479, Micro F1 = 0.8479, Macro F1 = 0.8371\n",
      "Epoch 171: Loss = 0.1237, Val Acc = 0.8464, Micro F1 = 0.8464, Macro F1 = 0.8366\n",
      "Epoch 172: Loss = 0.1281, Val Acc = 0.8508, Micro F1 = 0.8508, Macro F1 = 0.8419\n",
      "Epoch 173: Loss = 0.1190, Val Acc = 0.8523, Micro F1 = 0.8523, Macro F1 = 0.8431\n",
      "Epoch 174: Loss = 0.1210, Val Acc = 0.8508, Micro F1 = 0.8508, Macro F1 = 0.8419\n",
      "Epoch 175: Loss = 0.1323, Val Acc = 0.8493, Micro F1 = 0.8493, Macro F1 = 0.8388\n",
      "Epoch 176: Loss = 0.1261, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8291\n",
      "Epoch 177: Loss = 0.1274, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8267\n",
      "Epoch 178: Loss = 0.1195, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8236\n",
      "Epoch 179: Loss = 0.1274, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8235\n",
      "Epoch 180: Loss = 0.1215, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8231\n",
      "Epoch 181: Loss = 0.1395, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8269\n",
      "Epoch 182: Loss = 0.1232, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8190\n",
      "Epoch 183: Loss = 0.1355, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8280\n",
      "Epoch 184: Loss = 0.1167, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8327\n",
      "Epoch 185: Loss = 0.1223, Val Acc = 0.8479, Micro F1 = 0.8479, Macro F1 = 0.8383\n",
      "Epoch 186: Loss = 0.1226, Val Acc = 0.8493, Micro F1 = 0.8493, Macro F1 = 0.8396\n",
      "Epoch 187: Loss = 0.1161, Val Acc = 0.8493, Micro F1 = 0.8493, Macro F1 = 0.8401\n",
      "Epoch 188: Loss = 0.1238, Val Acc = 0.8464, Micro F1 = 0.8464, Macro F1 = 0.8359\n",
      "Epoch 189: Loss = 0.1194, Val Acc = 0.8464, Micro F1 = 0.8464, Macro F1 = 0.8343\n",
      "Epoch 190: Loss = 0.1261, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8270\n",
      "Epoch 191: Loss = 0.1174, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8291\n",
      "Epoch 192: Loss = 0.1159, Val Acc = 0.8419, Micro F1 = 0.8419, Macro F1 = 0.8292\n",
      "Epoch 193: Loss = 0.1300, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8268\n",
      "Epoch 194: Loss = 0.1197, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8269\n",
      "Epoch 195: Loss = 0.1234, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8269\n",
      "Epoch 196: Loss = 0.1260, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8254\n",
      "Epoch 197: Loss = 0.1276, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8229\n",
      "Epoch 198: Loss = 0.1213, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8303\n",
      "Epoch 199: Loss = 0.1207, Val Acc = 0.8479, Micro F1 = 0.8479, Macro F1 = 0.8381\n",
      "Epoch 200: Loss = 0.1188, Val Acc = 0.8434, Micro F1 = 0.8434, Macro F1 = 0.8330\n",
      "Epoch 001: Loss = 1.9576, Val Acc = 0.1004, Micro F1 = 0.1004, Macro F1 = 0.0444\n",
      "Epoch 002: Loss = 1.9362, Val Acc = 0.1595, Micro F1 = 0.1595, Macro F1 = 0.0727\n",
      "Epoch 003: Loss = 1.9160, Val Acc = 0.2260, Micro F1 = 0.2260, Macro F1 = 0.0939\n",
      "Epoch 004: Loss = 1.8959, Val Acc = 0.2585, Micro F1 = 0.2585, Macro F1 = 0.0989\n",
      "Epoch 005: Loss = 1.8800, Val Acc = 0.2718, Micro F1 = 0.2718, Macro F1 = 0.1043\n",
      "Epoch 006: Loss = 1.8595, Val Acc = 0.2718, Micro F1 = 0.2718, Macro F1 = 0.1057\n",
      "Epoch 007: Loss = 1.8432, Val Acc = 0.2777, Micro F1 = 0.2777, Macro F1 = 0.1096\n",
      "Epoch 008: Loss = 1.8240, Val Acc = 0.2777, Micro F1 = 0.2777, Macro F1 = 0.1098\n",
      "Epoch 009: Loss = 1.8001, Val Acc = 0.2821, Micro F1 = 0.2821, Macro F1 = 0.1112\n",
      "Epoch 010: Loss = 1.7803, Val Acc = 0.2851, Micro F1 = 0.2851, Macro F1 = 0.1106\n",
      "Epoch 011: Loss = 1.7526, Val Acc = 0.2939, Micro F1 = 0.2939, Macro F1 = 0.1185\n",
      "Epoch 012: Loss = 1.7261, Val Acc = 0.3043, Micro F1 = 0.3043, Macro F1 = 0.1258\n",
      "Epoch 013: Loss = 1.6955, Val Acc = 0.3028, Micro F1 = 0.3028, Macro F1 = 0.1238\n",
      "Epoch 014: Loss = 1.6685, Val Acc = 0.3102, Micro F1 = 0.3102, Macro F1 = 0.1266\n",
      "Epoch 015: Loss = 1.6416, Val Acc = 0.3161, Micro F1 = 0.3161, Macro F1 = 0.1293\n",
      "Epoch 016: Loss = 1.6122, Val Acc = 0.3191, Micro F1 = 0.3191, Macro F1 = 0.1299\n",
      "Epoch 017: Loss = 1.5857, Val Acc = 0.3191, Micro F1 = 0.3191, Macro F1 = 0.1300\n",
      "Epoch 018: Loss = 1.5575, Val Acc = 0.3191, Micro F1 = 0.3191, Macro F1 = 0.1301\n",
      "Epoch 019: Loss = 1.5239, Val Acc = 0.3235, Micro F1 = 0.3235, Macro F1 = 0.1317\n",
      "Epoch 020: Loss = 1.4991, Val Acc = 0.3220, Micro F1 = 0.3220, Macro F1 = 0.1310\n",
      "Epoch 021: Loss = 1.4717, Val Acc = 0.3205, Micro F1 = 0.3205, Macro F1 = 0.1305\n",
      "Epoch 022: Loss = 1.4285, Val Acc = 0.3205, Micro F1 = 0.3205, Macro F1 = 0.1309\n",
      "Epoch 023: Loss = 1.4051, Val Acc = 0.3220, Micro F1 = 0.3220, Macro F1 = 0.1362\n",
      "Epoch 024: Loss = 1.3794, Val Acc = 0.3309, Micro F1 = 0.3309, Macro F1 = 0.1617\n",
      "Epoch 025: Loss = 1.3416, Val Acc = 0.3383, Micro F1 = 0.3383, Macro F1 = 0.1800\n",
      "Epoch 026: Loss = 1.3152, Val Acc = 0.3501, Micro F1 = 0.3501, Macro F1 = 0.2026\n",
      "Epoch 027: Loss = 1.2679, Val Acc = 0.3604, Micro F1 = 0.3604, Macro F1 = 0.2216\n",
      "Epoch 028: Loss = 1.2353, Val Acc = 0.3663, Micro F1 = 0.3663, Macro F1 = 0.2309\n",
      "Epoch 029: Loss = 1.2011, Val Acc = 0.3693, Micro F1 = 0.3693, Macro F1 = 0.2364\n",
      "Epoch 030: Loss = 1.1693, Val Acc = 0.3796, Micro F1 = 0.3796, Macro F1 = 0.2438\n",
      "Epoch 031: Loss = 1.1365, Val Acc = 0.3826, Micro F1 = 0.3826, Macro F1 = 0.2418\n",
      "Epoch 032: Loss = 1.1033, Val Acc = 0.3900, Micro F1 = 0.3900, Macro F1 = 0.2631\n",
      "Epoch 033: Loss = 1.0679, Val Acc = 0.4151, Micro F1 = 0.4151, Macro F1 = 0.3083\n",
      "Epoch 034: Loss = 1.0180, Val Acc = 0.4343, Micro F1 = 0.4343, Macro F1 = 0.3362\n",
      "Epoch 035: Loss = 0.9900, Val Acc = 0.4623, Micro F1 = 0.4623, Macro F1 = 0.3792\n",
      "Epoch 036: Loss = 0.9585, Val Acc = 0.4860, Micro F1 = 0.4860, Macro F1 = 0.4104\n",
      "Epoch 037: Loss = 0.9242, Val Acc = 0.4978, Micro F1 = 0.4978, Macro F1 = 0.4222\n",
      "Epoch 038: Loss = 0.8990, Val Acc = 0.5199, Micro F1 = 0.5199, Macro F1 = 0.4405\n",
      "Epoch 039: Loss = 0.8612, Val Acc = 0.5377, Micro F1 = 0.5377, Macro F1 = 0.4576\n",
      "Epoch 040: Loss = 0.8189, Val Acc = 0.5569, Micro F1 = 0.5569, Macro F1 = 0.4712\n",
      "Epoch 041: Loss = 0.8022, Val Acc = 0.5849, Micro F1 = 0.5849, Macro F1 = 0.5050\n",
      "Epoch 042: Loss = 0.7573, Val Acc = 0.6322, Micro F1 = 0.6322, Macro F1 = 0.5627\n",
      "Epoch 043: Loss = 0.7459, Val Acc = 0.6588, Micro F1 = 0.6588, Macro F1 = 0.5905\n",
      "Epoch 044: Loss = 0.7132, Val Acc = 0.6706, Micro F1 = 0.6706, Macro F1 = 0.6004\n",
      "Epoch 045: Loss = 0.6888, Val Acc = 0.6883, Micro F1 = 0.6883, Macro F1 = 0.6160\n",
      "Epoch 046: Loss = 0.6711, Val Acc = 0.7046, Micro F1 = 0.7046, Macro F1 = 0.6294\n",
      "Epoch 047: Loss = 0.6391, Val Acc = 0.7149, Micro F1 = 0.7149, Macro F1 = 0.6375\n",
      "Epoch 048: Loss = 0.6203, Val Acc = 0.7253, Micro F1 = 0.7253, Macro F1 = 0.6447\n",
      "Epoch 049: Loss = 0.5994, Val Acc = 0.7297, Micro F1 = 0.7297, Macro F1 = 0.6459\n",
      "Epoch 050: Loss = 0.5746, Val Acc = 0.7386, Micro F1 = 0.7386, Macro F1 = 0.6550\n",
      "Epoch 051: Loss = 0.5503, Val Acc = 0.7459, Micro F1 = 0.7459, Macro F1 = 0.6611\n",
      "Epoch 052: Loss = 0.5335, Val Acc = 0.7474, Micro F1 = 0.7474, Macro F1 = 0.6628\n",
      "Epoch 053: Loss = 0.5044, Val Acc = 0.7518, Micro F1 = 0.7518, Macro F1 = 0.6666\n",
      "Epoch 054: Loss = 0.4978, Val Acc = 0.7533, Micro F1 = 0.7533, Macro F1 = 0.6681\n",
      "Epoch 055: Loss = 0.4771, Val Acc = 0.7563, Micro F1 = 0.7563, Macro F1 = 0.6754\n",
      "Epoch 056: Loss = 0.4717, Val Acc = 0.7563, Micro F1 = 0.7563, Macro F1 = 0.6738\n",
      "Epoch 057: Loss = 0.4528, Val Acc = 0.7533, Micro F1 = 0.7533, Macro F1 = 0.6705\n",
      "Epoch 058: Loss = 0.4375, Val Acc = 0.7637, Micro F1 = 0.7637, Macro F1 = 0.6775\n",
      "Epoch 059: Loss = 0.4387, Val Acc = 0.7637, Micro F1 = 0.7637, Macro F1 = 0.6780\n",
      "Epoch 060: Loss = 0.4166, Val Acc = 0.7637, Micro F1 = 0.7637, Macro F1 = 0.6820\n",
      "Epoch 061: Loss = 0.3976, Val Acc = 0.7696, Micro F1 = 0.7696, Macro F1 = 0.6890\n",
      "Epoch 062: Loss = 0.3744, Val Acc = 0.7784, Micro F1 = 0.7784, Macro F1 = 0.7003\n",
      "Epoch 063: Loss = 0.3727, Val Acc = 0.7843, Micro F1 = 0.7843, Macro F1 = 0.7126\n",
      "Epoch 064: Loss = 0.3540, Val Acc = 0.7858, Micro F1 = 0.7858, Macro F1 = 0.7208\n",
      "Epoch 065: Loss = 0.3553, Val Acc = 0.7903, Micro F1 = 0.7903, Macro F1 = 0.7306\n",
      "Epoch 066: Loss = 0.3410, Val Acc = 0.7917, Micro F1 = 0.7917, Macro F1 = 0.7331\n",
      "Epoch 067: Loss = 0.3324, Val Acc = 0.7917, Micro F1 = 0.7917, Macro F1 = 0.7372\n",
      "Epoch 068: Loss = 0.3241, Val Acc = 0.7932, Micro F1 = 0.7932, Macro F1 = 0.7424\n",
      "Epoch 069: Loss = 0.3161, Val Acc = 0.7903, Micro F1 = 0.7903, Macro F1 = 0.7418\n",
      "Epoch 070: Loss = 0.3050, Val Acc = 0.7932, Micro F1 = 0.7932, Macro F1 = 0.7433\n",
      "Epoch 071: Loss = 0.2973, Val Acc = 0.7947, Micro F1 = 0.7947, Macro F1 = 0.7462\n",
      "Epoch 072: Loss = 0.2967, Val Acc = 0.7917, Micro F1 = 0.7917, Macro F1 = 0.7462\n",
      "Epoch 073: Loss = 0.2909, Val Acc = 0.7947, Micro F1 = 0.7947, Macro F1 = 0.7499\n",
      "Epoch 074: Loss = 0.2837, Val Acc = 0.8035, Micro F1 = 0.8035, Macro F1 = 0.7631\n",
      "Epoch 075: Loss = 0.2746, Val Acc = 0.8065, Micro F1 = 0.8065, Macro F1 = 0.7653\n",
      "Epoch 076: Loss = 0.2730, Val Acc = 0.8050, Micro F1 = 0.8050, Macro F1 = 0.7678\n",
      "Epoch 077: Loss = 0.2611, Val Acc = 0.8065, Micro F1 = 0.8065, Macro F1 = 0.7686\n",
      "Epoch 078: Loss = 0.2639, Val Acc = 0.8124, Micro F1 = 0.8124, Macro F1 = 0.7799\n",
      "Epoch 079: Loss = 0.2577, Val Acc = 0.8198, Micro F1 = 0.8198, Macro F1 = 0.7883\n",
      "Epoch 080: Loss = 0.2528, Val Acc = 0.8198, Micro F1 = 0.8198, Macro F1 = 0.7886\n",
      "Epoch 081: Loss = 0.2475, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.7909\n",
      "Epoch 082: Loss = 0.2384, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.7916\n",
      "Epoch 083: Loss = 0.2385, Val Acc = 0.8198, Micro F1 = 0.8198, Macro F1 = 0.7904\n",
      "Epoch 084: Loss = 0.2350, Val Acc = 0.8227, Micro F1 = 0.8227, Macro F1 = 0.7958\n",
      "Epoch 085: Loss = 0.2284, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.7994\n",
      "Epoch 086: Loss = 0.2327, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.7973\n",
      "Epoch 087: Loss = 0.2244, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.7971\n",
      "Epoch 088: Loss = 0.2183, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.7936\n",
      "Epoch 089: Loss = 0.2174, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.7944\n",
      "Epoch 090: Loss = 0.2130, Val Acc = 0.8198, Micro F1 = 0.8198, Macro F1 = 0.7940\n",
      "Epoch 091: Loss = 0.2115, Val Acc = 0.8168, Micro F1 = 0.8168, Macro F1 = 0.7910\n",
      "Epoch 092: Loss = 0.2051, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.7938\n",
      "Epoch 093: Loss = 0.2177, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.7980\n",
      "Epoch 094: Loss = 0.1992, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8002\n",
      "Epoch 095: Loss = 0.1990, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8016\n",
      "Epoch 096: Loss = 0.1883, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8065\n",
      "Epoch 097: Loss = 0.2001, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8137\n",
      "Epoch 098: Loss = 0.1905, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8238\n",
      "Epoch 099: Loss = 0.1955, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8166\n",
      "Epoch 100: Loss = 0.1875, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8124\n",
      "Epoch 101: Loss = 0.1725, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8119\n",
      "Epoch 102: Loss = 0.1867, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8093\n",
      "Epoch 103: Loss = 0.1862, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8174\n",
      "Epoch 104: Loss = 0.1774, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8136\n",
      "Epoch 105: Loss = 0.1833, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8143\n",
      "Epoch 106: Loss = 0.1738, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8206\n",
      "Epoch 107: Loss = 0.1742, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8211\n",
      "Epoch 108: Loss = 0.1742, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8190\n",
      "Epoch 109: Loss = 0.1701, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8164\n",
      "Epoch 110: Loss = 0.1768, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8165\n",
      "Epoch 111: Loss = 0.1694, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8104\n",
      "Epoch 112: Loss = 0.1744, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8071\n",
      "Epoch 113: Loss = 0.1628, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8082\n",
      "Epoch 114: Loss = 0.1725, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8148\n",
      "Epoch 115: Loss = 0.1618, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8145\n",
      "Epoch 116: Loss = 0.1591, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8153\n",
      "Epoch 117: Loss = 0.1502, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8113\n",
      "Epoch 118: Loss = 0.1587, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8109\n",
      "Epoch 119: Loss = 0.1569, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8086\n",
      "Epoch 120: Loss = 0.1605, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8055\n",
      "Epoch 121: Loss = 0.1476, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8088\n",
      "Epoch 122: Loss = 0.1518, Val Acc = 0.8227, Micro F1 = 0.8227, Macro F1 = 0.8068\n",
      "Epoch 123: Loss = 0.1580, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8113\n",
      "Epoch 124: Loss = 0.1556, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8095\n",
      "Epoch 125: Loss = 0.1584, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8127\n",
      "Epoch 126: Loss = 0.1511, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8185\n",
      "Epoch 127: Loss = 0.1510, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8191\n",
      "Epoch 128: Loss = 0.1375, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8120\n",
      "Epoch 129: Loss = 0.1522, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8126\n",
      "Epoch 130: Loss = 0.1535, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8178\n",
      "Epoch 131: Loss = 0.1487, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8173\n",
      "Epoch 132: Loss = 0.1403, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8189\n",
      "Epoch 133: Loss = 0.1506, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8189\n",
      "Epoch 134: Loss = 0.1491, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8154\n",
      "Epoch 135: Loss = 0.1502, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8199\n",
      "Epoch 136: Loss = 0.1423, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8169\n",
      "Epoch 137: Loss = 0.1434, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8185\n",
      "Epoch 138: Loss = 0.1434, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8188\n",
      "Epoch 139: Loss = 0.1451, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8190\n",
      "Epoch 140: Loss = 0.1384, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8135\n",
      "Epoch 141: Loss = 0.1477, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8196\n",
      "Epoch 142: Loss = 0.1359, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8163\n",
      "Epoch 143: Loss = 0.1475, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8133\n",
      "Epoch 144: Loss = 0.1312, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8139\n",
      "Epoch 145: Loss = 0.1370, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8166\n",
      "Epoch 146: Loss = 0.1304, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8176\n",
      "Epoch 147: Loss = 0.1291, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8089\n",
      "Epoch 148: Loss = 0.1304, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8105\n",
      "Epoch 149: Loss = 0.1369, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8089\n",
      "Epoch 150: Loss = 0.1300, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8165\n",
      "Epoch 151: Loss = 0.1356, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8166\n",
      "Epoch 152: Loss = 0.1379, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8173\n",
      "Epoch 153: Loss = 0.1310, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8197\n",
      "Epoch 154: Loss = 0.1341, Val Acc = 0.8316, Micro F1 = 0.8316, Macro F1 = 0.8190\n",
      "Epoch 155: Loss = 0.1345, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8109\n",
      "Epoch 156: Loss = 0.1337, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8121\n",
      "Epoch 157: Loss = 0.1251, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8158\n",
      "Epoch 158: Loss = 0.1230, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8167\n",
      "Epoch 159: Loss = 0.1322, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8155\n",
      "Epoch 160: Loss = 0.1349, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8162\n",
      "Epoch 161: Loss = 0.1243, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8145\n",
      "Epoch 162: Loss = 0.1276, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8128\n",
      "Epoch 163: Loss = 0.1272, Val Acc = 0.8198, Micro F1 = 0.8198, Macro F1 = 0.8032\n",
      "Epoch 164: Loss = 0.1294, Val Acc = 0.8198, Micro F1 = 0.8198, Macro F1 = 0.8030\n",
      "Epoch 165: Loss = 0.1304, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8050\n",
      "Epoch 166: Loss = 0.1314, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8105\n",
      "Epoch 167: Loss = 0.1262, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8136\n",
      "Epoch 168: Loss = 0.1198, Val Acc = 0.8272, Micro F1 = 0.8272, Macro F1 = 0.8147\n",
      "Epoch 169: Loss = 0.1172, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8169\n",
      "Epoch 170: Loss = 0.1220, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8115\n",
      "Epoch 171: Loss = 0.1256, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8193\n",
      "Epoch 172: Loss = 0.1148, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8184\n",
      "Epoch 173: Loss = 0.1234, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8176\n",
      "Epoch 174: Loss = 0.1234, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8176\n",
      "Epoch 175: Loss = 0.1278, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8181\n",
      "Epoch 176: Loss = 0.1347, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8176\n",
      "Epoch 177: Loss = 0.1236, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8107\n",
      "Epoch 178: Loss = 0.1248, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8139\n",
      "Epoch 179: Loss = 0.1218, Val Acc = 0.8405, Micro F1 = 0.8405, Macro F1 = 0.8290\n",
      "Epoch 180: Loss = 0.1196, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8266\n",
      "Epoch 181: Loss = 0.1260, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8271\n",
      "Epoch 182: Loss = 0.1162, Val Acc = 0.8375, Micro F1 = 0.8375, Macro F1 = 0.8265\n",
      "Epoch 183: Loss = 0.1350, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8244\n",
      "Epoch 184: Loss = 0.1307, Val Acc = 0.8287, Micro F1 = 0.8287, Macro F1 = 0.8170\n",
      "Epoch 185: Loss = 0.1224, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8116\n",
      "Epoch 186: Loss = 0.1248, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8101\n",
      "Epoch 187: Loss = 0.1240, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8175\n",
      "Epoch 188: Loss = 0.1209, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8219\n",
      "Epoch 189: Loss = 0.1210, Val Acc = 0.8390, Micro F1 = 0.8390, Macro F1 = 0.8261\n",
      "Epoch 190: Loss = 0.1153, Val Acc = 0.8331, Micro F1 = 0.8331, Macro F1 = 0.8200\n",
      "Epoch 191: Loss = 0.1146, Val Acc = 0.8346, Micro F1 = 0.8346, Macro F1 = 0.8222\n",
      "Epoch 192: Loss = 0.1158, Val Acc = 0.8360, Micro F1 = 0.8360, Macro F1 = 0.8225\n",
      "Epoch 193: Loss = 0.1223, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8131\n",
      "Epoch 194: Loss = 0.1150, Val Acc = 0.8257, Micro F1 = 0.8257, Macro F1 = 0.8150\n",
      "Epoch 195: Loss = 0.1237, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8068\n",
      "Epoch 196: Loss = 0.1195, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8075\n",
      "Epoch 197: Loss = 0.1173, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8082\n",
      "Epoch 198: Loss = 0.1148, Val Acc = 0.8213, Micro F1 = 0.8213, Macro F1 = 0.8086\n",
      "Epoch 199: Loss = 0.1147, Val Acc = 0.8301, Micro F1 = 0.8301, Macro F1 = 0.8166\n",
      "Epoch 200: Loss = 0.1180, Val Acc = 0.8242, Micro F1 = 0.8242, Macro F1 = 0.8116\n",
      "Micro F1: 83.46$\\pm$0.79\n",
      "Macro F1: 82.32$\\pm$0.89\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_Cora.num_classes\n",
    "in_channels = dataset_Cora.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_HGPSL(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_HGPSL, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(HGPSLPool(hidden_channels, ratio=pool_ratios[i], sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        edge_attr = None\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, edge_attr, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "        micro_f1, macro_f1 = compute_f1_scores(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "        # Print epoch progress\n",
    "        print(f'Epoch {epoch:03d}: Loss = {loss.item():.4f}, Val Acc = {val_acc:.4f}, Micro F1 = {micro_f1:.4f}, Macro F1 = {macro_f1:.4f}')\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "# Example usage with the loop over seeds\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_HGPSL(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    # Compute F1 scores\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "# Calculate mean and std deviation of F1 scores\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "# \n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CG-ODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
