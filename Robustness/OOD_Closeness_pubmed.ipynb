{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical GPU ID being used: 1\n",
      "Logical GPU ID used by PyTorch: 0\n",
      "Device Name: NVIDIA A100-SXM4-80GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Physical GPU ID being used: 1\")\n",
    "    print(f\"Logical GPU ID used by PyTorch: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No CUDA device is available.\")\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn.models.mlp import Linear\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import BatchNorm\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "import random\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import os.path as osp\n",
    "import time\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseGCNConv, TopKPooling, SAGPooling\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Mask: 9858 nodes\n",
      "Val Mask: 4929 nodes\n",
      "Test Mask: 4930 nodes\n",
      "Train Mask Average Closeness: 0.1448\n",
      "Val Mask Average Closeness: 0.1669\n",
      "Test Mask Average Closeness: 0.1850\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "data_path = \"/data/Zeyu/Pooling/\"\n",
    "dataset_PubMed = Planetoid(root=data_path, name=\"PubMed\")\n",
    "data = dataset_PubMed[0]\n",
    "\n",
    "G = to_networkx(data, to_undirected=True)\n",
    "\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "closeness_centrality_tensor = torch.tensor([closeness_centrality[i] for i in range(data.num_nodes)])\n",
    "\n",
    "sorted_indices = torch.argsort(closeness_centrality_tensor)\n",
    "\n",
    "num_nodes = data.num_nodes\n",
    "train_size = int(0.5 * num_nodes)\n",
    "val_size = int(0.25 * num_nodes)\n",
    "test_size = num_nodes - train_size - val_size\n",
    "\n",
    "train_indices = sorted_indices[:train_size]\n",
    "val_indices = sorted_indices[train_size:train_size + val_size]\n",
    "test_indices = sorted_indices[train_size + val_size:]\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "train_mask[train_indices] = True\n",
    "val_mask[val_indices] = True\n",
    "test_mask[test_indices] = True\n",
    "\n",
    "data.train_mask = train_mask\n",
    "data.val_mask = val_mask\n",
    "data.test_mask = test_mask\n",
    "\n",
    "print(f\"Train Mask: {train_mask.sum().item()} nodes\")\n",
    "print(f\"Val Mask: {val_mask.sum().item()} nodes\")\n",
    "print(f\"Test Mask: {test_mask.sum().item()} nodes\")\n",
    "\n",
    "train_closeness_avg = closeness_centrality_tensor[train_mask].float().mean().item()\n",
    "val_closeness_avg = closeness_centrality_tensor[val_mask].float().mean().item()\n",
    "test_closeness_avg = closeness_centrality_tensor[test_mask].float().mean().item()\n",
    "\n",
    "print(f\"Train Mask Average Closeness: {train_closeness_avg:.4f}\")\n",
    "print(f\"Val Mask Average Closeness: {val_closeness_avg:.4f}\")\n",
    "print(f\"Test Mask Average Closeness: {test_closeness_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 85.66$\\pm$0.09\n",
      "Macro F1: 82.36$\\pm$0.24\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_PubMed.num_classes\n",
    "in_channels = dataset_PubMed.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_TOPK(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_TOPK, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(TopKPooling(channels, ratio=pool_ratios[i]))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义训练和评估函数\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "# 在不同的种子上训练和评估模型\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_TOPK(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # 训练模型\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    # 计算验证集上的F1分数\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "# 计算F1分数的均值和方差\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 85.96$\\pm$0.34\n",
      "Macro F1: 82.73$\\pm$0.38\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGPooling\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_PubMed.num_classes\n",
    "in_channels = dataset_PubMed.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_SAG(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_SAG, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(SAGPooling(channels, ratio=pool_ratios[i]))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义训练和评估函数\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_SAG(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASAPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss = 1.0992, Val Acc = 0.4112\n",
      "Epoch 002: Loss = 1.0975, Val Acc = 0.4374\n",
      "Epoch 003: Loss = 1.0960, Val Acc = 0.4332\n",
      "Epoch 004: Loss = 1.0943, Val Acc = 0.4323\n",
      "Epoch 005: Loss = 1.0928, Val Acc = 0.4323\n",
      "Epoch 006: Loss = 1.0912, Val Acc = 0.4323\n",
      "Epoch 007: Loss = 1.0895, Val Acc = 0.4323\n",
      "Epoch 008: Loss = 1.0876, Val Acc = 0.4323\n",
      "Epoch 009: Loss = 1.0855, Val Acc = 0.4323\n",
      "Epoch 010: Loss = 1.0834, Val Acc = 0.4323\n",
      "Epoch 011: Loss = 1.0811, Val Acc = 0.4323\n",
      "Epoch 012: Loss = 1.0786, Val Acc = 0.4323\n",
      "Epoch 013: Loss = 1.0759, Val Acc = 0.4323\n",
      "Epoch 014: Loss = 1.0731, Val Acc = 0.4323\n",
      "Epoch 015: Loss = 1.0699, Val Acc = 0.4327\n",
      "Epoch 016: Loss = 1.0666, Val Acc = 0.4338\n",
      "Epoch 017: Loss = 1.0630, Val Acc = 0.4348\n",
      "Epoch 018: Loss = 1.0592, Val Acc = 0.4392\n",
      "Epoch 019: Loss = 1.0548, Val Acc = 0.4451\n",
      "Epoch 020: Loss = 1.0505, Val Acc = 0.4508\n",
      "Epoch 021: Loss = 1.0458, Val Acc = 0.4561\n",
      "Epoch 022: Loss = 1.0403, Val Acc = 0.4622\n",
      "Epoch 023: Loss = 1.0344, Val Acc = 0.4689\n",
      "Epoch 024: Loss = 1.0283, Val Acc = 0.4751\n",
      "Epoch 025: Loss = 1.0217, Val Acc = 0.4806\n",
      "Epoch 026: Loss = 1.0145, Val Acc = 0.4845\n",
      "Epoch 027: Loss = 1.0074, Val Acc = 0.4902\n",
      "Epoch 028: Loss = 0.9996, Val Acc = 0.4924\n",
      "Epoch 029: Loss = 0.9907, Val Acc = 0.4948\n",
      "Epoch 030: Loss = 0.9825, Val Acc = 0.4971\n",
      "Epoch 031: Loss = 0.9724, Val Acc = 0.4973\n",
      "Epoch 032: Loss = 0.9640, Val Acc = 0.4991\n",
      "Epoch 033: Loss = 0.9538, Val Acc = 0.5009\n",
      "Epoch 034: Loss = 0.9412, Val Acc = 0.5009\n",
      "Epoch 035: Loss = 0.9292, Val Acc = 0.5019\n",
      "Epoch 036: Loss = 0.9181, Val Acc = 0.5036\n",
      "Epoch 037: Loss = 0.9049, Val Acc = 0.5050\n",
      "Epoch 038: Loss = 0.8908, Val Acc = 0.5050\n",
      "Epoch 039: Loss = 0.8783, Val Acc = 0.5056\n",
      "Epoch 040: Loss = 0.8642, Val Acc = 0.5084\n",
      "Epoch 041: Loss = 0.8489, Val Acc = 0.5107\n",
      "Epoch 042: Loss = 0.8337, Val Acc = 0.5117\n",
      "Epoch 043: Loss = 0.8180, Val Acc = 0.5151\n",
      "Epoch 044: Loss = 0.8045, Val Acc = 0.5206\n",
      "Epoch 045: Loss = 0.7886, Val Acc = 0.5305\n",
      "Epoch 046: Loss = 0.7752, Val Acc = 0.5382\n",
      "Epoch 047: Loss = 0.7632, Val Acc = 0.5608\n",
      "Epoch 048: Loss = 0.7476, Val Acc = 0.5993\n",
      "Epoch 049: Loss = 0.7341, Val Acc = 0.6336\n",
      "Epoch 050: Loss = 0.7224, Val Acc = 0.6559\n",
      "Epoch 051: Loss = 0.7118, Val Acc = 0.6835\n",
      "Epoch 052: Loss = 0.7005, Val Acc = 0.7016\n",
      "Epoch 053: Loss = 0.6912, Val Acc = 0.7243\n",
      "Epoch 054: Loss = 0.6813, Val Acc = 0.7460\n",
      "Epoch 055: Loss = 0.6763, Val Acc = 0.7590\n",
      "Epoch 056: Loss = 0.6658, Val Acc = 0.7679\n",
      "Epoch 057: Loss = 0.6610, Val Acc = 0.7693\n",
      "Epoch 058: Loss = 0.6500, Val Acc = 0.7707\n",
      "Epoch 059: Loss = 0.6441, Val Acc = 0.7732\n",
      "Epoch 060: Loss = 0.6344, Val Acc = 0.7827\n",
      "Epoch 061: Loss = 0.6234, Val Acc = 0.7880\n",
      "Epoch 062: Loss = 0.6150, Val Acc = 0.7882\n",
      "Epoch 063: Loss = 0.6056, Val Acc = 0.7914\n",
      "Epoch 064: Loss = 0.5961, Val Acc = 0.7955\n",
      "Epoch 065: Loss = 0.5856, Val Acc = 0.7994\n",
      "Epoch 066: Loss = 0.5777, Val Acc = 0.8012\n",
      "Epoch 067: Loss = 0.5683, Val Acc = 0.8054\n",
      "Epoch 068: Loss = 0.5633, Val Acc = 0.8067\n",
      "Epoch 069: Loss = 0.5551, Val Acc = 0.8093\n",
      "Epoch 070: Loss = 0.5472, Val Acc = 0.8103\n",
      "Epoch 071: Loss = 0.5394, Val Acc = 0.8117\n",
      "Epoch 072: Loss = 0.5302, Val Acc = 0.8136\n",
      "Epoch 073: Loss = 0.5270, Val Acc = 0.8142\n",
      "Epoch 074: Loss = 0.5246, Val Acc = 0.8158\n",
      "Epoch 075: Loss = 0.5155, Val Acc = 0.8178\n",
      "Epoch 076: Loss = 0.5123, Val Acc = 0.8196\n",
      "Epoch 077: Loss = 0.5089, Val Acc = 0.8253\n",
      "Epoch 078: Loss = 0.5031, Val Acc = 0.8259\n",
      "Epoch 079: Loss = 0.5008, Val Acc = 0.8280\n",
      "Epoch 080: Loss = 0.5022, Val Acc = 0.8280\n",
      "Epoch 081: Loss = 0.4991, Val Acc = 0.8294\n",
      "Epoch 082: Loss = 0.4974, Val Acc = 0.8316\n",
      "Epoch 083: Loss = 0.4914, Val Acc = 0.8310\n",
      "Epoch 084: Loss = 0.4906, Val Acc = 0.8312\n",
      "Epoch 085: Loss = 0.4896, Val Acc = 0.8359\n",
      "Epoch 086: Loss = 0.4909, Val Acc = 0.8355\n",
      "Epoch 087: Loss = 0.4897, Val Acc = 0.8365\n",
      "Epoch 088: Loss = 0.4849, Val Acc = 0.8373\n",
      "Epoch 089: Loss = 0.4802, Val Acc = 0.8371\n",
      "Epoch 090: Loss = 0.4842, Val Acc = 0.8377\n",
      "Epoch 091: Loss = 0.4779, Val Acc = 0.8387\n",
      "Epoch 092: Loss = 0.4771, Val Acc = 0.8385\n",
      "Epoch 093: Loss = 0.4741, Val Acc = 0.8403\n",
      "Epoch 094: Loss = 0.4774, Val Acc = 0.8403\n",
      "Epoch 095: Loss = 0.4736, Val Acc = 0.8391\n",
      "Epoch 096: Loss = 0.4727, Val Acc = 0.8405\n",
      "Epoch 097: Loss = 0.4718, Val Acc = 0.8399\n",
      "Epoch 098: Loss = 0.4672, Val Acc = 0.8411\n",
      "Epoch 099: Loss = 0.4657, Val Acc = 0.8413\n",
      "Epoch 100: Loss = 0.4681, Val Acc = 0.8405\n",
      "Epoch 101: Loss = 0.4633, Val Acc = 0.8397\n",
      "Epoch 102: Loss = 0.4634, Val Acc = 0.8407\n",
      "Epoch 103: Loss = 0.4623, Val Acc = 0.8416\n",
      "Epoch 104: Loss = 0.4631, Val Acc = 0.8413\n",
      "Epoch 105: Loss = 0.4614, Val Acc = 0.8409\n",
      "Epoch 106: Loss = 0.4624, Val Acc = 0.8416\n",
      "Epoch 107: Loss = 0.4615, Val Acc = 0.8420\n",
      "Epoch 108: Loss = 0.4584, Val Acc = 0.8434\n",
      "Epoch 109: Loss = 0.4572, Val Acc = 0.8434\n",
      "Epoch 110: Loss = 0.4567, Val Acc = 0.8438\n",
      "Epoch 111: Loss = 0.4514, Val Acc = 0.8444\n",
      "Epoch 112: Loss = 0.4531, Val Acc = 0.8444\n",
      "Epoch 113: Loss = 0.4534, Val Acc = 0.8444\n",
      "Epoch 114: Loss = 0.4545, Val Acc = 0.8444\n",
      "Epoch 115: Loss = 0.4530, Val Acc = 0.8424\n",
      "Epoch 116: Loss = 0.4533, Val Acc = 0.8446\n",
      "Epoch 117: Loss = 0.4490, Val Acc = 0.8478\n",
      "Epoch 118: Loss = 0.4522, Val Acc = 0.8458\n",
      "Epoch 119: Loss = 0.4487, Val Acc = 0.8444\n",
      "Epoch 120: Loss = 0.4493, Val Acc = 0.8434\n",
      "Epoch 121: Loss = 0.4494, Val Acc = 0.8466\n",
      "Epoch 122: Loss = 0.4469, Val Acc = 0.8482\n",
      "Epoch 123: Loss = 0.4457, Val Acc = 0.8450\n",
      "Epoch 124: Loss = 0.4448, Val Acc = 0.8466\n",
      "Epoch 125: Loss = 0.4446, Val Acc = 0.8462\n",
      "Epoch 126: Loss = 0.4468, Val Acc = 0.8499\n",
      "Epoch 127: Loss = 0.4434, Val Acc = 0.8495\n",
      "Epoch 128: Loss = 0.4422, Val Acc = 0.8464\n",
      "Epoch 129: Loss = 0.4454, Val Acc = 0.8474\n",
      "Epoch 130: Loss = 0.4414, Val Acc = 0.8501\n",
      "Epoch 131: Loss = 0.4425, Val Acc = 0.8501\n",
      "Epoch 132: Loss = 0.4416, Val Acc = 0.8474\n",
      "Epoch 133: Loss = 0.4412, Val Acc = 0.8491\n",
      "Epoch 134: Loss = 0.4414, Val Acc = 0.8521\n",
      "Epoch 135: Loss = 0.4418, Val Acc = 0.8499\n",
      "Epoch 136: Loss = 0.4377, Val Acc = 0.8474\n",
      "Epoch 137: Loss = 0.4381, Val Acc = 0.8487\n",
      "Epoch 138: Loss = 0.4338, Val Acc = 0.8537\n",
      "Epoch 139: Loss = 0.4353, Val Acc = 0.8539\n",
      "Epoch 140: Loss = 0.4366, Val Acc = 0.8525\n",
      "Epoch 141: Loss = 0.4342, Val Acc = 0.8489\n",
      "Epoch 142: Loss = 0.4379, Val Acc = 0.8509\n",
      "Epoch 143: Loss = 0.4346, Val Acc = 0.8523\n",
      "Epoch 144: Loss = 0.4368, Val Acc = 0.8535\n",
      "Epoch 145: Loss = 0.4363, Val Acc = 0.8533\n",
      "Epoch 146: Loss = 0.4341, Val Acc = 0.8497\n",
      "Epoch 147: Loss = 0.4349, Val Acc = 0.8484\n",
      "Epoch 148: Loss = 0.4315, Val Acc = 0.8539\n",
      "Epoch 149: Loss = 0.4344, Val Acc = 0.8539\n",
      "Epoch 150: Loss = 0.4336, Val Acc = 0.8545\n",
      "Epoch 151: Loss = 0.4303, Val Acc = 0.8507\n",
      "Epoch 152: Loss = 0.4335, Val Acc = 0.8511\n",
      "Epoch 153: Loss = 0.4322, Val Acc = 0.8533\n",
      "Epoch 154: Loss = 0.4292, Val Acc = 0.8543\n",
      "Epoch 155: Loss = 0.4329, Val Acc = 0.8535\n",
      "Epoch 156: Loss = 0.4332, Val Acc = 0.8501\n",
      "Epoch 157: Loss = 0.4332, Val Acc = 0.8507\n",
      "Epoch 158: Loss = 0.4298, Val Acc = 0.8549\n",
      "Epoch 159: Loss = 0.4296, Val Acc = 0.8562\n",
      "Epoch 160: Loss = 0.4340, Val Acc = 0.8537\n",
      "Epoch 161: Loss = 0.4281, Val Acc = 0.8511\n",
      "Epoch 162: Loss = 0.4287, Val Acc = 0.8503\n",
      "Epoch 163: Loss = 0.4266, Val Acc = 0.8547\n",
      "Epoch 164: Loss = 0.4280, Val Acc = 0.8543\n",
      "Epoch 165: Loss = 0.4287, Val Acc = 0.8551\n",
      "Epoch 166: Loss = 0.4293, Val Acc = 0.8497\n",
      "Epoch 167: Loss = 0.4271, Val Acc = 0.8509\n",
      "Epoch 168: Loss = 0.4276, Val Acc = 0.8539\n",
      "Epoch 169: Loss = 0.4237, Val Acc = 0.8553\n",
      "Epoch 170: Loss = 0.4271, Val Acc = 0.8531\n",
      "Epoch 171: Loss = 0.4251, Val Acc = 0.8511\n",
      "Epoch 172: Loss = 0.4234, Val Acc = 0.8562\n",
      "Epoch 173: Loss = 0.4260, Val Acc = 0.8566\n",
      "Epoch 174: Loss = 0.4219, Val Acc = 0.8555\n",
      "Epoch 175: Loss = 0.4266, Val Acc = 0.8539\n",
      "Epoch 176: Loss = 0.4225, Val Acc = 0.8529\n",
      "Epoch 177: Loss = 0.4251, Val Acc = 0.8558\n",
      "Epoch 178: Loss = 0.4233, Val Acc = 0.8543\n",
      "Epoch 179: Loss = 0.4275, Val Acc = 0.8555\n",
      "Epoch 180: Loss = 0.4249, Val Acc = 0.8549\n",
      "Epoch 181: Loss = 0.4207, Val Acc = 0.8555\n",
      "Epoch 182: Loss = 0.4226, Val Acc = 0.8558\n",
      "Epoch 183: Loss = 0.4203, Val Acc = 0.8562\n",
      "Epoch 184: Loss = 0.4219, Val Acc = 0.8558\n",
      "Epoch 185: Loss = 0.4169, Val Acc = 0.8533\n",
      "Epoch 186: Loss = 0.4196, Val Acc = 0.8564\n",
      "Epoch 187: Loss = 0.4201, Val Acc = 0.8551\n",
      "Epoch 188: Loss = 0.4208, Val Acc = 0.8570\n",
      "Epoch 189: Loss = 0.4190, Val Acc = 0.8547\n",
      "Epoch 190: Loss = 0.4182, Val Acc = 0.8533\n",
      "Epoch 191: Loss = 0.4222, Val Acc = 0.8566\n",
      "Epoch 192: Loss = 0.4189, Val Acc = 0.8566\n",
      "Epoch 193: Loss = 0.4181, Val Acc = 0.8560\n",
      "Epoch 194: Loss = 0.4183, Val Acc = 0.8560\n",
      "Epoch 195: Loss = 0.4169, Val Acc = 0.8558\n",
      "Epoch 196: Loss = 0.4181, Val Acc = 0.8566\n",
      "Epoch 197: Loss = 0.4184, Val Acc = 0.8566\n",
      "Epoch 198: Loss = 0.4163, Val Acc = 0.8574\n",
      "Epoch 199: Loss = 0.4136, Val Acc = 0.8572\n",
      "Epoch 200: Loss = 0.4166, Val Acc = 0.8564\n",
      "Epoch 001: Loss = 1.0986, Val Acc = 0.4323\n",
      "Epoch 002: Loss = 1.0972, Val Acc = 0.4323\n",
      "Epoch 003: Loss = 1.0957, Val Acc = 0.4323\n",
      "Epoch 004: Loss = 1.0945, Val Acc = 0.4323\n",
      "Epoch 005: Loss = 1.0931, Val Acc = 0.4323\n",
      "Epoch 006: Loss = 1.0917, Val Acc = 0.4323\n",
      "Epoch 007: Loss = 1.0903, Val Acc = 0.4323\n",
      "Epoch 008: Loss = 1.0885, Val Acc = 0.4323\n",
      "Epoch 009: Loss = 1.0867, Val Acc = 0.4323\n",
      "Epoch 010: Loss = 1.0848, Val Acc = 0.4323\n",
      "Epoch 011: Loss = 1.0827, Val Acc = 0.4323\n",
      "Epoch 012: Loss = 1.0806, Val Acc = 0.4323\n",
      "Epoch 013: Loss = 1.0784, Val Acc = 0.4323\n",
      "Epoch 014: Loss = 1.0757, Val Acc = 0.4323\n",
      "Epoch 015: Loss = 1.0732, Val Acc = 0.4323\n",
      "Epoch 016: Loss = 1.0706, Val Acc = 0.4323\n",
      "Epoch 017: Loss = 1.0676, Val Acc = 0.4323\n",
      "Epoch 018: Loss = 1.0645, Val Acc = 0.4323\n",
      "Epoch 019: Loss = 1.0610, Val Acc = 0.4323\n",
      "Epoch 020: Loss = 1.0572, Val Acc = 0.4323\n",
      "Epoch 021: Loss = 1.0533, Val Acc = 0.4323\n",
      "Epoch 022: Loss = 1.0489, Val Acc = 0.4329\n",
      "Epoch 023: Loss = 1.0442, Val Acc = 0.4344\n",
      "Epoch 024: Loss = 1.0393, Val Acc = 0.4386\n",
      "Epoch 025: Loss = 1.0345, Val Acc = 0.4453\n",
      "Epoch 026: Loss = 1.0283, Val Acc = 0.4534\n",
      "Epoch 027: Loss = 1.0233, Val Acc = 0.4609\n",
      "Epoch 028: Loss = 1.0161, Val Acc = 0.4687\n",
      "Epoch 029: Loss = 1.0089, Val Acc = 0.4753\n",
      "Epoch 030: Loss = 1.0018, Val Acc = 0.4810\n",
      "Epoch 031: Loss = 0.9939, Val Acc = 0.4885\n",
      "Epoch 032: Loss = 0.9861, Val Acc = 0.4932\n",
      "Epoch 033: Loss = 0.9769, Val Acc = 0.4971\n",
      "Epoch 034: Loss = 0.9676, Val Acc = 0.4993\n",
      "Epoch 035: Loss = 0.9579, Val Acc = 0.5005\n",
      "Epoch 036: Loss = 0.9464, Val Acc = 0.5023\n",
      "Epoch 037: Loss = 0.9351, Val Acc = 0.5036\n",
      "Epoch 038: Loss = 0.9228, Val Acc = 0.5050\n",
      "Epoch 039: Loss = 0.9104, Val Acc = 0.5062\n",
      "Epoch 040: Loss = 0.8974, Val Acc = 0.5070\n",
      "Epoch 041: Loss = 0.8848, Val Acc = 0.5070\n",
      "Epoch 042: Loss = 0.8699, Val Acc = 0.5078\n",
      "Epoch 043: Loss = 0.8544, Val Acc = 0.5090\n",
      "Epoch 044: Loss = 0.8385, Val Acc = 0.5139\n",
      "Epoch 045: Loss = 0.8246, Val Acc = 0.5249\n",
      "Epoch 046: Loss = 0.8075, Val Acc = 0.5447\n",
      "Epoch 047: Loss = 0.7923, Val Acc = 0.5766\n",
      "Epoch 048: Loss = 0.7774, Val Acc = 0.6155\n",
      "Epoch 049: Loss = 0.7607, Val Acc = 0.6456\n",
      "Epoch 050: Loss = 0.7453, Val Acc = 0.6772\n",
      "Epoch 051: Loss = 0.7308, Val Acc = 0.6999\n",
      "Epoch 052: Loss = 0.7161, Val Acc = 0.7247\n",
      "Epoch 053: Loss = 0.7023, Val Acc = 0.7501\n",
      "Epoch 054: Loss = 0.6875, Val Acc = 0.7638\n",
      "Epoch 055: Loss = 0.6778, Val Acc = 0.7707\n",
      "Epoch 056: Loss = 0.6657, Val Acc = 0.7787\n",
      "Epoch 057: Loss = 0.6559, Val Acc = 0.7831\n",
      "Epoch 058: Loss = 0.6468, Val Acc = 0.7864\n",
      "Epoch 059: Loss = 0.6360, Val Acc = 0.7900\n",
      "Epoch 060: Loss = 0.6267, Val Acc = 0.7925\n",
      "Epoch 061: Loss = 0.6143, Val Acc = 0.7965\n",
      "Epoch 062: Loss = 0.6038, Val Acc = 0.7971\n",
      "Epoch 063: Loss = 0.5952, Val Acc = 0.7991\n",
      "Epoch 064: Loss = 0.5868, Val Acc = 0.8008\n",
      "Epoch 065: Loss = 0.5764, Val Acc = 0.8026\n",
      "Epoch 066: Loss = 0.5670, Val Acc = 0.8042\n",
      "Epoch 067: Loss = 0.5618, Val Acc = 0.8050\n",
      "Epoch 068: Loss = 0.5483, Val Acc = 0.8067\n",
      "Epoch 069: Loss = 0.5421, Val Acc = 0.8085\n",
      "Epoch 070: Loss = 0.5360, Val Acc = 0.8099\n",
      "Epoch 071: Loss = 0.5282, Val Acc = 0.8123\n",
      "Epoch 072: Loss = 0.5204, Val Acc = 0.8138\n",
      "Epoch 073: Loss = 0.5153, Val Acc = 0.8160\n",
      "Epoch 074: Loss = 0.5126, Val Acc = 0.8154\n",
      "Epoch 075: Loss = 0.5101, Val Acc = 0.8192\n",
      "Epoch 076: Loss = 0.5034, Val Acc = 0.8219\n",
      "Epoch 077: Loss = 0.5015, Val Acc = 0.8219\n",
      "Epoch 078: Loss = 0.5013, Val Acc = 0.8215\n",
      "Epoch 079: Loss = 0.5004, Val Acc = 0.8239\n",
      "Epoch 080: Loss = 0.4920, Val Acc = 0.8278\n",
      "Epoch 081: Loss = 0.4930, Val Acc = 0.8273\n",
      "Epoch 082: Loss = 0.4907, Val Acc = 0.8294\n",
      "Epoch 083: Loss = 0.4921, Val Acc = 0.8300\n",
      "Epoch 084: Loss = 0.4910, Val Acc = 0.8340\n",
      "Epoch 085: Loss = 0.4893, Val Acc = 0.8320\n",
      "Epoch 086: Loss = 0.4884, Val Acc = 0.8359\n",
      "Epoch 087: Loss = 0.4843, Val Acc = 0.8383\n",
      "Epoch 088: Loss = 0.4859, Val Acc = 0.8351\n",
      "Epoch 089: Loss = 0.4788, Val Acc = 0.8342\n",
      "Epoch 090: Loss = 0.4797, Val Acc = 0.8359\n",
      "Epoch 091: Loss = 0.4748, Val Acc = 0.8393\n",
      "Epoch 092: Loss = 0.4751, Val Acc = 0.8399\n",
      "Epoch 093: Loss = 0.4724, Val Acc = 0.8371\n",
      "Epoch 094: Loss = 0.4748, Val Acc = 0.8375\n",
      "Epoch 095: Loss = 0.4696, Val Acc = 0.8397\n",
      "Epoch 096: Loss = 0.4709, Val Acc = 0.8416\n",
      "Epoch 097: Loss = 0.4672, Val Acc = 0.8413\n",
      "Epoch 098: Loss = 0.4690, Val Acc = 0.8430\n",
      "Epoch 099: Loss = 0.4660, Val Acc = 0.8383\n",
      "Epoch 100: Loss = 0.4651, Val Acc = 0.8426\n",
      "Epoch 101: Loss = 0.4654, Val Acc = 0.8430\n",
      "Epoch 102: Loss = 0.4646, Val Acc = 0.8430\n",
      "Epoch 103: Loss = 0.4654, Val Acc = 0.8444\n",
      "Epoch 104: Loss = 0.4617, Val Acc = 0.8403\n",
      "Epoch 105: Loss = 0.4635, Val Acc = 0.8403\n",
      "Epoch 106: Loss = 0.4617, Val Acc = 0.8446\n",
      "Epoch 107: Loss = 0.4626, Val Acc = 0.8438\n",
      "Epoch 108: Loss = 0.4580, Val Acc = 0.8448\n",
      "Epoch 109: Loss = 0.4607, Val Acc = 0.8422\n",
      "Epoch 110: Loss = 0.4536, Val Acc = 0.8420\n",
      "Epoch 111: Loss = 0.4568, Val Acc = 0.8440\n",
      "Epoch 112: Loss = 0.4555, Val Acc = 0.8462\n",
      "Epoch 113: Loss = 0.4592, Val Acc = 0.8462\n",
      "Epoch 114: Loss = 0.4578, Val Acc = 0.8428\n",
      "Epoch 115: Loss = 0.4541, Val Acc = 0.8438\n",
      "Epoch 116: Loss = 0.4518, Val Acc = 0.8446\n",
      "Epoch 117: Loss = 0.4547, Val Acc = 0.8472\n",
      "Epoch 118: Loss = 0.4517, Val Acc = 0.8478\n",
      "Epoch 119: Loss = 0.4534, Val Acc = 0.8452\n",
      "Epoch 120: Loss = 0.4501, Val Acc = 0.8446\n",
      "Epoch 121: Loss = 0.4498, Val Acc = 0.8450\n",
      "Epoch 122: Loss = 0.4484, Val Acc = 0.8468\n",
      "Epoch 123: Loss = 0.4482, Val Acc = 0.8478\n",
      "Epoch 124: Loss = 0.4477, Val Acc = 0.8468\n",
      "Epoch 125: Loss = 0.4480, Val Acc = 0.8446\n",
      "Epoch 126: Loss = 0.4489, Val Acc = 0.8450\n",
      "Epoch 127: Loss = 0.4473, Val Acc = 0.8458\n",
      "Epoch 128: Loss = 0.4440, Val Acc = 0.8478\n",
      "Epoch 129: Loss = 0.4441, Val Acc = 0.8491\n",
      "Epoch 130: Loss = 0.4431, Val Acc = 0.8460\n",
      "Epoch 131: Loss = 0.4421, Val Acc = 0.8450\n",
      "Epoch 132: Loss = 0.4420, Val Acc = 0.8464\n",
      "Epoch 133: Loss = 0.4472, Val Acc = 0.8491\n",
      "Epoch 134: Loss = 0.4412, Val Acc = 0.8480\n",
      "Epoch 135: Loss = 0.4419, Val Acc = 0.8480\n",
      "Epoch 136: Loss = 0.4420, Val Acc = 0.8484\n",
      "Epoch 137: Loss = 0.4442, Val Acc = 0.8513\n",
      "Epoch 138: Loss = 0.4370, Val Acc = 0.8509\n",
      "Epoch 139: Loss = 0.4407, Val Acc = 0.8472\n",
      "Epoch 140: Loss = 0.4395, Val Acc = 0.8474\n",
      "Epoch 141: Loss = 0.4381, Val Acc = 0.8499\n",
      "Epoch 142: Loss = 0.4410, Val Acc = 0.8501\n",
      "Epoch 143: Loss = 0.4427, Val Acc = 0.8495\n",
      "Epoch 144: Loss = 0.4389, Val Acc = 0.8499\n",
      "Epoch 145: Loss = 0.4377, Val Acc = 0.8497\n",
      "Epoch 146: Loss = 0.4360, Val Acc = 0.8513\n",
      "Epoch 147: Loss = 0.4357, Val Acc = 0.8501\n",
      "Epoch 148: Loss = 0.4342, Val Acc = 0.8497\n",
      "Epoch 149: Loss = 0.4394, Val Acc = 0.8509\n",
      "Epoch 150: Loss = 0.4389, Val Acc = 0.8501\n",
      "Epoch 151: Loss = 0.4319, Val Acc = 0.8489\n",
      "Epoch 152: Loss = 0.4367, Val Acc = 0.8503\n",
      "Epoch 153: Loss = 0.4370, Val Acc = 0.8545\n",
      "Epoch 154: Loss = 0.4354, Val Acc = 0.8543\n",
      "Epoch 155: Loss = 0.4362, Val Acc = 0.8509\n",
      "Epoch 156: Loss = 0.4345, Val Acc = 0.8503\n",
      "Epoch 157: Loss = 0.4339, Val Acc = 0.8521\n",
      "Epoch 158: Loss = 0.4304, Val Acc = 0.8547\n",
      "Epoch 159: Loss = 0.4314, Val Acc = 0.8545\n",
      "Epoch 160: Loss = 0.4336, Val Acc = 0.8507\n",
      "Epoch 161: Loss = 0.4288, Val Acc = 0.8503\n",
      "Epoch 162: Loss = 0.4286, Val Acc = 0.8523\n",
      "Epoch 163: Loss = 0.4307, Val Acc = 0.8547\n",
      "Epoch 164: Loss = 0.4310, Val Acc = 0.8523\n",
      "Epoch 165: Loss = 0.4256, Val Acc = 0.8519\n",
      "Epoch 166: Loss = 0.4280, Val Acc = 0.8499\n",
      "Epoch 167: Loss = 0.4311, Val Acc = 0.8551\n",
      "Epoch 168: Loss = 0.4328, Val Acc = 0.8525\n",
      "Epoch 169: Loss = 0.4286, Val Acc = 0.8517\n",
      "Epoch 170: Loss = 0.4263, Val Acc = 0.8521\n",
      "Epoch 171: Loss = 0.4259, Val Acc = 0.8521\n",
      "Epoch 172: Loss = 0.4281, Val Acc = 0.8523\n",
      "Epoch 173: Loss = 0.4290, Val Acc = 0.8535\n",
      "Epoch 174: Loss = 0.4294, Val Acc = 0.8537\n",
      "Epoch 175: Loss = 0.4241, Val Acc = 0.8537\n",
      "Epoch 176: Loss = 0.4264, Val Acc = 0.8541\n",
      "Epoch 177: Loss = 0.4244, Val Acc = 0.8537\n",
      "Epoch 178: Loss = 0.4255, Val Acc = 0.8549\n",
      "Epoch 179: Loss = 0.4292, Val Acc = 0.8539\n",
      "Epoch 180: Loss = 0.4267, Val Acc = 0.8541\n",
      "Epoch 181: Loss = 0.4250, Val Acc = 0.8549\n",
      "Epoch 182: Loss = 0.4271, Val Acc = 0.8566\n",
      "Epoch 183: Loss = 0.4215, Val Acc = 0.8549\n",
      "Epoch 184: Loss = 0.4226, Val Acc = 0.8537\n",
      "Epoch 185: Loss = 0.4231, Val Acc = 0.8535\n",
      "Epoch 186: Loss = 0.4230, Val Acc = 0.8562\n",
      "Epoch 187: Loss = 0.4231, Val Acc = 0.8562\n",
      "Epoch 188: Loss = 0.4238, Val Acc = 0.8558\n",
      "Epoch 189: Loss = 0.4240, Val Acc = 0.8545\n",
      "Epoch 190: Loss = 0.4227, Val Acc = 0.8547\n",
      "Epoch 191: Loss = 0.4237, Val Acc = 0.8572\n",
      "Epoch 192: Loss = 0.4221, Val Acc = 0.8570\n",
      "Epoch 193: Loss = 0.4197, Val Acc = 0.8568\n",
      "Epoch 194: Loss = 0.4168, Val Acc = 0.8568\n",
      "Epoch 195: Loss = 0.4195, Val Acc = 0.8562\n",
      "Epoch 196: Loss = 0.4176, Val Acc = 0.8562\n",
      "Epoch 197: Loss = 0.4198, Val Acc = 0.8578\n",
      "Epoch 198: Loss = 0.4200, Val Acc = 0.8566\n",
      "Epoch 199: Loss = 0.4185, Val Acc = 0.8568\n",
      "Epoch 200: Loss = 0.4185, Val Acc = 0.8576\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: insufficient resources when calling `cusparseSpGEMM_workEstimation( handle, opA, opB, &alpha_, descA.descriptor(), descB.descriptor(), &beta_, descC.descriptor(), compute_type, CUSPARSE_SPGEMM_DEFAULT, spgemm_desc.descriptor(), &buffer_size1, buffer1.get())`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\n\u001b[1;32m    140\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m--> 142\u001b[0m model, best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_node_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m micro_f1, macro_f1 \u001b[38;5;241m=\u001b[39m compute_f1_scores(model, data, data\u001b[38;5;241m.\u001b[39mval_mask)\n\u001b[1;32m    145\u001b[0m micro_f1_scores\u001b[38;5;241m.\u001b[39mappend(micro_f1)\n",
      "Cell \u001b[0;32mIn[6], line 87\u001b[0m, in \u001b[0;36mtrain_node_classifier\u001b[0;34m(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs, patience, min_delta)\u001b[0m\n\u001b[1;32m     85\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 87\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[train_mask], graph\u001b[38;5;241m.\u001b[39my[train_mask])\n\u001b[1;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 53\u001b[0m, in \u001b[0;36mHierarchicalGCN_ASA.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     50\u001b[0m perms \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 53\u001b[0m     x, edge_index, _, batch, perm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpools\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_convs[i](x, edge_index)\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch_geometric/nn/pool/asap.py:150\u001b[0m, in \u001b[0;36mASAPooling.forward\u001b[0;34m(self, x, edge_index, edge_weight, batch)\u001b[0m\n\u001b[1;32m    148\u001b[0m S \u001b[38;5;241m=\u001b[39m to_torch_coo_tensor(edge_index, score, size\u001b[38;5;241m=\u001b[39m(N, N))\n\u001b[1;32m    149\u001b[0m S \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m.\u001b[39mindex_select(\u001b[38;5;241m1\u001b[39m, perm)\u001b[38;5;241m.\u001b[39mto_sparse_csr()\n\u001b[0;32m--> 150\u001b[0m A \u001b[38;5;241m=\u001b[39m S\u001b[38;5;241m.\u001b[39mt()\u001b[38;5;241m.\u001b[39mto_sparse_csr() \u001b[38;5;241m@\u001b[39m (\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m edge_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     edge_index, _ \u001b[38;5;241m=\u001b[39m to_edge_index(A)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: insufficient resources when calling `cusparseSpGEMM_workEstimation( handle, opA, opB, &alpha_, descA.descriptor(), descB.descriptor(), &beta_, descC.descriptor(), compute_type, CUSPARSE_SPGEMM_DEFAULT, spgemm_desc.descriptor(), &buffer_size1, buffer1.get())`"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import ASAPooling\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_PubMed.num_classes\n",
    "in_channels = dataset_PubMed.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_ASA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_ASA, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(ASAPooling(channels, ratio=pool_ratios[i]))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch:03d}: Loss = {loss.item():.4f}, Val Acc = {val_acc:.4f}')\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_ASA(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PANPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "class PANPooling(torch.nn.Module):\n",
    "    r\"\"\" General Graph pooling layer based on PAN, which can work with all layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, ratio=0.5, pan_pool_weight=None, min_score=None, multiplier=1,\n",
    "                 nonlinearity=torch.tanh, filter_size=3, panpool_filter_weight=None):\n",
    "        super(PANPooling, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.min_score = min_score\n",
    "        self.multiplier = multiplier\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        self.filter_size = filter_size\n",
    "        if panpool_filter_weight is None:\n",
    "            self.panpool_filter_weight = torch.nn.Parameter(0.5 * torch.ones(filter_size), requires_grad=True)\n",
    "\n",
    "        self.transform = Parameter(torch.ones(in_channels), requires_grad=True)\n",
    "\n",
    "        if pan_pool_weight is None:\n",
    "            #self.weight = torch.tensor([0.7, 0.3], device=self.transform.device)\n",
    "            self.pan_pool_weight = torch.nn.Parameter(0.5 * torch.ones(2), requires_grad=True)\n",
    "        else:\n",
    "            self.pan_pool_weight = pan_pool_weight\n",
    "\n",
    "    def forward(self, x, edge_index, M=None, batch=None, num_nodes=None):\n",
    "\n",
    "        \"\"\"\"\"\"\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        # Path integral\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "        edge_index, edge_weight = self.panentropy_sparse(edge_index, num_nodes)\n",
    "\n",
    "        # weighted degree\n",
    "        num_nodes = x.size(0)\n",
    "        degree = torch.zeros(num_nodes, device=edge_index.device)\n",
    "        degree = scatter_add(edge_weight, edge_index[0], out=degree)\n",
    "\n",
    "        # linear transform\n",
    "        xtransform = torch.matmul(x, self.transform)\n",
    "\n",
    "        # aggregate score\n",
    "        x_transform_norm = xtransform #/ xtransform.norm(p=2, dim=-1)\n",
    "        degree_norm = degree #/ degree.norm(p=2, dim=-1)\n",
    "        score = self.pan_pool_weight[0] * x_transform_norm + self.pan_pool_weight[1] * degree_norm\n",
    "\n",
    "        if self.min_score is None:\n",
    "            score = self.nonlinearity(score)\n",
    "        else:\n",
    "            score = softmax(score, batch)\n",
    "\n",
    "        perm = self.topk(score, self.ratio, batch, self.min_score)\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        x = self.multiplier * x if self.multiplier != 1 else x\n",
    "\n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_weight = self.filter_adj(edge_index, edge_weight, perm, num_nodes=score.size(0))\n",
    "\n",
    "        return x, edge_index, edge_weight, batch, perm, score[perm]\n",
    "\n",
    "    def topk(self, x, ratio, batch, min_score=None, tol=1e-7):\n",
    "\n",
    "        if min_score is not None:\n",
    "            # Make sure that we do not drop all nodes in a graph.\n",
    "            scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "            scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "            perm = torch.nonzero(x > scores_min).view(-1)\n",
    "        else:\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "            cum_num_nodes = torch.cat(\n",
    "                [num_nodes.new_zeros(1),\n",
    "                 num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "            index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "            index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "            dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n",
    "            dense_x[index] = x\n",
    "            dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "            _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "            perm = perm + cum_num_nodes.view(-1, 1)\n",
    "            perm = perm.view(-1)\n",
    "\n",
    "            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "            mask = [\n",
    "                torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "                i * max_num_nodes for i in range(batch_size)\n",
    "            ]\n",
    "            mask = torch.cat(mask, dim=0)\n",
    "\n",
    "            perm = perm[mask]\n",
    "\n",
    "        return perm\n",
    "\n",
    "    def filter_adj(self, edge_index, edge_weight, perm, num_nodes=None):\n",
    "\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "        mask = perm.new_full((num_nodes, ), -1)\n",
    "        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "        mask[perm] = i\n",
    "\n",
    "        row, col = edge_index\n",
    "        row, col = mask[row], mask[col]\n",
    "        mask = (row >= 0) & (col >= 0)\n",
    "        row, col = row[mask], col[mask]\n",
    "\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight[mask]\n",
    "\n",
    "        return torch.stack([row, col], dim=0), edge_weight\n",
    "\n",
    "    def panentropy_sparse(self, edge_index, num_nodes):\n",
    "\n",
    "        edge_value = torch.ones(edge_index.size(1), device=edge_index.device)\n",
    "        edge_index, edge_value = coalesce(edge_index, edge_value, num_nodes, num_nodes)\n",
    "\n",
    "        # iteratively add weighted matrix power\n",
    "        pan_index, pan_value = eye(num_nodes, device=edge_index.device)\n",
    "        indextmp = pan_index.clone().to(edge_index.device)\n",
    "        valuetmp = pan_value.clone().to(edge_index.device)\n",
    "\n",
    "        pan_value = self.panpool_filter_weight[0] * pan_value\n",
    "\n",
    "        for i in range(self.filter_size - 1):\n",
    "            #indextmp, valuetmp = coalesce(indextmp, valuetmp, num_nodes, num_nodes)\n",
    "            indextmp, valuetmp = spspmm(indextmp, valuetmp, edge_index, edge_value, num_nodes, num_nodes, num_nodes)\n",
    "            valuetmp = valuetmp * self.panpool_filter_weight[i+1]\n",
    "            indextmp, valuetmp = coalesce(indextmp, valuetmp, num_nodes, num_nodes)\n",
    "            pan_index = torch.cat((pan_index, indextmp), 1)\n",
    "            pan_value = torch.cat((pan_value, valuetmp))\n",
    "\n",
    "        return coalesce(pan_index, pan_value, num_nodes, num_nodes, op='add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: Loss = 1.0989, Val Acc = 0.4431\n",
      "Epoch 002: Loss = 1.0975, Val Acc = 0.4317\n",
      "Epoch 003: Loss = 1.0963, Val Acc = 0.4323\n",
      "Epoch 004: Loss = 1.0949, Val Acc = 0.4323\n",
      "Epoch 005: Loss = 1.0933, Val Acc = 0.4323\n",
      "Epoch 006: Loss = 1.0919, Val Acc = 0.4323\n",
      "Epoch 007: Loss = 1.0903, Val Acc = 0.4323\n",
      "Epoch 008: Loss = 1.0886, Val Acc = 0.4323\n",
      "Epoch 009: Loss = 1.0865, Val Acc = 0.4323\n",
      "Epoch 010: Loss = 1.0845, Val Acc = 0.4323\n",
      "Epoch 011: Loss = 1.0825, Val Acc = 0.4323\n",
      "Epoch 012: Loss = 1.0801, Val Acc = 0.4323\n",
      "Epoch 013: Loss = 1.0777, Val Acc = 0.4323\n",
      "Epoch 014: Loss = 1.0752, Val Acc = 0.4323\n",
      "Epoch 015: Loss = 1.0721, Val Acc = 0.4323\n",
      "Epoch 016: Loss = 1.0693, Val Acc = 0.4323\n",
      "Epoch 017: Loss = 1.0660, Val Acc = 0.4323\n",
      "Epoch 018: Loss = 1.0626, Val Acc = 0.4323\n",
      "Epoch 019: Loss = 1.0587, Val Acc = 0.4323\n",
      "Epoch 020: Loss = 1.0549, Val Acc = 0.4323\n",
      "Epoch 021: Loss = 1.0510, Val Acc = 0.4323\n",
      "Epoch 022: Loss = 1.0464, Val Acc = 0.4323\n",
      "Epoch 023: Loss = 1.0412, Val Acc = 0.4323\n",
      "Epoch 024: Loss = 1.0365, Val Acc = 0.4323\n",
      "Epoch 025: Loss = 1.0308, Val Acc = 0.4323\n",
      "Epoch 026: Loss = 1.0248, Val Acc = 0.4325\n",
      "Epoch 027: Loss = 1.0193, Val Acc = 0.4332\n",
      "Epoch 028: Loss = 1.0127, Val Acc = 0.4338\n",
      "Epoch 029: Loss = 1.0053, Val Acc = 0.4348\n",
      "Epoch 030: Loss = 0.9987, Val Acc = 0.4400\n",
      "Epoch 031: Loss = 0.9898, Val Acc = 0.4447\n",
      "Epoch 032: Loss = 0.9823, Val Acc = 0.4506\n",
      "Epoch 033: Loss = 0.9737, Val Acc = 0.4581\n",
      "Epoch 034: Loss = 0.9631, Val Acc = 0.4644\n",
      "Epoch 035: Loss = 0.9536, Val Acc = 0.4709\n",
      "Epoch 036: Loss = 0.9441, Val Acc = 0.4762\n",
      "Epoch 037: Loss = 0.9329, Val Acc = 0.4806\n",
      "Epoch 038: Loss = 0.9204, Val Acc = 0.4835\n",
      "Epoch 039: Loss = 0.9097, Val Acc = 0.4863\n",
      "Epoch 040: Loss = 0.8974, Val Acc = 0.4910\n",
      "Epoch 041: Loss = 0.8837, Val Acc = 0.4922\n",
      "Epoch 042: Loss = 0.8713, Val Acc = 0.4942\n",
      "Epoch 043: Loss = 0.8562, Val Acc = 0.4954\n",
      "Epoch 044: Loss = 0.8441, Val Acc = 0.4956\n",
      "Epoch 045: Loss = 0.8281, Val Acc = 0.4964\n",
      "Epoch 046: Loss = 0.8149, Val Acc = 0.5007\n",
      "Epoch 047: Loss = 0.8022, Val Acc = 0.5050\n",
      "Epoch 048: Loss = 0.7866, Val Acc = 0.5078\n",
      "Epoch 049: Loss = 0.7728, Val Acc = 0.5102\n",
      "Epoch 050: Loss = 0.7588, Val Acc = 0.5141\n",
      "Epoch 051: Loss = 0.7457, Val Acc = 0.5171\n",
      "Epoch 052: Loss = 0.7332, Val Acc = 0.5244\n",
      "Epoch 053: Loss = 0.7207, Val Acc = 0.5344\n",
      "Epoch 054: Loss = 0.7094, Val Acc = 0.5498\n",
      "Epoch 055: Loss = 0.7009, Val Acc = 0.5602\n",
      "Epoch 056: Loss = 0.6880, Val Acc = 0.5713\n",
      "Epoch 057: Loss = 0.6812, Val Acc = 0.5794\n",
      "Epoch 058: Loss = 0.6710, Val Acc = 0.5896\n",
      "Epoch 059: Loss = 0.6614, Val Acc = 0.6054\n",
      "Epoch 060: Loss = 0.6554, Val Acc = 0.6251\n",
      "Epoch 061: Loss = 0.6457, Val Acc = 0.6468\n",
      "Epoch 062: Loss = 0.6376, Val Acc = 0.6543\n",
      "Epoch 063: Loss = 0.6298, Val Acc = 0.6723\n",
      "Epoch 064: Loss = 0.6214, Val Acc = 0.7076\n",
      "Epoch 065: Loss = 0.6124, Val Acc = 0.7344\n",
      "Epoch 066: Loss = 0.6063, Val Acc = 0.7501\n",
      "Epoch 067: Loss = 0.5975, Val Acc = 0.7553\n",
      "Epoch 068: Loss = 0.5910, Val Acc = 0.7750\n",
      "Epoch 069: Loss = 0.5810, Val Acc = 0.7896\n",
      "Epoch 070: Loss = 0.5749, Val Acc = 0.8054\n",
      "Epoch 071: Loss = 0.5644, Val Acc = 0.8093\n",
      "Epoch 072: Loss = 0.5579, Val Acc = 0.8075\n",
      "Epoch 073: Loss = 0.5514, Val Acc = 0.8091\n",
      "Epoch 074: Loss = 0.5461, Val Acc = 0.8140\n",
      "Epoch 075: Loss = 0.5352, Val Acc = 0.8154\n",
      "Epoch 076: Loss = 0.5321, Val Acc = 0.8144\n",
      "Epoch 077: Loss = 0.5260, Val Acc = 0.8142\n",
      "Epoch 078: Loss = 0.5174, Val Acc = 0.8180\n",
      "Epoch 079: Loss = 0.5114, Val Acc = 0.8182\n",
      "Epoch 080: Loss = 0.5090, Val Acc = 0.8202\n",
      "Epoch 081: Loss = 0.5049, Val Acc = 0.8200\n",
      "Epoch 082: Loss = 0.4994, Val Acc = 0.8215\n",
      "Epoch 083: Loss = 0.4951, Val Acc = 0.8200\n",
      "Epoch 084: Loss = 0.4900, Val Acc = 0.8223\n",
      "Epoch 085: Loss = 0.4900, Val Acc = 0.8245\n",
      "Epoch 086: Loss = 0.4884, Val Acc = 0.8223\n",
      "Epoch 087: Loss = 0.4885, Val Acc = 0.8213\n",
      "Epoch 088: Loss = 0.4848, Val Acc = 0.8259\n",
      "Epoch 089: Loss = 0.4817, Val Acc = 0.8265\n",
      "Epoch 090: Loss = 0.4846, Val Acc = 0.8259\n",
      "Epoch 091: Loss = 0.4788, Val Acc = 0.8253\n",
      "Epoch 092: Loss = 0.4767, Val Acc = 0.8273\n",
      "Epoch 093: Loss = 0.4766, Val Acc = 0.8288\n",
      "Epoch 094: Loss = 0.4771, Val Acc = 0.8284\n",
      "Epoch 095: Loss = 0.4763, Val Acc = 0.8298\n",
      "Epoch 096: Loss = 0.4759, Val Acc = 0.8302\n",
      "Epoch 097: Loss = 0.4719, Val Acc = 0.8282\n",
      "Epoch 098: Loss = 0.4686, Val Acc = 0.8286\n",
      "Epoch 099: Loss = 0.4663, Val Acc = 0.8290\n",
      "Epoch 100: Loss = 0.4695, Val Acc = 0.8320\n",
      "Epoch 101: Loss = 0.4642, Val Acc = 0.8318\n",
      "Epoch 102: Loss = 0.4619, Val Acc = 0.8326\n",
      "Epoch 103: Loss = 0.4626, Val Acc = 0.8318\n",
      "Epoch 104: Loss = 0.4637, Val Acc = 0.8334\n",
      "Epoch 105: Loss = 0.4615, Val Acc = 0.8328\n",
      "Epoch 106: Loss = 0.4593, Val Acc = 0.8367\n",
      "Epoch 107: Loss = 0.4590, Val Acc = 0.8359\n",
      "Epoch 108: Loss = 0.4569, Val Acc = 0.8373\n",
      "Epoch 109: Loss = 0.4540, Val Acc = 0.8387\n",
      "Epoch 110: Loss = 0.4566, Val Acc = 0.8363\n",
      "Epoch 111: Loss = 0.4511, Val Acc = 0.8369\n",
      "Epoch 112: Loss = 0.4522, Val Acc = 0.8393\n",
      "Epoch 113: Loss = 0.4519, Val Acc = 0.8411\n",
      "Epoch 114: Loss = 0.4536, Val Acc = 0.8399\n",
      "Epoch 115: Loss = 0.4531, Val Acc = 0.8351\n",
      "Epoch 116: Loss = 0.4506, Val Acc = 0.8351\n",
      "Epoch 117: Loss = 0.4499, Val Acc = 0.8442\n",
      "Epoch 118: Loss = 0.4499, Val Acc = 0.8458\n",
      "Epoch 119: Loss = 0.4480, Val Acc = 0.8418\n",
      "Epoch 120: Loss = 0.4474, Val Acc = 0.8375\n",
      "Epoch 121: Loss = 0.4479, Val Acc = 0.8381\n",
      "Epoch 122: Loss = 0.4454, Val Acc = 0.8416\n",
      "Epoch 123: Loss = 0.4443, Val Acc = 0.8397\n",
      "Epoch 124: Loss = 0.4453, Val Acc = 0.8409\n",
      "Epoch 125: Loss = 0.4408, Val Acc = 0.8428\n",
      "Epoch 126: Loss = 0.4444, Val Acc = 0.8436\n",
      "Epoch 127: Loss = 0.4401, Val Acc = 0.8442\n",
      "Epoch 128: Loss = 0.4404, Val Acc = 0.8401\n",
      "Epoch 129: Loss = 0.4423, Val Acc = 0.8405\n",
      "Epoch 130: Loss = 0.4392, Val Acc = 0.8422\n",
      "Epoch 131: Loss = 0.4381, Val Acc = 0.8426\n",
      "Epoch 132: Loss = 0.4373, Val Acc = 0.8424\n",
      "Epoch 133: Loss = 0.4380, Val Acc = 0.8440\n",
      "Epoch 134: Loss = 0.4367, Val Acc = 0.8444\n",
      "Epoch 135: Loss = 0.4387, Val Acc = 0.8426\n",
      "Epoch 136: Loss = 0.4365, Val Acc = 0.8393\n",
      "Epoch 137: Loss = 0.4388, Val Acc = 0.8426\n",
      "Epoch 138: Loss = 0.4321, Val Acc = 0.8470\n",
      "Epoch 139: Loss = 0.4324, Val Acc = 0.8466\n",
      "Epoch 140: Loss = 0.4353, Val Acc = 0.8450\n",
      "Epoch 141: Loss = 0.4311, Val Acc = 0.8403\n",
      "Epoch 142: Loss = 0.4356, Val Acc = 0.8432\n",
      "Epoch 143: Loss = 0.4315, Val Acc = 0.8450\n",
      "Epoch 144: Loss = 0.4341, Val Acc = 0.8487\n",
      "Epoch 145: Loss = 0.4334, Val Acc = 0.8472\n",
      "Epoch 146: Loss = 0.4318, Val Acc = 0.8430\n",
      "Epoch 147: Loss = 0.4336, Val Acc = 0.8424\n",
      "Epoch 148: Loss = 0.4289, Val Acc = 0.8468\n",
      "Epoch 149: Loss = 0.4303, Val Acc = 0.8468\n",
      "Epoch 150: Loss = 0.4318, Val Acc = 0.8484\n",
      "Epoch 151: Loss = 0.4243, Val Acc = 0.8442\n",
      "Epoch 152: Loss = 0.4308, Val Acc = 0.8440\n",
      "Epoch 153: Loss = 0.4303, Val Acc = 0.8499\n",
      "Epoch 154: Loss = 0.4268, Val Acc = 0.8472\n",
      "Epoch 155: Loss = 0.4302, Val Acc = 0.8476\n",
      "Epoch 156: Loss = 0.4274, Val Acc = 0.8448\n",
      "Epoch 157: Loss = 0.4296, Val Acc = 0.8458\n",
      "Epoch 158: Loss = 0.4275, Val Acc = 0.8519\n",
      "Epoch 159: Loss = 0.4266, Val Acc = 0.8519\n",
      "Epoch 160: Loss = 0.4301, Val Acc = 0.8501\n",
      "Epoch 161: Loss = 0.4230, Val Acc = 0.8458\n",
      "Epoch 162: Loss = 0.4248, Val Acc = 0.8460\n",
      "Epoch 163: Loss = 0.4219, Val Acc = 0.8497\n",
      "Epoch 164: Loss = 0.4239, Val Acc = 0.8525\n",
      "Epoch 165: Loss = 0.4240, Val Acc = 0.8529\n",
      "Epoch 166: Loss = 0.4236, Val Acc = 0.8509\n",
      "Epoch 167: Loss = 0.4214, Val Acc = 0.8491\n",
      "Epoch 168: Loss = 0.4235, Val Acc = 0.8484\n",
      "Epoch 169: Loss = 0.4201, Val Acc = 0.8480\n",
      "Epoch 170: Loss = 0.4220, Val Acc = 0.8489\n",
      "Epoch 171: Loss = 0.4186, Val Acc = 0.8515\n",
      "Epoch 172: Loss = 0.4185, Val Acc = 0.8547\n",
      "Epoch 173: Loss = 0.4221, Val Acc = 0.8495\n",
      "Epoch 174: Loss = 0.4177, Val Acc = 0.8491\n",
      "Epoch 175: Loss = 0.4216, Val Acc = 0.8507\n",
      "Epoch 176: Loss = 0.4193, Val Acc = 0.8511\n",
      "Epoch 177: Loss = 0.4185, Val Acc = 0.8531\n",
      "Epoch 178: Loss = 0.4210, Val Acc = 0.8541\n",
      "Epoch 179: Loss = 0.4198, Val Acc = 0.8509\n",
      "Epoch 180: Loss = 0.4205, Val Acc = 0.8547\n",
      "Epoch 181: Loss = 0.4161, Val Acc = 0.8555\n",
      "Epoch 182: Loss = 0.4188, Val Acc = 0.8553\n",
      "Epoch 183: Loss = 0.4146, Val Acc = 0.8519\n",
      "Epoch 184: Loss = 0.4153, Val Acc = 0.8497\n",
      "Epoch 185: Loss = 0.4128, Val Acc = 0.8493\n",
      "Epoch 186: Loss = 0.4145, Val Acc = 0.8527\n",
      "Epoch 187: Loss = 0.4127, Val Acc = 0.8543\n",
      "Epoch 188: Loss = 0.4159, Val Acc = 0.8560\n",
      "Epoch 189: Loss = 0.4137, Val Acc = 0.8489\n",
      "Epoch 190: Loss = 0.4143, Val Acc = 0.8505\n",
      "Epoch 191: Loss = 0.4146, Val Acc = 0.8545\n",
      "Epoch 192: Loss = 0.4152, Val Acc = 0.8560\n",
      "Epoch 193: Loss = 0.4108, Val Acc = 0.8497\n",
      "Epoch 194: Loss = 0.4138, Val Acc = 0.8493\n",
      "Epoch 195: Loss = 0.4118, Val Acc = 0.8523\n",
      "Epoch 196: Loss = 0.4109, Val Acc = 0.8555\n",
      "Epoch 197: Loss = 0.4123, Val Acc = 0.8560\n",
      "Epoch 198: Loss = 0.4111, Val Acc = 0.8551\n",
      "Epoch 199: Loss = 0.4098, Val Acc = 0.8545\n",
      "Epoch 200: Loss = 0.4123, Val Acc = 0.8553\n",
      "Epoch 001: Loss = 1.0985, Val Acc = 0.4350\n",
      "Epoch 002: Loss = 1.0971, Val Acc = 0.4342\n",
      "Epoch 003: Loss = 1.0957, Val Acc = 0.4334\n",
      "Epoch 004: Loss = 1.0943, Val Acc = 0.4332\n",
      "Epoch 005: Loss = 1.0929, Val Acc = 0.4334\n",
      "Epoch 006: Loss = 1.0914, Val Acc = 0.4340\n",
      "Epoch 007: Loss = 1.0897, Val Acc = 0.4352\n",
      "Epoch 008: Loss = 1.0878, Val Acc = 0.4362\n",
      "Epoch 009: Loss = 1.0857, Val Acc = 0.4366\n",
      "Epoch 010: Loss = 1.0835, Val Acc = 0.4370\n",
      "Epoch 011: Loss = 1.0812, Val Acc = 0.4380\n",
      "Epoch 012: Loss = 1.0787, Val Acc = 0.4394\n",
      "Epoch 013: Loss = 1.0759, Val Acc = 0.4427\n",
      "Epoch 014: Loss = 1.0724, Val Acc = 0.4441\n",
      "Epoch 015: Loss = 1.0694, Val Acc = 0.4459\n",
      "Epoch 016: Loss = 1.0656, Val Acc = 0.4480\n",
      "Epoch 017: Loss = 1.0618, Val Acc = 0.4500\n",
      "Epoch 018: Loss = 1.0576, Val Acc = 0.4524\n",
      "Epoch 019: Loss = 1.0524, Val Acc = 0.4553\n",
      "Epoch 020: Loss = 1.0473, Val Acc = 0.4577\n",
      "Epoch 021: Loss = 1.0416, Val Acc = 0.4618\n",
      "Epoch 022: Loss = 1.0358, Val Acc = 0.4640\n",
      "Epoch 023: Loss = 1.0292, Val Acc = 0.4672\n",
      "Epoch 024: Loss = 1.0219, Val Acc = 0.4701\n",
      "Epoch 025: Loss = 1.0148, Val Acc = 0.4723\n",
      "Epoch 026: Loss = 1.0062, Val Acc = 0.4756\n",
      "Epoch 027: Loss = 0.9988, Val Acc = 0.4786\n",
      "Epoch 028: Loss = 0.9888, Val Acc = 0.4820\n",
      "Epoch 029: Loss = 0.9787, Val Acc = 0.4839\n",
      "Epoch 030: Loss = 0.9681, Val Acc = 0.4855\n",
      "Epoch 031: Loss = 0.9564, Val Acc = 0.4875\n",
      "Epoch 032: Loss = 0.9448, Val Acc = 0.4889\n",
      "Epoch 033: Loss = 0.9329, Val Acc = 0.4904\n",
      "Epoch 034: Loss = 0.9193, Val Acc = 0.4918\n",
      "Epoch 035: Loss = 0.9058, Val Acc = 0.4932\n",
      "Epoch 036: Loss = 0.8914, Val Acc = 0.4938\n",
      "Epoch 037: Loss = 0.8755, Val Acc = 0.4944\n",
      "Epoch 038: Loss = 0.8591, Val Acc = 0.4964\n",
      "Epoch 039: Loss = 0.8443, Val Acc = 0.4971\n",
      "Epoch 040: Loss = 0.8286, Val Acc = 0.4973\n",
      "Epoch 041: Loss = 0.8137, Val Acc = 0.4991\n",
      "Epoch 042: Loss = 0.7973, Val Acc = 0.5023\n",
      "Epoch 043: Loss = 0.7804, Val Acc = 0.5127\n",
      "Epoch 044: Loss = 0.7647, Val Acc = 0.5212\n",
      "Epoch 045: Loss = 0.7512, Val Acc = 0.5311\n",
      "Epoch 046: Loss = 0.7359, Val Acc = 0.5455\n",
      "Epoch 047: Loss = 0.7248, Val Acc = 0.5555\n",
      "Epoch 048: Loss = 0.7124, Val Acc = 0.5632\n",
      "Epoch 049: Loss = 0.7005, Val Acc = 0.5819\n",
      "Epoch 050: Loss = 0.6897, Val Acc = 0.5999\n",
      "Epoch 051: Loss = 0.6793, Val Acc = 0.6174\n",
      "Epoch 052: Loss = 0.6668, Val Acc = 0.6364\n",
      "Epoch 053: Loss = 0.6606, Val Acc = 0.6547\n",
      "Epoch 054: Loss = 0.6506, Val Acc = 0.6770\n",
      "Epoch 055: Loss = 0.6431, Val Acc = 0.7028\n",
      "Epoch 056: Loss = 0.6348, Val Acc = 0.7186\n",
      "Epoch 057: Loss = 0.6250, Val Acc = 0.7354\n",
      "Epoch 058: Loss = 0.6202, Val Acc = 0.7472\n",
      "Epoch 059: Loss = 0.6084, Val Acc = 0.7576\n",
      "Epoch 060: Loss = 0.6014, Val Acc = 0.7738\n",
      "Epoch 061: Loss = 0.5936, Val Acc = 0.7791\n",
      "Epoch 062: Loss = 0.5851, Val Acc = 0.7856\n",
      "Epoch 063: Loss = 0.5777, Val Acc = 0.7892\n",
      "Epoch 064: Loss = 0.5688, Val Acc = 0.7925\n",
      "Epoch 065: Loss = 0.5615, Val Acc = 0.7979\n",
      "Epoch 066: Loss = 0.5553, Val Acc = 0.8018\n",
      "Epoch 067: Loss = 0.5498, Val Acc = 0.8020\n",
      "Epoch 068: Loss = 0.5396, Val Acc = 0.8034\n",
      "Epoch 069: Loss = 0.5341, Val Acc = 0.8060\n",
      "Epoch 070: Loss = 0.5305, Val Acc = 0.8109\n",
      "Epoch 071: Loss = 0.5239, Val Acc = 0.8107\n",
      "Epoch 072: Loss = 0.5161, Val Acc = 0.8107\n",
      "Epoch 073: Loss = 0.5113, Val Acc = 0.8121\n",
      "Epoch 074: Loss = 0.5085, Val Acc = 0.8164\n",
      "Epoch 075: Loss = 0.5066, Val Acc = 0.8146\n",
      "Epoch 076: Loss = 0.5020, Val Acc = 0.8156\n",
      "Epoch 077: Loss = 0.5018, Val Acc = 0.8194\n",
      "Epoch 078: Loss = 0.4974, Val Acc = 0.8211\n",
      "Epoch 079: Loss = 0.4946, Val Acc = 0.8176\n",
      "Epoch 080: Loss = 0.4916, Val Acc = 0.8178\n",
      "Epoch 081: Loss = 0.4900, Val Acc = 0.8227\n",
      "Epoch 082: Loss = 0.4891, Val Acc = 0.8235\n",
      "Epoch 083: Loss = 0.4868, Val Acc = 0.8227\n",
      "Epoch 084: Loss = 0.4888, Val Acc = 0.8209\n",
      "Epoch 085: Loss = 0.4840, Val Acc = 0.8282\n",
      "Epoch 086: Loss = 0.4841, Val Acc = 0.8288\n",
      "Epoch 087: Loss = 0.4818, Val Acc = 0.8253\n",
      "Epoch 088: Loss = 0.4812, Val Acc = 0.8235\n",
      "Epoch 089: Loss = 0.4773, Val Acc = 0.8302\n",
      "Epoch 090: Loss = 0.4763, Val Acc = 0.8328\n",
      "Epoch 091: Loss = 0.4753, Val Acc = 0.8302\n",
      "Epoch 092: Loss = 0.4698, Val Acc = 0.8271\n",
      "Epoch 093: Loss = 0.4723, Val Acc = 0.8318\n",
      "Epoch 094: Loss = 0.4717, Val Acc = 0.8347\n",
      "Epoch 095: Loss = 0.4696, Val Acc = 0.8349\n",
      "Epoch 096: Loss = 0.4660, Val Acc = 0.8302\n",
      "Epoch 097: Loss = 0.4664, Val Acc = 0.8306\n",
      "Epoch 098: Loss = 0.4643, Val Acc = 0.8342\n",
      "Epoch 099: Loss = 0.4645, Val Acc = 0.8361\n",
      "Epoch 100: Loss = 0.4615, Val Acc = 0.8365\n",
      "Epoch 101: Loss = 0.4651, Val Acc = 0.8336\n",
      "Epoch 102: Loss = 0.4617, Val Acc = 0.8332\n",
      "Epoch 103: Loss = 0.4599, Val Acc = 0.8353\n",
      "Epoch 104: Loss = 0.4587, Val Acc = 0.8371\n",
      "Epoch 105: Loss = 0.4586, Val Acc = 0.8387\n",
      "Epoch 106: Loss = 0.4574, Val Acc = 0.8353\n",
      "Epoch 107: Loss = 0.4595, Val Acc = 0.8357\n",
      "Epoch 108: Loss = 0.4543, Val Acc = 0.8369\n",
      "Epoch 109: Loss = 0.4579, Val Acc = 0.8375\n",
      "Epoch 110: Loss = 0.4513, Val Acc = 0.8375\n",
      "Epoch 111: Loss = 0.4537, Val Acc = 0.8381\n",
      "Epoch 112: Loss = 0.4534, Val Acc = 0.8397\n",
      "Epoch 113: Loss = 0.4536, Val Acc = 0.8387\n",
      "Epoch 114: Loss = 0.4535, Val Acc = 0.8379\n",
      "Epoch 115: Loss = 0.4506, Val Acc = 0.8375\n",
      "Epoch 116: Loss = 0.4485, Val Acc = 0.8397\n",
      "Epoch 117: Loss = 0.4498, Val Acc = 0.8409\n",
      "Epoch 118: Loss = 0.4486, Val Acc = 0.8397\n",
      "Epoch 119: Loss = 0.4475, Val Acc = 0.8381\n",
      "Epoch 120: Loss = 0.4462, Val Acc = 0.8381\n",
      "Epoch 121: Loss = 0.4456, Val Acc = 0.8385\n",
      "Epoch 122: Loss = 0.4435, Val Acc = 0.8416\n",
      "Epoch 123: Loss = 0.4449, Val Acc = 0.8434\n",
      "Epoch 124: Loss = 0.4444, Val Acc = 0.8424\n",
      "Epoch 125: Loss = 0.4445, Val Acc = 0.8405\n",
      "Epoch 126: Loss = 0.4446, Val Acc = 0.8397\n",
      "Epoch 127: Loss = 0.4432, Val Acc = 0.8393\n",
      "Epoch 128: Loss = 0.4400, Val Acc = 0.8432\n",
      "Epoch 129: Loss = 0.4391, Val Acc = 0.8432\n",
      "Epoch 130: Loss = 0.4386, Val Acc = 0.8432\n",
      "Epoch 131: Loss = 0.4361, Val Acc = 0.8413\n",
      "Epoch 132: Loss = 0.4364, Val Acc = 0.8401\n",
      "Epoch 133: Loss = 0.4408, Val Acc = 0.8416\n",
      "Epoch 134: Loss = 0.4357, Val Acc = 0.8430\n",
      "Epoch 135: Loss = 0.4382, Val Acc = 0.8448\n",
      "Epoch 136: Loss = 0.4375, Val Acc = 0.8456\n",
      "Epoch 137: Loss = 0.4383, Val Acc = 0.8428\n",
      "Epoch 138: Loss = 0.4310, Val Acc = 0.8409\n",
      "Epoch 139: Loss = 0.4349, Val Acc = 0.8434\n",
      "Epoch 140: Loss = 0.4337, Val Acc = 0.8454\n",
      "Epoch 141: Loss = 0.4318, Val Acc = 0.8484\n",
      "Epoch 142: Loss = 0.4341, Val Acc = 0.8448\n",
      "Epoch 143: Loss = 0.4337, Val Acc = 0.8430\n",
      "Epoch 144: Loss = 0.4333, Val Acc = 0.8436\n",
      "Epoch 145: Loss = 0.4300, Val Acc = 0.8474\n",
      "Epoch 146: Loss = 0.4300, Val Acc = 0.8491\n",
      "Epoch 147: Loss = 0.4298, Val Acc = 0.8440\n",
      "Epoch 148: Loss = 0.4285, Val Acc = 0.8448\n",
      "Epoch 149: Loss = 0.4323, Val Acc = 0.8497\n",
      "Epoch 150: Loss = 0.4311, Val Acc = 0.8474\n",
      "Epoch 151: Loss = 0.4272, Val Acc = 0.8436\n",
      "Epoch 152: Loss = 0.4296, Val Acc = 0.8458\n",
      "Epoch 153: Loss = 0.4286, Val Acc = 0.8527\n",
      "Epoch 154: Loss = 0.4294, Val Acc = 0.8521\n",
      "Epoch 155: Loss = 0.4321, Val Acc = 0.8460\n",
      "Epoch 156: Loss = 0.4257, Val Acc = 0.8440\n",
      "Epoch 157: Loss = 0.4247, Val Acc = 0.8452\n",
      "Epoch 158: Loss = 0.4215, Val Acc = 0.8497\n",
      "Epoch 159: Loss = 0.4232, Val Acc = 0.8519\n",
      "Epoch 160: Loss = 0.4286, Val Acc = 0.8466\n",
      "Epoch 161: Loss = 0.4234, Val Acc = 0.8462\n",
      "Epoch 162: Loss = 0.4218, Val Acc = 0.8523\n",
      "Epoch 163: Loss = 0.4238, Val Acc = 0.8489\n",
      "Epoch 164: Loss = 0.4252, Val Acc = 0.8450\n",
      "Epoch 165: Loss = 0.4174, Val Acc = 0.8450\n",
      "Epoch 166: Loss = 0.4205, Val Acc = 0.8489\n",
      "Epoch 167: Loss = 0.4233, Val Acc = 0.8517\n",
      "Epoch 168: Loss = 0.4271, Val Acc = 0.8505\n",
      "Epoch 169: Loss = 0.4208, Val Acc = 0.8460\n",
      "Epoch 170: Loss = 0.4189, Val Acc = 0.8466\n",
      "Epoch 171: Loss = 0.4161, Val Acc = 0.8515\n",
      "Epoch 172: Loss = 0.4195, Val Acc = 0.8517\n",
      "Epoch 173: Loss = 0.4185, Val Acc = 0.8491\n",
      "Epoch 174: Loss = 0.4201, Val Acc = 0.8472\n",
      "Epoch 175: Loss = 0.4167, Val Acc = 0.8478\n",
      "Epoch 176: Loss = 0.4170, Val Acc = 0.8515\n",
      "Epoch 177: Loss = 0.4165, Val Acc = 0.8509\n",
      "Epoch 178: Loss = 0.4163, Val Acc = 0.8497\n",
      "Epoch 179: Loss = 0.4197, Val Acc = 0.8474\n",
      "Epoch 180: Loss = 0.4167, Val Acc = 0.8482\n",
      "Epoch 181: Loss = 0.4170, Val Acc = 0.8523\n",
      "Epoch 182: Loss = 0.4181, Val Acc = 0.8539\n",
      "Epoch 183: Loss = 0.4141, Val Acc = 0.8517\n",
      "Epoch 184: Loss = 0.4162, Val Acc = 0.8491\n",
      "Epoch 185: Loss = 0.4151, Val Acc = 0.8491\n",
      "Epoch 186: Loss = 0.4140, Val Acc = 0.8535\n",
      "Epoch 187: Loss = 0.4156, Val Acc = 0.8539\n",
      "Epoch 188: Loss = 0.4126, Val Acc = 0.8519\n",
      "Epoch 189: Loss = 0.4154, Val Acc = 0.8525\n",
      "Epoch 190: Loss = 0.4130, Val Acc = 0.8521\n",
      "Epoch 191: Loss = 0.4159, Val Acc = 0.8531\n",
      "Epoch 192: Loss = 0.4118, Val Acc = 0.8533\n",
      "Epoch 193: Loss = 0.4108, Val Acc = 0.8539\n",
      "Epoch 194: Loss = 0.4083, Val Acc = 0.8519\n",
      "Epoch 195: Loss = 0.4090, Val Acc = 0.8543\n",
      "Epoch 196: Loss = 0.4106, Val Acc = 0.8539\n",
      "Epoch 197: Loss = 0.4130, Val Acc = 0.8525\n",
      "Epoch 198: Loss = 0.4104, Val Acc = 0.8527\n",
      "Epoch 199: Loss = 0.4107, Val Acc = 0.8547\n",
      "Epoch 200: Loss = 0.4102, Val Acc = 0.8560\n",
      "Epoch 001: Loss = 1.0988, Val Acc = 0.5194\n",
      "Epoch 002: Loss = 1.0972, Val Acc = 0.4611\n",
      "Epoch 003: Loss = 1.0957, Val Acc = 0.4394\n",
      "Epoch 004: Loss = 1.0943, Val Acc = 0.4350\n",
      "Epoch 005: Loss = 1.0926, Val Acc = 0.4336\n",
      "Epoch 006: Loss = 1.0910, Val Acc = 0.4329\n",
      "Epoch 007: Loss = 1.0892, Val Acc = 0.4327\n",
      "Epoch 008: Loss = 1.0870, Val Acc = 0.4327\n",
      "Epoch 009: Loss = 1.0851, Val Acc = 0.4327\n",
      "Epoch 010: Loss = 1.0824, Val Acc = 0.4327\n",
      "Epoch 011: Loss = 1.0800, Val Acc = 0.4327\n",
      "Epoch 012: Loss = 1.0770, Val Acc = 0.4329\n",
      "Epoch 013: Loss = 1.0743, Val Acc = 0.4332\n",
      "Epoch 014: Loss = 1.0710, Val Acc = 0.4334\n",
      "Epoch 015: Loss = 1.0674, Val Acc = 0.4346\n",
      "Epoch 016: Loss = 1.0637, Val Acc = 0.4352\n",
      "Epoch 017: Loss = 1.0592, Val Acc = 0.4372\n",
      "Epoch 018: Loss = 1.0547, Val Acc = 0.4386\n",
      "Epoch 019: Loss = 1.0501, Val Acc = 0.4403\n",
      "Epoch 020: Loss = 1.0449, Val Acc = 0.4417\n",
      "Epoch 021: Loss = 1.0396, Val Acc = 0.4449\n",
      "Epoch 022: Loss = 1.0337, Val Acc = 0.4492\n",
      "Epoch 023: Loss = 1.0274, Val Acc = 0.4534\n",
      "Epoch 024: Loss = 1.0209, Val Acc = 0.4567\n",
      "Epoch 025: Loss = 1.0133, Val Acc = 0.4601\n",
      "Epoch 026: Loss = 1.0062, Val Acc = 0.4650\n",
      "Epoch 027: Loss = 0.9972, Val Acc = 0.4697\n",
      "Epoch 028: Loss = 0.9891, Val Acc = 0.4768\n",
      "Epoch 029: Loss = 0.9792, Val Acc = 0.4800\n",
      "Epoch 030: Loss = 0.9699, Val Acc = 0.4845\n",
      "Epoch 031: Loss = 0.9596, Val Acc = 0.4875\n",
      "Epoch 032: Loss = 0.9495, Val Acc = 0.4908\n",
      "Epoch 033: Loss = 0.9377, Val Acc = 0.4946\n",
      "Epoch 034: Loss = 0.9254, Val Acc = 0.4952\n",
      "Epoch 035: Loss = 0.9121, Val Acc = 0.4954\n",
      "Epoch 036: Loss = 0.8985, Val Acc = 0.4964\n",
      "Epoch 037: Loss = 0.8856, Val Acc = 0.4973\n",
      "Epoch 038: Loss = 0.8724, Val Acc = 0.4979\n",
      "Epoch 039: Loss = 0.8577, Val Acc = 0.4983\n",
      "Epoch 040: Loss = 0.8427, Val Acc = 0.4989\n",
      "Epoch 041: Loss = 0.8287, Val Acc = 0.4995\n",
      "Epoch 042: Loss = 0.8152, Val Acc = 0.5021\n",
      "Epoch 043: Loss = 0.8011, Val Acc = 0.5060\n",
      "Epoch 044: Loss = 0.7879, Val Acc = 0.5086\n",
      "Epoch 045: Loss = 0.7737, Val Acc = 0.5107\n",
      "Epoch 046: Loss = 0.7624, Val Acc = 0.5131\n",
      "Epoch 047: Loss = 0.7500, Val Acc = 0.5161\n",
      "Epoch 048: Loss = 0.7377, Val Acc = 0.5259\n",
      "Epoch 049: Loss = 0.7259, Val Acc = 0.5382\n",
      "Epoch 050: Loss = 0.7165, Val Acc = 0.5518\n",
      "Epoch 051: Loss = 0.7077, Val Acc = 0.5642\n",
      "Epoch 052: Loss = 0.6970, Val Acc = 0.5786\n",
      "Epoch 053: Loss = 0.6892, Val Acc = 0.5928\n",
      "Epoch 054: Loss = 0.6818, Val Acc = 0.6048\n",
      "Epoch 055: Loss = 0.6711, Val Acc = 0.6188\n",
      "Epoch 056: Loss = 0.6618, Val Acc = 0.6381\n",
      "Epoch 057: Loss = 0.6532, Val Acc = 0.6583\n",
      "Epoch 058: Loss = 0.6427, Val Acc = 0.6732\n",
      "Epoch 059: Loss = 0.6367, Val Acc = 0.7030\n",
      "Epoch 060: Loss = 0.6275, Val Acc = 0.7216\n",
      "Epoch 061: Loss = 0.6192, Val Acc = 0.7367\n",
      "Epoch 062: Loss = 0.6080, Val Acc = 0.7547\n",
      "Epoch 063: Loss = 0.6036, Val Acc = 0.7819\n",
      "Epoch 064: Loss = 0.5930, Val Acc = 0.7971\n",
      "Epoch 065: Loss = 0.5843, Val Acc = 0.8032\n",
      "Epoch 066: Loss = 0.5761, Val Acc = 0.8069\n",
      "Epoch 067: Loss = 0.5642, Val Acc = 0.8075\n",
      "Epoch 068: Loss = 0.5563, Val Acc = 0.8093\n",
      "Epoch 069: Loss = 0.5510, Val Acc = 0.8123\n",
      "Epoch 070: Loss = 0.5448, Val Acc = 0.8123\n",
      "Epoch 071: Loss = 0.5379, Val Acc = 0.8113\n",
      "Epoch 072: Loss = 0.5298, Val Acc = 0.8123\n",
      "Epoch 073: Loss = 0.5249, Val Acc = 0.8136\n",
      "Epoch 074: Loss = 0.5176, Val Acc = 0.8162\n",
      "Epoch 075: Loss = 0.5121, Val Acc = 0.8182\n",
      "Epoch 076: Loss = 0.5106, Val Acc = 0.8174\n",
      "Epoch 077: Loss = 0.5068, Val Acc = 0.8184\n",
      "Epoch 078: Loss = 0.5012, Val Acc = 0.8196\n",
      "Epoch 079: Loss = 0.4971, Val Acc = 0.8215\n",
      "Epoch 080: Loss = 0.4932, Val Acc = 0.8200\n",
      "Epoch 081: Loss = 0.4946, Val Acc = 0.8229\n",
      "Epoch 082: Loss = 0.4894, Val Acc = 0.8245\n",
      "Epoch 083: Loss = 0.4863, Val Acc = 0.8219\n",
      "Epoch 084: Loss = 0.4869, Val Acc = 0.8245\n",
      "Epoch 085: Loss = 0.4851, Val Acc = 0.8269\n",
      "Epoch 086: Loss = 0.4827, Val Acc = 0.8278\n",
      "Epoch 087: Loss = 0.4790, Val Acc = 0.8251\n",
      "Epoch 088: Loss = 0.4791, Val Acc = 0.8245\n",
      "Epoch 089: Loss = 0.4797, Val Acc = 0.8280\n",
      "Epoch 090: Loss = 0.4757, Val Acc = 0.8302\n",
      "Epoch 091: Loss = 0.4747, Val Acc = 0.8278\n",
      "Epoch 092: Loss = 0.4710, Val Acc = 0.8249\n",
      "Epoch 093: Loss = 0.4744, Val Acc = 0.8276\n",
      "Epoch 094: Loss = 0.4733, Val Acc = 0.8298\n",
      "Epoch 095: Loss = 0.4687, Val Acc = 0.8324\n",
      "Epoch 096: Loss = 0.4667, Val Acc = 0.8322\n",
      "Epoch 097: Loss = 0.4649, Val Acc = 0.8296\n",
      "Epoch 098: Loss = 0.4654, Val Acc = 0.8324\n",
      "Epoch 099: Loss = 0.4622, Val Acc = 0.8340\n",
      "Epoch 100: Loss = 0.4596, Val Acc = 0.8344\n",
      "Epoch 101: Loss = 0.4616, Val Acc = 0.8357\n",
      "Epoch 102: Loss = 0.4574, Val Acc = 0.8330\n",
      "Epoch 103: Loss = 0.4561, Val Acc = 0.8344\n",
      "Epoch 104: Loss = 0.4578, Val Acc = 0.8340\n",
      "Epoch 105: Loss = 0.4561, Val Acc = 0.8377\n",
      "Epoch 106: Loss = 0.4580, Val Acc = 0.8407\n",
      "Epoch 107: Loss = 0.4521, Val Acc = 0.8391\n",
      "Epoch 108: Loss = 0.4536, Val Acc = 0.8407\n",
      "Epoch 109: Loss = 0.4546, Val Acc = 0.8381\n",
      "Epoch 110: Loss = 0.4510, Val Acc = 0.8381\n",
      "Epoch 111: Loss = 0.4509, Val Acc = 0.8428\n",
      "Epoch 112: Loss = 0.4499, Val Acc = 0.8440\n",
      "Epoch 113: Loss = 0.4487, Val Acc = 0.8426\n",
      "Epoch 114: Loss = 0.4493, Val Acc = 0.8381\n",
      "Epoch 115: Loss = 0.4466, Val Acc = 0.8397\n",
      "Epoch 116: Loss = 0.4455, Val Acc = 0.8401\n",
      "Epoch 117: Loss = 0.4454, Val Acc = 0.8444\n",
      "Epoch 118: Loss = 0.4453, Val Acc = 0.8399\n",
      "Epoch 119: Loss = 0.4490, Val Acc = 0.8401\n",
      "Epoch 120: Loss = 0.4408, Val Acc = 0.8448\n",
      "Epoch 121: Loss = 0.4419, Val Acc = 0.8428\n",
      "Epoch 122: Loss = 0.4413, Val Acc = 0.8422\n",
      "Epoch 123: Loss = 0.4440, Val Acc = 0.8379\n",
      "Epoch 124: Loss = 0.4440, Val Acc = 0.8420\n",
      "Epoch 125: Loss = 0.4390, Val Acc = 0.8448\n",
      "Epoch 126: Loss = 0.4422, Val Acc = 0.8460\n",
      "Epoch 127: Loss = 0.4369, Val Acc = 0.8420\n",
      "Epoch 128: Loss = 0.4369, Val Acc = 0.8426\n",
      "Epoch 129: Loss = 0.4399, Val Acc = 0.8442\n",
      "Epoch 130: Loss = 0.4364, Val Acc = 0.8446\n",
      "Epoch 131: Loss = 0.4343, Val Acc = 0.8450\n",
      "Epoch 132: Loss = 0.4332, Val Acc = 0.8432\n",
      "Epoch 133: Loss = 0.4348, Val Acc = 0.8452\n",
      "Epoch 134: Loss = 0.4383, Val Acc = 0.8487\n",
      "Epoch 135: Loss = 0.4329, Val Acc = 0.8464\n",
      "Epoch 136: Loss = 0.4331, Val Acc = 0.8430\n",
      "Epoch 137: Loss = 0.4315, Val Acc = 0.8413\n",
      "Epoch 138: Loss = 0.4352, Val Acc = 0.8430\n",
      "Epoch 139: Loss = 0.4286, Val Acc = 0.8470\n",
      "Epoch 140: Loss = 0.4308, Val Acc = 0.8482\n",
      "Epoch 141: Loss = 0.4308, Val Acc = 0.8476\n",
      "Epoch 142: Loss = 0.4267, Val Acc = 0.8448\n",
      "Epoch 143: Loss = 0.4287, Val Acc = 0.8450\n",
      "Epoch 144: Loss = 0.4306, Val Acc = 0.8472\n",
      "Epoch 145: Loss = 0.4263, Val Acc = 0.8464\n",
      "Epoch 146: Loss = 0.4254, Val Acc = 0.8468\n",
      "Epoch 147: Loss = 0.4254, Val Acc = 0.8466\n",
      "Epoch 148: Loss = 0.4259, Val Acc = 0.8480\n",
      "Epoch 149: Loss = 0.4272, Val Acc = 0.8499\n",
      "Epoch 150: Loss = 0.4277, Val Acc = 0.8484\n",
      "Epoch 151: Loss = 0.4260, Val Acc = 0.8480\n",
      "Epoch 152: Loss = 0.4248, Val Acc = 0.8446\n",
      "Epoch 153: Loss = 0.4284, Val Acc = 0.8480\n",
      "Epoch 154: Loss = 0.4232, Val Acc = 0.8503\n",
      "Epoch 155: Loss = 0.4263, Val Acc = 0.8521\n",
      "Epoch 156: Loss = 0.4248, Val Acc = 0.8480\n",
      "Epoch 157: Loss = 0.4206, Val Acc = 0.8466\n",
      "Epoch 158: Loss = 0.4211, Val Acc = 0.8517\n",
      "Epoch 159: Loss = 0.4229, Val Acc = 0.8517\n",
      "Epoch 160: Loss = 0.4207, Val Acc = 0.8507\n",
      "Epoch 161: Loss = 0.4190, Val Acc = 0.8517\n",
      "Epoch 162: Loss = 0.4208, Val Acc = 0.8501\n",
      "Epoch 163: Loss = 0.4215, Val Acc = 0.8515\n",
      "Epoch 164: Loss = 0.4181, Val Acc = 0.8529\n",
      "Epoch 165: Loss = 0.4166, Val Acc = 0.8513\n",
      "Epoch 166: Loss = 0.4201, Val Acc = 0.8468\n",
      "Epoch 167: Loss = 0.4226, Val Acc = 0.8495\n",
      "Epoch 168: Loss = 0.4195, Val Acc = 0.8519\n",
      "Epoch 169: Loss = 0.4188, Val Acc = 0.8537\n",
      "Epoch 170: Loss = 0.4191, Val Acc = 0.8470\n",
      "Epoch 171: Loss = 0.4164, Val Acc = 0.8462\n",
      "Epoch 172: Loss = 0.4165, Val Acc = 0.8515\n",
      "Epoch 173: Loss = 0.4169, Val Acc = 0.8515\n",
      "Epoch 174: Loss = 0.4200, Val Acc = 0.8541\n",
      "Epoch 175: Loss = 0.4174, Val Acc = 0.8507\n",
      "Epoch 176: Loss = 0.4174, Val Acc = 0.8503\n",
      "Epoch 177: Loss = 0.4174, Val Acc = 0.8535\n",
      "Epoch 178: Loss = 0.4145, Val Acc = 0.8539\n",
      "Epoch 179: Loss = 0.4124, Val Acc = 0.8537\n",
      "Epoch 180: Loss = 0.4145, Val Acc = 0.8529\n",
      "Epoch 181: Loss = 0.4149, Val Acc = 0.8535\n",
      "Epoch 182: Loss = 0.4144, Val Acc = 0.8545\n",
      "Epoch 183: Loss = 0.4116, Val Acc = 0.8553\n",
      "Epoch 184: Loss = 0.4120, Val Acc = 0.8547\n",
      "Epoch 185: Loss = 0.4133, Val Acc = 0.8547\n",
      "Epoch 186: Loss = 0.4157, Val Acc = 0.8558\n",
      "Epoch 187: Loss = 0.4119, Val Acc = 0.8547\n",
      "Epoch 188: Loss = 0.4122, Val Acc = 0.8537\n",
      "Epoch 189: Loss = 0.4126, Val Acc = 0.8529\n",
      "Epoch 190: Loss = 0.4093, Val Acc = 0.8543\n",
      "Epoch 191: Loss = 0.4091, Val Acc = 0.8549\n",
      "Epoch 192: Loss = 0.4121, Val Acc = 0.8560\n",
      "Epoch 193: Loss = 0.4099, Val Acc = 0.8541\n",
      "Epoch 194: Loss = 0.4081, Val Acc = 0.8535\n",
      "Epoch 195: Loss = 0.4103, Val Acc = 0.8564\n",
      "Epoch 196: Loss = 0.4120, Val Acc = 0.8568\n",
      "Epoch 197: Loss = 0.4057, Val Acc = 0.8541\n",
      "Epoch 198: Loss = 0.4080, Val Acc = 0.8541\n",
      "Epoch 199: Loss = 0.4102, Val Acc = 0.8549\n",
      "Epoch 200: Loss = 0.4092, Val Acc = 0.8551\n",
      "Micro F1: 85.55$\\pm$0.03\n",
      "Macro F1: 82.16$\\pm$0.20\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import ASAPooling\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_PubMed.num_classes\n",
    "in_channels = dataset_PubMed.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_PAN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_PAN, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(PANPooling(channels, ratio=pool_ratios[i]))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, score_perm = self.pools[i - 1](x, edge_index, batch=batch, M=None)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义训练和评估函数\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "        # Print progress\n",
    "        print(f'Epoch {epoch:03d}: Loss = {loss.item():.4f}, Val Acc = {val_acc:.4f}')\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_PAN(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "class GPR_prop(MessagePassing):\n",
    "    '''\n",
    "    propagation class for GPR_GNN\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, alpha, Init, Gamma=None, bias=True, **kwargs):\n",
    "        super(GPR_prop, self).__init__(aggr='add', **kwargs)\n",
    "        self.K = K\n",
    "        self.Init = Init\n",
    "        self.alpha = alpha\n",
    "\n",
    "        assert Init in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']\n",
    "        if Init == 'SGC':\n",
    "            # SGC-like\n",
    "            TEMP = 0.0*np.ones(K+1)\n",
    "            TEMP[alpha] = 1.0\n",
    "        elif Init == 'PPR':\n",
    "            # PPR-like\n",
    "            TEMP = alpha*(1-alpha)**np.arange(K+1)\n",
    "            TEMP[-1] = (1-alpha)**K\n",
    "        elif Init == 'NPPR':\n",
    "            # Negative PPR\n",
    "            TEMP = (alpha)**np.arange(K+1)\n",
    "            TEMP = TEMP/np.sum(np.abs(TEMP))\n",
    "        elif Init == 'Random':\n",
    "            # Random\n",
    "            bound = np.sqrt(3/(K+1))\n",
    "            TEMP = np.random.uniform(-bound, bound, K+1)\n",
    "            TEMP = TEMP/np.sum(np.abs(TEMP))\n",
    "        elif Init == 'WS':\n",
    "            # Specify Gamma\n",
    "            TEMP = Gamma\n",
    "\n",
    "        self.temp = Parameter(torch.tensor(TEMP))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.zeros_(self.temp)\n",
    "        for k in range(self.K+1):\n",
    "            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n",
    "        self.temp.data[-1] = (1-self.alpha)**self.K\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        edge_index, norm = gcn_norm(\n",
    "            edge_index, edge_weight, num_nodes=x.size(0), dtype=x.dtype)\n",
    "\n",
    "        hidden = x*(self.temp[0])\n",
    "        for k in range(self.K):\n",
    "            x = self.propagate(edge_index, x=x, norm=norm)\n",
    "            gamma = self.temp[k+1]\n",
    "            hidden = hidden + gamma*x\n",
    "        return hidden\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n",
    "                                           self.temp)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) # in case all the edges are removed\n",
    "\n",
    "        edge_index = edge_index.type(torch.long)\n",
    "        row, col = edge_index\n",
    "        # print(row, col)\n",
    "        # print(edge_weight.shape, row.shape, num_nodes)\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        # row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "class graph_attention(torch.nn.Module):\n",
    "    # reference: https://github.com/gordicaleksa/pytorch-GAT/blob/39c8f0ee634477033e8b1a6e9a6da3c7ed71bbd1/models/definitions/GAT.py#L324\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    nodes_dim = 0      # node dimension/axis\n",
    "    head_dim = 1       # attention head dimension/axis\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, dropout_prob=0.6, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Saving these as we'll need them in forward propagation in children layers (imp1/2/3)\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        # You can treat this one matrix as num_of_heads independent W matrices\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the additive scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
    "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
    "        Feel free to experiment - there may be better initializations depending on your problem.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features = x  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = scores_source_lifted + scores_target_lifted\n",
    "\n",
    "        return torch.sigmoid(scores_per_edge)\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "\n",
    "\n",
    "class CoPooling(torch.nn.Module):\n",
    "    # reference for GAT code: https://github.com/PetarV-/GAT\n",
    "    # reference for generalized pagerank code: https://github.com/jianhao2016/GPRGNN\n",
    "    def __init__(self, ratio=0.5, K=0.05, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=None):\n",
    "        super(CoPooling, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "        self.edge_ratio = edge_ratio\n",
    "\n",
    "        self.prop1 = GPR_prop(K, alpha, Init, Gamma)\n",
    "\n",
    "        score_dim = 32\n",
    "        self.G_att = graph_attention(num_in_features=nhid, num_out_features=score_dim, num_of_heads=1)\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(2*nhid, nhid))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        self.bias = Parameter(torch.Tensor(nhid))\n",
    "        nn.init.zeros_(self.bias.data)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        nn.init.zeros_(self.bias.data)\n",
    "        self.prop1.reset_parameters()\n",
    "        self.G_att.init_params()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None, nodes_index=None, node_attr=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        ori_batch = batch.clone()\n",
    "        device = x.device\n",
    "        num_nodes = x.shape[0]\n",
    "\n",
    "        # cut edges based on scores\n",
    "        x_cut = self.prop1(x, edge_index) # run generalized pagerank to update features\n",
    "\n",
    "        attention = self.G_att(x_cut, edge_index) # get the attention weights after sigmoid\n",
    "        attention = attention.sum(dim=1) #sum the weights on head dim\n",
    "        edge_index, attention = add_self_loops(edge_index, attention, 1.0, num_nodes) # add self loops in case no edges\n",
    "\n",
    "        # to get a systemitic adj matrix\n",
    "        edge_index_t, attention_t = transpose(edge_index, attention, num_nodes, num_nodes)\n",
    "        edge_tmp = torch.cat((edge_index, edge_index_t), 1)\n",
    "        att_tmp = torch.cat((attention, attention_t),0)\n",
    "        edge_index, attention = coalesce(edge_tmp, att_tmp, num_nodes, num_nodes, 'mean')\n",
    "\n",
    "        attention_np = attention.cpu().data.numpy()\n",
    "        cut_val = np.percentile(attention_np, int(100*(1-self.edge_ratio))) # this is for keep the top edge_ratio edges\n",
    "        attention = attention * (attention >= cut_val) # keep the edge_ratio higher weights of edges\n",
    "\n",
    "        kep_idx = attention > 0.0\n",
    "        cut_edge_index, cut_edge_attr = edge_index[:, kep_idx], attention[kep_idx]\n",
    "\n",
    "        # Graph Pooling based on nodes\n",
    "        x_information_score = self.calc_information_score(x, cut_edge_index, cut_edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x_topk = x[perm]\n",
    "        batch = batch[perm]\n",
    "        if nodes_index is not None:\n",
    "            nodes_index = nodes_index[perm]\n",
    "\n",
    "        if node_attr is not None:\n",
    "            node_attr = node_attr[perm]\n",
    "        if cut_edge_index is not None or cut_edge_index.nelement() != 0:\n",
    "            induced_edge_index, induced_edge_attr = filter_adj(cut_edge_index, cut_edge_attr, perm, num_nodes=num_nodes)\n",
    "        else:\n",
    "            print('All edges are cut!')\n",
    "            induced_edge_index, induced_edge_attr = cut_edge_index, cut_edge_attr\n",
    "\n",
    "        # update node features\n",
    "        attention_dense = (to_dense_adj(cut_edge_index, edge_attr=cut_edge_attr, max_num_nodes=num_nodes)).squeeze()\n",
    "        x = F.relu(torch.matmul(torch.cat((x_topk, torch.matmul(attention_dense[perm],x)), 1), self.weight) + self.bias)\n",
    "\n",
    "        return x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 84.99$\\pm$0.35\n",
      "Macro F1: 81.81$\\pm$0.29\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import ASAPooling\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_PubMed.num_classes\n",
    "in_channels = dataset_PubMed.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_CO(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_CO, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(CoPooling(ratio=pool_ratios[i], K=1, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=1.0))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, perm, _, batch, _, _, _ = self.pools[i - 1](x, edge_index, edge_attr=None, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义训练和评估函数\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_CO(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGIPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "@dataclass(init=False)\n",
    "class SelectOutput:\n",
    "    r\"\"\"The output of the :class:`Select` method, which holds an assignment\n",
    "    from selected nodes to their respective cluster(s).\n",
    "\n",
    "    Args:\n",
    "        node_index (torch.Tensor): The indices of the selected nodes.\n",
    "        num_nodes (int): The number of nodes.\n",
    "        cluster_index (torch.Tensor): The indices of the clusters each node in\n",
    "            :obj:`node_index` is assigned to.\n",
    "        num_clusters (int): The number of clusters.\n",
    "        weight (torch.Tensor, optional): A weight vector, denoting the strength\n",
    "            of the assignment of a node to its cluster. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    node_index: Tensor\n",
    "    num_nodes: int\n",
    "    cluster_index: Tensor\n",
    "    num_clusters: int\n",
    "    weight: Optional[Tensor] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_index: Tensor,\n",
    "        num_nodes: int,\n",
    "        cluster_index: Tensor,\n",
    "        num_clusters: int,\n",
    "        weight: Optional[Tensor] = None,\n",
    "    ):\n",
    "        if node_index.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'node_index' to be one-dimensional \"\n",
    "                             f\"(got {node_index.dim()} dimensions)\")\n",
    "\n",
    "        if cluster_index.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'cluster_index' to be one-dimensional \"\n",
    "                             f\"(got {cluster_index.dim()} dimensions)\")\n",
    "\n",
    "        if node_index.numel() != cluster_index.numel():\n",
    "            raise ValueError(f\"Expected 'node_index' and 'cluster_index' to \"\n",
    "                             f\"hold the same number of values (got \"\n",
    "                             f\"{node_index.numel()} and \"\n",
    "                             f\"{cluster_index.numel()} values)\")\n",
    "\n",
    "        if weight is not None and weight.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'weight' vector to be one-dimensional \"\n",
    "                             f\"(got {weight.dim()} dimensions)\")\n",
    "\n",
    "        if weight is not None and weight.numel() != node_index.numel():\n",
    "            raise ValueError(f\"Expected 'weight' to hold {node_index.numel()} \"\n",
    "                             f\"values (got {weight.numel()} values)\")\n",
    "\n",
    "        self.node_index = node_index\n",
    "        self.num_nodes = num_nodes\n",
    "        self.cluster_index = cluster_index\n",
    "        self.num_clusters = num_clusters\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Select(torch.nn.Module):\n",
    "    r\"\"\"An abstract base class for implementing custom node selections as\n",
    "    described in the `\"Understanding Pooling in Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1905.05178>`_ paper, which maps the nodes of an\n",
    "    input graph to supernodes in the coarsened graph.\n",
    "\n",
    "    Specifically, :class:`Select` returns a :class:`SelectOutput` output, which\n",
    "    holds a (sparse) mapping :math:`\\mathbf{C} \\in {[0, 1]}^{N \\times C}` that\n",
    "    assigns selected nodes to one or more of :math:`C` super nodes.\n",
    "    \"\"\"\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> SelectOutput:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'\n",
    "\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels * 2, in_channels)\n",
    "        self.fc2 = nn.Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CGIPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.5, non_lin=torch.tanh):\n",
    "        super(CGIPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.non_lin = non_lin\n",
    "        self.hidden_dim = in_channels\n",
    "        self.transform = GraphConv(in_channels, self.hidden_dim)\n",
    "        self.pp_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n",
    "        self.np_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        self.positive_pooling = GraphConv(self.hidden_dim, 1)\n",
    "        self.negative_pooling = GraphConv(self.hidden_dim, 1)\n",
    "\n",
    "        self.discriminator = Discriminator(self.hidden_dim)\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        device = x.device  # 获取输入张量的设备信息\n",
    "\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_transform = F.leaky_relu(self.transform(x, edge_index), 0.2)\n",
    "        x_tp = F.leaky_relu(self.pp_conv(x, edge_index), 0.2)\n",
    "        x_tn = F.leaky_relu(self.np_conv(x, edge_index), 0.2)\n",
    "        s_pp = self.positive_pooling(x_tp, edge_index).squeeze()\n",
    "        s_np = self.negative_pooling(x_tn, edge_index).squeeze()\n",
    "\n",
    "        perm_positive = topk(s_pp, 1, batch)\n",
    "        perm_negative = topk(s_np, 1, batch)\n",
    "        x_pp = x_transform[perm_positive] * self.non_lin(s_pp[perm_positive]).view(-1, 1)\n",
    "        x_np = x_transform[perm_negative] * self.non_lin(s_np[perm_negative]).view(-1, 1)\n",
    "\n",
    "        x_pp_readout = gap(x_pp, batch[perm_positive])\n",
    "        x_np_readout = gap(x_np, batch[perm_negative])\n",
    "        x_readout = gap(x_transform, batch)\n",
    "\n",
    "        positive_pair = torch.cat([x_pp_readout, x_readout], dim=1)\n",
    "        negative_pair = torch.cat([x_np_readout, x_readout], dim=1)\n",
    "\n",
    "        real = torch.ones(positive_pair.shape[0], device=device)  # 将张量移动到相应设备\n",
    "        fake = torch.zeros(negative_pair.shape[0], device=device)  # 将张量移动到相应设备\n",
    "        #real_loss = self.loss_fn(self.discriminator(positive_pair), real)\n",
    "        #fake_loss = self.loss_fn(self.discriminator(negative_pair), fake)\n",
    "        #discrimination_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        score = (s_pp - s_np)\n",
    "\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x_transform[perm] * self.non_lin(score[perm]).view(-1, 1)\n",
    "        batch = batch[perm]\n",
    "\n",
    "        filter_edge_index, filter_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "        return x, filter_edge_index, filter_edge_attr, batch, perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 84.97$\\pm$0.75\n",
      "Macro F1: 81.74$\\pm$0.90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_PubMed.num_classes\n",
    "in_channels = dataset_PubMed.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_CGI(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_CGI, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(CGIPool(channels, ratio=pool_ratios[i]))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义训练和评估函数\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_CGI(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMISPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Tuple, Union\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\n",
    "Scorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\n",
    "from torch_sparse import SparseTensor, remove_diag\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.nn.dense import Linear\n",
    "from torch.nn import Module\n",
    "from torch_scatter import scatter_max, scatter_min\n",
    "\n",
    "def maximal_independent_set(edge_index: Adj, k: int = 1,\n",
    "                            perm: OptTensor = None) -> Tensor:\n",
    "    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n",
    "    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n",
    "    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n",
    "    returned set.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method follows `Blelloch's Alogirithm\n",
    "    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n",
    "    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n",
    "    higher values of :math:`k`.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: :class:`ByteTensor`\n",
    "    \"\"\"\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "        device = edge_index.device()\n",
    "        n = edge_index.size(0)\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        device = row.device\n",
    "        n = edge_index.max().item() + 1\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    mis = torch.zeros(n, dtype=torch.bool, device=device)\n",
    "    mask = mis.clone()\n",
    "    min_rank = rank.clone()\n",
    "\n",
    "    while not mask.all():\n",
    "        for _ in range(k):\n",
    "            min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "            scatter_min(min_rank[row], col, out=min_neigh)\n",
    "            torch.minimum(min_neigh, min_rank, out=min_rank)  # self-loops\n",
    "\n",
    "        mis = mis | torch.eq(rank, min_rank)\n",
    "        mask = mis.clone().byte()\n",
    "\n",
    "        for _ in range(k):\n",
    "            max_neigh = torch.full_like(mask, fill_value=0)\n",
    "            scatter_max(mask[row], col, out=max_neigh)\n",
    "            torch.maximum(max_neigh, mask, out=mask)  # self-loops\n",
    "\n",
    "        mask = mask.to(dtype=torch.bool)\n",
    "        min_rank = rank.clone()\n",
    "        min_rank[mask] = n\n",
    "\n",
    "    return mis\n",
    "\n",
    "def maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n",
    "                                    perm: OptTensor = None) -> PairTensor:\n",
    "    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n",
    "    clustering of a graph, as defined in `\"Generalizing Downsampling from\n",
    "    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method returns both the :math:`k`-MIS and the clustering, where the\n",
    "    :math:`c`-th cluster refers to the :math:`c`-th element of the\n",
    "    :math:`k`-MIS.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n",
    "    \"\"\"\n",
    "    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n",
    "    n, device = mis.size(0), mis.device\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n",
    "    rank_mis = rank[mis]\n",
    "    min_rank[mis] = rank_mis\n",
    "\n",
    "    for _ in range(k):\n",
    "        min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "        scatter_min(min_rank[row], col, out=min_neigh)\n",
    "        torch.minimum(min_neigh, min_rank, out=min_rank)\n",
    "\n",
    "    _, clusters = torch.unique(min_rank, return_inverse=True)\n",
    "    perm = torch.argsort(rank_mis)\n",
    "    return mis, perm[clusters]\n",
    "\n",
    "\n",
    "class KMISPooling(Module):\n",
    "\n",
    "    _heuristics = {None, 'greedy', 'w-greedy'}\n",
    "    _passthroughs = {None, 'before', 'after'}\n",
    "    _scorers = {\n",
    "        'linear',\n",
    "        'random',\n",
    "        'constant',\n",
    "        'canonical',\n",
    "        'first',\n",
    "        'last',\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n",
    "                 scorer: Union[Scorer, str] = 'linear',\n",
    "                 score_heuristic: Optional[str] = 'greedy',\n",
    "                 score_passthrough: Optional[str] = 'before',\n",
    "                 aggr_x: Optional[Union[str, Aggregation]] = None,\n",
    "                 aggr_edge: str = 'sum',\n",
    "                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n",
    "                 remove_self_loops: bool = True) -> None:\n",
    "        super(KMISPooling, self).__init__()\n",
    "        assert score_heuristic in self._heuristics, \\\n",
    "            \"Unrecognized `score_heuristic` value.\"\n",
    "        assert score_passthrough in self._passthroughs, \\\n",
    "            \"Unrecognized `score_passthrough` value.\"\n",
    "\n",
    "        if not callable(scorer):\n",
    "            assert scorer in self._scorers, \\\n",
    "                \"Unrecognized `scorer` value.\"\n",
    "\n",
    "        self.k = k\n",
    "        self.scorer = scorer\n",
    "        self.score_heuristic = score_heuristic\n",
    "        self.score_passthrough = score_passthrough\n",
    "\n",
    "        self.aggr_x = aggr_x\n",
    "        self.aggr_edge = aggr_edge\n",
    "        self.aggr_score = aggr_score\n",
    "        self.remove_self_loops = remove_self_loops\n",
    "\n",
    "        if scorer == 'linear':\n",
    "            assert self.score_passthrough is not None, \\\n",
    "                \"`'score_passthrough'` must not be `None`\" \\\n",
    "                \" when using `'linear'` scorer\"\n",
    "\n",
    "            self.lin = nn.Linear(in_channels, 1)\n",
    "\n",
    "    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n",
    "        if self.score_heuristic is None:\n",
    "            return x\n",
    "\n",
    "        row, col, _ = adj.coo()\n",
    "        x = x.view(-1)\n",
    "\n",
    "        if self.score_heuristic == 'greedy':\n",
    "            k_sums = torch.ones_like(x)\n",
    "        else:\n",
    "            k_sums = x.clone()\n",
    "\n",
    "        for _ in range(self.k):\n",
    "            scatter_add(k_sums[row], col, out=k_sums)\n",
    "\n",
    "        return x / k_sums\n",
    "\n",
    "    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) -> Tensor:\n",
    "        if self.scorer == 'linear':\n",
    "            return self.lin(x).sigmoid()\n",
    "\n",
    "        if self.scorer == 'random':\n",
    "            return torch.rand((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'constant':\n",
    "            return torch.ones((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'canonical':\n",
    "            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n",
    "\n",
    "        if self.scorer == 'first':\n",
    "            return x[..., [0]]\n",
    "\n",
    "        if self.scorer == 'last':\n",
    "            return x[..., [-1]]\n",
    "\n",
    "        return self.scorer(x, edge_index, edge_attr, batch)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) \\\n",
    "            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n",
    "        \"\"\"\"\"\"\n",
    "        edge_index = edge_index.long()\n",
    "        adj, n = edge_index, x.size(0)\n",
    "\n",
    "        if not isinstance(edge_index, SparseTensor):\n",
    "            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n",
    "\n",
    "        score = self._scorer(x, edge_index, edge_attr, batch)\n",
    "        updated_score = self._apply_heuristic(score, adj)\n",
    "        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n",
    "\n",
    "        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n",
    "\n",
    "        row, col, val = adj.coo()\n",
    "        c = mis.sum()\n",
    "\n",
    "        if val is None:\n",
    "            val = torch.ones_like(row, dtype=torch.float)\n",
    "\n",
    "        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n",
    "                           is_sorted=False,\n",
    "                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n",
    "\n",
    "        if self.remove_self_loops:\n",
    "            adj = remove_diag(adj)\n",
    "\n",
    "        if self.score_passthrough == 'before':\n",
    "            x = self.aggr_score(x, score)\n",
    "\n",
    "        if self.aggr_x is None:\n",
    "            x = x[mis]\n",
    "        elif isinstance(self.aggr_x, str):\n",
    "            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n",
    "                        reduce=self.aggr_x)\n",
    "        else:\n",
    "            x = self.aggr_x(x, cluster, dim_size=c)\n",
    "\n",
    "        if self.score_passthrough == 'after':\n",
    "            x = self.aggr_score(x, score[mis])\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_index, edge_attr = adj, None\n",
    "\n",
    "        else:\n",
    "            row, col, edge_attr = adj.coo()\n",
    "            edge_index = torch.stack([row, col])\n",
    "\n",
    "        if batch is not None:\n",
    "            batch = batch[mis]\n",
    "\n",
    "        #attn = x\n",
    "        #select_out = topk(attn, batch)\n",
    "        perm = perm[mis]\n",
    "\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, mis, cluster, perm\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.scorer == 'linear':\n",
    "            channels = f\"in_channels={self.lin.in_channels}, \"\n",
    "        else:\n",
    "            channels = \"\"\n",
    "\n",
    "        return f'{self.__class__.__name__}({channels}k={self.k})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 85.66$\\pm$0.07\n",
      "Macro F1: 82.50$\\pm$0.12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_PubMed.num_classes\n",
    "in_channels = dataset_PubMed.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(64, k=1, aggr_x='sum'))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义训练和评估函数\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_KMIS(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSAPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Union, Optional, Callable\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, ChebConv, GraphConv\n",
    "\n",
    "def uniform(size, tensor):\n",
    "    if tensor is not None:\n",
    "        bound = 1.0 / math.sqrt(size)\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "\n",
    "def topk(x, ratio, batch, min_score=None, tol=1e-7):\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n",
    "    else:\n",
    "        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "        cum_num_nodes = torch.cat(\n",
    "            [num_nodes.new_zeros(1),\n",
    "             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "        dense_x = x.new_full((batch_size * max_num_nodes, ),\n",
    "                             torch.finfo(x.dtype).min)\n",
    "        dense_x[index] = x\n",
    "        dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "        _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "        perm = perm + cum_num_nodes.view(-1, 1)\n",
    "        perm = perm.view(-1)\n",
    "\n",
    "        if isinstance(ratio, int):\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n",
    "            k = torch.min(k, num_nodes)\n",
    "        else:\n",
    "            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "\n",
    "        mask = [\n",
    "            torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "            i * max_num_nodes for i in range(batch_size)\n",
    "        ]\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "\n",
    "        perm = perm[mask]\n",
    "\n",
    "    return perm\n",
    "\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "\n",
    "class GSAPool(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n",
    "                        min_score=None, multiplier=1,\n",
    "                        non_linearity=torch.tanh,\n",
    "                        cus_drop_ratio =0):\n",
    "        super(GSAPool,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.ratio = pooling_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.sbtl_layer = GCNConv(in_channels,1)\n",
    "        self.fbtl_layer = nn.Linear(in_channels, 1)\n",
    "        self.fusion = GCNConv(in_channels,in_channels)\n",
    "\n",
    "        self.min_score = min_score\n",
    "        self.multiplier = multiplier\n",
    "        self.fusion_flag = 0\n",
    "        self.non_linearity = non_linearity\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n",
    "\n",
    "    def conv_selection(self, conv, in_channels, conv_type=0):\n",
    "        if(conv_type == 0):\n",
    "            out_channels = 1\n",
    "        elif(conv_type == 1):\n",
    "            out_channels = in_channels\n",
    "        if(conv == \"GCNConv\"):\n",
    "            return GCNConv(in_channels,out_channels)\n",
    "        elif(conv == \"ChebConv\"):\n",
    "            return ChebConv(in_channels,out_channels,1)\n",
    "        elif(conv == \"SAGEConv\"):\n",
    "            return SAGEConv(in_channels,out_channels)\n",
    "        elif(conv == \"GATConv\"):\n",
    "            return GATConv(in_channels,out_channels, heads=1, concat=True)\n",
    "        elif(conv == \"GraphConv\"):\n",
    "            return GraphConv(in_channels,out_channels)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "\n",
    "        #SBTL\n",
    "        score_s = self.sbtl_layer(x,edge_index).squeeze()\n",
    "        #FBTL\n",
    "        score_f = self.fbtl_layer(x).squeeze()\n",
    "        #hyperparametr alpha\n",
    "        score = score_s*self.alpha + score_f*(1-self.alpha)\n",
    "\n",
    "        score = score.unsqueeze(-1) if score.dim()==0 else score\n",
    "\n",
    "        if self.min_score is None:\n",
    "            score = self.non_linearity(score)\n",
    "        else:\n",
    "            score = softmax(score, batch)\n",
    "\n",
    "        sc = self.dropout(score)\n",
    "        perm = topk(sc, self.ratio, batch)\n",
    "\n",
    "        #fusion\n",
    "        if(self.fusion_flag == 1):\n",
    "            x = self.fusion(x, edge_index)\n",
    "        x_ae = x[perm]\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        x = self.multiplier * x if self.multiplier != 1 else x\n",
    "\n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_attr = filter_adj(\n",
    "            edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, perm, x_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1: 85.83$\\pm$0.32\n",
      "Macro F1: 82.70$\\pm$0.50\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_PubMed.num_classes\n",
    "in_channels = dataset_PubMed.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(GSAPool(64, pooling_ratio=pool_ratios[i], alpha = 0.6, cus_drop_ratio = 0))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, edge_attr, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义训练和评估函数\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "def compute_f1_scores(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    y_true = graph.y[mask].cpu().numpy()\n",
    "    y_pred = pred[mask].cpu().numpy()\n",
    "    micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return micro_f1, macro_f1\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_GSA(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HGPSLPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5344, 0.0000, 0.0000, 0.4656, 0.0613, 0.0000, 0.0000, 0.0000, 0.5640,\n",
      "        0.3748])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
    "def topk(x, ratio, batch, min_score=None, tol=1e-7):\n",
    "\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = torch.nonzero(x > scores_min).view(-1)\n",
    "    else:\n",
    "        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "        cum_num_nodes = torch.cat(\n",
    "            [num_nodes.new_zeros(1),\n",
    "            num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n",
    "        dense_x[index] = x\n",
    "        dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "        _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "        perm = perm + cum_num_nodes.view(-1, 1)\n",
    "        perm = perm.view(-1)\n",
    "\n",
    "        k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "        mask = [\n",
    "            torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "            i * max_num_nodes for i in range(batch_size)\n",
    "        ]\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "\n",
    "        perm = perm[mask]\n",
    "\n",
    "    return perm\n",
    "\n",
    "def filter_adj(edge_index, edge_weight, perm, num_nodes=None):\n",
    "\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "        mask = perm.new_full((num_nodes, ), -1)\n",
    "        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "        mask[perm] = i\n",
    "\n",
    "        row, col = edge_index\n",
    "        row, col = mask[row], mask[col]\n",
    "        mask = (row >= 0) & (col >= 0)\n",
    "        row, col = row[mask], col[mask]\n",
    "\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight[mask]\n",
    "\n",
    "        return torch.stack([row, col], dim=0), edge_weight\n",
    "\n",
    "def scatter_sort(x, batch, fill_value=-1e16):\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n",
    "    dense_x[index] = x\n",
    "    dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n",
    "    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n",
    "    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n",
    "\n",
    "    sorted_x = sorted_x.view(-1)\n",
    "    filled_index = sorted_x != fill_value\n",
    "\n",
    "    sorted_x = sorted_x[filled_index]\n",
    "    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n",
    "\n",
    "    return sorted_x, cumsum_sorted_x\n",
    "\n",
    "\n",
    "def _make_ix_like(batch):\n",
    "    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n",
    "    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n",
    "    idx = torch.cat(idx, dim=0)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _threshold_and_support(x, batch):\n",
    "    \"\"\"Sparsemax building block: compute the threshold\n",
    "    Args:\n",
    "        x: input tensor to apply the sparsemax\n",
    "        batch: group indicators\n",
    "    Returns:\n",
    "        the threshold value\n",
    "    \"\"\"\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    sorted_input, input_cumsum = scatter_sort(x, batch)\n",
    "    input_cumsum = input_cumsum - 1.0\n",
    "    rhos = _make_ix_like(batch).to(x.dtype)\n",
    "    support = rhos * sorted_input > input_cumsum\n",
    "\n",
    "    support_size = scatter_add(support.to(batch.dtype), batch)\n",
    "    # mask invalid index, for example, if batch is not start from 0 or not continuous, it may result in negative index\n",
    "    idx = support_size + cum_num_nodes - 1\n",
    "    mask = idx < 0\n",
    "    idx[mask] = 0\n",
    "    tau = input_cumsum.gather(0, idx)\n",
    "    tau /= support_size.to(x.dtype)\n",
    "\n",
    "    return tau, support_size\n",
    "\n",
    "\n",
    "class SparsemaxFunction(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, batch):\n",
    "        \"\"\"sparsemax: normalizing sparse transform\n",
    "        Parameters:\n",
    "            ctx: context object\n",
    "            x (Tensor): shape (N, )\n",
    "            batch: group indicator\n",
    "        Returns:\n",
    "            output (Tensor): same shape as input\n",
    "        \"\"\"\n",
    "        max_val, _ = scatter_max(x, batch)\n",
    "        x -= max_val[batch]\n",
    "        tau, supp_size = _threshold_and_support(x, batch)\n",
    "        output = torch.clamp(x - tau[batch], min=0)\n",
    "        ctx.save_for_backward(supp_size, output, batch)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output, batch = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n",
    "\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        return sparsemax(x, batch)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sparse_attention = Sparsemax()\n",
    "    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n",
    "    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n",
    "    res = sparse_attention(input_x, input_batch)\n",
    "    print(res)\n",
    "\n",
    "class TwoHopNeighborhood(object):\n",
    "    def __call__(self, data):\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "        n = data.num_nodes\n",
    "\n",
    "        fill = 1e16\n",
    "        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n",
    "\n",
    "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n",
    "\n",
    "        edge_index = torch.cat([edge_index, index], dim=1)\n",
    "        if edge_attr is None:\n",
    "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
    "        else:\n",
    "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
    "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
    "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
    "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min', fill_value=fill)\n",
    "            edge_attr[edge_attr >= fill] = 0\n",
    "            data.edge_attr = edge_attr\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)\n",
    "\n",
    "\n",
    "class GCN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
    "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            nn.init.zeros_(self.bias.data)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
    "\n",
    "        row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "class HGPSLPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n",
    "        super(HGPSLPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.sample = sample\n",
    "        self.sparse = sparse\n",
    "        self.sl = sl\n",
    "        self.negative_slop = negative_slop\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att.data)\n",
    "        self.sparse_attention = Sparsemax()\n",
    "        self.neighbor_augment = TwoHopNeighborhood()\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "\n",
    "        # Graph Pooling\n",
    "        original_x = x\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x[perm]\n",
    "        batch = batch[perm]\n",
    "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        # Discard structure learning layer, directly return\n",
    "        if self.sl is False:\n",
    "            return x, induced_edge_index, induced_edge_attr, batch\n",
    "\n",
    "        # Structure Learning\n",
    "        if self.sample:\n",
    "            # A fast mode for large graphs.\n",
    "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
    "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
    "            # edge weights between them.\n",
    "            k_hop = 3\n",
    "            if edge_attr is None:\n",
    "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
    "\n",
    "            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            for _ in range(k_hop - 1):\n",
    "                hop_data = self.neighbor_augment(hop_data)\n",
    "            hop_edge_index = hop_data.edge_index\n",
    "            hop_edge_attr = hop_data.edge_attr\n",
    "            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
    "            row, col = new_edge_index\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            adj[row, col] = weights\n",
    "            new_edge_index, weights = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
    "            if edge_attr is None:\n",
    "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
    "                                               device=induced_edge_index.device)\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            # Construct batch fully connected graph in block diagonal matirx format\n",
    "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
    "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
    "            new_edge_index, _ = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop)\n",
    "            adj[row, col] = weights\n",
    "            induced_row, induced_col = induced_edge_index\n",
    "\n",
    "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
    "            weights = adj[row, col]\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, num_nodes=x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x, new_edge_index, new_edge_attr, batch, perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 45.42 GiB. GPU 0 has a total capacty of 79.21 GiB of which 1.85 GiB is free. Process 303209 has 4.89 GiB memory in use. Including non-PyTorch memory, this process has 72.46 GiB memory in use. Of the allocated memory 27.87 GiB is allocated by PyTorch, and 43.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m model, best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_node_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Compute F1 scores\u001b[39;00m\n\u001b[1;32m    131\u001b[0m micro_f1, macro_f1 \u001b[38;5;241m=\u001b[39m compute_f1_scores(model, data, data\u001b[38;5;241m.\u001b[39mval_mask)\n",
      "Cell \u001b[0;32mIn[19], line 87\u001b[0m, in \u001b[0;36mtrain_node_classifier\u001b[0;34m(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs, patience, min_delta)\u001b[0m\n\u001b[1;32m     85\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     86\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 87\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[train_mask], graph\u001b[38;5;241m.\u001b[39my[train_mask])\n\u001b[1;32m     89\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 53\u001b[0m, in \u001b[0;36mHierarchicalGCN_HGPSL.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     50\u001b[0m perms \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 53\u001b[0m     x, edge_index, _, batch, perm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpools\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_convs[i](x, edge_index)\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 402\u001b[0m, in \u001b[0;36mHGPSLPool.forward\u001b[0;34m(self, x, edge_index, edge_attr, batch)\u001b[0m\n\u001b[1;32m    399\u001b[0m new_edge_index, _ \u001b[38;5;241m=\u001b[39m dense_to_sparse(adj)\n\u001b[1;32m    400\u001b[0m row, col \u001b[38;5;241m=\u001b[39m new_edge_index\n\u001b[0;32m--> 402\u001b[0m weights \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m, x[col]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    403\u001b[0m weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_slop)\n\u001b[1;32m    404\u001b[0m adj[row, col] \u001b[38;5;241m=\u001b[39m weights\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 45.42 GiB. GPU 0 has a total capacty of 79.21 GiB of which 1.85 GiB is free. Process 303209 has 4.89 GiB memory in use. Including non-PyTorch memory, this process has 72.46 GiB memory in use. Of the allocated memory 27.87 GiB is allocated by PyTorch, and 43.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = dataset_PubMed.num_classes\n",
    "in_channels = dataset_PubMed.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.7, 0.7]\n",
    "class HierarchicalGCN_HGPSL(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_HGPSL, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(HGPSLPool(hidden_channels, ratio=pool_ratios[i], sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        edge_attr = None\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, edge_attr, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "        micro_f1, macro_f1 = compute_f1_scores(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            # Early stopping\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "        # Print epoch progress\n",
    "        print(f'Epoch {epoch:03d}: Loss = {loss.item():.4f}, Val Acc = {val_acc:.4f}, Micro F1 = {micro_f1:.4f}, Macro F1 = {macro_f1:.4f}')\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "# Example usage with the loop over seeds\n",
    "seeds = [42, 123, 456]\n",
    "micro_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "\n",
    "for seed in seeds:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    model = HierarchicalGCN_HGPSL(in_channels=in_channels, hidden_channels=hidden_channels,\n",
    "                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model\n",
    "    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n",
    "\n",
    "    # Compute F1 scores\n",
    "    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n",
    "    micro_f1_scores.append(micro_f1)\n",
    "    macro_f1_scores.append(macro_f1)\n",
    "\n",
    "# Calculate mean and std deviation of F1 scores\n",
    "micro_f1_mean = np.mean(micro_f1_scores)\n",
    "micro_f1_std = np.std(micro_f1_scores)\n",
    "macro_f1_mean = np.mean(macro_f1_scores)\n",
    "macro_f1_std = np.std(macro_f1_scores)\n",
    "\n",
    "micro_f1_mean = np.mean(micro_f1_scores) * 100\n",
    "micro_f1_std = np.std(micro_f1_scores) * 100\n",
    "macro_f1_mean = np.mean(macro_f1_scores) * 100\n",
    "macro_f1_std = np.std(macro_f1_scores) * 100\n",
    "\n",
    "# 打印输出格式化后的结果\n",
    "print(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\n",
    "print(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CG-ODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
