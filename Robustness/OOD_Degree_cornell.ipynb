{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/boot/anaconda3/envs/ 1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\nimport sys\nimport torch\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom ogb.graphproppred import PygGraphPropPredDataset, Evaluator\nfrom torch_geometric.loader import DataLoader\nimport os\nimport random\nimport pandas as pd\nimport torch\nimport torch_geometric.transforms as T\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data\nfrom torch_geometric.data.datapipes import functional_transform\nfrom torch_geometric.transforms import BaseTransform\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import Actor\nfrom torch_geometric.datasets import GNNBenchmarkDataset\nfrom torch_geometric.datasets import TUDataset\nfrom sklearn.metrics import r2_score\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import MoleculeNet\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.utils import to_networkx\nfrom torch.nn import Linear\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport time\nimport psutil\nimport torch\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom typing import List, Optional, Tuple, Union\nimport math\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.nn.models.mlp import Linear\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import BatchNorm\nimport os.path as osp\nimport time\nfrom math import ceil\nfrom torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nfrom torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nimport random\nfrom torch_geometric.nn import GCNConv\nimport os.path as osp\nimport time\nfrom math import ceil\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, dense_diff_pool\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, TopKPooling, SAGPooling\nfrom torch_geometric.utils import to_dense_batch\nfrom sklearn.metrics import f1_score"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train Mask: 91 nodes\n",
                        "Val Mask: 45 nodes\n",
                        "Test Mask: 47 nodes\n",
                        "Train Mask Average Degree: 3.2198\n",
                        "Val Mask Average Degree: 0.1111\n",
                        "Test Mask Average Degree: 0.0000\n"
                    ]
                }
            ],
            "source": "import torch\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.utils import degree\ndata_path = \"/data/ /Pooling/\"\ndataset_Cornell = WebKB(root=data_path, name=\"Cornell\")\ndata = dataset_Cornell[0]\ndeg = degree(data.edge_index[0], data.num_nodes)\nsorted_indices = torch.argsort(deg, descending=True)\nnum_nodes = data.num_nodes\ntrain_size = int(0.5 * num_nodes)\nval_size = int(0.25 * num_nodes)\ntest_size = num_nodes - train_size - val_size\ntrain_indices = sorted_indices[:train_size]\nval_indices = sorted_indices[train_size:train_size + val_size]\ntest_indices = sorted_indices[train_size + val_size:]\ntrain_mask = torch.zeros(num_nodes, dtype=torch.bool)\nval_mask = torch.zeros(num_nodes, dtype=torch.bool)\ntest_mask = torch.zeros(num_nodes, dtype=torch.bool)\ntrain_mask[train_indices] = True\nval_mask[val_indices] = True\ntest_mask[test_indices] = True\ndata.train_mask = train_mask\ndata.val_mask = val_mask\ndata.test_mask = test_mask\nprint(f\"Train Mask: {train_mask.sum().item()} nodes\")\nprint(f\"Val Mask: {val_mask.sum().item()} nodes\")\nprint(f\"Test Mask: {test_mask.sum().item()} nodes\")\ntrain_deg_avg = deg[train_mask].float().mean().item()\nval_deg_avg = deg[val_mask].float().mean().item()\ntest_deg_avg = deg[test_mask].float().mean().item()\nprint(f\"Train Mask Average Degree: {train_deg_avg:.4f}\")\nprint(f\"Val Mask Average Degree: {val_deg_avg:.4f}\")\nprint(f\"Test Mask Average Degree: {test_deg_avg:.4f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TopKPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 34.07$\\pm$2.10\n",
                        "Macro F1: 22.35$\\pm$2.92\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Cornell.num_classes\nin_channels = dataset_Cornell.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_TOPK(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_TOPK, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(TopKPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_TOPK(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SAGPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 36.30$\\pm$4.19\n",
                        "Macro F1: 24.39$\\pm$4.03\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import SAGPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Cornell.num_classes\nin_channels = dataset_Cornell.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_SAG(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_SAG, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(SAGPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_SAG(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ASAPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 34.07$\\pm$1.05\n",
                        "Macro F1: 20.94$\\pm$3.40\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Cornell.num_classes\nin_channels = dataset_Cornell.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_ASA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_ASA, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(ASAPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_ASA(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PANPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nclass PANPooling(torch.nn.Module):\n    r\"\"\" General Graph pooling layer based on PAN, which can work with all layers.\n    \"\"\"\n    def __init__(self, in_channels, ratio=0.5, pan_pool_weight=None, min_score=None, multiplier=1,\n                 nonlinearity=torch.tanh, filter_size=3, panpool_filter_weight=None):\n        super(PANPooling, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.nonlinearity = nonlinearity\n        self.filter_size = filter_size\n        if panpool_filter_weight is None:\n            self.panpool_filter_weight = torch.nn.Parameter(0.5 * torch.ones(filter_size), requires_grad=True)\n        self.transform = Parameter(torch.ones(in_channels), requires_grad=True)\n        if pan_pool_weight is None:\n            self.pan_pool_weight = torch.nn.Parameter(0.5 * torch.ones(2), requires_grad=True)\n        else:\n            self.pan_pool_weight = pan_pool_weight\n    def forward(self, x, edge_index, M=None, batch=None, num_nodes=None):\n        \"\"\"\"\"\"\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        edge_index, edge_weight = self.panentropy_sparse(edge_index, num_nodes)\n        num_nodes = x.size(0)\n        degree = torch.zeros(num_nodes, device=edge_index.device)\n        degree = scatter_add(edge_weight, edge_index[0], out=degree)\n        xtransform = torch.matmul(x, self.transform)\n        x_transform_norm = xtransform \n        degree_norm = degree \n        score = self.pan_pool_weight[0] * x_transform_norm + self.pan_pool_weight[1] * degree_norm\n        if self.min_score is None:\n            score = self.nonlinearity(score)\n        else:\n            score = softmax(score, batch)\n        perm = self.topk(score, self.ratio, batch, self.min_score)\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n        batch = batch[perm]\n        edge_index, edge_weight = self.filter_adj(edge_index, edge_weight, perm, num_nodes=score.size(0))\n        return x, edge_index, edge_weight, batch, perm, score[perm]\n    def topk(self, x, ratio, batch, min_score=None, tol=1e-7):\n        if min_score is not None:\n            scores_max = scatter_max(x, batch)[0][batch] - tol\n            scores_min = scores_max.clamp(max=min_score)\n            perm = torch.nonzero(x > scores_min).view(-1)\n        else:\n            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n            batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n            cum_num_nodes = torch.cat(\n                [num_nodes.new_zeros(1),\n                 num_nodes.cumsum(dim=0)[:-1]], dim=0)\n            index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n            index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n            dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n            dense_x[index] = x\n            dense_x = dense_x.view(batch_size, max_num_nodes)\n            _, perm = dense_x.sort(dim=-1, descending=True)\n            perm = perm + cum_num_nodes.view(-1, 1)\n            perm = perm.view(-1)\n            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n            mask = [\n                torch.arange(k[i], dtype=torch.long, device=x.device) +\n                i * max_num_nodes for i in range(batch_size)\n            ]\n            mask = torch.cat(mask, dim=0)\n            perm = perm[mask]\n        return perm\n    def filter_adj(self, edge_index, edge_weight, perm, num_nodes=None):\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        mask = perm.new_full((num_nodes, ), -1)\n        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n        mask[perm] = i\n        row, col = edge_index\n        row, col = mask[row], mask[col]\n        mask = (row >= 0) & (col >= 0)\n        row, col = row[mask], col[mask]\n        if edge_weight is not None:\n            edge_weight = edge_weight[mask]\n        return torch.stack([row, col], dim=0), edge_weight\n    def panentropy_sparse(self, edge_index, num_nodes):\n        edge_value = torch.ones(edge_index.size(1), device=edge_index.device)\n        edge_index, edge_value = coalesce(edge_index, edge_value, num_nodes, num_nodes)\n        pan_index, pan_value = eye(num_nodes, device=edge_index.device)\n        indextmp = pan_index.clone().to(edge_index.device)\n        valuetmp = pan_value.clone().to(edge_index.device)\n        pan_value = self.panpool_filter_weight[0] * pan_value\n        for i in range(self.filter_size - 1):\n            indextmp, valuetmp = spspmm(indextmp, valuetmp, edge_index, edge_value, num_nodes, num_nodes, num_nodes)\n            valuetmp = valuetmp * self.panpool_filter_weight[i+1]\n            indextmp, valuetmp = coalesce(indextmp, valuetmp, num_nodes, num_nodes)\n            pan_index = torch.cat((pan_index, indextmp), 1)\n            pan_value = torch.cat((pan_value, valuetmp))\n        return coalesce(pan_index, pan_value, num_nodes, num_nodes, op='add')"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 35.56$\\pm$0.00\n",
                        "Macro F1: 23.55$\\pm$0.90\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Cornell.num_classes\nin_channels = dataset_Cornell.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_PAN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_PAN, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(PANPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, score_perm = self.pools[i - 1](x, edge_index, batch=batch, M=None)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_PAN(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CoPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\ndef cumsum(x: Tensor, dim: int = 0) -> Tensor:\n    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n    Args:\n        x (torch.Tensor): The input tensor.\n        dim (int, optional): The dimension to do the operation over.\n            (default: :obj:`0`)\n    Example:\n        >>> x = torch.tensor([2, 4, 1])\n        >>> cumsum(x)\n        tensor([0, 2, 6, 7])\n    \"\"\"\n    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n    out = x.new_empty(size)\n    out.narrow(dim, 0, 1).zero_()\n    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n    return out\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\ndef topk(\n    x: Tensor,\n    ratio: Optional[Union[float, int]],\n    batch: Tensor,\n    min_score: Optional[float] = None,\n    tol: float = 1e-7,\n) -> Tensor:\n    if min_score is not None:\n        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero().view(-1)\n        return perm\n    if ratio is not None:\n        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n        if ratio >= 1:\n            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n        else:\n            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n        x, x_perm = torch.sort(x.view(-1), descending=True)\n        batch = batch[x_perm]\n        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n        ptr = cumsum(num_nodes)\n        batched_arange = arange - ptr[batch]\n        mask = batched_arange < k[batch]\n        return x_perm[batch_perm[mask]]\n    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n                     \"must be specified\")\nclass GPR_prop(MessagePassing):\n    '''\n    propagation class for GPR_GNN\n    '''\n    def __init__(self, K, alpha, Init, Gamma=None, bias=True, **kwargs):\n        super(GPR_prop, self).__init__(aggr='add', **kwargs)\n        self.K = K\n        self.Init = Init\n        self.alpha = alpha\n        assert Init in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']\n        if Init == 'SGC':\n            TEMP = 0.0*np.ones(K+1)\n            TEMP[alpha] = 1.0\n        elif Init == 'PPR':\n            TEMP = alpha*(1-alpha)**np.arange(K+1)\n            TEMP[-1] = (1-alpha)**K\n        elif Init == 'NPPR':\n            TEMP = (alpha)**np.arange(K+1)\n            TEMP = TEMP/np.sum(np.abs(TEMP))\n        elif Init == 'Random':\n            bound = np.sqrt(3/(K+1))\n            TEMP = np.random.uniform(-bound, bound, K+1)\n            TEMP = TEMP/np.sum(np.abs(TEMP))\n        elif Init == 'WS':\n            TEMP = Gamma\n        self.temp = Parameter(torch.tensor(TEMP))\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.temp)\n        for k in range(self.K+1):\n            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n        self.temp.data[-1] = (1-self.alpha)**self.K\n    def forward(self, x, edge_index, edge_weight=None):\n        edge_index, norm = gcn_norm(\n            edge_index, edge_weight, num_nodes=x.size(0), dtype=x.dtype)\n        hidden = x*(self.temp[0])\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=norm)\n            gamma = self.temp[k+1]\n            hidden = hidden + gamma*x\n        return hidden\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def __repr__(self):\n        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n                                           self.temp)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) \n        edge_index = edge_index.type(torch.long)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass graph_attention(torch.nn.Module):\n    src_nodes_dim = 0  \n    trg_nodes_dim = 1  \n    nodes_dim = 0      \n    head_dim = 1       \n    def __init__(self, num_in_features, num_out_features, num_of_heads, dropout_prob=0.6, log_attention_weights=False):\n        super().__init__()\n        self.num_of_heads = num_of_heads\n        self.num_out_features = num_out_features\n        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.init_params()\n    def init_params(self):\n        \"\"\"\n        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n        Feel free to experiment - there may be better initializations depending on your problem.\n        \"\"\"\n        nn.init.xavier_uniform_(self.linear_proj.weight)\n        nn.init.xavier_uniform_(self.scoring_fn_target)\n        nn.init.xavier_uniform_(self.scoring_fn_source)\n    def forward(self, x, edge_index):\n        in_nodes_features = x  \n        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n        scores_per_edge = scores_source_lifted + scores_target_lifted\n        return torch.sigmoid(scores_per_edge)\n    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n        \"\"\"\n        Lifts i.e. duplicates certain vectors depending on the edge index.\n        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n        \"\"\"\n        src_nodes_index = edge_index[self.src_nodes_dim]\n        trg_nodes_index = edge_index[self.trg_nodes_dim]\n        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n        return scores_source, scores_target, nodes_features_matrix_proj_lifted\nclass CoPooling(torch.nn.Module):\n    def __init__(self, ratio=0.5, K=0.05, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=None):\n        super(CoPooling, self).__init__()\n        self.ratio = ratio\n        self.calc_information_score = NodeInformationScore()\n        self.edge_ratio = edge_ratio\n        self.prop1 = GPR_prop(K, alpha, Init, Gamma)\n        score_dim = 32\n        self.G_att = graph_attention(num_in_features=nhid, num_out_features=score_dim, num_of_heads=1)\n        self.weight = Parameter(torch.Tensor(2*nhid, nhid))\n        nn.init.xavier_uniform_(self.weight.data)\n        self.bias = Parameter(torch.Tensor(nhid))\n        nn.init.zeros_(self.bias.data)\n        self.reset_parameters()\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight.data)\n        nn.init.zeros_(self.bias.data)\n        self.prop1.reset_parameters()\n        self.G_att.init_params()\n    def forward(self, x, edge_index, edge_attr, batch=None, nodes_index=None, node_attr=None):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        ori_batch = batch.clone()\n        device = x.device\n        num_nodes = x.shape[0]\n        x_cut = self.prop1(x, edge_index) \n        attention = self.G_att(x_cut, edge_index) \n        attention = attention.sum(dim=1) \n        edge_index, attention = add_self_loops(edge_index, attention, 1.0, num_nodes) \n        edge_index_t, attention_t = transpose(edge_index, attention, num_nodes, num_nodes)\n        edge_tmp = torch.cat((edge_index, edge_index_t), 1)\n        att_tmp = torch.cat((attention, attention_t),0)\n        edge_index, attention = coalesce(edge_tmp, att_tmp, num_nodes, num_nodes, 'mean')\n        attention_np = attention.cpu().data.numpy()\n        cut_val = np.percentile(attention_np, int(100*(1-self.edge_ratio))) \n        attention = attention * (attention >= cut_val) \n        kep_idx = attention > 0.0\n        cut_edge_index, cut_edge_attr = edge_index[:, kep_idx], attention[kep_idx]\n        x_information_score = self.calc_information_score(x, cut_edge_index, cut_edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        perm = topk(score, self.ratio, batch)\n        x_topk = x[perm]\n        batch = batch[perm]\n        if nodes_index is not None:\n            nodes_index = nodes_index[perm]\n        if node_attr is not None:\n            node_attr = node_attr[perm]\n        if cut_edge_index is not None or cut_edge_index.nelement() != 0:\n            induced_edge_index, induced_edge_attr = filter_adj(cut_edge_index, cut_edge_attr, perm, num_nodes=num_nodes)\n        else:\n            print('All edges are cut!')\n            induced_edge_index, induced_edge_attr = cut_edge_index, cut_edge_attr\n        attention_dense = (to_dense_adj(cut_edge_index, edge_attr=cut_edge_attr, max_num_nodes=num_nodes)).squeeze()\n        x = F.relu(torch.matmul(torch.cat((x_topk, torch.matmul(attention_dense[perm],x)), 1), self.weight) + self.bias)\n        return x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 35.56$\\pm$1.81\n",
                        "Macro F1: 22.64$\\pm$3.21\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Cornell.num_classes\nin_channels = dataset_Cornell.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_CO(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_CO, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(CoPooling(ratio=pool_ratios[i], K=1, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=1.0))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, perm, _, batch, _, _, _ = self.pools[i - 1](x, edge_index, edge_attr=None, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_CO(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CGIPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "from torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\n@dataclass(init=False)\nclass SelectOutput:\n    r\"\"\"The output of the :class:`Select` method, which holds an assignment\n    from selected nodes to their respective cluster(s).\n    Args:\n        node_index (torch.Tensor): The indices of the selected nodes.\n        num_nodes (int): The number of nodes.\n        cluster_index (torch.Tensor): The indices of the clusters each node in\n            :obj:`node_index` is assigned to.\n        num_clusters (int): The number of clusters.\n        weight (torch.Tensor, optional): A weight vector, denoting the strength\n            of the assignment of a node to its cluster. (default: :obj:`None`)\n    \"\"\"\n    node_index: Tensor\n    num_nodes: int\n    cluster_index: Tensor\n    num_clusters: int\n    weight: Optional[Tensor] = None\n    def __init__(\n        self,\n        node_index: Tensor,\n        num_nodes: int,\n        cluster_index: Tensor,\n        num_clusters: int,\n        weight: Optional[Tensor] = None,\n    ):\n        if node_index.dim() != 1:\n            raise ValueError(f\"Expected 'node_index' to be one-dimensional \"\n                             f\"(got {node_index.dim()} dimensions)\")\n        if cluster_index.dim() != 1:\n            raise ValueError(f\"Expected 'cluster_index' to be one-dimensional \"\n                             f\"(got {cluster_index.dim()} dimensions)\")\n        if node_index.numel() != cluster_index.numel():\n            raise ValueError(f\"Expected 'node_index' and 'cluster_index' to \"\n                             f\"hold the same number of values (got \"\n                             f\"{node_index.numel()} and \"\n                             f\"{cluster_index.numel()} values)\")\n        if weight is not None and weight.dim() != 1:\n            raise ValueError(f\"Expected 'weight' vector to be one-dimensional \"\n                             f\"(got {weight.dim()} dimensions)\")\n        if weight is not None and weight.numel() != node_index.numel():\n            raise ValueError(f\"Expected 'weight' to hold {node_index.numel()} \"\n                             f\"values (got {weight.numel()} values)\")\n        self.node_index = node_index\n        self.num_nodes = num_nodes\n        self.cluster_index = cluster_index\n        self.num_clusters = num_clusters\n        self.weight = weight\nclass Select(torch.nn.Module):\n    r\"\"\"An abstract base class for implementing custom node selections as\n    described in the `\"Understanding Pooling in Graph Neural Networks\"\n    <https://arxiv.org/abs/1905.05178>`_ paper, which maps the nodes of an\n    input graph to supernodes in the coarsened graph.\n    Specifically, :class:`Select` returns a :class:`SelectOutput` output, which\n    holds a (sparse) mapping :math:`\\mathbf{C} \\in {[0, 1]}^{N \\times C}` that\n    assigns selected nodes to one or more of :math:`C` super nodes.\n    \"\"\"\n    def reset_parameters(self):\n        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n        pass\n    def forward(self, *args, **kwargs) -> SelectOutput:\n        raise NotImplementedError\n    def __repr__(self) -> str:\n        return f'{self.__class__.__name__}()'\ndef cumsum(x: Tensor, dim: int = 0) -> Tensor:\n    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n    Args:\n        x (torch.Tensor): The input tensor.\n        dim (int, optional): The dimension to do the operation over.\n            (default: :obj:`0`)\n    Example:\n        >>> x = torch.tensor([2, 4, 1])\n        >>> cumsum(x)\n        tensor([0, 2, 6, 7])\n    \"\"\"\n    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n    out = x.new_empty(size)\n    out.narrow(dim, 0, 1).zero_()\n    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n    return out\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\ndef topk(\n    x: Tensor,\n    ratio: Optional[Union[float, int]],\n    batch: Tensor,\n    min_score: Optional[float] = None,\n    tol: float = 1e-7,\n) -> Tensor:\n    if min_score is not None:\n        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero().view(-1)\n        return perm\n    if ratio is not None:\n        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n        if ratio >= 1:\n            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n        else:\n            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n        x, x_perm = torch.sort(x.view(-1), descending=True)\n        batch = batch[x_perm]\n        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n        ptr = cumsum(num_nodes)\n        batched_arange = arange - ptr[batch]\n        mask = batched_arange < k[batch]\n        return x_perm[batch_perm[mask]]\n    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n                     \"must be specified\")\nclass Discriminator(torch.nn.Module):\n    def __init__(self, in_channels):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(in_channels * 2, in_channels)\n        self.fc2 = nn.Linear(in_channels, 1)\n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = F.sigmoid(self.fc2(x))\n        return x\nclass CGIPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, non_lin=torch.tanh):\n        super(CGIPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.non_lin = non_lin\n        self.hidden_dim = in_channels\n        self.transform = GraphConv(in_channels, self.hidden_dim)\n        self.pp_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n        self.np_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n        self.positive_pooling = GraphConv(self.hidden_dim, 1)\n        self.negative_pooling = GraphConv(self.hidden_dim, 1)\n        self.discriminator = Discriminator(self.hidden_dim)\n        self.loss_fn = torch.nn.BCELoss()\n    def forward(self, x, edge_index, edge_attr=None, batch=None):\n        device = x.device  \n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_transform = F.leaky_relu(self.transform(x, edge_index), 0.2)\n        x_tp = F.leaky_relu(self.pp_conv(x, edge_index), 0.2)\n        x_tn = F.leaky_relu(self.np_conv(x, edge_index), 0.2)\n        s_pp = self.positive_pooling(x_tp, edge_index).squeeze()\n        s_np = self.negative_pooling(x_tn, edge_index).squeeze()\n        perm_positive = topk(s_pp, 1, batch)\n        perm_negative = topk(s_np, 1, batch)\n        x_pp = x_transform[perm_positive] * self.non_lin(s_pp[perm_positive]).view(-1, 1)\n        x_np = x_transform[perm_negative] * self.non_lin(s_np[perm_negative]).view(-1, 1)\n        x_pp_readout = gap(x_pp, batch[perm_positive])\n        x_np_readout = gap(x_np, batch[perm_negative])\n        x_readout = gap(x_transform, batch)\n        positive_pair = torch.cat([x_pp_readout, x_readout], dim=1)\n        negative_pair = torch.cat([x_np_readout, x_readout], dim=1)\n        real = torch.ones(positive_pair.shape[0], device=device)  \n        fake = torch.zeros(negative_pair.shape[0], device=device)  \n        score = (s_pp - s_np)\n        perm = topk(score, self.ratio, batch)\n        x = x_transform[perm] * self.non_lin(score[perm]).view(-1, 1)\n        batch = batch[perm]\n        filter_edge_index, filter_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        return x, filter_edge_index, filter_edge_attr, batch, perm"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 36.30$\\pm$4.19\n",
                        "Macro F1: 25.34$\\pm$4.15\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Cornell.num_classes\nin_channels = dataset_Cornell.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_CGI(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_CGI, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(CGIPool(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_CGI(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### KMISPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "from typing import Callable, Optional, Tuple, Union\nfrom torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\nScorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\nfrom torch_sparse import SparseTensor, remove_diag\nfrom torch_geometric.nn.aggr import Aggregation\nfrom torch_geometric.nn.dense import Linear\nfrom torch.nn import Module\nfrom torch_scatter import scatter_max, scatter_min\ndef maximal_independent_set(edge_index: Adj, k: int = 1,\n                            perm: OptTensor = None) -> Tensor:\n    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n    returned set.\n    The algorithm greedily selects the nodes in their canonical order. If a\n    permutation :obj:`perm` is provided, the nodes are extracted following\n    that permutation instead.\n    This method follows `Blelloch's Alogirithm\n    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n    higher values of :math:`k`.\n    Args:\n        edge_index (Tensor or SparseTensor): The graph connectivity.\n        k (int): The :math:`k` value (defaults to 1).\n        perm (LongTensor, optional): Permutation vector. Must be of size\n            :obj:`(n,)` (defaults to :obj:`None`).\n    :rtype: :class:`ByteTensor`\n    \"\"\"\n    if isinstance(edge_index, SparseTensor):\n        row, col, _ = edge_index.coo()\n        device = edge_index.device()\n        n = edge_index.size(0)\n    else:\n        row, col = edge_index[0], edge_index[1]\n        device = row.device\n        n = edge_index.max().item() + 1\n    if perm is None:\n        rank = torch.arange(n, dtype=torch.long, device=device)\n    else:\n        rank = torch.zeros_like(perm)\n        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n    mis = torch.zeros(n, dtype=torch.bool, device=device)\n    mask = mis.clone()\n    min_rank = rank.clone()\n    while not mask.all():\n        for _ in range(k):\n            min_neigh = torch.full_like(min_rank, fill_value=n)\n            scatter_min(min_rank[row], col, out=min_neigh)\n            torch.minimum(min_neigh, min_rank, out=min_rank)  \n        mis = mis | torch.eq(rank, min_rank)\n        mask = mis.clone().byte()\n        for _ in range(k):\n            max_neigh = torch.full_like(mask, fill_value=0)\n            scatter_max(mask[row], col, out=max_neigh)\n            torch.maximum(max_neigh, mask, out=mask)  \n        mask = mask.to(dtype=torch.bool)\n        min_rank = rank.clone()\n        min_rank[mask] = n\n    return mis\ndef maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n                                    perm: OptTensor = None) -> PairTensor:\n    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n    clustering of a graph, as defined in `\"Generalizing Downsampling from\n    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n    The algorithm greedily selects the nodes in their canonical order. If a\n    permutation :obj:`perm` is provided, the nodes are extracted following\n    that permutation instead.\n    This method returns both the :math:`k`-MIS and the clustering, where the\n    :math:`c`-th cluster refers to the :math:`c`-th element of the\n    :math:`k`-MIS.\n    Args:\n        edge_index (Tensor or SparseTensor): The graph connectivity.\n        k (int): The :math:`k` value (defaults to 1).\n        perm (LongTensor, optional): Permutation vector. Must be of size\n            :obj:`(n,)` (defaults to :obj:`None`).\n    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n    \"\"\"\n    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n    n, device = mis.size(0), mis.device\n    if isinstance(edge_index, SparseTensor):\n        row, col, _ = edge_index.coo()\n    else:\n        row, col = edge_index[0], edge_index[1]\n    if perm is None:\n        rank = torch.arange(n, dtype=torch.long, device=device)\n    else:\n        rank = torch.zeros_like(perm)\n        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n    rank_mis = rank[mis]\n    min_rank[mis] = rank_mis\n    for _ in range(k):\n        min_neigh = torch.full_like(min_rank, fill_value=n)\n        scatter_min(min_rank[row], col, out=min_neigh)\n        torch.minimum(min_neigh, min_rank, out=min_rank)\n    _, clusters = torch.unique(min_rank, return_inverse=True)\n    perm = torch.argsort(rank_mis)\n    return mis, perm[clusters]\nclass KMISPooling(Module):\n    _heuristics = {None, 'greedy', 'w-greedy'}\n    _passthroughs = {None, 'before', 'after'}\n    _scorers = {\n        'linear',\n        'random',\n        'constant',\n        'canonical',\n        'first',\n        'last',\n    }\n    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n                 scorer: Union[Scorer, str] = 'linear',\n                 score_heuristic: Optional[str] = 'greedy',\n                 score_passthrough: Optional[str] = 'before',\n                 aggr_x: Optional[Union[str, Aggregation]] = None,\n                 aggr_edge: str = 'sum',\n                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n                 remove_self_loops: bool = True) -> None:\n        super(KMISPooling, self).__init__()\n        assert score_heuristic in self._heuristics, \\\n            \"Unrecognized `score_heuristic` value.\"\n        assert score_passthrough in self._passthroughs, \\\n            \"Unrecognized `score_passthrough` value.\"\n        if not callable(scorer):\n            assert scorer in self._scorers, \\\n                \"Unrecognized `scorer` value.\"\n        self.k = k\n        self.scorer = scorer\n        self.score_heuristic = score_heuristic\n        self.score_passthrough = score_passthrough\n        self.aggr_x = aggr_x\n        self.aggr_edge = aggr_edge\n        self.aggr_score = aggr_score\n        self.remove_self_loops = remove_self_loops\n        if scorer == 'linear':\n            assert self.score_passthrough is not None, \\\n                \"`'score_passthrough'` must not be `None`\" \\\n                \" when using `'linear'` scorer\"\n            self.lin = nn.Linear(in_channels, 1)\n    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n        if self.score_heuristic is None:\n            return x\n        row, col, _ = adj.coo()\n        x = x.view(-1)\n        if self.score_heuristic == 'greedy':\n            k_sums = torch.ones_like(x)\n        else:\n            k_sums = x.clone()\n        for _ in range(self.k):\n            scatter_add(k_sums[row], col, out=k_sums)\n        return x / k_sums\n    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n                batch: OptTensor = None) -> Tensor:\n        if self.scorer == 'linear':\n            return self.lin(x).sigmoid()\n        if self.scorer == 'random':\n            return torch.rand((x.size(0), 1), device=x.device)\n        if self.scorer == 'constant':\n            return torch.ones((x.size(0), 1), device=x.device)\n        if self.scorer == 'canonical':\n            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n        if self.scorer == 'first':\n            return x[..., [0]]\n        if self.scorer == 'last':\n            return x[..., [-1]]\n        return self.scorer(x, edge_index, edge_attr, batch)\n    def forward(self, x: Tensor, edge_index: Adj,\n                edge_attr: OptTensor = None,\n                batch: OptTensor = None) \\\n            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n        \"\"\"\"\"\"\n        edge_index = edge_index.long()\n        adj, n = edge_index, x.size(0)\n        if not isinstance(edge_index, SparseTensor):\n            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n        score = self._scorer(x, edge_index, edge_attr, batch)\n        updated_score = self._apply_heuristic(score, adj)\n        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n        row, col, val = adj.coo()\n        c = mis.sum()\n        if val is None:\n            val = torch.ones_like(row, dtype=torch.float)\n        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n                           is_sorted=False,\n                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n        if self.remove_self_loops:\n            adj = remove_diag(adj)\n        if self.score_passthrough == 'before':\n            x = self.aggr_score(x, score)\n        if self.aggr_x is None:\n            x = x[mis]\n        elif isinstance(self.aggr_x, str):\n            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n                        reduce=self.aggr_x)\n        else:\n            x = self.aggr_x(x, cluster, dim_size=c)\n        if self.score_passthrough == 'after':\n            x = self.aggr_score(x, score[mis])\n        if isinstance(edge_index, SparseTensor):\n            edge_index, edge_attr = adj, None\n        else:\n            row, col, edge_attr = adj.coo()\n            edge_index = torch.stack([row, col])\n        if batch is not None:\n            batch = batch[mis]\n        perm = perm[mis]\n        return x, edge_index, edge_attr, batch, mis, cluster, perm\n    def __repr__(self):\n        if self.scorer == 'linear':\n            channels = f\"in_channels={self.lin.in_channels}, \"\n        else:\n            channels = \"\"\n        return f'{self.__class__.__name__}({channels}k={self.k})'"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 34.81$\\pm$1.05\n",
                        "Macro F1: 22.99$\\pm$2.00\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Cornell.num_classes\nin_channels = dataset_Cornell.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\nclass HierarchicalGCN_KMIS(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_KMIS, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(KMISPooling(64, k=1, aggr_x='sum'))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_KMIS(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### GSAPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "import math\nfrom typing import Union, Optional, Callable\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax\nfrom torch_geometric.nn import GCNConv, SAGEConv, GATConv, ChebConv, GraphConv\ndef uniform(size, tensor):\n    if tensor is not None:\n        bound = 1.0 / math.sqrt(size)\n        tensor.data.uniform_(-bound, bound)\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ),\n                             torch.finfo(x.dtype).min)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        if isinstance(ratio, int):\n            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n            k = torch.min(k, num_nodes)\n        else:\n            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\nclass GSAPool(torch.nn.Module):\n    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n                        min_score=None, multiplier=1,\n                        non_linearity=torch.tanh,\n                        cus_drop_ratio =0):\n        super(GSAPool,self).__init__()\n        self.in_channels = in_channels\n        self.ratio = pooling_ratio\n        self.alpha = alpha\n        self.sbtl_layer = GCNConv(in_channels,1)\n        self.fbtl_layer = nn.Linear(in_channels, 1)\n        self.fusion = GCNConv(in_channels,in_channels)\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.fusion_flag = 0\n        self.non_linearity = non_linearity\n        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n    def conv_selection(self, conv, in_channels, conv_type=0):\n        if(conv_type == 0):\n            out_channels = 1\n        elif(conv_type == 1):\n            out_channels = in_channels\n        if(conv == \"GCNConv\"):\n            return GCNConv(in_channels,out_channels)\n        elif(conv == \"ChebConv\"):\n            return ChebConv(in_channels,out_channels,1)\n        elif(conv == \"SAGEConv\"):\n            return SAGEConv(in_channels,out_channels)\n        elif(conv == \"GATConv\"):\n            return GATConv(in_channels,out_channels, heads=1, concat=True)\n        elif(conv == \"GraphConv\"):\n            return GraphConv(in_channels,out_channels)\n        else:\n            raise ValueError\n    def forward(self, x, edge_index, edge_attr=None, batch=None):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x = x.unsqueeze(-1) if x.dim() == 1 else x\n        score_s = self.sbtl_layer(x,edge_index).squeeze()\n        score_f = self.fbtl_layer(x).squeeze()\n        score = score_s*self.alpha + score_f*(1-self.alpha)\n        score = score.unsqueeze(-1) if score.dim()==0 else score\n        if self.min_score is None:\n            score = self.non_linearity(score)\n        else:\n            score = softmax(score, batch)\n        sc = self.dropout(score)\n        perm = topk(sc, self.ratio, batch)\n        if(self.fusion_flag == 1):\n            x = self.fusion(x, edge_index)\n        x_ae = x[perm]\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n        batch = batch[perm]\n        edge_index, edge_attr = filter_adj(\n            edge_index, edge_attr, perm, num_nodes=score.size(0))\n        return x, edge_index, edge_attr, batch, perm, x_ae"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 35.56$\\pm$1.81\n",
                        "Macro F1: 23.23$\\pm$1.21\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Cornell.num_classes\nin_channels = dataset_Cornell.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_GSA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_GSA, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(GSAPool(64, pooling_ratio=pool_ratios[i], alpha = 0.6, cus_drop_ratio = 0))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, edge_attr, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_GSA(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### HGPSLPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([0.5344, 0.0000, 0.0000, 0.4656, 0.0613, 0.0000, 0.0000, 0.0000, 0.5640,\n",
                        "        0.3748])\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = torch.nonzero(x > scores_min).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n            num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_weight, perm, num_nodes=None):\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        mask = perm.new_full((num_nodes, ), -1)\n        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n        mask[perm] = i\n        row, col = edge_index\n        row, col = mask[row], mask[col]\n        mask = (row >= 0) & (col >= 0)\n        row, col = row[mask], col[mask]\n        if edge_weight is not None:\n            edge_weight = edge_weight[mask]\n        return torch.stack([row, col], dim=0), edge_weight\ndef scatter_sort(x, batch, fill_value=-1e16):\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n    dense_x[index] = x\n    dense_x = dense_x.view(batch_size, max_num_nodes)\n    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n    sorted_x = sorted_x.view(-1)\n    filled_index = sorted_x != fill_value\n    sorted_x = sorted_x[filled_index]\n    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n    return sorted_x, cumsum_sorted_x\ndef _make_ix_like(batch):\n    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n    idx = torch.cat(idx, dim=0)\n    return idx\ndef _threshold_and_support(x, batch):\n    \"\"\"Sparsemax building block: compute the threshold\n    Args:\n        x: input tensor to apply the sparsemax\n        batch: group indicators\n    Returns:\n        the threshold value\n    \"\"\"\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    sorted_input, input_cumsum = scatter_sort(x, batch)\n    input_cumsum = input_cumsum - 1.0\n    rhos = _make_ix_like(batch).to(x.dtype)\n    support = rhos * sorted_input > input_cumsum\n    support_size = scatter_add(support.to(batch.dtype), batch)\n    idx = support_size + cum_num_nodes - 1\n    mask = idx < 0\n    idx[mask] = 0\n    tau = input_cumsum.gather(0, idx)\n    tau /= support_size.to(x.dtype)\n    return tau, support_size\nclass SparsemaxFunction(Function):\n    @staticmethod\n    def forward(ctx, x, batch):\n        \"\"\"sparsemax: normalizing sparse transform\n        Parameters:\n            ctx: context object\n            x (Tensor): shape (N, )\n            batch: group indicator\n        Returns:\n            output (Tensor): same shape as input\n        \"\"\"\n        max_val, _ = scatter_max(x, batch)\n        x -= max_val[batch]\n        tau, supp_size = _threshold_and_support(x, batch)\n        output = torch.clamp(x - tau[batch], min=0)\n        ctx.save_for_backward(supp_size, output, batch)\n        return output\n    @staticmethod\n    def backward(ctx, grad_output):\n        supp_size, output, batch = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n        return grad_input, None\nsparsemax = SparsemaxFunction.apply\nclass Sparsemax(nn.Module):\n    def __init__(self):\n        super(Sparsemax, self).__init__()\n    def forward(self, x, batch):\n        return sparsemax(x, batch)\nif __name__ == '__main__':\n    sparse_attention = Sparsemax()\n    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n    res = sparse_attention(input_x, input_batch)\n    print(res)\nclass TwoHopNeighborhood(object):\n    def __call__(self, data):\n        edge_index, edge_attr = data.edge_index, data.edge_attr\n        n = data.num_nodes\n        fill = 1e16\n        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n        edge_index = torch.cat([edge_index, index], dim=1)\n        if edge_attr is None:\n            data.edge_index, _ = coalesce(edge_index, None, n, n)\n        else:\n            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n            value = value.expand(-1, *list(edge_attr.size())[1:])\n            edge_attr = torch.cat([edge_attr, value], dim=0)\n            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min', fill_value=fill)\n            edge_attr[edge_attr >= fill] = 0\n            data.edge_attr = edge_attr\n        return data\n    def __repr__(self):\n        return '{}()'.format(self.__class__.__name__)\nclass GCN(MessagePassing):\n    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n        super(GCN, self).__init__(aggr='add', **kwargs)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n        nn.init.xavier_uniform_(self.weight.data)\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n            nn.init.zeros_(self.bias.data)\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n    def reset_parameters(self):\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight=None):\n        x = torch.matmul(x, self.weight)\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        if self.bias is not None:\n            aggr_out = aggr_out + self.bias\n        return aggr_out\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n        row, col = edge_index\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass HGPSLPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n        super(HGPSLPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.sample = sample\n        self.sparse = sparse\n        self.sl = sl\n        self.negative_slop = negative_slop\n        self.lamb = lamb\n        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n        nn.init.xavier_uniform_(self.att.data)\n        self.sparse_attention = Sparsemax()\n        self.neighbor_augment = TwoHopNeighborhood()\n        self.calc_information_score = NodeInformationScore()\n    def forward(self, x, edge_index, edge_attr, batch):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        original_x = x\n        perm = topk(score, self.ratio, batch)\n        x = x[perm]\n        batch = batch[perm]\n        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        if self.sl is False:\n            return x, induced_edge_index, induced_edge_attr, batch\n        if self.sample:\n            k_hop = 3\n            if edge_attr is None:\n                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n            for _ in range(k_hop - 1):\n                hop_data = self.neighbor_augment(hop_data)\n            hop_edge_index = hop_data.edge_index\n            hop_edge_attr = hop_data.edge_attr\n            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            adj[row, col] = weights\n            new_edge_index, weights = dense_to_sparse(adj)\n            row, col = new_edge_index\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        else:\n            if edge_attr is None:\n                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n                                               device=induced_edge_index.device)\n            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n            cum_num_nodes = num_nodes.cumsum(dim=0)\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n            new_edge_index, _ = dense_to_sparse(adj)\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop)\n            adj[row, col] = weights\n            induced_row, induced_col = induced_edge_index\n            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n            weights = adj[row, col]\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, num_nodes=x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        return x, new_edge_index, new_edge_attr, batch, perm"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 001: Loss = 1.6256, Val Acc = 0.0667, Micro F1 = 0.0667, Macro F1 = 0.0729\n",
                        "Epoch 002: Loss = 1.5627, Val Acc = 0.1111, Micro F1 = 0.1111, Macro F1 = 0.1238\n",
                        "Epoch 003: Loss = 1.5267, Val Acc = 0.0889, Micro F1 = 0.0889, Macro F1 = 0.1641\n",
                        "Epoch 004: Loss = 1.5141, Val Acc = 0.0889, Micro F1 = 0.0889, Macro F1 = 0.0382\n",
                        "Epoch 005: Loss = 1.4868, Val Acc = 0.1111, Micro F1 = 0.1111, Macro F1 = 0.0502\n",
                        "Epoch 006: Loss = 1.4597, Val Acc = 0.1333, Micro F1 = 0.1333, Macro F1 = 0.0612\n",
                        "Epoch 007: Loss = 1.4400, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1103\n",
                        "Epoch 008: Loss = 1.4176, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.1433\n",
                        "Epoch 009: Loss = 1.4215, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.1507\n",
                        "Epoch 010: Loss = 1.3777, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.1543\n",
                        "Epoch 011: Loss = 1.3807, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.1543\n",
                        "Epoch 012: Loss = 1.3479, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.1543\n",
                        "Epoch 013: Loss = 1.3506, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.1433\n",
                        "Epoch 014: Loss = 1.3107, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.1313\n",
                        "Epoch 015: Loss = 1.3023, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.1590\n",
                        "Epoch 016: Loss = 1.2730, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.1590\n",
                        "Epoch 017: Loss = 1.2491, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.1477\n",
                        "Epoch 018: Loss = 1.2405, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.1441\n",
                        "Epoch 019: Loss = 1.2056, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.1452\n",
                        "Epoch 020: Loss = 1.1889, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.1450\n",
                        "Epoch 021: Loss = 1.1783, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2272\n",
                        "Epoch 022: Loss = 1.1494, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2272\n",
                        "Epoch 023: Loss = 1.1343, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2272\n",
                        "Epoch 024: Loss = 1.1281, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2219\n",
                        "Epoch 025: Loss = 1.0961, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2267\n",
                        "Epoch 026: Loss = 1.0886, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2212\n",
                        "Epoch 027: Loss = 1.0767, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2074\n",
                        "Epoch 028: Loss = 1.0556, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2074\n",
                        "Epoch 029: Loss = 1.0330, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2021\n",
                        "Epoch 030: Loss = 1.0118, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1966\n",
                        "Epoch 031: Loss = 1.0105, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1897\n",
                        "Epoch 032: Loss = 1.0281, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.1961\n",
                        "Epoch 033: Loss = 0.9595, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2014\n",
                        "Epoch 034: Loss = 0.9325, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2147\n",
                        "Epoch 035: Loss = 0.9643, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2029\n",
                        "Epoch 036: Loss = 0.9082, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1654\n",
                        "Epoch 037: Loss = 0.8952, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1540\n",
                        "Epoch 038: Loss = 0.8765, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1540\n",
                        "Epoch 039: Loss = 0.8687, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1540\n",
                        "Epoch 040: Loss = 0.8827, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1598\n",
                        "Epoch 041: Loss = 0.8398, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1598\n",
                        "Epoch 042: Loss = 0.8357, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1598\n",
                        "Epoch 043: Loss = 0.7950, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1540\n",
                        "Epoch 044: Loss = 0.7936, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1540\n",
                        "Epoch 045: Loss = 0.7957, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2298\n",
                        "Epoch 046: Loss = 0.7942, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1957\n",
                        "Epoch 047: Loss = 0.7885, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1520\n",
                        "Epoch 048: Loss = 0.7561, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1404\n",
                        "Epoch 049: Loss = 0.7411, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 050: Loss = 0.7330, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1484\n",
                        "Epoch 051: Loss = 0.7342, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1484\n",
                        "Epoch 052: Loss = 0.7399, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1424\n",
                        "Epoch 053: Loss = 0.7137, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1424\n",
                        "Epoch 054: Loss = 0.7031, Val Acc = 0.2667, Micro F1 = 0.2667, Macro F1 = 0.1361\n",
                        "Epoch 055: Loss = 0.7085, Val Acc = 0.2667, Micro F1 = 0.2667, Macro F1 = 0.1417\n",
                        "Epoch 056: Loss = 0.7090, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1480\n",
                        "Epoch 057: Loss = 0.6608, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1540\n",
                        "Epoch 058: Loss = 0.6387, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1480\n",
                        "Epoch 059: Loss = 0.6704, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1480\n",
                        "Epoch 060: Loss = 0.6888, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 061: Loss = 0.6525, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 062: Loss = 0.6531, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1929\n",
                        "Epoch 063: Loss = 0.6411, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2652\n",
                        "Epoch 064: Loss = 0.6065, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2607\n",
                        "Epoch 065: Loss = 0.6592, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2652\n",
                        "Epoch 066: Loss = 0.6207, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1996\n",
                        "Epoch 067: Loss = 0.6084, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1404\n",
                        "Epoch 068: Loss = 0.6016, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1659\n",
                        "Epoch 069: Loss = 0.5815, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1944\n",
                        "Epoch 070: Loss = 0.6017, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2220\n",
                        "Epoch 071: Loss = 0.6096, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2368\n",
                        "Epoch 072: Loss = 0.5783, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2514\n",
                        "Epoch 073: Loss = 0.5777, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2514\n",
                        "Epoch 074: Loss = 0.5960, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2868\n",
                        "Epoch 075: Loss = 0.5494, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.3102\n",
                        "Epoch 076: Loss = 0.5671, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2603\n",
                        "Epoch 077: Loss = 0.5766, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1715\n",
                        "Epoch 078: Loss = 0.5822, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 079: Loss = 0.5426, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 080: Loss = 0.5808, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1715\n",
                        "Epoch 081: Loss = 0.5454, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1715\n",
                        "Epoch 082: Loss = 0.5246, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1639\n",
                        "Epoch 083: Loss = 0.4982, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1658\n",
                        "Epoch 084: Loss = 0.5414, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1658\n",
                        "Epoch 085: Loss = 0.4957, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2012\n",
                        "Epoch 086: Loss = 0.5257, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2263\n",
                        "Epoch 087: Loss = 0.5182, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2033\n",
                        "Epoch 088: Loss = 0.5025, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2271\n",
                        "Epoch 089: Loss = 0.5027, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2271\n",
                        "Epoch 090: Loss = 0.5202, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1787\n",
                        "Epoch 091: Loss = 0.4909, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1790\n",
                        "Epoch 092: Loss = 0.4866, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1775\n",
                        "Epoch 093: Loss = 0.5040, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1775\n",
                        "Epoch 094: Loss = 0.4622, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1775\n",
                        "Epoch 095: Loss = 0.4938, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1715\n",
                        "Epoch 096: Loss = 0.4436, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2343\n",
                        "Epoch 097: Loss = 0.4843, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.3028\n",
                        "Epoch 098: Loss = 0.4914, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.3032\n",
                        "Epoch 099: Loss = 0.4766, Val Acc = 0.2667, Micro F1 = 0.2667, Macro F1 = 0.2261\n",
                        "Epoch 100: Loss = 0.4960, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.3032\n",
                        "Epoch 101: Loss = 0.4900, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.3061\n",
                        "Epoch 102: Loss = 0.4984, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1992\n",
                        "Epoch 103: Loss = 0.4947, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2054\n",
                        "Epoch 104: Loss = 0.4567, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1762\n",
                        "Epoch 105: Loss = 0.4668, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1982\n",
                        "Epoch 106: Loss = 0.4606, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1900\n",
                        "Epoch 107: Loss = 0.4261, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1791\n",
                        "Epoch 108: Loss = 0.4807, Val Acc = 0.2667, Micro F1 = 0.2667, Macro F1 = 0.1573\n",
                        "Epoch 109: Loss = 0.5028, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1891\n",
                        "Epoch 110: Loss = 0.4990, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2240\n",
                        "Epoch 111: Loss = 0.5293, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2240\n",
                        "Epoch 112: Loss = 0.4224, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1972\n",
                        "Epoch 113: Loss = 0.4417, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2482\n",
                        "Epoch 114: Loss = 0.4081, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1881\n",
                        "Epoch 115: Loss = 0.4571, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2209\n",
                        "Epoch 116: Loss = 0.4227, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2211\n",
                        "Epoch 117: Loss = 0.4783, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2494\n",
                        "Epoch 118: Loss = 0.4498, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2744\n",
                        "Epoch 119: Loss = 0.4507, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2935\n",
                        "Epoch 120: Loss = 0.4206, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2647\n",
                        "Epoch 121: Loss = 0.5106, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2838\n",
                        "Epoch 122: Loss = 0.4784, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2838\n",
                        "Epoch 123: Loss = 0.4060, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2421\n",
                        "Epoch 124: Loss = 0.4253, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2603\n",
                        "Epoch 125: Loss = 0.4538, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2514\n",
                        "Epoch 126: Loss = 0.4660, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2514\n",
                        "Epoch 127: Loss = 0.4593, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2687\n",
                        "Epoch 128: Loss = 0.4553, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2778\n",
                        "Epoch 129: Loss = 0.4809, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2299\n",
                        "Epoch 130: Loss = 0.4104, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2013\n",
                        "Epoch 131: Loss = 0.3877, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2080\n",
                        "Epoch 132: Loss = 0.4392, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2266\n",
                        "Epoch 133: Loss = 0.4219, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2184\n",
                        "Epoch 134: Loss = 0.4164, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2438\n",
                        "Epoch 135: Loss = 0.4209, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2334\n",
                        "Epoch 136: Loss = 0.4168, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2334\n",
                        "Epoch 137: Loss = 0.4178, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2209\n",
                        "Epoch 138: Loss = 0.3873, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1746\n",
                        "Epoch 139: Loss = 0.3821, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1746\n",
                        "Epoch 140: Loss = 0.4338, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1746\n",
                        "Epoch 141: Loss = 0.4239, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1746\n",
                        "Epoch 142: Loss = 0.4434, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2321\n",
                        "Epoch 143: Loss = 0.4451, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2873\n",
                        "Epoch 144: Loss = 0.4706, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2687\n",
                        "Epoch 145: Loss = 0.4059, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2424\n",
                        "Epoch 146: Loss = 0.3812, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1639\n",
                        "Epoch 147: Loss = 0.3834, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1659\n",
                        "Epoch 148: Loss = 0.4355, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2080\n",
                        "Epoch 149: Loss = 0.4376, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1997\n",
                        "Epoch 150: Loss = 0.4165, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2422\n",
                        "Epoch 151: Loss = 0.4050, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2551\n",
                        "Epoch 152: Loss = 0.3814, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2782\n",
                        "Epoch 153: Loss = 0.3924, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2782\n",
                        "Epoch 154: Loss = 0.3880, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2782\n",
                        "Epoch 155: Loss = 0.4633, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2560\n",
                        "Epoch 156: Loss = 0.3886, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2136\n",
                        "Epoch 157: Loss = 0.4000, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.1943\n",
                        "Epoch 158: Loss = 0.3857, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1870\n",
                        "Epoch 159: Loss = 0.4187, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1722\n",
                        "Early stopping at epoch 160\n",
                        "Epoch 001: Loss = 1.6441, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.1855\n",
                        "Epoch 002: Loss = 1.5948, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.1919\n",
                        "Epoch 003: Loss = 1.5227, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.1960\n",
                        "Epoch 004: Loss = 1.5116, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.1887\n",
                        "Epoch 005: Loss = 1.4796, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.1821\n",
                        "Epoch 006: Loss = 1.4576, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.2096\n",
                        "Epoch 007: Loss = 1.4310, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2338\n",
                        "Epoch 008: Loss = 1.4176, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2423\n",
                        "Epoch 009: Loss = 1.3964, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2423\n",
                        "Epoch 010: Loss = 1.3599, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2338\n",
                        "Epoch 011: Loss = 1.3473, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.1552\n",
                        "Epoch 012: Loss = 1.3414, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.1477\n",
                        "Epoch 013: Loss = 1.3301, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.1770\n",
                        "Epoch 014: Loss = 1.3024, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.1709\n",
                        "Epoch 015: Loss = 1.2565, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.1667\n",
                        "Epoch 016: Loss = 1.2531, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.1667\n",
                        "Epoch 017: Loss = 1.2247, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.1667\n",
                        "Epoch 018: Loss = 1.1990, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.1686\n",
                        "Epoch 019: Loss = 1.1480, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.1686\n",
                        "Epoch 020: Loss = 1.1714, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.1424\n",
                        "Epoch 021: Loss = 1.1464, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.1995\n",
                        "Epoch 022: Loss = 1.1098, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2038\n",
                        "Epoch 023: Loss = 1.0903, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2038\n",
                        "Epoch 024: Loss = 1.0796, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.1967\n",
                        "Epoch 025: Loss = 1.0725, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.1879\n",
                        "Epoch 026: Loss = 1.0359, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.1786\n",
                        "Epoch 027: Loss = 1.0765, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1632\n",
                        "Epoch 028: Loss = 1.0173, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 029: Loss = 0.9943, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1577\n",
                        "Epoch 030: Loss = 0.9492, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.1630\n",
                        "Epoch 031: Loss = 0.9859, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.1730\n",
                        "Epoch 032: Loss = 0.9527, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.1879\n",
                        "Epoch 033: Loss = 0.9640, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.1786\n",
                        "Epoch 034: Loss = 0.9318, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.1685\n",
                        "Epoch 035: Loss = 0.8847, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1632\n",
                        "Epoch 036: Loss = 0.9063, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1520\n",
                        "Epoch 037: Loss = 0.8718, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1787\n",
                        "Epoch 038: Loss = 0.8563, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 039: Loss = 0.8186, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1520\n",
                        "Epoch 040: Loss = 0.8518, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1520\n",
                        "Epoch 041: Loss = 0.8362, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1520\n",
                        "Epoch 042: Loss = 0.8116, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 043: Loss = 0.8014, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 044: Loss = 0.7685, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 045: Loss = 0.7894, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2121\n",
                        "Epoch 046: Loss = 0.7451, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2121\n",
                        "Epoch 047: Loss = 0.7700, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2385\n",
                        "Epoch 048: Loss = 0.7367, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2121\n",
                        "Epoch 049: Loss = 0.7514, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1818\n",
                        "Epoch 050: Loss = 0.7110, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 051: Loss = 0.7064, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 052: Loss = 0.7098, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 053: Loss = 0.7183, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 054: Loss = 0.6809, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1734\n",
                        "Epoch 055: Loss = 0.7059, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2369\n",
                        "Epoch 056: Loss = 0.7003, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2369\n",
                        "Epoch 057: Loss = 0.6969, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2121\n",
                        "Epoch 058: Loss = 0.6722, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 059: Loss = 0.6824, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 060: Loss = 0.6438, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1734\n",
                        "Epoch 061: Loss = 0.6248, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1734\n",
                        "Epoch 062: Loss = 0.6242, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2155\n",
                        "Epoch 063: Loss = 0.6583, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2369\n",
                        "Epoch 064: Loss = 0.6682, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2459\n",
                        "Epoch 065: Loss = 0.6132, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2548\n",
                        "Epoch 066: Loss = 0.6145, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1818\n",
                        "Epoch 067: Loss = 0.6134, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2393\n",
                        "Epoch 068: Loss = 0.5871, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2459\n",
                        "Epoch 069: Loss = 0.6041, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2514\n",
                        "Epoch 070: Loss = 0.6026, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2459\n",
                        "Epoch 071: Loss = 0.6176, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2369\n",
                        "Epoch 072: Loss = 0.6052, Val Acc = 0.2667, Micro F1 = 0.2667, Macro F1 = 0.1342\n",
                        "Epoch 073: Loss = 0.6333, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 074: Loss = 0.5806, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 075: Loss = 0.5879, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1577\n",
                        "Epoch 076: Loss = 0.6075, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1577\n",
                        "Epoch 077: Loss = 0.5624, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1577\n",
                        "Epoch 078: Loss = 0.6056, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 079: Loss = 0.5713, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1404\n",
                        "Epoch 080: Loss = 0.5623, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1669\n",
                        "Epoch 081: Loss = 0.5613, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2159\n",
                        "Epoch 082: Loss = 0.5302, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2424\n",
                        "Epoch 083: Loss = 0.5727, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2122\n",
                        "Epoch 084: Loss = 0.5613, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1715\n",
                        "Epoch 085: Loss = 0.5354, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1520\n",
                        "Epoch 086: Loss = 0.5381, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1577\n",
                        "Epoch 087: Loss = 0.5695, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1577\n",
                        "Epoch 088: Loss = 0.5447, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1404\n",
                        "Epoch 089: Loss = 0.5174, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2013\n",
                        "Epoch 090: Loss = 0.6167, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2689\n",
                        "Epoch 091: Loss = 0.4977, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.3018\n",
                        "Epoch 092: Loss = 0.5284, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2441\n",
                        "Epoch 093: Loss = 0.5158, Val Acc = 0.2444, Micro F1 = 0.2444, Macro F1 = 0.1920\n",
                        "Epoch 094: Loss = 0.5648, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2768\n",
                        "Epoch 095: Loss = 0.5280, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2505\n",
                        "Epoch 096: Loss = 0.5485, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 097: Loss = 0.5583, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.1685\n",
                        "Epoch 098: Loss = 0.5529, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.1685\n",
                        "Epoch 099: Loss = 0.5177, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1577\n",
                        "Epoch 100: Loss = 0.5294, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1577\n",
                        "Epoch 101: Loss = 0.4978, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1790\n",
                        "Epoch 102: Loss = 0.5023, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1730\n",
                        "Epoch 103: Loss = 0.5374, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2058\n",
                        "Epoch 104: Loss = 0.5068, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2287\n",
                        "Epoch 105: Loss = 0.4759, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2287\n",
                        "Epoch 106: Loss = 0.5223, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1997\n",
                        "Epoch 107: Loss = 0.4654, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1710\n",
                        "Epoch 108: Loss = 0.4725, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1404\n",
                        "Epoch 109: Loss = 0.4724, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1997\n",
                        "Epoch 110: Loss = 0.5046, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2287\n",
                        "Epoch 111: Loss = 0.4880, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2369\n",
                        "Epoch 112: Loss = 0.4723, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2068\n",
                        "Epoch 113: Loss = 0.4486, Val Acc = 0.2667, Micro F1 = 0.2667, Macro F1 = 0.1629\n",
                        "Epoch 114: Loss = 0.5103, Val Acc = 0.2667, Micro F1 = 0.2667, Macro F1 = 0.1629\n",
                        "Epoch 115: Loss = 0.4542, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1697\n",
                        "Epoch 116: Loss = 0.4705, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1751\n",
                        "Epoch 117: Loss = 0.4440, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1944\n",
                        "Epoch 118: Loss = 0.4655, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2585\n",
                        "Epoch 119: Loss = 0.4648, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2577\n",
                        "Epoch 120: Loss = 0.4763, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2817\n",
                        "Epoch 121: Loss = 0.4519, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2817\n",
                        "Epoch 122: Loss = 0.4689, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2761\n",
                        "Epoch 123: Loss = 0.4362, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2761\n",
                        "Epoch 124: Loss = 0.4579, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2426\n",
                        "Epoch 125: Loss = 0.4466, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2438\n",
                        "Epoch 126: Loss = 0.4601, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1825\n",
                        "Epoch 127: Loss = 0.4815, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1659\n",
                        "Epoch 128: Loss = 0.4339, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1720\n",
                        "Epoch 129: Loss = 0.4365, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1720\n",
                        "Epoch 130: Loss = 0.4258, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1720\n",
                        "Epoch 131: Loss = 0.4244, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 132: Loss = 0.4648, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1577\n",
                        "Epoch 133: Loss = 0.4701, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 134: Loss = 0.4758, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 135: Loss = 0.4307, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 136: Loss = 0.4745, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1978\n",
                        "Epoch 137: Loss = 0.4455, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2102\n",
                        "Epoch 138: Loss = 0.4143, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2346\n",
                        "Epoch 139: Loss = 0.4959, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2346\n",
                        "Epoch 140: Loss = 0.4039, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2068\n",
                        "Epoch 141: Loss = 0.4219, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1714\n",
                        "Epoch 142: Loss = 0.4350, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1720\n",
                        "Epoch 143: Loss = 0.4191, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1792\n",
                        "Epoch 144: Loss = 0.4153, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1720\n",
                        "Epoch 145: Loss = 0.4212, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1802\n",
                        "Epoch 146: Loss = 0.4323, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2402\n",
                        "Epoch 147: Loss = 0.4452, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2387\n",
                        "Epoch 148: Loss = 0.3744, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2158\n",
                        "Epoch 149: Loss = 0.4284, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2102\n",
                        "Epoch 150: Loss = 0.3870, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2266\n",
                        "Epoch 151: Loss = 0.4015, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1611\n",
                        "Epoch 152: Loss = 0.3733, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1611\n",
                        "Epoch 153: Loss = 0.4105, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1611\n",
                        "Epoch 154: Loss = 0.4069, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1596\n",
                        "Epoch 155: Loss = 0.4039, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1596\n",
                        "Epoch 156: Loss = 0.3813, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1596\n",
                        "Early stopping at epoch 157\n",
                        "Epoch 001: Loss = 1.5737, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.2043\n",
                        "Epoch 002: Loss = 1.5152, Val Acc = 0.6444, Micro F1 = 0.6444, Macro F1 = 0.2077\n",
                        "Epoch 003: Loss = 1.5094, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2183\n",
                        "Epoch 004: Loss = 1.4789, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2183\n",
                        "Epoch 005: Loss = 1.4425, Val Acc = 0.6444, Micro F1 = 0.6444, Macro F1 = 0.2044\n",
                        "Epoch 006: Loss = 1.4517, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.1824\n",
                        "Epoch 007: Loss = 1.4075, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.1954\n",
                        "Epoch 008: Loss = 1.3916, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.1627\n",
                        "Epoch 009: Loss = 1.3672, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.1627\n",
                        "Epoch 010: Loss = 1.3665, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.1691\n",
                        "Epoch 011: Loss = 1.3245, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.1821\n",
                        "Epoch 012: Loss = 1.3168, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.1821\n",
                        "Epoch 013: Loss = 1.2987, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.2057\n",
                        "Epoch 014: Loss = 1.2765, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.2057\n",
                        "Epoch 015: Loss = 1.2488, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.1792\n",
                        "Epoch 016: Loss = 1.2307, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.2817\n",
                        "Epoch 017: Loss = 1.1960, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.2750\n",
                        "Epoch 018: Loss = 1.1938, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.2067\n",
                        "Epoch 019: Loss = 1.1874, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.2067\n",
                        "Epoch 020: Loss = 1.1329, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.2067\n",
                        "Epoch 021: Loss = 1.1122, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.1977\n",
                        "Epoch 022: Loss = 1.1041, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.1905\n",
                        "Epoch 023: Loss = 1.1223, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.1905\n",
                        "Epoch 024: Loss = 1.0714, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.1905\n",
                        "Epoch 025: Loss = 1.0697, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.1924\n",
                        "Epoch 026: Loss = 1.0406, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.1924\n",
                        "Epoch 027: Loss = 0.9975, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.1786\n",
                        "Epoch 028: Loss = 0.9850, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.1736\n",
                        "Epoch 029: Loss = 0.9615, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.1685\n",
                        "Epoch 030: Loss = 0.9705, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1632\n",
                        "Epoch 031: Loss = 0.9610, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1632\n",
                        "Epoch 032: Loss = 0.9239, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 033: Loss = 0.9223, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 034: Loss = 0.9016, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 035: Loss = 0.8617, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1424\n",
                        "Epoch 036: Loss = 0.8998, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1424\n",
                        "Epoch 037: Loss = 0.8556, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1808\n",
                        "Epoch 038: Loss = 0.8540, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2162\n",
                        "Epoch 039: Loss = 0.8477, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1978\n",
                        "Epoch 040: Loss = 0.8251, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1791\n",
                        "Epoch 041: Loss = 0.8011, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1484\n",
                        "Epoch 042: Loss = 0.7970, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1598\n",
                        "Epoch 043: Loss = 0.8074, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1577\n",
                        "Epoch 044: Loss = 0.7989, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 045: Loss = 0.7679, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2239\n",
                        "Epoch 046: Loss = 0.7554, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2502\n",
                        "Epoch 047: Loss = 0.7574, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2733\n",
                        "Epoch 048: Loss = 0.7177, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2922\n",
                        "Epoch 049: Loss = 0.7408, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2946\n",
                        "Epoch 050: Loss = 0.7140, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2557\n",
                        "Epoch 051: Loss = 0.7605, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1735\n",
                        "Epoch 052: Loss = 0.7151, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1484\n",
                        "Epoch 053: Loss = 0.7166, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1542\n",
                        "Epoch 054: Loss = 0.6918, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1484\n",
                        "Epoch 055: Loss = 0.6897, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1484\n",
                        "Epoch 056: Loss = 0.7340, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2034\n",
                        "Epoch 057: Loss = 0.6611, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1990\n",
                        "Epoch 058: Loss = 0.6553, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2272\n",
                        "Epoch 059: Loss = 0.6341, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2272\n",
                        "Epoch 060: Loss = 0.6607, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2272\n",
                        "Epoch 061: Loss = 0.6655, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2013\n",
                        "Epoch 062: Loss = 0.6615, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1659\n",
                        "Epoch 063: Loss = 0.6294, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 064: Loss = 0.6101, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 065: Loss = 0.6516, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 066: Loss = 0.6563, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1404\n",
                        "Epoch 067: Loss = 0.6394, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2538\n",
                        "Epoch 068: Loss = 0.6149, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2977\n",
                        "Epoch 069: Loss = 0.6096, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2841\n",
                        "Epoch 070: Loss = 0.6371, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2977\n",
                        "Epoch 071: Loss = 0.5948, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2680\n",
                        "Epoch 072: Loss = 0.5690, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2287\n",
                        "Epoch 073: Loss = 0.5791, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 074: Loss = 0.6157, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 075: Loss = 0.5906, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 076: Loss = 0.5712, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1464\n",
                        "Epoch 077: Loss = 0.6124, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1583\n",
                        "Epoch 078: Loss = 0.5601, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1936\n",
                        "Epoch 079: Loss = 0.5780, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2281\n",
                        "Epoch 080: Loss = 0.5552, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.2188\n",
                        "Epoch 081: Loss = 0.6706, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2459\n",
                        "Epoch 082: Loss = 0.5768, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2052\n",
                        "Epoch 083: Loss = 0.5251, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2052\n",
                        "Epoch 084: Loss = 0.5957, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 085: Loss = 0.5754, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1460\n",
                        "Epoch 086: Loss = 0.6050, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1735\n",
                        "Epoch 087: Loss = 0.5838, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1658\n",
                        "Epoch 088: Loss = 0.5527, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1957\n",
                        "Epoch 089: Loss = 0.5169, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2863\n",
                        "Epoch 090: Loss = 0.5396, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2863\n",
                        "Epoch 091: Loss = 0.5247, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.3163\n",
                        "Epoch 092: Loss = 0.5550, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2761\n",
                        "Epoch 093: Loss = 0.5521, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2538\n",
                        "Epoch 094: Loss = 0.5036, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2016\n",
                        "Epoch 095: Loss = 0.5382, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 096: Loss = 0.5928, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 097: Loss = 0.5918, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1561\n",
                        "Epoch 098: Loss = 0.5232, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1765\n",
                        "Epoch 099: Loss = 0.5419, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2081\n",
                        "Epoch 100: Loss = 0.5234, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2632\n",
                        "Epoch 101: Loss = 0.4919, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2597\n",
                        "Epoch 102: Loss = 0.4843, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2913\n",
                        "Epoch 103: Loss = 0.5333, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2719\n",
                        "Epoch 104: Loss = 0.4604, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2287\n",
                        "Epoch 105: Loss = 0.4725, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2287\n",
                        "Epoch 106: Loss = 0.4583, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1720\n",
                        "Epoch 107: Loss = 0.5189, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1572\n",
                        "Epoch 108: Loss = 0.5041, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.1762\n",
                        "Epoch 109: Loss = 0.4948, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1695\n",
                        "Epoch 110: Loss = 0.4640, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2369\n",
                        "Epoch 111: Loss = 0.5288, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2266\n",
                        "Epoch 112: Loss = 0.4448, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2505\n",
                        "Epoch 113: Loss = 0.4988, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2505\n",
                        "Epoch 114: Loss = 0.5007, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2505\n",
                        "Epoch 115: Loss = 0.4716, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2266\n",
                        "Epoch 116: Loss = 0.4625, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2266\n",
                        "Epoch 117: Loss = 0.4611, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2208\n",
                        "Epoch 118: Loss = 0.5046, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2127\n",
                        "Epoch 119: Loss = 0.4615, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2369\n",
                        "Epoch 120: Loss = 0.4579, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2369\n",
                        "Epoch 121: Loss = 0.4532, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2346\n",
                        "Epoch 122: Loss = 0.5929, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2376\n",
                        "Epoch 123: Loss = 0.4577, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2376\n",
                        "Epoch 124: Loss = 0.4438, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2422\n",
                        "Epoch 125: Loss = 0.4551, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2216\n",
                        "Epoch 126: Loss = 0.5109, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2271\n",
                        "Epoch 127: Loss = 0.5162, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2033\n",
                        "Epoch 128: Loss = 0.4609, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2033\n",
                        "Epoch 129: Loss = 0.5026, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2321\n",
                        "Epoch 130: Loss = 0.4862, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2478\n",
                        "Epoch 131: Loss = 0.4403, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2651\n",
                        "Epoch 132: Loss = 0.4403, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2625\n",
                        "Epoch 133: Loss = 0.4625, Val Acc = 0.2444, Micro F1 = 0.2444, Macro F1 = 0.2417\n",
                        "Epoch 134: Loss = 0.5730, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.2544\n",
                        "Epoch 135: Loss = 0.6285, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2596\n",
                        "Epoch 136: Loss = 0.4453, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2366\n",
                        "Epoch 137: Loss = 0.4273, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1792\n",
                        "Epoch 138: Loss = 0.4761, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1577\n",
                        "Epoch 139: Loss = 0.5205, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.1577\n",
                        "Epoch 140: Loss = 0.4562, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1521\n",
                        "Epoch 141: Loss = 0.4764, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.1792\n",
                        "Epoch 142: Loss = 0.4333, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2316\n",
                        "Epoch 143: Loss = 0.4266, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2376\n",
                        "Epoch 144: Loss = 0.4374, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2336\n",
                        "Epoch 145: Loss = 0.4361, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.2054\n",
                        "Epoch 146: Loss = 0.4643, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1999\n",
                        "Epoch 147: Loss = 0.4348, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.1999\n",
                        "Epoch 148: Loss = 0.4718, Val Acc = 0.3111, Micro F1 = 0.3111, Macro F1 = 0.2078\n",
                        "Epoch 149: Loss = 0.4195, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2013\n",
                        "Epoch 150: Loss = 0.3788, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2080\n",
                        "Epoch 151: Loss = 0.4170, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2366\n",
                        "Epoch 152: Loss = 0.4071, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2366\n",
                        "Early stopping at epoch 153\n",
                        "Micro F1: 34.07$\\pm$1.05\n",
                        "Macro F1: 21.39$\\pm$1.41\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Cornell.num_classes\nin_channels = dataset_Cornell.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_HGPSL(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_HGPSL, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(HGPSLPool(hidden_channels, ratio=pool_ratios[i], sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        edge_attr = None\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, edge_attr, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        micro_f1, macro_f1 = compute_f1_scores(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            print(f'Early stopping at epoch {epoch}')\n            break\n        print(f'Epoch {epoch:03d}: Loss = {loss.item():.4f}, Val Acc = {val_acc:.4f}, Micro F1 = {micro_f1:.4f}, Macro F1 = {macro_f1:.4f}')\n    return model, best_val_acc\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_HGPSL(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "CG-ODE",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}