{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\nimport sys\nimport torch\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom ogb.graphproppred import PygGraphPropPredDataset, Evaluator\nfrom torch_geometric.loader import DataLoader\nimport os\nimport random\nimport pandas as pd\nimport torch\nimport torch_geometric.transforms as T\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data\nfrom torch_geometric.data.datapipes import functional_transform\nfrom torch_geometric.transforms import BaseTransform\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import Actor\nfrom torch_geometric.datasets import GNNBenchmarkDataset\nfrom torch_geometric.datasets import TUDataset\nfrom sklearn.metrics import r2_score\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import MoleculeNet\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.utils import to_networkx\nfrom torch.nn import Linear\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport time\nimport psutil\nimport torch\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom typing import List, Optional, Tuple, Union\nimport math\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.nn.models.mlp import Linear\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import BatchNorm\nimport os.path as osp\nimport time\nfrom math import ceil\nfrom torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nfrom torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nimport random\nfrom torch_geometric.nn import GCNConv\nimport os.path as osp\nimport time\nfrom math import ceil\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, dense_diff_pool\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, TopKPooling, SAGPooling\nfrom torch_geometric.utils import to_dense_batch\nfrom sklearn.metrics import f1_score"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train Mask: 91 nodes\n",
                        "Val Mask: 45 nodes\n",
                        "Test Mask: 47 nodes\n",
                        "Train Mask Average Degree: 3.3626\n",
                        "Val Mask Average Degree: 0.4222\n",
                        "Test Mask Average Degree: 0.0000\n"
                    ]
                }
            ],
            "source": "import torch\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.utils import degree\ndata_path = \"/data/ /Pooling/\"\ndataset_Texas = WebKB(root=data_path, name=\"Texas\")\ndata = dataset_Texas[0]\ndeg = degree(data.edge_index[0], data.num_nodes)\nsorted_indices = torch.argsort(deg, descending=True)\nnum_nodes = data.num_nodes\ntrain_size = int(0.5 * num_nodes)\nval_size = int(0.25 * num_nodes)\ntest_size = num_nodes - train_size - val_size\ntrain_indices = sorted_indices[:train_size]\nval_indices = sorted_indices[train_size:train_size + val_size]\ntest_indices = sorted_indices[train_size + val_size:]\ntrain_mask = torch.zeros(num_nodes, dtype=torch.bool)\nval_mask = torch.zeros(num_nodes, dtype=torch.bool)\ntest_mask = torch.zeros(num_nodes, dtype=torch.bool)\ntrain_mask[train_indices] = True\nval_mask[val_indices] = True\ntest_mask[test_indices] = True\ndata.train_mask = train_mask\ndata.val_mask = val_mask\ndata.test_mask = test_mask\nprint(f\"Train Mask: {train_mask.sum().item()} nodes\")\nprint(f\"Val Mask: {val_mask.sum().item()} nodes\")\nprint(f\"Test Mask: {test_mask.sum().item()} nodes\")\ntrain_deg_avg = deg[train_mask].float().mean().item()\nval_deg_avg = deg[val_mask].float().mean().item()\ntest_deg_avg = deg[test_mask].float().mean().item()\nprint(f\"Train Mask Average Degree: {train_deg_avg:.4f}\")\nprint(f\"Val Mask Average Degree: {val_deg_avg:.4f}\")\nprint(f\"Test Mask Average Degree: {test_deg_avg:.4f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TopKPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 40.74$\\pm$2.10\n",
                        "Macro F1: 27.06$\\pm$2.71\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Texas.num_classes\nin_channels = dataset_Texas.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_TOPK(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_TOPK, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(TopKPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_TOPK(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SAGPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 39.26$\\pm$1.05\n",
                        "Macro F1: 23.52$\\pm$0.35\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import SAGPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Texas.num_classes\nin_channels = dataset_Texas.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_SAG(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_SAG, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(SAGPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_SAG(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ASAPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 40.74$\\pm$2.77\n",
                        "Macro F1: 23.87$\\pm$0.67\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Texas.num_classes\nin_channels = dataset_Texas.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_ASA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_ASA, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(ASAPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_ASA(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PANPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nclass PANPooling(torch.nn.Module):\n    r\"\"\" General Graph pooling layer based on PAN, which can work with all layers.\n    \"\"\"\n    def __init__(self, in_channels, ratio=0.5, pan_pool_weight=None, min_score=None, multiplier=1,\n                 nonlinearity=torch.tanh, filter_size=3, panpool_filter_weight=None):\n        super(PANPooling, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.nonlinearity = nonlinearity\n        self.filter_size = filter_size\n        if panpool_filter_weight is None:\n            self.panpool_filter_weight = torch.nn.Parameter(0.5 * torch.ones(filter_size), requires_grad=True)\n        self.transform = Parameter(torch.ones(in_channels), requires_grad=True)\n        if pan_pool_weight is None:\n            self.pan_pool_weight = torch.nn.Parameter(0.5 * torch.ones(2), requires_grad=True)\n        else:\n            self.pan_pool_weight = pan_pool_weight\n    def forward(self, x, edge_index, M=None, batch=None, num_nodes=None):\n        \"\"\"\"\"\"\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        edge_index, edge_weight = self.panentropy_sparse(edge_index, num_nodes)\n        num_nodes = x.size(0)\n        degree = torch.zeros(num_nodes, device=edge_index.device)\n        degree = scatter_add(edge_weight, edge_index[0], out=degree)\n        xtransform = torch.matmul(x, self.transform)\n        x_transform_norm = xtransform \n        degree_norm = degree \n        score = self.pan_pool_weight[0] * x_transform_norm + self.pan_pool_weight[1] * degree_norm\n        if self.min_score is None:\n            score = self.nonlinearity(score)\n        else:\n            score = softmax(score, batch)\n        perm = self.topk(score, self.ratio, batch, self.min_score)\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n        batch = batch[perm]\n        edge_index, edge_weight = self.filter_adj(edge_index, edge_weight, perm, num_nodes=score.size(0))\n        return x, edge_index, edge_weight, batch, perm, score[perm]\n    def topk(self, x, ratio, batch, min_score=None, tol=1e-7):\n        if min_score is not None:\n            scores_max = scatter_max(x, batch)[0][batch] - tol\n            scores_min = scores_max.clamp(max=min_score)\n            perm = torch.nonzero(x > scores_min).view(-1)\n        else:\n            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n            batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n            cum_num_nodes = torch.cat(\n                [num_nodes.new_zeros(1),\n                 num_nodes.cumsum(dim=0)[:-1]], dim=0)\n            index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n            index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n            dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n            dense_x[index] = x\n            dense_x = dense_x.view(batch_size, max_num_nodes)\n            _, perm = dense_x.sort(dim=-1, descending=True)\n            perm = perm + cum_num_nodes.view(-1, 1)\n            perm = perm.view(-1)\n            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n            mask = [\n                torch.arange(k[i], dtype=torch.long, device=x.device) +\n                i * max_num_nodes for i in range(batch_size)\n            ]\n            mask = torch.cat(mask, dim=0)\n            perm = perm[mask]\n        return perm\n    def filter_adj(self, edge_index, edge_weight, perm, num_nodes=None):\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        mask = perm.new_full((num_nodes, ), -1)\n        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n        mask[perm] = i\n        row, col = edge_index\n        row, col = mask[row], mask[col]\n        mask = (row >= 0) & (col >= 0)\n        row, col = row[mask], col[mask]\n        if edge_weight is not None:\n            edge_weight = edge_weight[mask]\n        return torch.stack([row, col], dim=0), edge_weight\n    def panentropy_sparse(self, edge_index, num_nodes):\n        edge_value = torch.ones(edge_index.size(1), device=edge_index.device)\n        edge_index, edge_value = coalesce(edge_index, edge_value, num_nodes, num_nodes)\n        pan_index, pan_value = eye(num_nodes, device=edge_index.device)\n        indextmp = pan_index.clone().to(edge_index.device)\n        valuetmp = pan_value.clone().to(edge_index.device)\n        pan_value = self.panpool_filter_weight[0] * pan_value\n        for i in range(self.filter_size - 1):\n            indextmp, valuetmp = spspmm(indextmp, valuetmp, edge_index, edge_value, num_nodes, num_nodes, num_nodes)\n            valuetmp = valuetmp * self.panpool_filter_weight[i+1]\n            indextmp, valuetmp = coalesce(indextmp, valuetmp, num_nodes, num_nodes)\n            pan_index = torch.cat((pan_index, indextmp), 1)\n            pan_value = torch.cat((pan_value, valuetmp))\n        return coalesce(pan_index, pan_value, num_nodes, num_nodes, op='add')"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 40.00$\\pm$3.63\n",
                        "Macro F1: 23.64$\\pm$1.13\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Texas.num_classes\nin_channels = dataset_Texas.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_PAN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_PAN, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(PANPooling(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, score_perm = self.pools[i - 1](x, edge_index, batch=batch, M=None)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_PAN(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CoPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\ndef cumsum(x: Tensor, dim: int = 0) -> Tensor:\n    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n    Args:\n        x (torch.Tensor): The input tensor.\n        dim (int, optional): The dimension to do the operation over.\n            (default: :obj:`0`)\n    Example:\n        >>> x = torch.tensor([2, 4, 1])\n        >>> cumsum(x)\n        tensor([0, 2, 6, 7])\n    \"\"\"\n    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n    out = x.new_empty(size)\n    out.narrow(dim, 0, 1).zero_()\n    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n    return out\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\ndef topk(\n    x: Tensor,\n    ratio: Optional[Union[float, int]],\n    batch: Tensor,\n    min_score: Optional[float] = None,\n    tol: float = 1e-7,\n) -> Tensor:\n    if min_score is not None:\n        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero().view(-1)\n        return perm\n    if ratio is not None:\n        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n        if ratio >= 1:\n            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n        else:\n            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n        x, x_perm = torch.sort(x.view(-1), descending=True)\n        batch = batch[x_perm]\n        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n        ptr = cumsum(num_nodes)\n        batched_arange = arange - ptr[batch]\n        mask = batched_arange < k[batch]\n        return x_perm[batch_perm[mask]]\n    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n                     \"must be specified\")\nclass GPR_prop(MessagePassing):\n    '''\n    propagation class for GPR_GNN\n    '''\n    def __init__(self, K, alpha, Init, Gamma=None, bias=True, **kwargs):\n        super(GPR_prop, self).__init__(aggr='add', **kwargs)\n        self.K = K\n        self.Init = Init\n        self.alpha = alpha\n        assert Init in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']\n        if Init == 'SGC':\n            TEMP = 0.0*np.ones(K+1)\n            TEMP[alpha] = 1.0\n        elif Init == 'PPR':\n            TEMP = alpha*(1-alpha)**np.arange(K+1)\n            TEMP[-1] = (1-alpha)**K\n        elif Init == 'NPPR':\n            TEMP = (alpha)**np.arange(K+1)\n            TEMP = TEMP/np.sum(np.abs(TEMP))\n        elif Init == 'Random':\n            bound = np.sqrt(3/(K+1))\n            TEMP = np.random.uniform(-bound, bound, K+1)\n            TEMP = TEMP/np.sum(np.abs(TEMP))\n        elif Init == 'WS':\n            TEMP = Gamma\n        self.temp = Parameter(torch.tensor(TEMP))\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.temp)\n        for k in range(self.K+1):\n            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n        self.temp.data[-1] = (1-self.alpha)**self.K\n    def forward(self, x, edge_index, edge_weight=None):\n        edge_index, norm = gcn_norm(\n            edge_index, edge_weight, num_nodes=x.size(0), dtype=x.dtype)\n        hidden = x*(self.temp[0])\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=norm)\n            gamma = self.temp[k+1]\n            hidden = hidden + gamma*x\n        return hidden\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def __repr__(self):\n        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n                                           self.temp)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) \n        edge_index = edge_index.type(torch.long)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass graph_attention(torch.nn.Module):\n    src_nodes_dim = 0  \n    trg_nodes_dim = 1  \n    nodes_dim = 0      \n    head_dim = 1       \n    def __init__(self, num_in_features, num_out_features, num_of_heads, dropout_prob=0.6, log_attention_weights=False):\n        super().__init__()\n        self.num_of_heads = num_of_heads\n        self.num_out_features = num_out_features\n        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.init_params()\n    def init_params(self):\n        \"\"\"\n        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n        Feel free to experiment - there may be better initializations depending on your problem.\n        \"\"\"\n        nn.init.xavier_uniform_(self.linear_proj.weight)\n        nn.init.xavier_uniform_(self.scoring_fn_target)\n        nn.init.xavier_uniform_(self.scoring_fn_source)\n    def forward(self, x, edge_index):\n        in_nodes_features = x  \n        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n        scores_per_edge = scores_source_lifted + scores_target_lifted\n        return torch.sigmoid(scores_per_edge)\n    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n        \"\"\"\n        Lifts i.e. duplicates certain vectors depending on the edge index.\n        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n        \"\"\"\n        src_nodes_index = edge_index[self.src_nodes_dim]\n        trg_nodes_index = edge_index[self.trg_nodes_dim]\n        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n        return scores_source, scores_target, nodes_features_matrix_proj_lifted\nclass CoPooling(torch.nn.Module):\n    def __init__(self, ratio=0.5, K=0.05, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=None):\n        super(CoPooling, self).__init__()\n        self.ratio = ratio\n        self.calc_information_score = NodeInformationScore()\n        self.edge_ratio = edge_ratio\n        self.prop1 = GPR_prop(K, alpha, Init, Gamma)\n        score_dim = 32\n        self.G_att = graph_attention(num_in_features=nhid, num_out_features=score_dim, num_of_heads=1)\n        self.weight = Parameter(torch.Tensor(2*nhid, nhid))\n        nn.init.xavier_uniform_(self.weight.data)\n        self.bias = Parameter(torch.Tensor(nhid))\n        nn.init.zeros_(self.bias.data)\n        self.reset_parameters()\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight.data)\n        nn.init.zeros_(self.bias.data)\n        self.prop1.reset_parameters()\n        self.G_att.init_params()\n    def forward(self, x, edge_index, edge_attr, batch=None, nodes_index=None, node_attr=None):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        ori_batch = batch.clone()\n        device = x.device\n        num_nodes = x.shape[0]\n        x_cut = self.prop1(x, edge_index) \n        attention = self.G_att(x_cut, edge_index) \n        attention = attention.sum(dim=1) \n        edge_index, attention = add_self_loops(edge_index, attention, 1.0, num_nodes) \n        edge_index_t, attention_t = transpose(edge_index, attention, num_nodes, num_nodes)\n        edge_tmp = torch.cat((edge_index, edge_index_t), 1)\n        att_tmp = torch.cat((attention, attention_t),0)\n        edge_index, attention = coalesce(edge_tmp, att_tmp, num_nodes, num_nodes, 'mean')\n        attention_np = attention.cpu().data.numpy()\n        cut_val = np.percentile(attention_np, int(100*(1-self.edge_ratio))) \n        attention = attention * (attention >= cut_val) \n        kep_idx = attention > 0.0\n        cut_edge_index, cut_edge_attr = edge_index[:, kep_idx], attention[kep_idx]\n        x_information_score = self.calc_information_score(x, cut_edge_index, cut_edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        perm = topk(score, self.ratio, batch)\n        x_topk = x[perm]\n        batch = batch[perm]\n        if nodes_index is not None:\n            nodes_index = nodes_index[perm]\n        if node_attr is not None:\n            node_attr = node_attr[perm]\n        if cut_edge_index is not None or cut_edge_index.nelement() != 0:\n            induced_edge_index, induced_edge_attr = filter_adj(cut_edge_index, cut_edge_attr, perm, num_nodes=num_nodes)\n        else:\n            print('All edges are cut!')\n            induced_edge_index, induced_edge_attr = cut_edge_index, cut_edge_attr\n        attention_dense = (to_dense_adj(cut_edge_index, edge_attr=cut_edge_attr, max_num_nodes=num_nodes)).squeeze()\n        x = F.relu(torch.matmul(torch.cat((x_topk, torch.matmul(attention_dense[perm],x)), 1), self.weight) + self.bias)\n        return x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 39.26$\\pm$1.05\n",
                        "Macro F1: 23.44$\\pm$0.32\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Texas.num_classes\nin_channels = dataset_Texas.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_CO(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_CO, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(CoPooling(ratio=pool_ratios[i], K=1, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=1.0))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, perm, _, batch, _, _, _ = self.pools[i - 1](x, edge_index, edge_attr=None, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_CO(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CGIPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "from torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\n@dataclass(init=False)\nclass SelectOutput:\n    r\"\"\"The output of the :class:`Select` method, which holds an assignment\n    from selected nodes to their respective cluster(s).\n    Args:\n        node_index (torch.Tensor): The indices of the selected nodes.\n        num_nodes (int): The number of nodes.\n        cluster_index (torch.Tensor): The indices of the clusters each node in\n            :obj:`node_index` is assigned to.\n        num_clusters (int): The number of clusters.\n        weight (torch.Tensor, optional): A weight vector, denoting the strength\n            of the assignment of a node to its cluster. (default: :obj:`None`)\n    \"\"\"\n    node_index: Tensor\n    num_nodes: int\n    cluster_index: Tensor\n    num_clusters: int\n    weight: Optional[Tensor] = None\n    def __init__(\n        self,\n        node_index: Tensor,\n        num_nodes: int,\n        cluster_index: Tensor,\n        num_clusters: int,\n        weight: Optional[Tensor] = None,\n    ):\n        if node_index.dim() != 1:\n            raise ValueError(f\"Expected 'node_index' to be one-dimensional \"\n                             f\"(got {node_index.dim()} dimensions)\")\n        if cluster_index.dim() != 1:\n            raise ValueError(f\"Expected 'cluster_index' to be one-dimensional \"\n                             f\"(got {cluster_index.dim()} dimensions)\")\n        if node_index.numel() != cluster_index.numel():\n            raise ValueError(f\"Expected 'node_index' and 'cluster_index' to \"\n                             f\"hold the same number of values (got \"\n                             f\"{node_index.numel()} and \"\n                             f\"{cluster_index.numel()} values)\")\n        if weight is not None and weight.dim() != 1:\n            raise ValueError(f\"Expected 'weight' vector to be one-dimensional \"\n                             f\"(got {weight.dim()} dimensions)\")\n        if weight is not None and weight.numel() != node_index.numel():\n            raise ValueError(f\"Expected 'weight' to hold {node_index.numel()} \"\n                             f\"values (got {weight.numel()} values)\")\n        self.node_index = node_index\n        self.num_nodes = num_nodes\n        self.cluster_index = cluster_index\n        self.num_clusters = num_clusters\n        self.weight = weight\nclass Select(torch.nn.Module):\n    r\"\"\"An abstract base class for implementing custom node selections as\n    described in the `\"Understanding Pooling in Graph Neural Networks\"\n    <https://arxiv.org/abs/1905.05178>`_ paper, which maps the nodes of an\n    input graph to supernodes in the coarsened graph.\n    Specifically, :class:`Select` returns a :class:`SelectOutput` output, which\n    holds a (sparse) mapping :math:`\\mathbf{C} \\in {[0, 1]}^{N \\times C}` that\n    assigns selected nodes to one or more of :math:`C` super nodes.\n    \"\"\"\n    def reset_parameters(self):\n        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n        pass\n    def forward(self, *args, **kwargs) -> SelectOutput:\n        raise NotImplementedError\n    def __repr__(self) -> str:\n        return f'{self.__class__.__name__}()'\ndef cumsum(x: Tensor, dim: int = 0) -> Tensor:\n    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n    Args:\n        x (torch.Tensor): The input tensor.\n        dim (int, optional): The dimension to do the operation over.\n            (default: :obj:`0`)\n    Example:\n        >>> x = torch.tensor([2, 4, 1])\n        >>> cumsum(x)\n        tensor([0, 2, 6, 7])\n    \"\"\"\n    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n    out = x.new_empty(size)\n    out.narrow(dim, 0, 1).zero_()\n    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n    return out\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\ndef topk(\n    x: Tensor,\n    ratio: Optional[Union[float, int]],\n    batch: Tensor,\n    min_score: Optional[float] = None,\n    tol: float = 1e-7,\n) -> Tensor:\n    if min_score is not None:\n        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero().view(-1)\n        return perm\n    if ratio is not None:\n        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n        if ratio >= 1:\n            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n        else:\n            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n        x, x_perm = torch.sort(x.view(-1), descending=True)\n        batch = batch[x_perm]\n        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n        ptr = cumsum(num_nodes)\n        batched_arange = arange - ptr[batch]\n        mask = batched_arange < k[batch]\n        return x_perm[batch_perm[mask]]\n    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n                     \"must be specified\")\nclass Discriminator(torch.nn.Module):\n    def __init__(self, in_channels):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(in_channels * 2, in_channels)\n        self.fc2 = nn.Linear(in_channels, 1)\n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = F.sigmoid(self.fc2(x))\n        return x\nclass CGIPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, non_lin=torch.tanh):\n        super(CGIPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.non_lin = non_lin\n        self.hidden_dim = in_channels\n        self.transform = GraphConv(in_channels, self.hidden_dim)\n        self.pp_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n        self.np_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n        self.positive_pooling = GraphConv(self.hidden_dim, 1)\n        self.negative_pooling = GraphConv(self.hidden_dim, 1)\n        self.discriminator = Discriminator(self.hidden_dim)\n        self.loss_fn = torch.nn.BCELoss()\n    def forward(self, x, edge_index, edge_attr=None, batch=None):\n        device = x.device  \n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_transform = F.leaky_relu(self.transform(x, edge_index), 0.2)\n        x_tp = F.leaky_relu(self.pp_conv(x, edge_index), 0.2)\n        x_tn = F.leaky_relu(self.np_conv(x, edge_index), 0.2)\n        s_pp = self.positive_pooling(x_tp, edge_index).squeeze()\n        s_np = self.negative_pooling(x_tn, edge_index).squeeze()\n        perm_positive = topk(s_pp, 1, batch)\n        perm_negative = topk(s_np, 1, batch)\n        x_pp = x_transform[perm_positive] * self.non_lin(s_pp[perm_positive]).view(-1, 1)\n        x_np = x_transform[perm_negative] * self.non_lin(s_np[perm_negative]).view(-1, 1)\n        x_pp_readout = gap(x_pp, batch[perm_positive])\n        x_np_readout = gap(x_np, batch[perm_negative])\n        x_readout = gap(x_transform, batch)\n        positive_pair = torch.cat([x_pp_readout, x_readout], dim=1)\n        negative_pair = torch.cat([x_np_readout, x_readout], dim=1)\n        real = torch.ones(positive_pair.shape[0], device=device)  \n        fake = torch.zeros(negative_pair.shape[0], device=device)  \n        score = (s_pp - s_np)\n        perm = topk(score, self.ratio, batch)\n        x = x_transform[perm] * self.non_lin(score[perm]).view(-1, 1)\n        batch = batch[perm]\n        filter_edge_index, filter_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        return x, filter_edge_index, filter_edge_attr, batch, perm"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 40.00$\\pm$1.81\n",
                        "Macro F1: 23.85$\\pm$0.52\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Texas.num_classes\nin_channels = dataset_Texas.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_CGI(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_CGI, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(CGIPool(channels, ratio=pool_ratios[i]))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_CGI(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### KMISPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": "from typing import Callable, Optional, Tuple, Union\nfrom torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\nScorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\nfrom torch_sparse import SparseTensor, remove_diag\nfrom torch_geometric.nn.aggr import Aggregation\nfrom torch_geometric.nn.dense import Linear\nfrom torch.nn import Module\nfrom torch_scatter import scatter_max, scatter_min\ndef maximal_independent_set(edge_index: Adj, k: int = 1,\n                            perm: OptTensor = None) -> Tensor:\n    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n    returned set.\n    The algorithm greedily selects the nodes in their canonical order. If a\n    permutation :obj:`perm` is provided, the nodes are extracted following\n    that permutation instead.\n    This method follows `Blelloch's Alogirithm\n    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n    higher values of :math:`k`.\n    Args:\n        edge_index (Tensor or SparseTensor): The graph connectivity.\n        k (int): The :math:`k` value (defaults to 1).\n        perm (LongTensor, optional): Permutation vector. Must be of size\n            :obj:`(n,)` (defaults to :obj:`None`).\n    :rtype: :class:`ByteTensor`\n    \"\"\"\n    if isinstance(edge_index, SparseTensor):\n        row, col, _ = edge_index.coo()\n        device = edge_index.device()\n        n = edge_index.size(0)\n    else:\n        row, col = edge_index[0], edge_index[1]\n        device = row.device\n        n = edge_index.max().item() + 1\n    if perm is None:\n        rank = torch.arange(n, dtype=torch.long, device=device)\n    else:\n        rank = torch.zeros_like(perm)\n        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n    mis = torch.zeros(n, dtype=torch.bool, device=device)\n    mask = mis.clone()\n    min_rank = rank.clone()\n    while not mask.all():\n        for _ in range(k):\n            min_neigh = torch.full_like(min_rank, fill_value=n)\n            scatter_min(min_rank[row], col, out=min_neigh)\n            torch.minimum(min_neigh, min_rank, out=min_rank)  \n        mis = mis | torch.eq(rank, min_rank)\n        mask = mis.clone().byte()\n        for _ in range(k):\n            max_neigh = torch.full_like(mask, fill_value=0)\n            scatter_max(mask[row], col, out=max_neigh)\n            torch.maximum(max_neigh, mask, out=mask)  \n        mask = mask.to(dtype=torch.bool)\n        min_rank = rank.clone()\n        min_rank[mask] = n\n    return mis\ndef maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n                                    perm: OptTensor = None) -> PairTensor:\n    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n    clustering of a graph, as defined in `\"Generalizing Downsampling from\n    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n    The algorithm greedily selects the nodes in their canonical order. If a\n    permutation :obj:`perm` is provided, the nodes are extracted following\n    that permutation instead.\n    This method returns both the :math:`k`-MIS and the clustering, where the\n    :math:`c`-th cluster refers to the :math:`c`-th element of the\n    :math:`k`-MIS.\n    Args:\n        edge_index (Tensor or SparseTensor): The graph connectivity.\n        k (int): The :math:`k` value (defaults to 1).\n        perm (LongTensor, optional): Permutation vector. Must be of size\n            :obj:`(n,)` (defaults to :obj:`None`).\n    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n    \"\"\"\n    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n    n, device = mis.size(0), mis.device\n    if isinstance(edge_index, SparseTensor):\n        row, col, _ = edge_index.coo()\n    else:\n        row, col = edge_index[0], edge_index[1]\n    if perm is None:\n        rank = torch.arange(n, dtype=torch.long, device=device)\n    else:\n        rank = torch.zeros_like(perm)\n        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n    rank_mis = rank[mis]\n    min_rank[mis] = rank_mis\n    for _ in range(k):\n        min_neigh = torch.full_like(min_rank, fill_value=n)\n        scatter_min(min_rank[row], col, out=min_neigh)\n        torch.minimum(min_neigh, min_rank, out=min_rank)\n    _, clusters = torch.unique(min_rank, return_inverse=True)\n    perm = torch.argsort(rank_mis)\n    return mis, perm[clusters]\nclass KMISPooling(Module):\n    _heuristics = {None, 'greedy', 'w-greedy'}\n    _passthroughs = {None, 'before', 'after'}\n    _scorers = {\n        'linear',\n        'random',\n        'constant',\n        'canonical',\n        'first',\n        'last',\n    }\n    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n                 scorer: Union[Scorer, str] = 'linear',\n                 score_heuristic: Optional[str] = 'greedy',\n                 score_passthrough: Optional[str] = 'before',\n                 aggr_x: Optional[Union[str, Aggregation]] = None,\n                 aggr_edge: str = 'sum',\n                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n                 remove_self_loops: bool = True) -> None:\n        super(KMISPooling, self).__init__()\n        assert score_heuristic in self._heuristics, \\\n            \"Unrecognized `score_heuristic` value.\"\n        assert score_passthrough in self._passthroughs, \\\n            \"Unrecognized `score_passthrough` value.\"\n        if not callable(scorer):\n            assert scorer in self._scorers, \\\n                \"Unrecognized `scorer` value.\"\n        self.k = k\n        self.scorer = scorer\n        self.score_heuristic = score_heuristic\n        self.score_passthrough = score_passthrough\n        self.aggr_x = aggr_x\n        self.aggr_edge = aggr_edge\n        self.aggr_score = aggr_score\n        self.remove_self_loops = remove_self_loops\n        if scorer == 'linear':\n            assert self.score_passthrough is not None, \\\n                \"`'score_passthrough'` must not be `None`\" \\\n                \" when using `'linear'` scorer\"\n            self.lin = nn.Linear(in_channels, 1)\n    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n        if self.score_heuristic is None:\n            return x\n        row, col, _ = adj.coo()\n        x = x.view(-1)\n        if self.score_heuristic == 'greedy':\n            k_sums = torch.ones_like(x)\n        else:\n            k_sums = x.clone()\n        for _ in range(self.k):\n            scatter_add(k_sums[row], col, out=k_sums)\n        return x / k_sums\n    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n                batch: OptTensor = None) -> Tensor:\n        if self.scorer == 'linear':\n            return self.lin(x).sigmoid()\n        if self.scorer == 'random':\n            return torch.rand((x.size(0), 1), device=x.device)\n        if self.scorer == 'constant':\n            return torch.ones((x.size(0), 1), device=x.device)\n        if self.scorer == 'canonical':\n            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n        if self.scorer == 'first':\n            return x[..., [0]]\n        if self.scorer == 'last':\n            return x[..., [-1]]\n        return self.scorer(x, edge_index, edge_attr, batch)\n    def forward(self, x: Tensor, edge_index: Adj,\n                edge_attr: OptTensor = None,\n                batch: OptTensor = None) \\\n            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n        \"\"\"\"\"\"\n        edge_index = edge_index.long()\n        adj, n = edge_index, x.size(0)\n        if not isinstance(edge_index, SparseTensor):\n            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n        score = self._scorer(x, edge_index, edge_attr, batch)\n        updated_score = self._apply_heuristic(score, adj)\n        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n        row, col, val = adj.coo()\n        c = mis.sum()\n        if val is None:\n            val = torch.ones_like(row, dtype=torch.float)\n        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n                           is_sorted=False,\n                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n        if self.remove_self_loops:\n            adj = remove_diag(adj)\n        if self.score_passthrough == 'before':\n            x = self.aggr_score(x, score)\n        if self.aggr_x is None:\n            x = x[mis]\n        elif isinstance(self.aggr_x, str):\n            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n                        reduce=self.aggr_x)\n        else:\n            x = self.aggr_x(x, cluster, dim_size=c)\n        if self.score_passthrough == 'after':\n            x = self.aggr_score(x, score[mis])\n        if isinstance(edge_index, SparseTensor):\n            edge_index, edge_attr = adj, None\n        else:\n            row, col, edge_attr = adj.coo()\n            edge_index = torch.stack([row, col])\n        if batch is not None:\n            batch = batch[mis]\n        perm = perm[mis]\n        return x, edge_index, edge_attr, batch, mis, cluster, perm\n    def __repr__(self):\n        if self.scorer == 'linear':\n            channels = f\"in_channels={self.lin.in_channels}, \"\n        else:\n            channels = \"\"\n        return f'{self.__class__.__name__}({channels}k={self.k})'"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 38.52$\\pm$1.05\n",
                        "Macro F1: 23.19$\\pm$0.10\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Texas.num_classes\nin_channels = dataset_Texas.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\nclass HierarchicalGCN_KMIS(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_KMIS, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(KMISPooling(64, k=1, aggr_x='sum'))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_KMIS(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### GSAPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": "import math\nfrom typing import Union, Optional, Callable\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax\nfrom torch_geometric.nn import GCNConv, SAGEConv, GATConv, ChebConv, GraphConv\ndef uniform(size, tensor):\n    if tensor is not None:\n        bound = 1.0 / math.sqrt(size)\n        tensor.data.uniform_(-bound, bound)\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ),\n                             torch.finfo(x.dtype).min)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        if isinstance(ratio, int):\n            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n            k = torch.min(k, num_nodes)\n        else:\n            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\nclass GSAPool(torch.nn.Module):\n    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n                        min_score=None, multiplier=1,\n                        non_linearity=torch.tanh,\n                        cus_drop_ratio =0):\n        super(GSAPool,self).__init__()\n        self.in_channels = in_channels\n        self.ratio = pooling_ratio\n        self.alpha = alpha\n        self.sbtl_layer = GCNConv(in_channels,1)\n        self.fbtl_layer = nn.Linear(in_channels, 1)\n        self.fusion = GCNConv(in_channels,in_channels)\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.fusion_flag = 0\n        self.non_linearity = non_linearity\n        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n    def conv_selection(self, conv, in_channels, conv_type=0):\n        if(conv_type == 0):\n            out_channels = 1\n        elif(conv_type == 1):\n            out_channels = in_channels\n        if(conv == \"GCNConv\"):\n            return GCNConv(in_channels,out_channels)\n        elif(conv == \"ChebConv\"):\n            return ChebConv(in_channels,out_channels,1)\n        elif(conv == \"SAGEConv\"):\n            return SAGEConv(in_channels,out_channels)\n        elif(conv == \"GATConv\"):\n            return GATConv(in_channels,out_channels, heads=1, concat=True)\n        elif(conv == \"GraphConv\"):\n            return GraphConv(in_channels,out_channels)\n        else:\n            raise ValueError\n    def forward(self, x, edge_index, edge_attr=None, batch=None):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x = x.unsqueeze(-1) if x.dim() == 1 else x\n        score_s = self.sbtl_layer(x,edge_index).squeeze()\n        score_f = self.fbtl_layer(x).squeeze()\n        score = score_s*self.alpha + score_f*(1-self.alpha)\n        score = score.unsqueeze(-1) if score.dim()==0 else score\n        if self.min_score is None:\n            score = self.non_linearity(score)\n        else:\n            score = softmax(score, batch)\n        sc = self.dropout(score)\n        perm = topk(sc, self.ratio, batch)\n        if(self.fusion_flag == 1):\n            x = self.fusion(x, edge_index)\n        x_ae = x[perm]\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n        batch = batch[perm]\n        edge_index, edge_attr = filter_adj(\n            edge_index, edge_attr, perm, num_nodes=score.size(0))\n        return x, edge_index, edge_attr, batch, perm, x_ae"
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Micro F1: 40.74$\\pm$2.10\n",
                        "Macro F1: 26.90$\\pm$2.72\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Texas.num_classes\nin_channels = dataset_Texas.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_GSA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_GSA, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(GSAPool(64, pooling_ratio=pool_ratios[i], alpha = 0.6, cus_drop_ratio = 0))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, edge_attr, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_GSA(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### HGPSLPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([0.5344, 0.0000, 0.0000, 0.4656, 0.0613, 0.0000, 0.0000, 0.0000, 0.5640,\n",
                        "        0.3748])\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = torch.nonzero(x > scores_min).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n            num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_weight, perm, num_nodes=None):\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        mask = perm.new_full((num_nodes, ), -1)\n        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n        mask[perm] = i\n        row, col = edge_index\n        row, col = mask[row], mask[col]\n        mask = (row >= 0) & (col >= 0)\n        row, col = row[mask], col[mask]\n        if edge_weight is not None:\n            edge_weight = edge_weight[mask]\n        return torch.stack([row, col], dim=0), edge_weight\ndef scatter_sort(x, batch, fill_value=-1e16):\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n    dense_x[index] = x\n    dense_x = dense_x.view(batch_size, max_num_nodes)\n    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n    sorted_x = sorted_x.view(-1)\n    filled_index = sorted_x != fill_value\n    sorted_x = sorted_x[filled_index]\n    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n    return sorted_x, cumsum_sorted_x\ndef _make_ix_like(batch):\n    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n    idx = torch.cat(idx, dim=0)\n    return idx\ndef _threshold_and_support(x, batch):\n    \"\"\"Sparsemax building block: compute the threshold\n    Args:\n        x: input tensor to apply the sparsemax\n        batch: group indicators\n    Returns:\n        the threshold value\n    \"\"\"\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    sorted_input, input_cumsum = scatter_sort(x, batch)\n    input_cumsum = input_cumsum - 1.0\n    rhos = _make_ix_like(batch).to(x.dtype)\n    support = rhos * sorted_input > input_cumsum\n    support_size = scatter_add(support.to(batch.dtype), batch)\n    idx = support_size + cum_num_nodes - 1\n    mask = idx < 0\n    idx[mask] = 0\n    tau = input_cumsum.gather(0, idx)\n    tau /= support_size.to(x.dtype)\n    return tau, support_size\nclass SparsemaxFunction(Function):\n    @staticmethod\n    def forward(ctx, x, batch):\n        \"\"\"sparsemax: normalizing sparse transform\n        Parameters:\n            ctx: context object\n            x (Tensor): shape (N, )\n            batch: group indicator\n        Returns:\n            output (Tensor): same shape as input\n        \"\"\"\n        max_val, _ = scatter_max(x, batch)\n        x -= max_val[batch]\n        tau, supp_size = _threshold_and_support(x, batch)\n        output = torch.clamp(x - tau[batch], min=0)\n        ctx.save_for_backward(supp_size, output, batch)\n        return output\n    @staticmethod\n    def backward(ctx, grad_output):\n        supp_size, output, batch = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n        return grad_input, None\nsparsemax = SparsemaxFunction.apply\nclass Sparsemax(nn.Module):\n    def __init__(self):\n        super(Sparsemax, self).__init__()\n    def forward(self, x, batch):\n        return sparsemax(x, batch)\nif __name__ == '__main__':\n    sparse_attention = Sparsemax()\n    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n    res = sparse_attention(input_x, input_batch)\n    print(res)\nclass TwoHopNeighborhood(object):\n    def __call__(self, data):\n        edge_index, edge_attr = data.edge_index, data.edge_attr\n        n = data.num_nodes\n        fill = 1e16\n        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n        edge_index = torch.cat([edge_index, index], dim=1)\n        if edge_attr is None:\n            data.edge_index, _ = coalesce(edge_index, None, n, n)\n        else:\n            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n            value = value.expand(-1, *list(edge_attr.size())[1:])\n            edge_attr = torch.cat([edge_attr, value], dim=0)\n            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min', fill_value=fill)\n            edge_attr[edge_attr >= fill] = 0\n            data.edge_attr = edge_attr\n        return data\n    def __repr__(self):\n        return '{}()'.format(self.__class__.__name__)\nclass GCN(MessagePassing):\n    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n        super(GCN, self).__init__(aggr='add', **kwargs)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n        nn.init.xavier_uniform_(self.weight.data)\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n            nn.init.zeros_(self.bias.data)\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n    def reset_parameters(self):\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight=None):\n        x = torch.matmul(x, self.weight)\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        if self.bias is not None:\n            aggr_out = aggr_out + self.bias\n        return aggr_out\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n        row, col = edge_index\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass HGPSLPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n        super(HGPSLPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.sample = sample\n        self.sparse = sparse\n        self.sl = sl\n        self.negative_slop = negative_slop\n        self.lamb = lamb\n        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n        nn.init.xavier_uniform_(self.att.data)\n        self.sparse_attention = Sparsemax()\n        self.neighbor_augment = TwoHopNeighborhood()\n        self.calc_information_score = NodeInformationScore()\n    def forward(self, x, edge_index, edge_attr, batch):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        original_x = x\n        perm = topk(score, self.ratio, batch)\n        x = x[perm]\n        batch = batch[perm]\n        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        if self.sl is False:\n            return x, induced_edge_index, induced_edge_attr, batch\n        if self.sample:\n            k_hop = 3\n            if edge_attr is None:\n                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n            for _ in range(k_hop - 1):\n                hop_data = self.neighbor_augment(hop_data)\n            hop_edge_index = hop_data.edge_index\n            hop_edge_attr = hop_data.edge_attr\n            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            adj[row, col] = weights\n            new_edge_index, weights = dense_to_sparse(adj)\n            row, col = new_edge_index\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        else:\n            if edge_attr is None:\n                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n                                               device=induced_edge_index.device)\n            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n            cum_num_nodes = num_nodes.cumsum(dim=0)\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n            new_edge_index, _ = dense_to_sparse(adj)\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop)\n            adj[row, col] = weights\n            induced_row, induced_col = induced_edge_index\n            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n            weights = adj[row, col]\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, num_nodes=x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        return x, new_edge_index, new_edge_attr, batch, perm"
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 001: Loss = 1.6089, Val Acc = 0.0889, Micro F1 = 0.0889, Macro F1 = 0.0833\n",
                        "Epoch 002: Loss = 1.5320, Val Acc = 0.0889, Micro F1 = 0.0889, Macro F1 = 0.0694\n",
                        "Epoch 003: Loss = 1.4912, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2433\n",
                        "Epoch 004: Loss = 1.4254, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.3205\n",
                        "Epoch 005: Loss = 1.4227, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.3493\n",
                        "Epoch 006: Loss = 1.3507, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.3205\n",
                        "Epoch 007: Loss = 1.3087, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.3518\n",
                        "Epoch 008: Loss = 1.2565, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.2955\n",
                        "Epoch 009: Loss = 1.2657, Val Acc = 0.6444, Micro F1 = 0.6444, Macro F1 = 0.2684\n",
                        "Epoch 010: Loss = 1.2561, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2726\n",
                        "Epoch 011: Loss = 1.1911, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2726\n",
                        "Epoch 012: Loss = 1.1718, Val Acc = 0.6889, Micro F1 = 0.6889, Macro F1 = 0.3316\n",
                        "Epoch 013: Loss = 1.1459, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.3228\n",
                        "Epoch 014: Loss = 1.1561, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.3228\n",
                        "Epoch 015: Loss = 1.1050, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.4048\n",
                        "Epoch 016: Loss = 1.0985, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.3810\n",
                        "Epoch 017: Loss = 1.0637, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3736\n",
                        "Epoch 018: Loss = 1.0770, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3745\n",
                        "Epoch 019: Loss = 1.0277, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3745\n",
                        "Epoch 020: Loss = 1.0265, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.3344\n",
                        "Epoch 021: Loss = 0.9873, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.3344\n",
                        "Epoch 022: Loss = 0.9684, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3560\n",
                        "Epoch 023: Loss = 0.9749, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3560\n",
                        "Epoch 024: Loss = 0.9572, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3530\n",
                        "Epoch 025: Loss = 0.9704, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3394\n",
                        "Epoch 026: Loss = 0.9757, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.3816\n",
                        "Epoch 027: Loss = 0.9240, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3589\n",
                        "Epoch 028: Loss = 0.9497, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.3324\n",
                        "Epoch 029: Loss = 0.9253, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3028\n",
                        "Epoch 030: Loss = 0.9057, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3028\n",
                        "Epoch 031: Loss = 0.8900, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.3373\n",
                        "Epoch 032: Loss = 0.9190, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2717\n",
                        "Epoch 033: Loss = 0.8870, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2717\n",
                        "Epoch 034: Loss = 0.8799, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.2792\n",
                        "Epoch 035: Loss = 0.8560, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.3317\n",
                        "Epoch 036: Loss = 0.8584, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.3103\n",
                        "Epoch 037: Loss = 0.8581, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.3164\n",
                        "Epoch 038: Loss = 0.8544, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.3164\n",
                        "Epoch 039: Loss = 0.8200, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.2662\n",
                        "Epoch 040: Loss = 0.8379, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2616\n",
                        "Epoch 041: Loss = 0.7994, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2599\n",
                        "Epoch 042: Loss = 0.7894, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2616\n",
                        "Epoch 043: Loss = 0.8464, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2616\n",
                        "Epoch 044: Loss = 0.8003, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2616\n",
                        "Epoch 045: Loss = 0.7970, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2616\n",
                        "Epoch 046: Loss = 0.7636, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2616\n",
                        "Epoch 047: Loss = 0.7784, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2599\n",
                        "Epoch 048: Loss = 0.7716, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2599\n",
                        "Epoch 049: Loss = 0.7465, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2585\n",
                        "Epoch 050: Loss = 0.7549, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2585\n",
                        "Epoch 051: Loss = 0.7531, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2585\n",
                        "Epoch 052: Loss = 0.7686, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2437\n",
                        "Epoch 053: Loss = 0.7269, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2437\n",
                        "Epoch 054: Loss = 0.7381, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2525\n",
                        "Epoch 055: Loss = 0.7302, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3011\n",
                        "Epoch 056: Loss = 0.7456, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3011\n",
                        "Epoch 057: Loss = 0.7212, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3011\n",
                        "Epoch 058: Loss = 0.7280, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3011\n",
                        "Epoch 059: Loss = 0.6973, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.3109\n",
                        "Epoch 060: Loss = 0.7327, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2599\n",
                        "Epoch 061: Loss = 0.7097, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2585\n",
                        "Epoch 062: Loss = 0.6785, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2511\n",
                        "Epoch 063: Loss = 0.6770, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2373\n",
                        "Epoch 064: Loss = 0.6726, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 065: Loss = 0.7250, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2977\n",
                        "Epoch 066: Loss = 0.6796, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2928\n",
                        "Epoch 067: Loss = 0.6831, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2928\n",
                        "Epoch 068: Loss = 0.6927, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2928\n",
                        "Epoch 069: Loss = 0.7791, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 070: Loss = 0.6761, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2334\n",
                        "Epoch 071: Loss = 0.6571, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2334\n",
                        "Epoch 072: Loss = 0.6713, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2334\n",
                        "Epoch 073: Loss = 0.6739, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 074: Loss = 0.6594, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 075: Loss = 0.6536, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2977\n",
                        "Epoch 076: Loss = 0.6187, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2948\n",
                        "Epoch 077: Loss = 0.6946, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 078: Loss = 0.6274, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 079: Loss = 0.6368, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 080: Loss = 0.6217, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 081: Loss = 0.6334, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 082: Loss = 0.6142, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2413\n",
                        "Epoch 083: Loss = 0.6159, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2413\n",
                        "Epoch 084: Loss = 0.5977, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2580\n",
                        "Epoch 085: Loss = 0.6687, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2413\n",
                        "Epoch 086: Loss = 0.6060, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2449\n",
                        "Epoch 087: Loss = 0.6216, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3028\n",
                        "Epoch 088: Loss = 0.6085, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3028\n",
                        "Epoch 089: Loss = 0.5915, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3028\n",
                        "Epoch 090: Loss = 0.6388, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3028\n",
                        "Epoch 091: Loss = 0.5912, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 092: Loss = 0.6007, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 093: Loss = 0.5794, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 094: Loss = 0.6092, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 095: Loss = 0.6234, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 096: Loss = 0.5712, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2867\n",
                        "Epoch 097: Loss = 0.5906, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 098: Loss = 0.5644, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 099: Loss = 0.5679, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 100: Loss = 0.6129, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 101: Loss = 0.5568, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 102: Loss = 0.5855, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 103: Loss = 0.5708, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 104: Loss = 0.5397, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 105: Loss = 0.5679, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 106: Loss = 0.5815, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 107: Loss = 0.5645, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2817\n",
                        "Epoch 108: Loss = 0.5707, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 109: Loss = 0.5637, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2373\n",
                        "Epoch 110: Loss = 0.5595, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 111: Loss = 0.5886, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 112: Loss = 0.5321, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2449\n",
                        "Epoch 113: Loss = 0.5511, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2373\n",
                        "Epoch 114: Loss = 0.5183, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2618\n",
                        "Epoch 115: Loss = 0.5589, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2470\n",
                        "Epoch 116: Loss = 0.5576, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2470\n",
                        "Epoch 117: Loss = 0.5281, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.3056\n",
                        "Epoch 118: Loss = 0.5669, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2692\n",
                        "Epoch 119: Loss = 0.6123, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2692\n",
                        "Epoch 120: Loss = 0.5463, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 121: Loss = 0.5125, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2977\n",
                        "Epoch 122: Loss = 0.5314, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2977\n",
                        "Epoch 123: Loss = 0.5178, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 124: Loss = 0.5343, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 125: Loss = 0.5345, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 126: Loss = 0.5333, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2449\n",
                        "Epoch 127: Loss = 0.5246, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2373\n",
                        "Epoch 128: Loss = 0.5397, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2373\n",
                        "Epoch 129: Loss = 0.5170, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 130: Loss = 0.5059, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 131: Loss = 0.5286, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 132: Loss = 0.5499, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 133: Loss = 0.5261, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2817\n",
                        "Epoch 134: Loss = 0.5357, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2470\n",
                        "Epoch 135: Loss = 0.5010, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2470\n",
                        "Epoch 136: Loss = 0.5045, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2470\n",
                        "Epoch 137: Loss = 0.5114, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2470\n",
                        "Epoch 138: Loss = 0.5231, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 139: Loss = 0.5098, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 140: Loss = 0.5113, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3095\n",
                        "Epoch 141: Loss = 0.5183, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3042\n",
                        "Epoch 142: Loss = 0.6592, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2924\n",
                        "Epoch 143: Loss = 0.5382, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.3056\n",
                        "Epoch 144: Loss = 0.4610, Val Acc = 0.3333, Micro F1 = 0.3333, Macro F1 = 0.2411\n",
                        "Epoch 145: Loss = 0.5347, Val Acc = 0.2667, Micro F1 = 0.2667, Macro F1 = 0.2055\n",
                        "Epoch 146: Loss = 0.5596, Val Acc = 0.2667, Micro F1 = 0.2667, Macro F1 = 0.2055\n",
                        "Epoch 147: Loss = 0.4844, Val Acc = 0.2889, Micro F1 = 0.2889, Macro F1 = 0.2165\n",
                        "Epoch 148: Loss = 0.5541, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2510\n",
                        "Epoch 149: Loss = 0.4965, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2393\n",
                        "Epoch 150: Loss = 0.5147, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2963\n",
                        "Epoch 151: Loss = 0.4698, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2924\n",
                        "Epoch 152: Loss = 0.5299, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2897\n",
                        "Epoch 153: Loss = 0.5161, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2948\n",
                        "Epoch 154: Loss = 0.4611, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2867\n",
                        "Epoch 155: Loss = 0.4617, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2762\n",
                        "Epoch 156: Loss = 0.5328, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2785\n",
                        "Epoch 157: Loss = 0.5857, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 158: Loss = 0.4429, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 159: Loss = 0.4879, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 160: Loss = 0.4852, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 161: Loss = 0.4580, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Early stopping at epoch 162\n",
                        "Epoch 001: Loss = 1.7376, Val Acc = 0.1111, Micro F1 = 0.1111, Macro F1 = 0.0932\n",
                        "Epoch 002: Loss = 1.6315, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.3280\n",
                        "Epoch 003: Loss = 1.5575, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.2491\n",
                        "Epoch 004: Loss = 1.4992, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.2467\n",
                        "Epoch 005: Loss = 1.4782, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.2394\n",
                        "Epoch 006: Loss = 1.4489, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.2485\n",
                        "Epoch 007: Loss = 1.4132, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.2485\n",
                        "Epoch 008: Loss = 1.3937, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.2485\n",
                        "Epoch 009: Loss = 1.3465, Val Acc = 0.6444, Micro F1 = 0.6444, Macro F1 = 0.2614\n",
                        "Epoch 010: Loss = 1.3097, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2696\n",
                        "Epoch 011: Loss = 1.2877, Val Acc = 0.6889, Micro F1 = 0.6889, Macro F1 = 0.2738\n",
                        "Epoch 012: Loss = 1.2391, Val Acc = 0.6889, Micro F1 = 0.6889, Macro F1 = 0.2738\n",
                        "Epoch 013: Loss = 1.2185, Val Acc = 0.6889, Micro F1 = 0.6889, Macro F1 = 0.2738\n",
                        "Epoch 014: Loss = 1.1942, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.2610\n",
                        "Epoch 015: Loss = 1.1475, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.2610\n",
                        "Epoch 016: Loss = 1.1618, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.2565\n",
                        "Epoch 017: Loss = 1.1225, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.2519\n",
                        "Epoch 018: Loss = 1.0671, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.2500\n",
                        "Epoch 019: Loss = 1.0969, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.2450\n",
                        "Epoch 020: Loss = 1.0234, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.2500\n",
                        "Epoch 021: Loss = 1.0520, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.2431\n",
                        "Epoch 022: Loss = 1.0085, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3460\n",
                        "Epoch 023: Loss = 0.9916, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3530\n",
                        "Epoch 024: Loss = 0.9782, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.3480\n",
                        "Epoch 025: Loss = 0.9789, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.2804\n",
                        "Epoch 026: Loss = 0.9874, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3882\n",
                        "Epoch 027: Loss = 0.9232, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3882\n",
                        "Epoch 028: Loss = 0.9349, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3736\n",
                        "Epoch 029: Loss = 0.8933, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3736\n",
                        "Epoch 030: Loss = 0.9214, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.3423\n",
                        "Epoch 031: Loss = 0.8842, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.3423\n",
                        "Epoch 032: Loss = 0.8942, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2758\n",
                        "Epoch 033: Loss = 0.8990, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2699\n",
                        "Epoch 034: Loss = 0.8580, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2586\n",
                        "Epoch 035: Loss = 0.8518, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2526\n",
                        "Epoch 036: Loss = 0.8513, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3020\n",
                        "Epoch 037: Loss = 0.8671, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.3103\n",
                        "Epoch 038: Loss = 0.8499, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2604\n",
                        "Epoch 039: Loss = 0.8191, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2585\n",
                        "Epoch 040: Loss = 0.8039, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2560\n",
                        "Epoch 041: Loss = 0.8627, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2437\n",
                        "Epoch 042: Loss = 0.8037, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2537\n",
                        "Epoch 043: Loss = 0.7846, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2433\n",
                        "Epoch 044: Loss = 0.8365, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2839\n",
                        "Epoch 045: Loss = 0.8372, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2919\n",
                        "Epoch 046: Loss = 0.8045, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3011\n",
                        "Epoch 047: Loss = 0.7901, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.3109\n",
                        "Epoch 048: Loss = 0.7615, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.2674\n",
                        "Epoch 049: Loss = 0.7550, Val Acc = 0.4889, Micro F1 = 0.4889, Macro F1 = 0.2657\n",
                        "Epoch 050: Loss = 0.7975, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2599\n",
                        "Epoch 051: Loss = 0.7997, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2525\n",
                        "Epoch 052: Loss = 0.7433, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2457\n",
                        "Epoch 053: Loss = 0.8077, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2403\n",
                        "Epoch 054: Loss = 0.7619, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2403\n",
                        "Epoch 055: Loss = 0.7391, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2752\n",
                        "Epoch 056: Loss = 0.7451, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2345\n",
                        "Epoch 057: Loss = 0.7362, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2599\n",
                        "Epoch 058: Loss = 0.7073, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2585\n",
                        "Epoch 059: Loss = 0.7747, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2585\n",
                        "Epoch 060: Loss = 0.7283, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2285\n",
                        "Epoch 061: Loss = 0.7297, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 062: Loss = 0.7441, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 063: Loss = 0.6883, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2317\n",
                        "Epoch 064: Loss = 0.7051, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 065: Loss = 0.6852, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 066: Loss = 0.6853, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 067: Loss = 0.7087, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 068: Loss = 0.6921, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 069: Loss = 0.7053, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 070: Loss = 0.6829, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 071: Loss = 0.7205, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 072: Loss = 0.6932, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2832\n",
                        "Epoch 073: Loss = 0.6472, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2928\n",
                        "Epoch 074: Loss = 0.6645, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2928\n",
                        "Epoch 075: Loss = 0.6706, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2928\n",
                        "Epoch 076: Loss = 0.6755, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2928\n",
                        "Epoch 077: Loss = 0.6474, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3055\n",
                        "Epoch 078: Loss = 0.6229, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.3371\n",
                        "Epoch 079: Loss = 0.6470, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.3371\n",
                        "Epoch 080: Loss = 0.6276, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3095\n",
                        "Epoch 081: Loss = 0.6128, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 082: Loss = 0.6033, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 083: Loss = 0.6233, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2785\n",
                        "Epoch 084: Loss = 0.6618, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 085: Loss = 0.6110, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 086: Loss = 0.6405, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 087: Loss = 0.6287, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 088: Loss = 0.6460, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2746\n",
                        "Epoch 089: Loss = 0.6183, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2762\n",
                        "Epoch 090: Loss = 0.5983, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2817\n",
                        "Epoch 091: Loss = 0.5791, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 092: Loss = 0.6314, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 093: Loss = 0.6091, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 094: Loss = 0.6356, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 095: Loss = 0.6150, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 096: Loss = 0.6174, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 097: Loss = 0.5907, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 098: Loss = 0.5882, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2817\n",
                        "Epoch 099: Loss = 0.6397, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2334\n",
                        "Epoch 100: Loss = 0.6787, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2800\n",
                        "Epoch 101: Loss = 0.6008, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2746\n",
                        "Epoch 102: Loss = 0.5836, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2832\n",
                        "Epoch 103: Loss = 0.5928, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2832\n",
                        "Epoch 104: Loss = 0.5846, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2867\n",
                        "Epoch 105: Loss = 0.5940, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 106: Loss = 0.5801, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 107: Loss = 0.5772, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2352\n",
                        "Epoch 108: Loss = 0.6027, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2352\n",
                        "Epoch 109: Loss = 0.5939, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 110: Loss = 0.5565, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2867\n",
                        "Epoch 111: Loss = 0.5683, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2650\n",
                        "Epoch 112: Loss = 0.5737, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2674\n",
                        "Epoch 113: Loss = 0.5916, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2741\n",
                        "Epoch 114: Loss = 0.5601, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3011\n",
                        "Epoch 115: Loss = 0.5959, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3028\n",
                        "Epoch 116: Loss = 0.6658, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 117: Loss = 0.5554, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2373\n",
                        "Epoch 118: Loss = 0.5642, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2711\n",
                        "Epoch 119: Loss = 0.5649, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2639\n",
                        "Epoch 120: Loss = 0.6175, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2639\n",
                        "Epoch 121: Loss = 0.6401, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 122: Loss = 0.5426, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2867\n",
                        "Epoch 123: Loss = 0.5523, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2928\n",
                        "Epoch 124: Loss = 0.5349, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3003\n",
                        "Epoch 125: Loss = 0.5372, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3003\n",
                        "Epoch 126: Loss = 0.5774, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3003\n",
                        "Epoch 127: Loss = 0.6030, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 128: Loss = 0.5615, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 129: Loss = 0.5459, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 130: Loss = 0.5461, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2683\n",
                        "Epoch 131: Loss = 0.5505, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2683\n",
                        "Epoch 132: Loss = 0.6254, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2470\n",
                        "Epoch 133: Loss = 0.5497, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 134: Loss = 0.5199, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 135: Loss = 0.5879, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 136: Loss = 0.5397, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 137: Loss = 0.5366, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 138: Loss = 0.5658, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2449\n",
                        "Epoch 139: Loss = 0.5118, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 140: Loss = 0.5120, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 141: Loss = 0.5480, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2470\n",
                        "Epoch 142: Loss = 0.5067, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2683\n",
                        "Epoch 143: Loss = 0.5648, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2683\n",
                        "Epoch 144: Loss = 0.5544, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 145: Loss = 0.4786, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 146: Loss = 0.5006, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2977\n",
                        "Epoch 147: Loss = 0.5243, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2977\n",
                        "Epoch 148: Loss = 0.5099, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2977\n",
                        "Epoch 149: Loss = 0.5362, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 150: Loss = 0.5614, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 151: Loss = 0.5028, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 152: Loss = 0.5145, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 153: Loss = 0.5441, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 154: Loss = 0.5012, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 155: Loss = 0.5688, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 156: Loss = 0.5251, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 157: Loss = 0.4859, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 158: Loss = 0.5242, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 159: Loss = 0.5288, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Epoch 160: Loss = 0.5074, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3018\n",
                        "Early stopping at epoch 161\n",
                        "Epoch 001: Loss = 1.5894, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2027\n",
                        "Epoch 002: Loss = 1.5348, Val Acc = 0.6444, Micro F1 = 0.6444, Macro F1 = 0.2014\n",
                        "Epoch 003: Loss = 1.4976, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2083\n",
                        "Epoch 004: Loss = 1.4768, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2083\n",
                        "Epoch 005: Loss = 1.4176, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2083\n",
                        "Epoch 006: Loss = 1.3857, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2083\n",
                        "Epoch 007: Loss = 1.3736, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2083\n",
                        "Epoch 008: Loss = 1.3530, Val Acc = 0.6667, Micro F1 = 0.6667, Macro F1 = 0.2083\n",
                        "Epoch 009: Loss = 1.3011, Val Acc = 0.6444, Micro F1 = 0.6444, Macro F1 = 0.2042\n",
                        "Epoch 010: Loss = 1.2810, Val Acc = 0.6889, Micro F1 = 0.6889, Macro F1 = 0.3382\n",
                        "Epoch 011: Loss = 1.2600, Val Acc = 0.6444, Micro F1 = 0.6444, Macro F1 = 0.3236\n",
                        "Epoch 012: Loss = 1.1975, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.3177\n",
                        "Epoch 013: Loss = 1.1628, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.3011\n",
                        "Epoch 014: Loss = 1.1677, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.3071\n",
                        "Epoch 015: Loss = 1.1539, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.3052\n",
                        "Epoch 016: Loss = 1.1675, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.3718\n",
                        "Epoch 017: Loss = 1.1169, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.3718\n",
                        "Epoch 018: Loss = 1.0636, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3647\n",
                        "Epoch 019: Loss = 1.0870, Val Acc = 0.5778, Micro F1 = 0.5778, Macro F1 = 0.3745\n",
                        "Epoch 020: Loss = 1.0589, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.3962\n",
                        "Epoch 021: Loss = 1.0968, Val Acc = 0.6444, Micro F1 = 0.6444, Macro F1 = 0.4099\n",
                        "Epoch 022: Loss = 1.0155, Val Acc = 0.6222, Micro F1 = 0.6222, Macro F1 = 0.4025\n",
                        "Epoch 023: Loss = 1.0771, Val Acc = 0.6000, Micro F1 = 0.6000, Macro F1 = 0.3962\n",
                        "Epoch 024: Loss = 0.9711, Val Acc = 0.5556, Micro F1 = 0.5556, Macro F1 = 0.3675\n",
                        "Epoch 025: Loss = 0.9707, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.3008\n",
                        "Epoch 026: Loss = 0.9666, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2552\n",
                        "Epoch 027: Loss = 0.9897, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2643\n",
                        "Epoch 028: Loss = 0.9674, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2643\n",
                        "Epoch 029: Loss = 0.9182, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.3564\n",
                        "Epoch 030: Loss = 0.9393, Val Acc = 0.5333, Micro F1 = 0.5333, Macro F1 = 0.3423\n",
                        "Epoch 031: Loss = 0.9147, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2777\n",
                        "Epoch 032: Loss = 0.8759, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2777\n",
                        "Epoch 033: Loss = 0.8897, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2758\n",
                        "Epoch 034: Loss = 0.8637, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2758\n",
                        "Epoch 035: Loss = 0.9125, Val Acc = 0.5111, Micro F1 = 0.5111, Macro F1 = 0.2758\n",
                        "Epoch 036: Loss = 0.8817, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2585\n",
                        "Epoch 037: Loss = 0.8348, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2449\n",
                        "Epoch 038: Loss = 0.8544, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2419\n",
                        "Epoch 039: Loss = 0.8382, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2450\n",
                        "Epoch 040: Loss = 0.8361, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2437\n",
                        "Epoch 041: Loss = 0.8330, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2419\n",
                        "Epoch 042: Loss = 0.8455, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2388\n",
                        "Epoch 043: Loss = 0.8204, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2506\n",
                        "Epoch 044: Loss = 0.8047, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2512\n",
                        "Epoch 045: Loss = 0.8234, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2512\n",
                        "Epoch 046: Loss = 0.8100, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2917\n",
                        "Epoch 047: Loss = 0.8134, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2512\n",
                        "Epoch 048: Loss = 0.7895, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2574\n",
                        "Epoch 049: Loss = 0.7917, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2555\n",
                        "Epoch 050: Loss = 0.7861, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2449\n",
                        "Epoch 051: Loss = 0.7811, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2540\n",
                        "Epoch 052: Loss = 0.7946, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2555\n",
                        "Epoch 053: Loss = 0.7778, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2701\n",
                        "Epoch 054: Loss = 0.7712, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2839\n",
                        "Epoch 055: Loss = 0.7659, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2917\n",
                        "Epoch 056: Loss = 0.7666, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2917\n",
                        "Epoch 057: Loss = 0.7018, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.3011\n",
                        "Epoch 058: Loss = 0.7323, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2917\n",
                        "Epoch 059: Loss = 0.7103, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2917\n",
                        "Epoch 060: Loss = 0.7068, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2928\n",
                        "Epoch 061: Loss = 0.7759, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2752\n",
                        "Epoch 062: Loss = 0.7223, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2375\n",
                        "Epoch 063: Loss = 0.7394, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2610\n",
                        "Epoch 064: Loss = 0.7649, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2610\n",
                        "Epoch 065: Loss = 0.7291, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2375\n",
                        "Epoch 066: Loss = 0.7216, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 067: Loss = 0.7041, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2880\n",
                        "Epoch 068: Loss = 0.7095, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2880\n",
                        "Epoch 069: Loss = 0.7046, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2928\n",
                        "Epoch 070: Loss = 0.7457, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 071: Loss = 0.6649, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 072: Loss = 0.6942, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2398\n",
                        "Epoch 073: Loss = 0.7048, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 074: Loss = 0.6928, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 075: Loss = 0.6815, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 076: Loss = 0.6512, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 077: Loss = 0.6747, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 078: Loss = 0.6569, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 079: Loss = 0.6483, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2825\n",
                        "Epoch 080: Loss = 0.7043, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2825\n",
                        "Epoch 081: Loss = 0.6630, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2825\n",
                        "Epoch 082: Loss = 0.6378, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2825\n",
                        "Epoch 083: Loss = 0.6226, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2825\n",
                        "Epoch 084: Loss = 0.6532, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 085: Loss = 0.6464, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2330\n",
                        "Epoch 086: Loss = 0.6422, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2334\n",
                        "Epoch 087: Loss = 0.6466, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2334\n",
                        "Epoch 088: Loss = 0.6223, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2334\n",
                        "Epoch 089: Loss = 0.6231, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2334\n",
                        "Epoch 090: Loss = 0.5975, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2334\n",
                        "Epoch 091: Loss = 0.6103, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2412\n",
                        "Epoch 092: Loss = 0.6249, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2842\n",
                        "Epoch 093: Loss = 0.6196, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2842\n",
                        "Epoch 094: Loss = 0.6315, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2427\n",
                        "Epoch 095: Loss = 0.6223, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2413\n",
                        "Epoch 096: Loss = 0.5863, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 097: Loss = 0.5916, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 098: Loss = 0.6201, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2550\n",
                        "Epoch 099: Loss = 0.5996, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2640\n",
                        "Epoch 100: Loss = 0.6019, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 101: Loss = 0.6010, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2508\n",
                        "Epoch 102: Loss = 0.6208, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2263\n",
                        "Epoch 103: Loss = 0.5788, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2692\n",
                        "Epoch 104: Loss = 0.5986, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 105: Loss = 0.6058, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 106: Loss = 0.5821, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 107: Loss = 0.5596, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2867\n",
                        "Epoch 108: Loss = 0.6611, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 109: Loss = 0.5793, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2898\n",
                        "Epoch 110: Loss = 0.5711, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 111: Loss = 0.5842, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 112: Loss = 0.5648, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 113: Loss = 0.5514, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 114: Loss = 0.5497, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 115: Loss = 0.6030, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 116: Loss = 0.5703, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 117: Loss = 0.5554, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 118: Loss = 0.5741, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 119: Loss = 0.5792, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 120: Loss = 0.5540, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2317\n",
                        "Epoch 121: Loss = 0.6147, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 122: Loss = 0.5420, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 123: Loss = 0.5670, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 124: Loss = 0.5499, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 125: Loss = 0.5132, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 126: Loss = 0.5557, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 127: Loss = 0.5720, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 128: Loss = 0.5445, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2412\n",
                        "Epoch 129: Loss = 0.5566, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2398\n",
                        "Epoch 130: Loss = 0.5547, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2398\n",
                        "Epoch 131: Loss = 0.5195, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2398\n",
                        "Epoch 132: Loss = 0.5429, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2398\n",
                        "Epoch 133: Loss = 0.5211, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2398\n",
                        "Epoch 134: Loss = 0.5170, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 135: Loss = 0.5117, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 136: Loss = 0.5304, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 137: Loss = 0.4933, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 138: Loss = 0.5015, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2867\n",
                        "Epoch 139: Loss = 0.5034, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2398\n",
                        "Epoch 140: Loss = 0.5133, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 141: Loss = 0.4997, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 142: Loss = 0.5202, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 143: Loss = 0.5046, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 144: Loss = 0.5413, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 145: Loss = 0.5336, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 146: Loss = 0.4805, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 147: Loss = 0.5359, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 148: Loss = 0.4903, Val Acc = 0.3556, Micro F1 = 0.3556, Macro F1 = 0.2248\n",
                        "Epoch 149: Loss = 0.5340, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2762\n",
                        "Epoch 150: Loss = 0.4961, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 151: Loss = 0.5079, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2845\n",
                        "Epoch 152: Loss = 0.4938, Val Acc = 0.4444, Micro F1 = 0.4444, Macro F1 = 0.2977\n",
                        "Epoch 153: Loss = 0.5179, Val Acc = 0.4667, Micro F1 = 0.4667, Macro F1 = 0.2585\n",
                        "Epoch 154: Loss = 0.5225, Val Acc = 0.4222, Micro F1 = 0.4222, Macro F1 = 0.2449\n",
                        "Epoch 155: Loss = 0.4995, Val Acc = 0.4000, Micro F1 = 0.4000, Macro F1 = 0.2384\n",
                        "Epoch 156: Loss = 0.4950, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 157: Loss = 0.4691, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 158: Loss = 0.4614, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Epoch 159: Loss = 0.4827, Val Acc = 0.3778, Micro F1 = 0.3778, Macro F1 = 0.2305\n",
                        "Early stopping at epoch 160\n",
                        "Micro F1: 40.74$\\pm$2.77\n",
                        "Macro F1: 25.69$\\pm$3.19\n"
                    ]
                }
            ],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nnum_classes = dataset_Texas.num_classes\nin_channels = dataset_Texas.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.7, 0.7]\nclass HierarchicalGCN_HGPSL(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_HGPSL, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(HGPSLPool(hidden_channels, ratio=pool_ratios[i], sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        edge_attr = None\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, edge_attr, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\ndef compute_f1_scores(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    y_true = graph.y[mask].cpu().numpy()\n    y_pred = pred[mask].cpu().numpy()\n    micro_f1 = f1_score(y_true, y_pred, average='micro')\n    macro_f1 = f1_score(y_true, y_pred, average='macro')\n    return micro_f1, macro_f1\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)  \n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        micro_f1, macro_f1 = compute_f1_scores(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            print(f'Early stopping at epoch {epoch}')\n            break\n        print(f'Epoch {epoch:03d}: Loss = {loss.item():.4f}, Val Acc = {val_acc:.4f}, Micro F1 = {micro_f1:.4f}, Macro F1 = {macro_f1:.4f}')\n    return model, best_val_acc\nseeds = [42, 123, 456]\nmicro_f1_scores = []\nmacro_f1_scores = []\nfor seed in seeds:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    model = HierarchicalGCN_HGPSL(in_channels=in_channels, hidden_channels=hidden_channels,\n                                 out_channels=out_channels, depth=depth, pool_ratios=pool_ratios)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n    criterion = torch.nn.CrossEntropyLoss()\n    model, best_val_acc = train_node_classifier(model, data, optimizer, criterion, data.train_mask, data.val_mask)\n    micro_f1, macro_f1 = compute_f1_scores(model, data, data.val_mask)\n    micro_f1_scores.append(micro_f1)\n    macro_f1_scores.append(macro_f1)\nmicro_f1_mean = np.mean(micro_f1_scores)\nmicro_f1_std = np.std(micro_f1_scores)\nmacro_f1_mean = np.mean(macro_f1_scores)\nmacro_f1_std = np.std(macro_f1_scores)\nmicro_f1_mean = np.mean(micro_f1_scores) * 100\nmicro_f1_std = np.std(micro_f1_scores) * 100\nmacro_f1_mean = np.mean(macro_f1_scores) * 100\nmacro_f1_std = np.std(macro_f1_scores) * 100\nprint(f\"Micro F1: {micro_f1_mean:.2f}$\\pm${micro_f1_std:.2f}\")\nprint(f\"Macro F1: {macro_f1_mean:.2f}$\\pm${macro_f1_std:.2f}\")"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "CG-ODE",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}