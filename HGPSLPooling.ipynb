{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Graph Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nimport sys\nimport torch\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom ogb.graphproppred import PygGraphPropPredDataset, Evaluator\nfrom torch_geometric.loader import DataLoader\nimport os\nimport random\nimport pandas as pd\nimport torch\nimport torch_geometric.transforms as T\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data\nfrom torch_geometric.data.datapipes import functional_transform\nfrom torch_geometric.transforms import BaseTransform\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import Actor\nfrom torch_geometric.datasets import GNNBenchmarkDataset\nfrom torch_geometric.datasets import TUDataset\nfrom sklearn.metrics import r2_score\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import MoleculeNet\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.utils import to_networkx\nfrom torch.nn import Linear\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport time\nimport psutil\nimport torch\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([0.5344, 0.0000, 0.0000, 0.4656, 0.0613, 0.0000, 0.0000, 0.0000, 0.5640,\n",
                        "        0.3748])\n"
                    ]
                }
            ],
            "source": "from torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom torch_geometric.nn import GCNConv\nfrom torch.nn import Parameter\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Union, Optional, Callable\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax\nimport math\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, GCNConv, GATConv, ChebConv, GraphConv\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\nfrom torch_scatter import scatter_add\nfrom torch_sparse import spspmm, coalesce\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch_scatter import scatter_add, scatter_max\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = torch.nonzero(x > scores_min).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n            num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_weight, perm, num_nodes=None):\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        mask = perm.new_full((num_nodes, ), -1)\n        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n        mask[perm] = i\n        row, col = edge_index\n        row, col = mask[row], mask[col]\n        mask = (row >= 0) & (col >= 0)\n        row, col = row[mask], col[mask]\n        if edge_weight is not None:\n            edge_weight = edge_weight[mask]\n        return torch.stack([row, col], dim=0), edge_weight\ndef scatter_sort(x, batch, fill_value=-1e16):\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n    dense_x[index] = x\n    dense_x = dense_x.view(batch_size, max_num_nodes)\n    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n    sorted_x = sorted_x.view(-1)\n    filled_index = sorted_x != fill_value\n    sorted_x = sorted_x[filled_index]\n    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n    return sorted_x, cumsum_sorted_x\ndef _make_ix_like(batch):\n    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n    idx = torch.cat(idx, dim=0)\n    return idx\ndef _threshold_and_support(x, batch):\n    \"\"\"Sparsemax building block: compute the threshold\n    Args:\n        x: input tensor to apply the sparsemax\n        batch: group indicators\n    Returns:\n        the threshold value\n    \"\"\"\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    sorted_input, input_cumsum = scatter_sort(x, batch)\n    input_cumsum = input_cumsum - 1.0\n    rhos = _make_ix_like(batch).to(x.dtype)\n    support = rhos * sorted_input > input_cumsum\n    support_size = scatter_add(support.to(batch.dtype), batch)\n    idx = support_size + cum_num_nodes - 1\n    mask = idx < 0\n    idx[mask] = 0\n    tau = input_cumsum.gather(0, idx)\n    tau /= support_size.to(x.dtype)\n    return tau, support_size\nclass SparsemaxFunction(Function):\n    @staticmethod\n    def forward(ctx, x, batch):\n        \"\"\"sparsemax: normalizing sparse transform\n        Parameters:\n            ctx: context object\n            x (Tensor): shape (N, )\n            batch: group indicator\n        Returns:\n            output (Tensor): same shape as input\n        \"\"\"\n        max_val, _ = scatter_max(x, batch)\n        x -= max_val[batch]\n        tau, supp_size = _threshold_and_support(x, batch)\n        output = torch.clamp(x - tau[batch], min=0)\n        ctx.save_for_backward(supp_size, output, batch)\n        return output\n    @staticmethod\n    def backward(ctx, grad_output):\n        supp_size, output, batch = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n        return grad_input, None\nsparsemax = SparsemaxFunction.apply\nclass Sparsemax(nn.Module):\n    def __init__(self):\n        super(Sparsemax, self).__init__()\n    def forward(self, x, batch):\n        return sparsemax(x, batch)\nif __name__ == '__main__':\n    sparse_attention = Sparsemax()\n    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n    res = sparse_attention(input_x, input_batch)\n    print(res)\nclass TwoHopNeighborhood(object):\n    def __call__(self, data):\n        edge_index, edge_attr = data.edge_index, data.edge_attr\n        n = data.num_nodes\n        fill = 1e16\n        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n        edge_index = torch.cat([edge_index, index], dim=1)\n        if edge_attr is None:\n            data.edge_index, _ = coalesce(edge_index, None, n, n)\n        else:\n            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n            value = value.expand(-1, *list(edge_attr.size())[1:])\n            edge_attr = torch.cat([edge_attr, value], dim=0)\n            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min', fill_value=fill)\n            edge_attr[edge_attr >= fill] = 0\n            data.edge_attr = edge_attr\n        return data\n    def __repr__(self):\n        return '{}()'.format(self.__class__.__name__)\nclass GCN(MessagePassing):\n    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n        super(GCN, self).__init__(aggr='add', **kwargs)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n        nn.init.xavier_uniform_(self.weight.data)\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n            nn.init.zeros_(self.bias.data)\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n    def reset_parameters(self):\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight=None):\n        x = torch.matmul(x, self.weight)\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        if self.bias is not None:\n            aggr_out = aggr_out + self.bias\n        return aggr_out\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n        row, col = edge_index\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass HGPSLPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n        super(HGPSLPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.sample = sample\n        self.sparse = sparse\n        self.sl = sl\n        self.negative_slop = negative_slop\n        self.lamb = lamb\n        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n        nn.init.xavier_uniform_(self.att.data)\n        self.sparse_attention = Sparsemax()\n        self.neighbor_augment = TwoHopNeighborhood()\n        self.calc_information_score = NodeInformationScore()\n    def forward(self, x, edge_index, edge_attr, batch):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        original_x = x\n        perm = topk(score, self.ratio, batch)\n        x = x[perm]\n        batch = batch[perm]\n        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        if self.sl is False:\n            return x, induced_edge_index, induced_edge_attr, batch\n        if self.sample:\n            k_hop = 3\n            if edge_attr is None:\n                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n            for _ in range(k_hop - 1):\n                hop_data = self.neighbor_augment(hop_data)\n            hop_edge_index = hop_data.edge_index\n            hop_edge_attr = hop_data.edge_attr\n            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            adj[row, col] = weights\n            new_edge_index, weights = dense_to_sparse(adj)\n            row, col = new_edge_index\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        else:\n            if edge_attr is None:\n                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n                                               device=induced_edge_index.device)\n            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n            cum_num_nodes = num_nodes.cumsum(dim=0)\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n            new_edge_index, _ = dense_to_sparse(adj)\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop)\n            adj[row, col] = weights\n            induced_row, induced_col = induced_edge_index\n            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n            weights = adj[row, col]\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, num_nodes=x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        return x, new_edge_index, new_edge_attr, batch"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### MUTAG"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Early stopping at epoch 165 for seed 43\n",
                        "Early stopping at epoch 152 for seed 44\n",
                        "Average Time: 11.07 seconds\n",
                        "Var Time: 2.06 seconds\n",
                        "Average Memory: 24.00 MB\n",
                        "Average Best Val Acc: 0.7976\n",
                        "Std Best Test Acc: 0.1270\n",
                        "Average Test Acc: 0.7126\n"
                    ]
                }
            ],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nmax_nodes = 150\ndata_path = \"/data1/Pooling/1\"\ndataset_sparse = TUDataset(root=data_path, name=\"MUTAG\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_geometric.nn import BatchNorm\nclass HierarchicalGCN_HGPSL(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_HGPSL, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = HGPSLPool(hidden_channels, ratio=0.9, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = HGPSLPool(hidden_channels, ratio=0.9, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv3 = GCNConv(hidden_channels, out_channels)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 32)\n        self.lin2 = torch.nn.Linear(32, num_classes)\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        edge_attr=None\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool1(x, edge_index, edge_attr, batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool2(x, edge_index, edge_attr, batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\ndef train():\n    model.train()\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        data = data.to(device)\n        out = model(data)\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n    return correct / len(loader.dataset)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 150\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    train_ratio = 0.7\n    val_ratio = 0.15\n    val_ratio = 0.15\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n    model = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 201):\n        loss = train()\n        val_acc = test(valid_loader)\n        test_acc = test(test_loader)\n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\nprint(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### DD"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Average Time: 122.73 seconds\n",
                        "Var Time: 0.90 seconds\n",
                        "Average Memory: 10767.33 MB\n",
                        "Average Best Val Acc: 0.7748\n",
                        "Std Best Test Acc: 0.0278\n",
                        "Average Test Acc: 0.7327\n"
                    ]
                }
            ],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nmax_nodes = 500\ndata_path = \"/data1/Pooling\"\ndataset_sparse = TUDataset(root=data_path, name=\"DD\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_geometric.nn import BatchNorm\nclass HierarchicalGCN_HGPSL(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_HGPSL, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = HGPSLPool(hidden_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = HGPSLPool(hidden_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv3 = GCNConv(hidden_channels, out_channels)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 32)\n        self.lin2 = torch.nn.Linear(32, num_classes)\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        edge_attr = None\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool1(x, edge_index, edge_attr, batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool2(x, edge_index, edge_attr, batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\ndef train():\n    model.train()\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        data = data.to(device)\n        out = model(data)\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n    return correct / len(loader.dataset)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 150\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    train_ratio = 0.7\n    val_ratio = 0.15\n    val_ratio = 0.15\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n    model = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 201):\n        loss = train()\n        val_acc = test(valid_loader)\n        test_acc = test(test_loader)\n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\nprint(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.6963, Val Acc: 0.4800, Test Acc: 0.4733\n",
                        "Seed: 42, Epoch: 002, Loss: 0.6957, Val Acc: 0.4800, Test Acc: 0.4733\n",
                        "Seed: 42, Epoch: 003, Loss: 0.6953, Val Acc: 0.4800, Test Acc: 0.4733\n",
                        "Seed: 42, Epoch: 004, Loss: 0.6949, Val Acc: 0.4800, Test Acc: 0.4733\n",
                        "Seed: 42, Epoch: 005, Loss: 0.6943, Val Acc: 0.4800, Test Acc: 0.4733\n",
                        "Seed: 42, Epoch: 006, Loss: 0.6937, Val Acc: 0.4800, Test Acc: 0.4733\n",
                        "Seed: 42, Epoch: 007, Loss: 0.6933, Val Acc: 0.4800, Test Acc: 0.4733\n",
                        "Seed: 42, Epoch: 008, Loss: 0.6927, Val Acc: 0.4800, Test Acc: 0.4733\n",
                        "Seed: 42, Epoch: 009, Loss: 0.6918, Val Acc: 0.4867, Test Acc: 0.4733\n",
                        "Seed: 42, Epoch: 010, Loss: 0.6907, Val Acc: 0.5267, Test Acc: 0.4733\n",
                        "Seed: 42, Epoch: 011, Loss: 0.6908, Val Acc: 0.5467, Test Acc: 0.5400\n",
                        "Seed: 42, Epoch: 012, Loss: 0.6889, Val Acc: 0.5533, Test Acc: 0.5600\n",
                        "Seed: 42, Epoch: 013, Loss: 0.6876, Val Acc: 0.6267, Test Acc: 0.6000\n",
                        "Seed: 42, Epoch: 014, Loss: 0.6852, Val Acc: 0.7267, Test Acc: 0.6000\n",
                        "Seed: 42, Epoch: 015, Loss: 0.6861, Val Acc: 0.7733, Test Acc: 0.6867\n",
                        "Seed: 42, Epoch: 016, Loss: 0.6830, Val Acc: 0.7733, Test Acc: 0.7133\n",
                        "Seed: 42, Epoch: 017, Loss: 0.6813, Val Acc: 0.7667, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 018, Loss: 0.6795, Val Acc: 0.7667, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 019, Loss: 0.6780, Val Acc: 0.7667, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 020, Loss: 0.6729, Val Acc: 0.7600, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 021, Loss: 0.6685, Val Acc: 0.7600, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 022, Loss: 0.6658, Val Acc: 0.7733, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 023, Loss: 0.6565, Val Acc: 0.7600, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 024, Loss: 0.6562, Val Acc: 0.7667, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 025, Loss: 0.6511, Val Acc: 0.7600, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 026, Loss: 0.6406, Val Acc: 0.7667, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 027, Loss: 0.6346, Val Acc: 0.7667, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 028, Loss: 0.6307, Val Acc: 0.7733, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 029, Loss: 0.6213, Val Acc: 0.7533, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 030, Loss: 0.6078, Val Acc: 0.7467, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 031, Loss: 0.6131, Val Acc: 0.7533, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 032, Loss: 0.5975, Val Acc: 0.7600, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 033, Loss: 0.5857, Val Acc: 0.7533, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 034, Loss: 0.5790, Val Acc: 0.7667, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 035, Loss: 0.5776, Val Acc: 0.7600, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 036, Loss: 0.5688, Val Acc: 0.7667, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 037, Loss: 0.5689, Val Acc: 0.7867, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 038, Loss: 0.5615, Val Acc: 0.8000, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 039, Loss: 0.5554, Val Acc: 0.8067, Test Acc: 0.7267\n",
                        "Seed: 42, Epoch: 040, Loss: 0.5560, Val Acc: 0.8067, Test Acc: 0.7200\n",
                        "Seed: 42, Epoch: 041, Loss: 0.5480, Val Acc: 0.8200, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 042, Loss: 0.5414, Val Acc: 0.8067, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 043, Loss: 0.5474, Val Acc: 0.8000, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 044, Loss: 0.5402, Val Acc: 0.8067, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 045, Loss: 0.5369, Val Acc: 0.8067, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 046, Loss: 0.5381, Val Acc: 0.8067, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 047, Loss: 0.5312, Val Acc: 0.8067, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 048, Loss: 0.5315, Val Acc: 0.8067, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 049, Loss: 0.5339, Val Acc: 0.8200, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 050, Loss: 0.5280, Val Acc: 0.8133, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 051, Loss: 0.5225, Val Acc: 0.8333, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 052, Loss: 0.5218, Val Acc: 0.8333, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 053, Loss: 0.5226, Val Acc: 0.8467, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 054, Loss: 0.5158, Val Acc: 0.8133, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 055, Loss: 0.5275, Val Acc: 0.8133, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 056, Loss: 0.5201, Val Acc: 0.8200, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 057, Loss: 0.5201, Val Acc: 0.8267, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 058, Loss: 0.5059, Val Acc: 0.8200, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 059, Loss: 0.5188, Val Acc: 0.8067, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 060, Loss: 0.5108, Val Acc: 0.8200, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 061, Loss: 0.5281, Val Acc: 0.8133, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 062, Loss: 0.5134, Val Acc: 0.8133, Test Acc: 0.7400\n",
                        "Seed: 42, Epoch: 063, Loss: 0.5158, Val Acc: 0.8133, Test Acc: 0.7400\n",
                        "Seed: 42, Epoch: 064, Loss: 0.5093, Val Acc: 0.8333, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 065, Loss: 0.5165, Val Acc: 0.8133, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 066, Loss: 0.5020, Val Acc: 0.7933, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 067, Loss: 0.5082, Val Acc: 0.7867, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 068, Loss: 0.5098, Val Acc: 0.7933, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 069, Loss: 0.5097, Val Acc: 0.8267, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 070, Loss: 0.4961, Val Acc: 0.8200, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 071, Loss: 0.5000, Val Acc: 0.8133, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 072, Loss: 0.5181, Val Acc: 0.8333, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 073, Loss: 0.4956, Val Acc: 0.8200, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 074, Loss: 0.4990, Val Acc: 0.8200, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 075, Loss: 0.5095, Val Acc: 0.8200, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 076, Loss: 0.4975, Val Acc: 0.8200, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 077, Loss: 0.5121, Val Acc: 0.8067, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 078, Loss: 0.4977, Val Acc: 0.8267, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 079, Loss: 0.4871, Val Acc: 0.8200, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 080, Loss: 0.5009, Val Acc: 0.8133, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 081, Loss: 0.4953, Val Acc: 0.8133, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 082, Loss: 0.4797, Val Acc: 0.8133, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 083, Loss: 0.4895, Val Acc: 0.8133, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 084, Loss: 0.4950, Val Acc: 0.8133, Test Acc: 0.8133\n",
                        "Seed: 42, Epoch: 085, Loss: 0.4908, Val Acc: 0.8067, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 086, Loss: 0.4840, Val Acc: 0.8133, Test Acc: 0.8133\n",
                        "Seed: 42, Epoch: 087, Loss: 0.4759, Val Acc: 0.8133, Test Acc: 0.8133\n",
                        "Seed: 42, Epoch: 088, Loss: 0.4842, Val Acc: 0.8200, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 089, Loss: 0.4880, Val Acc: 0.8133, Test Acc: 0.8133\n",
                        "Seed: 42, Epoch: 090, Loss: 0.4906, Val Acc: 0.8067, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 091, Loss: 0.4771, Val Acc: 0.8200, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 092, Loss: 0.4834, Val Acc: 0.8200, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 093, Loss: 0.4821, Val Acc: 0.8267, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 094, Loss: 0.4784, Val Acc: 0.8267, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 095, Loss: 0.4779, Val Acc: 0.8200, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 096, Loss: 0.4798, Val Acc: 0.8067, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 097, Loss: 0.4912, Val Acc: 0.8133, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 098, Loss: 0.4646, Val Acc: 0.8200, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 099, Loss: 0.4760, Val Acc: 0.8067, Test Acc: 0.7400\n",
                        "Seed: 42, Epoch: 100, Loss: 0.4698, Val Acc: 0.8000, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 101, Loss: 0.4704, Val Acc: 0.8133, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 102, Loss: 0.4684, Val Acc: 0.8133, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 103, Loss: 0.4739, Val Acc: 0.8067, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 104, Loss: 0.4695, Val Acc: 0.8200, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 105, Loss: 0.4613, Val Acc: 0.8133, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 106, Loss: 0.4729, Val Acc: 0.8133, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 107, Loss: 0.4604, Val Acc: 0.8133, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 108, Loss: 0.4641, Val Acc: 0.8133, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 109, Loss: 0.4567, Val Acc: 0.8133, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 110, Loss: 0.4676, Val Acc: 0.8133, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 111, Loss: 0.4641, Val Acc: 0.8200, Test Acc: 0.8133\n",
                        "Seed: 42, Epoch: 112, Loss: 0.4687, Val Acc: 0.8200, Test Acc: 0.8133\n",
                        "Seed: 42, Epoch: 113, Loss: 0.4621, Val Acc: 0.8000, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 114, Loss: 0.4598, Val Acc: 0.7933, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 115, Loss: 0.4776, Val Acc: 0.7933, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 116, Loss: 0.4609, Val Acc: 0.7933, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 117, Loss: 0.4518, Val Acc: 0.8000, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 118, Loss: 0.4616, Val Acc: 0.8000, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 119, Loss: 0.4577, Val Acc: 0.8000, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 120, Loss: 0.4523, Val Acc: 0.8000, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 121, Loss: 0.4766, Val Acc: 0.7867, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 122, Loss: 0.4528, Val Acc: 0.8000, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 123, Loss: 0.4506, Val Acc: 0.8200, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 124, Loss: 0.4680, Val Acc: 0.8067, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 125, Loss: 0.4693, Val Acc: 0.8000, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 126, Loss: 0.4599, Val Acc: 0.8000, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 127, Loss: 0.4555, Val Acc: 0.8000, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 128, Loss: 0.4443, Val Acc: 0.8133, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 129, Loss: 0.4448, Val Acc: 0.8000, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 130, Loss: 0.4562, Val Acc: 0.7800, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 131, Loss: 0.4538, Val Acc: 0.7867, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 132, Loss: 0.4565, Val Acc: 0.7933, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 133, Loss: 0.4544, Val Acc: 0.7867, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 134, Loss: 0.4500, Val Acc: 0.7933, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 135, Loss: 0.4578, Val Acc: 0.8067, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 136, Loss: 0.4565, Val Acc: 0.7867, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 137, Loss: 0.4575, Val Acc: 0.7933, Test Acc: 0.7267\n",
                        "Seed: 42, Epoch: 138, Loss: 0.4485, Val Acc: 0.8000, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 139, Loss: 0.4401, Val Acc: 0.8133, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 140, Loss: 0.4524, Val Acc: 0.8067, Test Acc: 0.8133\n",
                        "Seed: 42, Epoch: 141, Loss: 0.4556, Val Acc: 0.8000, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 142, Loss: 0.4453, Val Acc: 0.8000, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 143, Loss: 0.4418, Val Acc: 0.7867, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 144, Loss: 0.4412, Val Acc: 0.8067, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 145, Loss: 0.4358, Val Acc: 0.8067, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 146, Loss: 0.4415, Val Acc: 0.7933, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 147, Loss: 0.4308, Val Acc: 0.7867, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 148, Loss: 0.4423, Val Acc: 0.7867, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 149, Loss: 0.4664, Val Acc: 0.7867, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 150, Loss: 0.4474, Val Acc: 0.7800, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 151, Loss: 0.4431, Val Acc: 0.8067, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 152, Loss: 0.4307, Val Acc: 0.8067, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 153, Loss: 0.4442, Val Acc: 0.7800, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 154, Loss: 0.4427, Val Acc: 0.7800, Test Acc: 0.7400\n",
                        "Seed: 42, Epoch: 155, Loss: 0.4410, Val Acc: 0.7867, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 156, Loss: 0.4395, Val Acc: 0.7800, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 157, Loss: 0.4333, Val Acc: 0.7800, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 158, Loss: 0.4443, Val Acc: 0.7800, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 159, Loss: 0.4307, Val Acc: 0.7800, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 160, Loss: 0.4395, Val Acc: 0.7800, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 161, Loss: 0.4275, Val Acc: 0.8000, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 162, Loss: 0.4329, Val Acc: 0.8000, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 163, Loss: 0.4338, Val Acc: 0.7933, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 164, Loss: 0.4362, Val Acc: 0.7800, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 165, Loss: 0.4435, Val Acc: 0.7933, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 166, Loss: 0.4340, Val Acc: 0.7933, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 167, Loss: 0.4220, Val Acc: 0.8067, Test Acc: 0.8067\n",
                        "Seed: 42, Epoch: 168, Loss: 0.4378, Val Acc: 0.7800, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 169, Loss: 0.4393, Val Acc: 0.7800, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 170, Loss: 0.4357, Val Acc: 0.7800, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 171, Loss: 0.4221, Val Acc: 0.7800, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 172, Loss: 0.4324, Val Acc: 0.7800, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 173, Loss: 0.4162, Val Acc: 0.7800, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 174, Loss: 0.4186, Val Acc: 0.7800, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 175, Loss: 0.4188, Val Acc: 0.7933, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 176, Loss: 0.4176, Val Acc: 0.8067, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 177, Loss: 0.4485, Val Acc: 0.7800, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 178, Loss: 0.4287, Val Acc: 0.7733, Test Acc: 0.7267\n",
                        "Seed: 42, Epoch: 179, Loss: 0.4375, Val Acc: 0.7733, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 180, Loss: 0.4273, Val Acc: 0.7733, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 181, Loss: 0.4287, Val Acc: 0.7800, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 182, Loss: 0.4270, Val Acc: 0.7667, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 183, Loss: 0.4321, Val Acc: 0.7667, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 184, Loss: 0.4284, Val Acc: 0.7867, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 185, Loss: 0.4265, Val Acc: 0.7600, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 186, Loss: 0.4261, Val Acc: 0.7867, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 187, Loss: 0.4232, Val Acc: 0.7733, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 188, Loss: 0.4187, Val Acc: 0.7667, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 189, Loss: 0.4234, Val Acc: 0.7800, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 190, Loss: 0.4169, Val Acc: 0.7800, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 191, Loss: 0.4068, Val Acc: 0.7600, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 192, Loss: 0.4179, Val Acc: 0.7733, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 193, Loss: 0.4158, Val Acc: 0.7600, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 194, Loss: 0.4235, Val Acc: 0.7867, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 195, Loss: 0.4204, Val Acc: 0.7733, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 196, Loss: 0.4150, Val Acc: 0.7667, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 197, Loss: 0.4175, Val Acc: 0.7600, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 198, Loss: 0.4013, Val Acc: 0.7467, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 199, Loss: 0.4061, Val Acc: 0.7667, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 200, Loss: 0.4186, Val Acc: 0.7600, Test Acc: 0.7800\n",
                        "Seed: 43, Epoch: 001, Loss: 0.6996, Val Acc: 0.5867, Test Acc: 0.5400\n",
                        "Seed: 43, Epoch: 002, Loss: 0.6987, Val Acc: 0.5867, Test Acc: 0.5400\n",
                        "Seed: 43, Epoch: 003, Loss: 0.6976, Val Acc: 0.5867, Test Acc: 0.5400\n",
                        "Seed: 43, Epoch: 004, Loss: 0.6965, Val Acc: 0.5867, Test Acc: 0.5400\n",
                        "Seed: 43, Epoch: 005, Loss: 0.6960, Val Acc: 0.5867, Test Acc: 0.5400\n",
                        "Seed: 43, Epoch: 006, Loss: 0.6950, Val Acc: 0.5867, Test Acc: 0.5400\n",
                        "Seed: 43, Epoch: 007, Loss: 0.6937, Val Acc: 0.5867, Test Acc: 0.5667\n",
                        "Seed: 43, Epoch: 008, Loss: 0.6927, Val Acc: 0.6133, Test Acc: 0.5867\n",
                        "Seed: 43, Epoch: 009, Loss: 0.6920, Val Acc: 0.6467, Test Acc: 0.5867\n",
                        "Seed: 43, Epoch: 010, Loss: 0.6899, Val Acc: 0.6333, Test Acc: 0.6400\n",
                        "Seed: 43, Epoch: 011, Loss: 0.6892, Val Acc: 0.6600, Test Acc: 0.6333\n",
                        "Seed: 43, Epoch: 012, Loss: 0.6883, Val Acc: 0.5267, Test Acc: 0.5867\n",
                        "Seed: 43, Epoch: 013, Loss: 0.6865, Val Acc: 0.4333, Test Acc: 0.4800\n",
                        "Seed: 43, Epoch: 014, Loss: 0.6852, Val Acc: 0.4133, Test Acc: 0.4533\n",
                        "Seed: 43, Epoch: 015, Loss: 0.6831, Val Acc: 0.4133, Test Acc: 0.4533\n",
                        "Seed: 43, Epoch: 016, Loss: 0.6784, Val Acc: 0.4133, Test Acc: 0.4667\n",
                        "Seed: 43, Epoch: 017, Loss: 0.6736, Val Acc: 0.4467, Test Acc: 0.5000\n",
                        "Seed: 43, Epoch: 018, Loss: 0.6756, Val Acc: 0.4867, Test Acc: 0.5200\n",
                        "Seed: 43, Epoch: 019, Loss: 0.6708, Val Acc: 0.4933, Test Acc: 0.5333\n",
                        "Seed: 43, Epoch: 020, Loss: 0.6683, Val Acc: 0.4933, Test Acc: 0.6067\n",
                        "Seed: 43, Epoch: 021, Loss: 0.6637, Val Acc: 0.5667, Test Acc: 0.6267\n",
                        "Seed: 43, Epoch: 022, Loss: 0.6552, Val Acc: 0.5733, Test Acc: 0.6533\n",
                        "Seed: 43, Epoch: 023, Loss: 0.6512, Val Acc: 0.5667, Test Acc: 0.6467\n",
                        "Seed: 43, Epoch: 024, Loss: 0.6422, Val Acc: 0.5933, Test Acc: 0.6733\n",
                        "Seed: 43, Epoch: 025, Loss: 0.6354, Val Acc: 0.6333, Test Acc: 0.7200\n",
                        "Seed: 43, Epoch: 026, Loss: 0.6132, Val Acc: 0.6467, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 027, Loss: 0.6158, Val Acc: 0.6467, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 028, Loss: 0.6104, Val Acc: 0.6533, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 029, Loss: 0.5811, Val Acc: 0.6600, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 030, Loss: 0.5871, Val Acc: 0.6667, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 031, Loss: 0.5755, Val Acc: 0.6867, Test Acc: 0.7133\n",
                        "Seed: 43, Epoch: 032, Loss: 0.5650, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 033, Loss: 0.5555, Val Acc: 0.6733, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 034, Loss: 0.5480, Val Acc: 0.6667, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 035, Loss: 0.5423, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 036, Loss: 0.5408, Val Acc: 0.6733, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 037, Loss: 0.5135, Val Acc: 0.7000, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 038, Loss: 0.5305, Val Acc: 0.6933, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 039, Loss: 0.5074, Val Acc: 0.7200, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 040, Loss: 0.5172, Val Acc: 0.6933, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 041, Loss: 0.5118, Val Acc: 0.6933, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 042, Loss: 0.5079, Val Acc: 0.6933, Test Acc: 0.7133\n",
                        "Seed: 43, Epoch: 043, Loss: 0.4961, Val Acc: 0.6667, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 044, Loss: 0.4973, Val Acc: 0.6600, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 045, Loss: 0.4958, Val Acc: 0.6867, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 046, Loss: 0.4960, Val Acc: 0.7067, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 047, Loss: 0.4889, Val Acc: 0.7000, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 048, Loss: 0.4845, Val Acc: 0.6933, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 049, Loss: 0.4844, Val Acc: 0.6800, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 050, Loss: 0.4838, Val Acc: 0.6867, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 051, Loss: 0.4767, Val Acc: 0.6867, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 052, Loss: 0.4787, Val Acc: 0.6933, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 053, Loss: 0.4834, Val Acc: 0.6933, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 054, Loss: 0.4709, Val Acc: 0.6933, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 055, Loss: 0.4709, Val Acc: 0.6933, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 056, Loss: 0.4737, Val Acc: 0.6933, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 057, Loss: 0.4741, Val Acc: 0.6867, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 058, Loss: 0.4759, Val Acc: 0.7000, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 059, Loss: 0.4689, Val Acc: 0.7000, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 060, Loss: 0.4689, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 061, Loss: 0.4777, Val Acc: 0.6667, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 062, Loss: 0.4693, Val Acc: 0.6800, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 063, Loss: 0.4700, Val Acc: 0.6667, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 064, Loss: 0.4597, Val Acc: 0.6933, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 065, Loss: 0.4595, Val Acc: 0.6933, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 066, Loss: 0.4530, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 067, Loss: 0.4589, Val Acc: 0.6800, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 068, Loss: 0.4650, Val Acc: 0.6733, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 069, Loss: 0.4596, Val Acc: 0.6867, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 070, Loss: 0.4492, Val Acc: 0.6867, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 071, Loss: 0.4655, Val Acc: 0.7000, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 072, Loss: 0.4566, Val Acc: 0.6667, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 073, Loss: 0.4393, Val Acc: 0.6667, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 074, Loss: 0.4493, Val Acc: 0.6667, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 075, Loss: 0.4569, Val Acc: 0.6533, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 076, Loss: 0.4659, Val Acc: 0.6600, Test Acc: 0.7667\n",
                        "Seed: 43, Epoch: 077, Loss: 0.4687, Val Acc: 0.7133, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 078, Loss: 0.4472, Val Acc: 0.6600, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 079, Loss: 0.4576, Val Acc: 0.6533, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 080, Loss: 0.4378, Val Acc: 0.6533, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 081, Loss: 0.4592, Val Acc: 0.6600, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 082, Loss: 0.4408, Val Acc: 0.6600, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 083, Loss: 0.4551, Val Acc: 0.6800, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 084, Loss: 0.4544, Val Acc: 0.6800, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 085, Loss: 0.4484, Val Acc: 0.6667, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 086, Loss: 0.4382, Val Acc: 0.6600, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 087, Loss: 0.4483, Val Acc: 0.6667, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 088, Loss: 0.4492, Val Acc: 0.6600, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 089, Loss: 0.4479, Val Acc: 0.6667, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 090, Loss: 0.4472, Val Acc: 0.6867, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 091, Loss: 0.4525, Val Acc: 0.6867, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 092, Loss: 0.4431, Val Acc: 0.6600, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 093, Loss: 0.4346, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 094, Loss: 0.4508, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 095, Loss: 0.4423, Val Acc: 0.6600, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 096, Loss: 0.4385, Val Acc: 0.6867, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 097, Loss: 0.4502, Val Acc: 0.6733, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 098, Loss: 0.4386, Val Acc: 0.6800, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 099, Loss: 0.4410, Val Acc: 0.6733, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 100, Loss: 0.4312, Val Acc: 0.6600, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 101, Loss: 0.4442, Val Acc: 0.6667, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 102, Loss: 0.4313, Val Acc: 0.6600, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 103, Loss: 0.4222, Val Acc: 0.6733, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 104, Loss: 0.4359, Val Acc: 0.6800, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 105, Loss: 0.4379, Val Acc: 0.6800, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 106, Loss: 0.4219, Val Acc: 0.6733, Test Acc: 0.7667\n",
                        "Seed: 43, Epoch: 107, Loss: 0.4329, Val Acc: 0.6667, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 108, Loss: 0.4270, Val Acc: 0.6667, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 109, Loss: 0.4294, Val Acc: 0.6800, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 110, Loss: 0.4273, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 111, Loss: 0.4307, Val Acc: 0.6667, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 112, Loss: 0.4331, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 113, Loss: 0.4249, Val Acc: 0.6667, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 114, Loss: 0.4328, Val Acc: 0.6733, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 115, Loss: 0.4378, Val Acc: 0.6733, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 116, Loss: 0.4274, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 117, Loss: 0.4291, Val Acc: 0.6733, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 118, Loss: 0.4289, Val Acc: 0.6733, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 119, Loss: 0.4223, Val Acc: 0.6733, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 120, Loss: 0.4218, Val Acc: 0.6733, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 121, Loss: 0.4167, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 122, Loss: 0.4255, Val Acc: 0.6667, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 123, Loss: 0.4297, Val Acc: 0.6733, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 124, Loss: 0.4246, Val Acc: 0.6733, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 125, Loss: 0.4166, Val Acc: 0.6733, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 126, Loss: 0.4221, Val Acc: 0.6533, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 127, Loss: 0.4242, Val Acc: 0.6467, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 128, Loss: 0.4176, Val Acc: 0.6400, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 129, Loss: 0.4272, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 130, Loss: 0.4275, Val Acc: 0.6733, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 131, Loss: 0.4140, Val Acc: 0.6733, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 132, Loss: 0.4126, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 133, Loss: 0.4253, Val Acc: 0.6667, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 134, Loss: 0.4184, Val Acc: 0.6600, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 135, Loss: 0.4189, Val Acc: 0.6600, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 136, Loss: 0.4046, Val Acc: 0.6733, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 137, Loss: 0.4131, Val Acc: 0.6733, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 138, Loss: 0.4192, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 139, Loss: 0.4193, Val Acc: 0.6667, Test Acc: 0.7600\n",
                        "Seed: 43, Epoch: 140, Loss: 0.4125, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 141, Loss: 0.4270, Val Acc: 0.6867, Test Acc: 0.7200\n",
                        "Seed: 43, Epoch: 142, Loss: 0.4188, Val Acc: 0.7000, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 143, Loss: 0.4093, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 144, Loss: 0.4031, Val Acc: 0.6600, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 145, Loss: 0.4252, Val Acc: 0.6667, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 146, Loss: 0.4059, Val Acc: 0.6867, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 147, Loss: 0.4086, Val Acc: 0.6800, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 148, Loss: 0.4130, Val Acc: 0.6800, Test Acc: 0.7200\n",
                        "Seed: 43, Epoch: 149, Loss: 0.4053, Val Acc: 0.6600, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 150, Loss: 0.4089, Val Acc: 0.6667, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 151, Loss: 0.4105, Val Acc: 0.6533, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 152, Loss: 0.4103, Val Acc: 0.6733, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 153, Loss: 0.4019, Val Acc: 0.6733, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 154, Loss: 0.4025, Val Acc: 0.6600, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 155, Loss: 0.4150, Val Acc: 0.6733, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 156, Loss: 0.3982, Val Acc: 0.6533, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 157, Loss: 0.4014, Val Acc: 0.6667, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 158, Loss: 0.3975, Val Acc: 0.6733, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 159, Loss: 0.3935, Val Acc: 0.6800, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 160, Loss: 0.3938, Val Acc: 0.6800, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 161, Loss: 0.3977, Val Acc: 0.6867, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 162, Loss: 0.4010, Val Acc: 0.6867, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 163, Loss: 0.3926, Val Acc: 0.6800, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 164, Loss: 0.4024, Val Acc: 0.6800, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 165, Loss: 0.4025, Val Acc: 0.6867, Test Acc: 0.7133\n",
                        "Seed: 43, Epoch: 166, Loss: 0.3928, Val Acc: 0.6600, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 167, Loss: 0.4033, Val Acc: 0.6600, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 168, Loss: 0.4008, Val Acc: 0.6667, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 169, Loss: 0.3856, Val Acc: 0.6800, Test Acc: 0.7067\n",
                        "Seed: 43, Epoch: 170, Loss: 0.4016, Val Acc: 0.6667, Test Acc: 0.7067\n",
                        "Seed: 43, Epoch: 171, Loss: 0.4029, Val Acc: 0.6733, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 172, Loss: 0.3984, Val Acc: 0.6667, Test Acc: 0.7067\n",
                        "Seed: 43, Epoch: 173, Loss: 0.4034, Val Acc: 0.6733, Test Acc: 0.7133\n",
                        "Seed: 43, Epoch: 174, Loss: 0.3895, Val Acc: 0.6600, Test Acc: 0.7067\n",
                        "Seed: 43, Epoch: 175, Loss: 0.4073, Val Acc: 0.6600, Test Acc: 0.7067\n",
                        "Seed: 43, Epoch: 176, Loss: 0.4001, Val Acc: 0.6600, Test Acc: 0.7200\n",
                        "Seed: 43, Epoch: 177, Loss: 0.3881, Val Acc: 0.6533, Test Acc: 0.7467\n",
                        "Seed: 43, Epoch: 178, Loss: 0.3851, Val Acc: 0.6467, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 179, Loss: 0.3969, Val Acc: 0.6733, Test Acc: 0.7200\n",
                        "Seed: 43, Epoch: 180, Loss: 0.3973, Val Acc: 0.6867, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 181, Loss: 0.4010, Val Acc: 0.6867, Test Acc: 0.7400\n",
                        "Seed: 43, Epoch: 182, Loss: 0.3950, Val Acc: 0.6667, Test Acc: 0.7333\n",
                        "Seed: 43, Epoch: 183, Loss: 0.3958, Val Acc: 0.6400, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 184, Loss: 0.3926, Val Acc: 0.6733, Test Acc: 0.7200\n",
                        "Seed: 43, Epoch: 185, Loss: 0.3884, Val Acc: 0.6600, Test Acc: 0.7200\n",
                        "Seed: 43, Epoch: 186, Loss: 0.3779, Val Acc: 0.6467, Test Acc: 0.7133\n",
                        "Seed: 43, Epoch: 187, Loss: 0.3890, Val Acc: 0.6400, Test Acc: 0.7267\n",
                        "Seed: 43, Epoch: 188, Loss: 0.3968, Val Acc: 0.6667, Test Acc: 0.7067\n",
                        "Seed: 43, Epoch: 189, Loss: 0.3882, Val Acc: 0.6800, Test Acc: 0.7200\n",
                        "Early stopping at epoch 189 for seed 43\n",
                        "Seed: 44, Epoch: 001, Loss: 0.6928, Val Acc: 0.4600, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 002, Loss: 0.6925, Val Acc: 0.4600, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 003, Loss: 0.6923, Val Acc: 0.4600, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 004, Loss: 0.6921, Val Acc: 0.4600, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 005, Loss: 0.6918, Val Acc: 0.4600, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 006, Loss: 0.6914, Val Acc: 0.4600, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 007, Loss: 0.6909, Val Acc: 0.4600, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 008, Loss: 0.6899, Val Acc: 0.4600, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 009, Loss: 0.6897, Val Acc: 0.4600, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 010, Loss: 0.6885, Val Acc: 0.4600, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 011, Loss: 0.6874, Val Acc: 0.4667, Test Acc: 0.4867\n",
                        "Seed: 44, Epoch: 012, Loss: 0.6861, Val Acc: 0.4667, Test Acc: 0.5000\n",
                        "Seed: 44, Epoch: 013, Loss: 0.6834, Val Acc: 0.4933, Test Acc: 0.5400\n",
                        "Seed: 44, Epoch: 014, Loss: 0.6818, Val Acc: 0.5267, Test Acc: 0.6000\n",
                        "Seed: 44, Epoch: 015, Loss: 0.6787, Val Acc: 0.5467, Test Acc: 0.6200\n",
                        "Seed: 44, Epoch: 016, Loss: 0.6783, Val Acc: 0.5600, Test Acc: 0.6667\n",
                        "Seed: 44, Epoch: 017, Loss: 0.6742, Val Acc: 0.5733, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 018, Loss: 0.6703, Val Acc: 0.5933, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 019, Loss: 0.6642, Val Acc: 0.6133, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 020, Loss: 0.6621, Val Acc: 0.6400, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 021, Loss: 0.6551, Val Acc: 0.6800, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 022, Loss: 0.6495, Val Acc: 0.6867, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 023, Loss: 0.6445, Val Acc: 0.7000, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 024, Loss: 0.6341, Val Acc: 0.7200, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 025, Loss: 0.6267, Val Acc: 0.7267, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 026, Loss: 0.6200, Val Acc: 0.7267, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 027, Loss: 0.6117, Val Acc: 0.7200, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 028, Loss: 0.5968, Val Acc: 0.7267, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 029, Loss: 0.5867, Val Acc: 0.7333, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 030, Loss: 0.5759, Val Acc: 0.7267, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 031, Loss: 0.5690, Val Acc: 0.7267, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 032, Loss: 0.5553, Val Acc: 0.7467, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 033, Loss: 0.5437, Val Acc: 0.7533, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 034, Loss: 0.5354, Val Acc: 0.7467, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 035, Loss: 0.5296, Val Acc: 0.7533, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 036, Loss: 0.5189, Val Acc: 0.7600, Test Acc: 0.6667\n",
                        "Seed: 44, Epoch: 037, Loss: 0.5247, Val Acc: 0.7600, Test Acc: 0.6667\n",
                        "Seed: 44, Epoch: 038, Loss: 0.5133, Val Acc: 0.7667, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 039, Loss: 0.5050, Val Acc: 0.7800, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 040, Loss: 0.4983, Val Acc: 0.7667, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 041, Loss: 0.5030, Val Acc: 0.7733, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 042, Loss: 0.4923, Val Acc: 0.7867, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 043, Loss: 0.4867, Val Acc: 0.7733, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 044, Loss: 0.4895, Val Acc: 0.7733, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 045, Loss: 0.4874, Val Acc: 0.7867, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 046, Loss: 0.4824, Val Acc: 0.7867, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 047, Loss: 0.4854, Val Acc: 0.7800, Test Acc: 0.6600\n",
                        "Seed: 44, Epoch: 048, Loss: 0.4795, Val Acc: 0.7800, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 049, Loss: 0.4799, Val Acc: 0.7733, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 050, Loss: 0.4806, Val Acc: 0.7800, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 051, Loss: 0.4730, Val Acc: 0.7933, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 052, Loss: 0.4802, Val Acc: 0.7600, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 053, Loss: 0.4741, Val Acc: 0.7933, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 054, Loss: 0.4698, Val Acc: 0.7933, Test Acc: 0.6667\n",
                        "Seed: 44, Epoch: 055, Loss: 0.4673, Val Acc: 0.7733, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 056, Loss: 0.4683, Val Acc: 0.7867, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 057, Loss: 0.4641, Val Acc: 0.7933, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 058, Loss: 0.4689, Val Acc: 0.7867, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 059, Loss: 0.4654, Val Acc: 0.7933, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 060, Loss: 0.4623, Val Acc: 0.8000, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 061, Loss: 0.4628, Val Acc: 0.7733, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 062, Loss: 0.4619, Val Acc: 0.7867, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 063, Loss: 0.4598, Val Acc: 0.7933, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 064, Loss: 0.4561, Val Acc: 0.8000, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 065, Loss: 0.4584, Val Acc: 0.8000, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 066, Loss: 0.4566, Val Acc: 0.7867, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 067, Loss: 0.4548, Val Acc: 0.8000, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 068, Loss: 0.4532, Val Acc: 0.8000, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 069, Loss: 0.4577, Val Acc: 0.8000, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 070, Loss: 0.4558, Val Acc: 0.7867, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 071, Loss: 0.4591, Val Acc: 0.7867, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 072, Loss: 0.4513, Val Acc: 0.8000, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 073, Loss: 0.4526, Val Acc: 0.7933, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 074, Loss: 0.4531, Val Acc: 0.7933, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 075, Loss: 0.4533, Val Acc: 0.7867, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 076, Loss: 0.4499, Val Acc: 0.7867, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 077, Loss: 0.4510, Val Acc: 0.7867, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 078, Loss: 0.4484, Val Acc: 0.7867, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 079, Loss: 0.4465, Val Acc: 0.7867, Test Acc: 0.7267\n",
                        "Seed: 44, Epoch: 080, Loss: 0.4542, Val Acc: 0.8000, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 081, Loss: 0.4460, Val Acc: 0.8000, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 082, Loss: 0.4441, Val Acc: 0.7933, Test Acc: 0.7267\n",
                        "Seed: 44, Epoch: 083, Loss: 0.4433, Val Acc: 0.7867, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 084, Loss: 0.4426, Val Acc: 0.7800, Test Acc: 0.7333\n",
                        "Seed: 44, Epoch: 085, Loss: 0.4467, Val Acc: 0.7867, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 086, Loss: 0.4419, Val Acc: 0.7933, Test Acc: 0.7267\n",
                        "Seed: 44, Epoch: 087, Loss: 0.4485, Val Acc: 0.7800, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 088, Loss: 0.4573, Val Acc: 0.7800, Test Acc: 0.7267\n",
                        "Seed: 44, Epoch: 089, Loss: 0.4403, Val Acc: 0.7533, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 090, Loss: 0.4538, Val Acc: 0.7600, Test Acc: 0.7267\n",
                        "Seed: 44, Epoch: 091, Loss: 0.4397, Val Acc: 0.7800, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 092, Loss: 0.4441, Val Acc: 0.7733, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 093, Loss: 0.4483, Val Acc: 0.7867, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 094, Loss: 0.4358, Val Acc: 0.7067, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 095, Loss: 0.4542, Val Acc: 0.7133, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 096, Loss: 0.4481, Val Acc: 0.7800, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 097, Loss: 0.4400, Val Acc: 0.7800, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 098, Loss: 0.4436, Val Acc: 0.7733, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 099, Loss: 0.4422, Val Acc: 0.7933, Test Acc: 0.7267\n",
                        "Seed: 44, Epoch: 100, Loss: 0.4367, Val Acc: 0.7600, Test Acc: 0.7267\n",
                        "Seed: 44, Epoch: 101, Loss: 0.4367, Val Acc: 0.7800, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 102, Loss: 0.4390, Val Acc: 0.7867, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 103, Loss: 0.4320, Val Acc: 0.7867, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 104, Loss: 0.4359, Val Acc: 0.7867, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 105, Loss: 0.4300, Val Acc: 0.7867, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 106, Loss: 0.4322, Val Acc: 0.7800, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 107, Loss: 0.4301, Val Acc: 0.7933, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 108, Loss: 0.4288, Val Acc: 0.7800, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 109, Loss: 0.4308, Val Acc: 0.7867, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 110, Loss: 0.4286, Val Acc: 0.7933, Test Acc: 0.7267\n",
                        "Seed: 44, Epoch: 111, Loss: 0.4269, Val Acc: 0.7867, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 112, Loss: 0.4251, Val Acc: 0.7933, Test Acc: 0.7267\n",
                        "Seed: 44, Epoch: 113, Loss: 0.4286, Val Acc: 0.7800, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 114, Loss: 0.4291, Val Acc: 0.7800, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 115, Loss: 0.4264, Val Acc: 0.7800, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 116, Loss: 0.4253, Val Acc: 0.7867, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 117, Loss: 0.4270, Val Acc: 0.7867, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 118, Loss: 0.4204, Val Acc: 0.7800, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 119, Loss: 0.4247, Val Acc: 0.7800, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 120, Loss: 0.4266, Val Acc: 0.7800, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 121, Loss: 0.4232, Val Acc: 0.7733, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 122, Loss: 0.4197, Val Acc: 0.7600, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 123, Loss: 0.4237, Val Acc: 0.7533, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 124, Loss: 0.4212, Val Acc: 0.7667, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 125, Loss: 0.4226, Val Acc: 0.7533, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 126, Loss: 0.4208, Val Acc: 0.7533, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 127, Loss: 0.4169, Val Acc: 0.7600, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 128, Loss: 0.4179, Val Acc: 0.7467, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 129, Loss: 0.4241, Val Acc: 0.7800, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 130, Loss: 0.4183, Val Acc: 0.7733, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 131, Loss: 0.4183, Val Acc: 0.7533, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 132, Loss: 0.4205, Val Acc: 0.7533, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 133, Loss: 0.4144, Val Acc: 0.7533, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 134, Loss: 0.4144, Val Acc: 0.7533, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 135, Loss: 0.4140, Val Acc: 0.7533, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 136, Loss: 0.4133, Val Acc: 0.7533, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 137, Loss: 0.4134, Val Acc: 0.7667, Test Acc: 0.7267\n",
                        "Seed: 44, Epoch: 138, Loss: 0.4137, Val Acc: 0.7667, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 139, Loss: 0.4155, Val Acc: 0.7600, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 140, Loss: 0.4122, Val Acc: 0.7467, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 141, Loss: 0.4150, Val Acc: 0.7533, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 142, Loss: 0.4140, Val Acc: 0.7533, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 143, Loss: 0.4140, Val Acc: 0.7533, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 144, Loss: 0.4124, Val Acc: 0.7600, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 145, Loss: 0.4144, Val Acc: 0.7400, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 146, Loss: 0.4091, Val Acc: 0.7467, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 147, Loss: 0.4124, Val Acc: 0.7467, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 148, Loss: 0.4078, Val Acc: 0.7533, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 149, Loss: 0.4087, Val Acc: 0.7467, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 150, Loss: 0.4179, Val Acc: 0.7467, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 151, Loss: 0.4102, Val Acc: 0.7467, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 152, Loss: 0.4084, Val Acc: 0.7400, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 153, Loss: 0.4048, Val Acc: 0.7400, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 154, Loss: 0.4134, Val Acc: 0.7400, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 155, Loss: 0.4054, Val Acc: 0.7400, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 156, Loss: 0.4014, Val Acc: 0.7400, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 157, Loss: 0.4045, Val Acc: 0.7400, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 158, Loss: 0.4126, Val Acc: 0.7400, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 159, Loss: 0.4085, Val Acc: 0.7400, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 160, Loss: 0.4014, Val Acc: 0.7400, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 161, Loss: 0.4034, Val Acc: 0.7467, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 162, Loss: 0.4041, Val Acc: 0.7467, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 163, Loss: 0.4034, Val Acc: 0.7467, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 164, Loss: 0.4010, Val Acc: 0.7467, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 165, Loss: 0.4024, Val Acc: 0.7533, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 166, Loss: 0.4017, Val Acc: 0.7600, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 167, Loss: 0.3981, Val Acc: 0.7533, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 168, Loss: 0.4007, Val Acc: 0.7600, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 169, Loss: 0.3985, Val Acc: 0.7533, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 170, Loss: 0.4077, Val Acc: 0.7533, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 171, Loss: 0.3960, Val Acc: 0.7533, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 172, Loss: 0.3965, Val Acc: 0.7467, Test Acc: 0.6667\n",
                        "Seed: 44, Epoch: 173, Loss: 0.4000, Val Acc: 0.7533, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 174, Loss: 0.3965, Val Acc: 0.7467, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 175, Loss: 0.3962, Val Acc: 0.7467, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 176, Loss: 0.3961, Val Acc: 0.7533, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 177, Loss: 0.3961, Val Acc: 0.7533, Test Acc: 0.7133\n",
                        "Seed: 44, Epoch: 178, Loss: 0.3980, Val Acc: 0.7600, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 179, Loss: 0.3921, Val Acc: 0.7333, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 180, Loss: 0.4019, Val Acc: 0.7600, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 181, Loss: 0.3957, Val Acc: 0.7600, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 182, Loss: 0.3991, Val Acc: 0.7533, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 183, Loss: 0.3931, Val Acc: 0.7533, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 184, Loss: 0.3912, Val Acc: 0.7333, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 185, Loss: 0.4059, Val Acc: 0.7600, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 186, Loss: 0.3912, Val Acc: 0.7467, Test Acc: 0.6867\n",
                        "Seed: 44, Epoch: 187, Loss: 0.3984, Val Acc: 0.7533, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 188, Loss: 0.3907, Val Acc: 0.7467, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 189, Loss: 0.3916, Val Acc: 0.7467, Test Acc: 0.6800\n",
                        "Seed: 44, Epoch: 190, Loss: 0.3934, Val Acc: 0.7400, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 191, Loss: 0.3904, Val Acc: 0.7467, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 192, Loss: 0.3882, Val Acc: 0.7467, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 193, Loss: 0.3875, Val Acc: 0.7333, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 194, Loss: 0.3864, Val Acc: 0.7467, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 195, Loss: 0.3840, Val Acc: 0.7400, Test Acc: 0.6733\n",
                        "Seed: 44, Epoch: 196, Loss: 0.3844, Val Acc: 0.7533, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 197, Loss: 0.3905, Val Acc: 0.7267, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 198, Loss: 0.3844, Val Acc: 0.7333, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 199, Loss: 0.3848, Val Acc: 0.7467, Test Acc: 0.6933\n",
                        "Seed: 44, Epoch: 200, Loss: 0.3827, Val Acc: 0.7467, Test Acc: 0.7000\n",
                        "Average Time: 77.41 seconds\n",
                        "Var Time: 23.84 seconds\n",
                        "Average Memory: 18188.67 MB\n",
                        "Average Best Val Acc: 0.7889\n",
                        "Std Best Test Acc: 0.0437\n",
                        "Average Test Acc: 0.7289\n"
                    ]
                }
            ],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nmax_nodes = 500\ndata_path = \"/data1/Pooling\"\ndataset_sparse = TUDataset(root=data_path, name=\"IMDB-BINARY\", transform=T.Compose([T.OneHotDegree(136)]), use_node_attr=True)\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_geometric.nn import BatchNorm\nclass HierarchicalGCN_HGPSL(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_HGPSL, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = HGPSLPool(hidden_channels, ratio=0.9, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = HGPSLPool(hidden_channels, ratio=0.9, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv3 = GCNConv(hidden_channels, out_channels)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 32)\n        self.lin2 = torch.nn.Linear(32, num_classes)\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        edge_attr = None\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool1(x, edge_index, edge_attr, batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool2(x, edge_index, edge_attr, batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\ndef train():\n    model.train()\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        data = data.to(device)\n        out = model(data)\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n    return correct / len(loader.dataset)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 150\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    train_ratio = 0.7\n    val_ratio = 0.15\n    val_ratio = 0.15\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n    model = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 201):\n        loss = train()\n        val_acc = test(valid_loader)\n        test_acc = test(test_loader)\n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\nprint(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### IMDB-MULTI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 1.1004, Val Acc: 0.3378, Test Acc: 0.3822\n",
                        "Seed: 42, Epoch: 002, Loss: 1.0999, Val Acc: 0.2933, Test Acc: 0.3867\n",
                        "Seed: 42, Epoch: 003, Loss: 1.0993, Val Acc: 0.2800, Test Acc: 0.3778\n",
                        "Seed: 42, Epoch: 004, Loss: 1.0991, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 005, Loss: 1.0985, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 006, Loss: 1.0982, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 007, Loss: 1.0979, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 008, Loss: 1.0976, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 009, Loss: 1.0970, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 010, Loss: 1.0967, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 011, Loss: 1.0962, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 012, Loss: 1.0963, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 013, Loss: 1.0955, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 014, Loss: 1.0944, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 015, Loss: 1.0945, Val Acc: 0.2800, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 016, Loss: 1.0938, Val Acc: 0.2800, Test Acc: 0.3556\n",
                        "Seed: 42, Epoch: 017, Loss: 1.0929, Val Acc: 0.2889, Test Acc: 0.3556\n",
                        "Seed: 42, Epoch: 018, Loss: 1.0908, Val Acc: 0.2933, Test Acc: 0.3689\n",
                        "Seed: 42, Epoch: 019, Loss: 1.0901, Val Acc: 0.2978, Test Acc: 0.3867\n",
                        "Seed: 42, Epoch: 020, Loss: 1.0868, Val Acc: 0.3156, Test Acc: 0.3911\n",
                        "Seed: 42, Epoch: 021, Loss: 1.0864, Val Acc: 0.3467, Test Acc: 0.4133\n",
                        "Seed: 42, Epoch: 022, Loss: 1.0805, Val Acc: 0.3689, Test Acc: 0.4400\n",
                        "Seed: 42, Epoch: 023, Loss: 1.0769, Val Acc: 0.4400, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 024, Loss: 1.0762, Val Acc: 0.4400, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 025, Loss: 1.0690, Val Acc: 0.4444, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 026, Loss: 1.0613, Val Acc: 0.4444, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 027, Loss: 1.0613, Val Acc: 0.4667, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 028, Loss: 1.0551, Val Acc: 0.4667, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 029, Loss: 1.0492, Val Acc: 0.4533, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 030, Loss: 1.0443, Val Acc: 0.4489, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 031, Loss: 1.0333, Val Acc: 0.4889, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 032, Loss: 1.0300, Val Acc: 0.4756, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 033, Loss: 1.0252, Val Acc: 0.4800, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 034, Loss: 1.0296, Val Acc: 0.4711, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 035, Loss: 1.0267, Val Acc: 0.4844, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 036, Loss: 1.0210, Val Acc: 0.4889, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 037, Loss: 1.0196, Val Acc: 0.4756, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 038, Loss: 1.0143, Val Acc: 0.4667, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 039, Loss: 1.0056, Val Acc: 0.4711, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 040, Loss: 1.0074, Val Acc: 0.4711, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 041, Loss: 1.0041, Val Acc: 0.4844, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 042, Loss: 0.9986, Val Acc: 0.4844, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 043, Loss: 1.0005, Val Acc: 0.4756, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 044, Loss: 0.9929, Val Acc: 0.4667, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 045, Loss: 0.9909, Val Acc: 0.4711, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 046, Loss: 0.9933, Val Acc: 0.4711, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 047, Loss: 0.9921, Val Acc: 0.4711, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 048, Loss: 0.9905, Val Acc: 0.4800, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 049, Loss: 0.9881, Val Acc: 0.4756, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 050, Loss: 0.9845, Val Acc: 0.4844, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 051, Loss: 0.9847, Val Acc: 0.4844, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 052, Loss: 0.9854, Val Acc: 0.4800, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 053, Loss: 0.9846, Val Acc: 0.4844, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 054, Loss: 0.9860, Val Acc: 0.4800, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 055, Loss: 0.9863, Val Acc: 0.4400, Test Acc: 0.4267\n",
                        "Seed: 42, Epoch: 056, Loss: 0.9968, Val Acc: 0.4400, Test Acc: 0.4311\n",
                        "Seed: 42, Epoch: 057, Loss: 0.9853, Val Acc: 0.4756, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 058, Loss: 0.9804, Val Acc: 0.4667, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 059, Loss: 0.9782, Val Acc: 0.4533, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 060, Loss: 0.9770, Val Acc: 0.4667, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 061, Loss: 0.9813, Val Acc: 0.4622, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 062, Loss: 0.9783, Val Acc: 0.4622, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 063, Loss: 0.9787, Val Acc: 0.4756, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 064, Loss: 0.9738, Val Acc: 0.4756, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 065, Loss: 0.9727, Val Acc: 0.4800, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 066, Loss: 0.9728, Val Acc: 0.4800, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 067, Loss: 0.9703, Val Acc: 0.4844, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 068, Loss: 0.9754, Val Acc: 0.4667, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 069, Loss: 0.9749, Val Acc: 0.4844, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 070, Loss: 0.9704, Val Acc: 0.4756, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 071, Loss: 0.9724, Val Acc: 0.4578, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 072, Loss: 0.9677, Val Acc: 0.4800, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 073, Loss: 0.9684, Val Acc: 0.4800, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 074, Loss: 0.9713, Val Acc: 0.4889, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 075, Loss: 0.9678, Val Acc: 0.4800, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 076, Loss: 0.9683, Val Acc: 0.4756, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 077, Loss: 0.9682, Val Acc: 0.4756, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 078, Loss: 0.9723, Val Acc: 0.4756, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 079, Loss: 0.9696, Val Acc: 0.4756, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 080, Loss: 0.9639, Val Acc: 0.4711, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 081, Loss: 0.9668, Val Acc: 0.4756, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 082, Loss: 0.9679, Val Acc: 0.4844, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 083, Loss: 0.9660, Val Acc: 0.4756, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 084, Loss: 0.9638, Val Acc: 0.4667, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 085, Loss: 0.9644, Val Acc: 0.4667, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 086, Loss: 0.9535, Val Acc: 0.4667, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 087, Loss: 0.9553, Val Acc: 0.4667, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 088, Loss: 0.9593, Val Acc: 0.4667, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 089, Loss: 0.9559, Val Acc: 0.4756, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 090, Loss: 0.9601, Val Acc: 0.4756, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 091, Loss: 0.9496, Val Acc: 0.4622, Test Acc: 0.4489\n",
                        "Seed: 42, Epoch: 092, Loss: 0.9507, Val Acc: 0.4622, Test Acc: 0.4444\n",
                        "Seed: 42, Epoch: 093, Loss: 0.9593, Val Acc: 0.4667, Test Acc: 0.4489\n",
                        "Seed: 42, Epoch: 094, Loss: 0.9619, Val Acc: 0.4622, Test Acc: 0.4489\n",
                        "Seed: 42, Epoch: 095, Loss: 0.9588, Val Acc: 0.4711, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 096, Loss: 0.9613, Val Acc: 0.4667, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 097, Loss: 0.9559, Val Acc: 0.4578, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 098, Loss: 0.9550, Val Acc: 0.4578, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 099, Loss: 0.9578, Val Acc: 0.4578, Test Acc: 0.4444\n",
                        "Seed: 42, Epoch: 100, Loss: 0.9597, Val Acc: 0.4711, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 101, Loss: 0.9601, Val Acc: 0.4711, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 102, Loss: 0.9537, Val Acc: 0.4711, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 103, Loss: 0.9549, Val Acc: 0.4711, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 104, Loss: 0.9545, Val Acc: 0.4667, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 105, Loss: 0.9560, Val Acc: 0.4578, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 106, Loss: 0.9599, Val Acc: 0.4444, Test Acc: 0.5200\n",
                        "Seed: 42, Epoch: 107, Loss: 0.9629, Val Acc: 0.4489, Test Acc: 0.5200\n",
                        "Seed: 42, Epoch: 108, Loss: 0.9566, Val Acc: 0.4533, Test Acc: 0.5244\n",
                        "Seed: 42, Epoch: 109, Loss: 0.9561, Val Acc: 0.4667, Test Acc: 0.5111\n",
                        "Seed: 42, Epoch: 110, Loss: 0.9541, Val Acc: 0.4533, Test Acc: 0.5067\n",
                        "Seed: 42, Epoch: 111, Loss: 0.9514, Val Acc: 0.4533, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 112, Loss: 0.9475, Val Acc: 0.4622, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 113, Loss: 0.9483, Val Acc: 0.4667, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 114, Loss: 0.9463, Val Acc: 0.4622, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 115, Loss: 0.9442, Val Acc: 0.4533, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 116, Loss: 0.9470, Val Acc: 0.4578, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 117, Loss: 0.9437, Val Acc: 0.4622, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 118, Loss: 0.9479, Val Acc: 0.4711, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 119, Loss: 0.9531, Val Acc: 0.4622, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 120, Loss: 0.9427, Val Acc: 0.4667, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 121, Loss: 0.9494, Val Acc: 0.4667, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 122, Loss: 0.9488, Val Acc: 0.4800, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 123, Loss: 0.9581, Val Acc: 0.4711, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 124, Loss: 0.9623, Val Acc: 0.4711, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 125, Loss: 0.9642, Val Acc: 0.4756, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 126, Loss: 0.9583, Val Acc: 0.4756, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 127, Loss: 0.9545, Val Acc: 0.4667, Test Acc: 0.5111\n",
                        "Seed: 42, Epoch: 128, Loss: 0.9488, Val Acc: 0.4622, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 129, Loss: 0.9510, Val Acc: 0.4578, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 130, Loss: 0.9639, Val Acc: 0.4578, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 131, Loss: 0.9423, Val Acc: 0.4622, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 132, Loss: 0.9454, Val Acc: 0.4578, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 133, Loss: 0.9444, Val Acc: 0.4667, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 134, Loss: 0.9452, Val Acc: 0.4711, Test Acc: 0.5067\n",
                        "Seed: 42, Epoch: 135, Loss: 0.9474, Val Acc: 0.4711, Test Acc: 0.5156\n",
                        "Seed: 42, Epoch: 136, Loss: 0.9414, Val Acc: 0.4711, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 137, Loss: 0.9432, Val Acc: 0.4667, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 138, Loss: 0.9465, Val Acc: 0.4667, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 139, Loss: 0.9379, Val Acc: 0.4711, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 140, Loss: 0.9437, Val Acc: 0.4667, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 141, Loss: 0.9455, Val Acc: 0.4578, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 142, Loss: 0.9366, Val Acc: 0.4711, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 143, Loss: 0.9394, Val Acc: 0.4622, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 144, Loss: 0.9381, Val Acc: 0.4533, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 145, Loss: 0.9425, Val Acc: 0.4578, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 146, Loss: 0.9371, Val Acc: 0.4711, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 147, Loss: 0.9415, Val Acc: 0.4711, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 148, Loss: 0.9354, Val Acc: 0.4756, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 149, Loss: 0.9456, Val Acc: 0.4800, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 150, Loss: 0.9531, Val Acc: 0.4800, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 151, Loss: 0.9375, Val Acc: 0.4667, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 152, Loss: 0.9340, Val Acc: 0.4711, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 153, Loss: 0.9349, Val Acc: 0.4667, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 154, Loss: 0.9440, Val Acc: 0.4667, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 155, Loss: 0.9462, Val Acc: 0.4711, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 156, Loss: 0.9403, Val Acc: 0.4667, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 157, Loss: 0.9434, Val Acc: 0.4667, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 158, Loss: 0.9354, Val Acc: 0.4711, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 159, Loss: 0.9359, Val Acc: 0.4711, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 160, Loss: 0.9368, Val Acc: 0.4756, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 161, Loss: 0.9315, Val Acc: 0.4622, Test Acc: 0.5111\n",
                        "Seed: 42, Epoch: 162, Loss: 0.9336, Val Acc: 0.4667, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 163, Loss: 0.9388, Val Acc: 0.4711, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 164, Loss: 0.9303, Val Acc: 0.4711, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 165, Loss: 0.9355, Val Acc: 0.4622, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 166, Loss: 0.9348, Val Acc: 0.4667, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 167, Loss: 0.9349, Val Acc: 0.4667, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 168, Loss: 0.9325, Val Acc: 0.4711, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 169, Loss: 0.9295, Val Acc: 0.4622, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 170, Loss: 0.9299, Val Acc: 0.4756, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 171, Loss: 0.9322, Val Acc: 0.4667, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 172, Loss: 0.9403, Val Acc: 0.4889, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 173, Loss: 0.9294, Val Acc: 0.4756, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 174, Loss: 0.9423, Val Acc: 0.4756, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 175, Loss: 0.9360, Val Acc: 0.4756, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 176, Loss: 0.9331, Val Acc: 0.4844, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 177, Loss: 0.9424, Val Acc: 0.4756, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 178, Loss: 0.9347, Val Acc: 0.4756, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 179, Loss: 0.9301, Val Acc: 0.4800, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 180, Loss: 0.9420, Val Acc: 0.4756, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 181, Loss: 0.9327, Val Acc: 0.4756, Test Acc: 0.4844\n",
                        "Early stopping at epoch 181 for seed 42\n",
                        "Seed: 43, Epoch: 001, Loss: 1.1001, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 002, Loss: 1.0995, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 003, Loss: 1.0989, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 004, Loss: 1.0984, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 005, Loss: 1.0980, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 006, Loss: 1.0975, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 007, Loss: 1.0970, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 008, Loss: 1.0965, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 009, Loss: 1.0959, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 010, Loss: 1.0947, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 011, Loss: 1.0938, Val Acc: 0.3333, Test Acc: 0.3289\n",
                        "Seed: 43, Epoch: 012, Loss: 1.0927, Val Acc: 0.3422, Test Acc: 0.3289\n",
                        "Seed: 43, Epoch: 013, Loss: 1.0903, Val Acc: 0.3467, Test Acc: 0.3289\n",
                        "Seed: 43, Epoch: 014, Loss: 1.0884, Val Acc: 0.3467, Test Acc: 0.3467\n",
                        "Seed: 43, Epoch: 015, Loss: 1.0861, Val Acc: 0.4089, Test Acc: 0.3733\n",
                        "Seed: 43, Epoch: 016, Loss: 1.0829, Val Acc: 0.4311, Test Acc: 0.3911\n",
                        "Seed: 43, Epoch: 017, Loss: 1.0795, Val Acc: 0.4311, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 018, Loss: 1.0746, Val Acc: 0.4444, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 019, Loss: 1.0700, Val Acc: 0.4533, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 020, Loss: 1.0644, Val Acc: 0.4622, Test Acc: 0.4044\n",
                        "Seed: 43, Epoch: 021, Loss: 1.0558, Val Acc: 0.4756, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 022, Loss: 1.0535, Val Acc: 0.4800, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 023, Loss: 1.0480, Val Acc: 0.4756, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 024, Loss: 1.0395, Val Acc: 0.4933, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 025, Loss: 1.0323, Val Acc: 0.4978, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 026, Loss: 1.0224, Val Acc: 0.5022, Test Acc: 0.4533\n",
                        "Seed: 43, Epoch: 027, Loss: 1.0172, Val Acc: 0.4844, Test Acc: 0.4533\n",
                        "Seed: 43, Epoch: 028, Loss: 1.0040, Val Acc: 0.4800, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 029, Loss: 0.9979, Val Acc: 0.4933, Test Acc: 0.4578\n",
                        "Seed: 43, Epoch: 030, Loss: 0.9983, Val Acc: 0.4800, Test Acc: 0.4756\n",
                        "Seed: 43, Epoch: 031, Loss: 0.9948, Val Acc: 0.4800, Test Acc: 0.4756\n",
                        "Seed: 43, Epoch: 032, Loss: 0.9911, Val Acc: 0.4756, Test Acc: 0.4578\n",
                        "Seed: 43, Epoch: 033, Loss: 0.9843, Val Acc: 0.4933, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 034, Loss: 0.9824, Val Acc: 0.4889, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 035, Loss: 0.9813, Val Acc: 0.4889, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 036, Loss: 0.9785, Val Acc: 0.4756, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 037, Loss: 0.9741, Val Acc: 0.4711, Test Acc: 0.4533\n",
                        "Seed: 43, Epoch: 038, Loss: 0.9736, Val Acc: 0.4711, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 039, Loss: 0.9833, Val Acc: 0.4711, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 040, Loss: 0.9756, Val Acc: 0.4667, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 041, Loss: 0.9724, Val Acc: 0.4711, Test Acc: 0.4533\n",
                        "Seed: 43, Epoch: 042, Loss: 0.9769, Val Acc: 0.4622, Test Acc: 0.4622\n",
                        "Seed: 43, Epoch: 043, Loss: 0.9762, Val Acc: 0.4622, Test Acc: 0.4578\n",
                        "Seed: 43, Epoch: 044, Loss: 0.9809, Val Acc: 0.4622, Test Acc: 0.4667\n",
                        "Seed: 43, Epoch: 045, Loss: 0.9660, Val Acc: 0.4533, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 046, Loss: 0.9636, Val Acc: 0.4844, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 047, Loss: 0.9632, Val Acc: 0.4667, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 048, Loss: 0.9624, Val Acc: 0.4578, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 049, Loss: 0.9609, Val Acc: 0.4622, Test Acc: 0.4667\n",
                        "Seed: 43, Epoch: 050, Loss: 0.9688, Val Acc: 0.4533, Test Acc: 0.4578\n",
                        "Seed: 43, Epoch: 051, Loss: 0.9649, Val Acc: 0.4489, Test Acc: 0.4622\n",
                        "Seed: 43, Epoch: 052, Loss: 0.9589, Val Acc: 0.4622, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 053, Loss: 0.9561, Val Acc: 0.4711, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 054, Loss: 0.9611, Val Acc: 0.4978, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 055, Loss: 0.9633, Val Acc: 0.4889, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 056, Loss: 0.9637, Val Acc: 0.4933, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 057, Loss: 0.9677, Val Acc: 0.4889, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 058, Loss: 0.9711, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 059, Loss: 0.9676, Val Acc: 0.4889, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 060, Loss: 0.9595, Val Acc: 0.4844, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 061, Loss: 0.9550, Val Acc: 0.4933, Test Acc: 0.4533\n",
                        "Seed: 43, Epoch: 062, Loss: 0.9549, Val Acc: 0.4800, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 063, Loss: 0.9516, Val Acc: 0.4622, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 064, Loss: 0.9506, Val Acc: 0.4667, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 065, Loss: 0.9435, Val Acc: 0.4578, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 066, Loss: 0.9452, Val Acc: 0.4622, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 067, Loss: 0.9431, Val Acc: 0.4667, Test Acc: 0.4578\n",
                        "Seed: 43, Epoch: 068, Loss: 0.9414, Val Acc: 0.4933, Test Acc: 0.4578\n",
                        "Seed: 43, Epoch: 069, Loss: 0.9484, Val Acc: 0.4978, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 070, Loss: 0.9456, Val Acc: 0.4978, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 071, Loss: 0.9494, Val Acc: 0.4978, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 072, Loss: 0.9544, Val Acc: 0.4933, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 073, Loss: 0.9490, Val Acc: 0.4889, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 074, Loss: 0.9464, Val Acc: 0.4889, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 075, Loss: 0.9406, Val Acc: 0.4756, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 076, Loss: 0.9319, Val Acc: 0.4667, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 077, Loss: 0.9381, Val Acc: 0.4756, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 078, Loss: 0.9414, Val Acc: 0.4844, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 079, Loss: 0.9405, Val Acc: 0.4933, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 080, Loss: 0.9440, Val Acc: 0.4933, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 081, Loss: 0.9426, Val Acc: 0.4889, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 082, Loss: 0.9425, Val Acc: 0.4889, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 083, Loss: 0.9385, Val Acc: 0.4978, Test Acc: 0.4533\n",
                        "Seed: 43, Epoch: 084, Loss: 0.9414, Val Acc: 0.4667, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 085, Loss: 0.9418, Val Acc: 0.4667, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 086, Loss: 0.9458, Val Acc: 0.4667, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 087, Loss: 0.9414, Val Acc: 0.4622, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 088, Loss: 0.9440, Val Acc: 0.4578, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 089, Loss: 0.9357, Val Acc: 0.4578, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 090, Loss: 0.9365, Val Acc: 0.4533, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 091, Loss: 0.9392, Val Acc: 0.4578, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 092, Loss: 0.9379, Val Acc: 0.4711, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 093, Loss: 0.9380, Val Acc: 0.4844, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 094, Loss: 0.9380, Val Acc: 0.5022, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 095, Loss: 0.9337, Val Acc: 0.4800, Test Acc: 0.4578\n",
                        "Seed: 43, Epoch: 096, Loss: 0.9331, Val Acc: 0.4756, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 097, Loss: 0.9394, Val Acc: 0.4667, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 098, Loss: 0.9415, Val Acc: 0.4667, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 099, Loss: 0.9444, Val Acc: 0.4711, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 100, Loss: 0.9322, Val Acc: 0.5022, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 101, Loss: 0.9368, Val Acc: 0.4800, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 102, Loss: 0.9405, Val Acc: 0.4889, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 103, Loss: 0.9355, Val Acc: 0.4800, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 104, Loss: 0.9355, Val Acc: 0.5111, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 105, Loss: 0.9285, Val Acc: 0.4933, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 106, Loss: 0.9292, Val Acc: 0.4800, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 107, Loss: 0.9317, Val Acc: 0.4667, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 108, Loss: 0.9332, Val Acc: 0.4622, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 109, Loss: 0.9344, Val Acc: 0.4800, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 110, Loss: 0.9235, Val Acc: 0.5022, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 111, Loss: 0.9227, Val Acc: 0.4756, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 112, Loss: 0.9375, Val Acc: 0.4756, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 113, Loss: 0.9338, Val Acc: 0.4933, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 114, Loss: 0.9244, Val Acc: 0.5022, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 115, Loss: 0.9257, Val Acc: 0.4933, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 116, Loss: 0.9211, Val Acc: 0.4844, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 117, Loss: 0.9221, Val Acc: 0.4800, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 118, Loss: 0.9257, Val Acc: 0.4844, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 119, Loss: 0.9256, Val Acc: 0.4800, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 120, Loss: 0.9278, Val Acc: 0.4756, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 121, Loss: 0.9344, Val Acc: 0.4889, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 122, Loss: 0.9331, Val Acc: 0.5022, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 123, Loss: 0.9274, Val Acc: 0.5022, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 124, Loss: 0.9288, Val Acc: 0.5022, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 125, Loss: 0.9254, Val Acc: 0.4978, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 126, Loss: 0.9294, Val Acc: 0.4889, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 127, Loss: 0.9266, Val Acc: 0.4667, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 128, Loss: 0.9219, Val Acc: 0.4533, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 129, Loss: 0.9314, Val Acc: 0.4578, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 130, Loss: 0.9317, Val Acc: 0.4667, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 131, Loss: 0.9249, Val Acc: 0.4711, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 132, Loss: 0.9273, Val Acc: 0.4756, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 133, Loss: 0.9259, Val Acc: 0.4533, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 134, Loss: 0.9284, Val Acc: 0.4533, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 135, Loss: 0.9240, Val Acc: 0.4711, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 136, Loss: 0.9229, Val Acc: 0.4800, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 137, Loss: 0.9211, Val Acc: 0.4844, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 138, Loss: 0.9213, Val Acc: 0.4889, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 139, Loss: 0.9269, Val Acc: 0.4889, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 140, Loss: 0.9234, Val Acc: 0.4978, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 141, Loss: 0.9245, Val Acc: 0.4978, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 142, Loss: 0.9332, Val Acc: 0.4978, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 143, Loss: 0.9350, Val Acc: 0.4978, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 144, Loss: 0.9299, Val Acc: 0.5022, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 145, Loss: 0.9269, Val Acc: 0.5067, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 146, Loss: 0.9241, Val Acc: 0.4978, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 147, Loss: 0.9231, Val Acc: 0.5022, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 148, Loss: 0.9228, Val Acc: 0.4933, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 149, Loss: 0.9203, Val Acc: 0.4889, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 150, Loss: 0.9177, Val Acc: 0.4844, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 151, Loss: 0.9226, Val Acc: 0.4889, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 152, Loss: 0.9215, Val Acc: 0.4889, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 153, Loss: 0.9206, Val Acc: 0.4844, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 154, Loss: 0.9204, Val Acc: 0.4978, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 155, Loss: 0.9173, Val Acc: 0.4978, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 156, Loss: 0.9170, Val Acc: 0.4889, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 157, Loss: 0.9228, Val Acc: 0.4889, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 158, Loss: 0.9160, Val Acc: 0.5022, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 159, Loss: 0.9155, Val Acc: 0.4978, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 160, Loss: 0.9122, Val Acc: 0.4978, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 161, Loss: 0.9176, Val Acc: 0.4933, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 162, Loss: 0.9154, Val Acc: 0.4933, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 163, Loss: 0.9161, Val Acc: 0.4933, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 164, Loss: 0.9136, Val Acc: 0.4978, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 165, Loss: 0.9080, Val Acc: 0.4933, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 166, Loss: 0.9057, Val Acc: 0.4933, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 167, Loss: 0.9123, Val Acc: 0.5022, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 168, Loss: 0.9102, Val Acc: 0.4978, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 169, Loss: 0.9177, Val Acc: 0.4978, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 170, Loss: 0.9120, Val Acc: 0.4667, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 171, Loss: 0.9229, Val Acc: 0.4844, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 172, Loss: 0.9205, Val Acc: 0.4933, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 173, Loss: 0.9133, Val Acc: 0.4933, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 174, Loss: 0.9104, Val Acc: 0.4978, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 175, Loss: 0.9159, Val Acc: 0.4889, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 176, Loss: 0.9161, Val Acc: 0.4889, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 177, Loss: 0.9182, Val Acc: 0.4889, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 178, Loss: 0.9161, Val Acc: 0.4889, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 179, Loss: 0.9152, Val Acc: 0.4978, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 180, Loss: 0.9114, Val Acc: 0.5022, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 181, Loss: 0.9169, Val Acc: 0.4756, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 182, Loss: 0.9197, Val Acc: 0.4844, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 183, Loss: 0.9153, Val Acc: 0.4844, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 184, Loss: 0.9140, Val Acc: 0.5067, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 185, Loss: 0.9131, Val Acc: 0.5067, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 186, Loss: 0.9135, Val Acc: 0.5022, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 187, Loss: 0.9129, Val Acc: 0.5022, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 188, Loss: 0.9193, Val Acc: 0.5022, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 189, Loss: 0.9126, Val Acc: 0.4978, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 190, Loss: 0.9159, Val Acc: 0.4844, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 191, Loss: 0.9148, Val Acc: 0.4889, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 192, Loss: 0.9161, Val Acc: 0.4978, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 193, Loss: 0.9129, Val Acc: 0.4978, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 194, Loss: 0.9139, Val Acc: 0.4933, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 195, Loss: 0.9115, Val Acc: 0.4978, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 196, Loss: 0.9138, Val Acc: 0.4933, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 197, Loss: 0.9115, Val Acc: 0.4889, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 198, Loss: 0.9086, Val Acc: 0.5067, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 199, Loss: 0.9136, Val Acc: 0.5022, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 200, Loss: 0.9158, Val Acc: 0.5022, Test Acc: 0.4356\n",
                        "Seed: 44, Epoch: 001, Loss: 1.1032, Val Acc: 0.3556, Test Acc: 0.3200\n",
                        "Seed: 44, Epoch: 002, Loss: 1.1021, Val Acc: 0.3556, Test Acc: 0.3200\n",
                        "Seed: 44, Epoch: 003, Loss: 1.1012, Val Acc: 0.3556, Test Acc: 0.3200\n",
                        "Seed: 44, Epoch: 004, Loss: 1.1005, Val Acc: 0.3556, Test Acc: 0.3200\n",
                        "Seed: 44, Epoch: 005, Loss: 1.0999, Val Acc: 0.3556, Test Acc: 0.3200\n",
                        "Seed: 44, Epoch: 006, Loss: 1.0992, Val Acc: 0.3556, Test Acc: 0.3200\n",
                        "Seed: 44, Epoch: 007, Loss: 1.0985, Val Acc: 0.3556, Test Acc: 0.3200\n",
                        "Seed: 44, Epoch: 008, Loss: 1.0981, Val Acc: 0.3556, Test Acc: 0.3200\n",
                        "Seed: 44, Epoch: 009, Loss: 1.0976, Val Acc: 0.3556, Test Acc: 0.3200\n",
                        "Seed: 44, Epoch: 010, Loss: 1.0968, Val Acc: 0.3556, Test Acc: 0.3200\n",
                        "Seed: 44, Epoch: 011, Loss: 1.0963, Val Acc: 0.3556, Test Acc: 0.3289\n",
                        "Seed: 44, Epoch: 012, Loss: 1.0948, Val Acc: 0.3600, Test Acc: 0.3289\n",
                        "Seed: 44, Epoch: 013, Loss: 1.0944, Val Acc: 0.3600, Test Acc: 0.3289\n",
                        "Seed: 44, Epoch: 014, Loss: 1.0936, Val Acc: 0.3600, Test Acc: 0.3289\n",
                        "Seed: 44, Epoch: 015, Loss: 1.0924, Val Acc: 0.3600, Test Acc: 0.3244\n",
                        "Seed: 44, Epoch: 016, Loss: 1.0902, Val Acc: 0.3600, Test Acc: 0.3511\n",
                        "Seed: 44, Epoch: 017, Loss: 1.0894, Val Acc: 0.3600, Test Acc: 0.3511\n",
                        "Seed: 44, Epoch: 018, Loss: 1.0877, Val Acc: 0.3733, Test Acc: 0.3600\n",
                        "Seed: 44, Epoch: 019, Loss: 1.0844, Val Acc: 0.4178, Test Acc: 0.4356\n",
                        "Seed: 44, Epoch: 020, Loss: 1.0817, Val Acc: 0.4578, Test Acc: 0.4489\n",
                        "Seed: 44, Epoch: 021, Loss: 1.0791, Val Acc: 0.4711, Test Acc: 0.4578\n",
                        "Seed: 44, Epoch: 022, Loss: 1.0758, Val Acc: 0.4711, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 023, Loss: 1.0741, Val Acc: 0.4889, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 024, Loss: 1.0710, Val Acc: 0.4978, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 025, Loss: 1.0669, Val Acc: 0.4889, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 026, Loss: 1.0591, Val Acc: 0.4533, Test Acc: 0.4400\n",
                        "Seed: 44, Epoch: 027, Loss: 1.0549, Val Acc: 0.4622, Test Acc: 0.4400\n",
                        "Seed: 44, Epoch: 028, Loss: 1.0464, Val Acc: 0.4622, Test Acc: 0.4311\n",
                        "Seed: 44, Epoch: 029, Loss: 1.0406, Val Acc: 0.4533, Test Acc: 0.4356\n",
                        "Seed: 44, Epoch: 030, Loss: 1.0346, Val Acc: 0.4578, Test Acc: 0.4356\n",
                        "Seed: 44, Epoch: 031, Loss: 1.0314, Val Acc: 0.4711, Test Acc: 0.4400\n",
                        "Seed: 44, Epoch: 032, Loss: 1.0184, Val Acc: 0.4889, Test Acc: 0.4578\n",
                        "Seed: 44, Epoch: 033, Loss: 1.0162, Val Acc: 0.4889, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 034, Loss: 1.0133, Val Acc: 0.4756, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 035, Loss: 1.0102, Val Acc: 0.4578, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 036, Loss: 1.0056, Val Acc: 0.4622, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 037, Loss: 1.0027, Val Acc: 0.4622, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 038, Loss: 0.9946, Val Acc: 0.4756, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 039, Loss: 0.9889, Val Acc: 0.4800, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 040, Loss: 0.9772, Val Acc: 0.4978, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 041, Loss: 0.9804, Val Acc: 0.4933, Test Acc: 0.5467\n",
                        "Seed: 44, Epoch: 042, Loss: 0.9818, Val Acc: 0.4933, Test Acc: 0.5467\n",
                        "Seed: 44, Epoch: 043, Loss: 0.9770, Val Acc: 0.4800, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 044, Loss: 0.9689, Val Acc: 0.4756, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 045, Loss: 0.9694, Val Acc: 0.4711, Test Acc: 0.4444\n",
                        "Seed: 44, Epoch: 046, Loss: 0.9759, Val Acc: 0.4711, Test Acc: 0.4400\n",
                        "Seed: 44, Epoch: 047, Loss: 0.9790, Val Acc: 0.4711, Test Acc: 0.4444\n",
                        "Seed: 44, Epoch: 048, Loss: 0.9794, Val Acc: 0.4844, Test Acc: 0.4489\n",
                        "Seed: 44, Epoch: 049, Loss: 0.9753, Val Acc: 0.4889, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 050, Loss: 0.9702, Val Acc: 0.4978, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 051, Loss: 0.9695, Val Acc: 0.4889, Test Acc: 0.5422\n",
                        "Seed: 44, Epoch: 052, Loss: 0.9734, Val Acc: 0.4844, Test Acc: 0.5422\n",
                        "Seed: 44, Epoch: 053, Loss: 0.9746, Val Acc: 0.4844, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 054, Loss: 0.9766, Val Acc: 0.4889, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 055, Loss: 0.9757, Val Acc: 0.4889, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 056, Loss: 0.9756, Val Acc: 0.4978, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 057, Loss: 0.9812, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 44, Epoch: 058, Loss: 0.9809, Val Acc: 0.4889, Test Acc: 0.4533\n",
                        "Seed: 44, Epoch: 059, Loss: 0.9754, Val Acc: 0.4933, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 060, Loss: 0.9711, Val Acc: 0.5067, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 061, Loss: 0.9723, Val Acc: 0.4844, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 062, Loss: 0.9686, Val Acc: 0.4889, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 063, Loss: 0.9677, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 064, Loss: 0.9677, Val Acc: 0.5022, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 065, Loss: 0.9665, Val Acc: 0.4978, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 066, Loss: 0.9675, Val Acc: 0.4978, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 067, Loss: 0.9712, Val Acc: 0.5022, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 068, Loss: 0.9688, Val Acc: 0.4978, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 069, Loss: 0.9714, Val Acc: 0.5067, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 070, Loss: 0.9648, Val Acc: 0.5111, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 071, Loss: 0.9646, Val Acc: 0.4933, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 072, Loss: 0.9585, Val Acc: 0.4889, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 073, Loss: 0.9615, Val Acc: 0.4889, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 074, Loss: 0.9629, Val Acc: 0.5022, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 075, Loss: 0.9581, Val Acc: 0.4933, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 076, Loss: 0.9595, Val Acc: 0.4978, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 077, Loss: 0.9598, Val Acc: 0.4978, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 078, Loss: 0.9520, Val Acc: 0.4978, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 079, Loss: 0.9553, Val Acc: 0.4978, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 080, Loss: 0.9575, Val Acc: 0.5022, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 081, Loss: 0.9545, Val Acc: 0.4889, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 082, Loss: 0.9557, Val Acc: 0.4889, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 083, Loss: 0.9541, Val Acc: 0.5022, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 084, Loss: 0.9494, Val Acc: 0.5111, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 085, Loss: 0.9496, Val Acc: 0.5111, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 086, Loss: 0.9497, Val Acc: 0.5111, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 087, Loss: 0.9492, Val Acc: 0.5111, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 088, Loss: 0.9440, Val Acc: 0.5156, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 089, Loss: 0.9458, Val Acc: 0.5111, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 090, Loss: 0.9500, Val Acc: 0.5067, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 091, Loss: 0.9448, Val Acc: 0.5067, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 092, Loss: 0.9464, Val Acc: 0.5156, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 093, Loss: 0.9445, Val Acc: 0.5022, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 094, Loss: 0.9481, Val Acc: 0.5022, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 095, Loss: 0.9497, Val Acc: 0.5156, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 096, Loss: 0.9465, Val Acc: 0.5111, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 097, Loss: 0.9464, Val Acc: 0.4844, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 098, Loss: 0.9483, Val Acc: 0.4933, Test Acc: 0.5333\n",
                        "Seed: 44, Epoch: 099, Loss: 0.9469, Val Acc: 0.4800, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 100, Loss: 0.9506, Val Acc: 0.5022, Test Acc: 0.5378\n",
                        "Seed: 44, Epoch: 101, Loss: 0.9477, Val Acc: 0.4711, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 102, Loss: 0.9549, Val Acc: 0.4844, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 103, Loss: 0.9582, Val Acc: 0.4800, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 104, Loss: 0.9560, Val Acc: 0.4844, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 105, Loss: 0.9524, Val Acc: 0.4933, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 106, Loss: 0.9475, Val Acc: 0.4933, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 107, Loss: 0.9468, Val Acc: 0.5067, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 108, Loss: 0.9451, Val Acc: 0.4978, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 109, Loss: 0.9467, Val Acc: 0.4933, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 110, Loss: 0.9464, Val Acc: 0.4933, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 111, Loss: 0.9450, Val Acc: 0.4889, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 112, Loss: 0.9424, Val Acc: 0.4978, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 113, Loss: 0.9468, Val Acc: 0.4933, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 114, Loss: 0.9434, Val Acc: 0.4889, Test Acc: 0.5378\n",
                        "Seed: 44, Epoch: 115, Loss: 0.9444, Val Acc: 0.4800, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 116, Loss: 0.9466, Val Acc: 0.4844, Test Acc: 0.5467\n",
                        "Seed: 44, Epoch: 117, Loss: 0.9434, Val Acc: 0.4844, Test Acc: 0.5422\n",
                        "Seed: 44, Epoch: 118, Loss: 0.9444, Val Acc: 0.4844, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 119, Loss: 0.9431, Val Acc: 0.4933, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 120, Loss: 0.9389, Val Acc: 0.4933, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 121, Loss: 0.9394, Val Acc: 0.4978, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 122, Loss: 0.9428, Val Acc: 0.4978, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 123, Loss: 0.9415, Val Acc: 0.4978, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 124, Loss: 0.9387, Val Acc: 0.5067, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 125, Loss: 0.9360, Val Acc: 0.4889, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 126, Loss: 0.9371, Val Acc: 0.4844, Test Acc: 0.5333\n",
                        "Seed: 44, Epoch: 127, Loss: 0.9317, Val Acc: 0.4844, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 128, Loss: 0.9384, Val Acc: 0.4844, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 129, Loss: 0.9339, Val Acc: 0.4933, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 130, Loss: 0.9295, Val Acc: 0.5022, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 131, Loss: 0.9316, Val Acc: 0.4978, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 132, Loss: 0.9294, Val Acc: 0.4844, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 133, Loss: 0.9266, Val Acc: 0.4933, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 134, Loss: 0.9319, Val Acc: 0.4978, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 135, Loss: 0.9305, Val Acc: 0.5022, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 136, Loss: 0.9280, Val Acc: 0.4978, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 137, Loss: 0.9280, Val Acc: 0.4978, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 138, Loss: 0.9316, Val Acc: 0.4889, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 139, Loss: 0.9257, Val Acc: 0.4889, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 140, Loss: 0.9326, Val Acc: 0.4933, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 141, Loss: 0.9393, Val Acc: 0.4800, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 142, Loss: 0.9469, Val Acc: 0.4889, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 143, Loss: 0.9350, Val Acc: 0.4889, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 144, Loss: 0.9345, Val Acc: 0.4978, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 145, Loss: 0.9340, Val Acc: 0.5156, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 146, Loss: 0.9370, Val Acc: 0.5022, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 147, Loss: 0.9392, Val Acc: 0.5289, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 148, Loss: 0.9358, Val Acc: 0.5156, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 149, Loss: 0.9332, Val Acc: 0.5111, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 150, Loss: 0.9367, Val Acc: 0.5111, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 151, Loss: 0.9353, Val Acc: 0.5067, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 152, Loss: 0.9355, Val Acc: 0.4933, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 153, Loss: 0.9291, Val Acc: 0.4933, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 154, Loss: 0.9295, Val Acc: 0.4978, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 155, Loss: 0.9303, Val Acc: 0.5156, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 156, Loss: 0.9266, Val Acc: 0.5111, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 157, Loss: 0.9285, Val Acc: 0.5067, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 158, Loss: 0.9276, Val Acc: 0.5022, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 159, Loss: 0.9301, Val Acc: 0.4889, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 160, Loss: 0.9295, Val Acc: 0.4889, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 161, Loss: 0.9314, Val Acc: 0.4889, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 162, Loss: 0.9303, Val Acc: 0.4889, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 163, Loss: 0.9297, Val Acc: 0.4889, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 164, Loss: 0.9293, Val Acc: 0.4889, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 165, Loss: 0.9272, Val Acc: 0.5067, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 166, Loss: 0.9255, Val Acc: 0.5200, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 167, Loss: 0.9272, Val Acc: 0.5200, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 168, Loss: 0.9253, Val Acc: 0.5156, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 169, Loss: 0.9266, Val Acc: 0.5111, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 170, Loss: 0.9257, Val Acc: 0.4978, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 171, Loss: 0.9270, Val Acc: 0.4756, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 172, Loss: 0.9282, Val Acc: 0.4800, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 173, Loss: 0.9461, Val Acc: 0.4667, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 174, Loss: 0.9477, Val Acc: 0.4711, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 175, Loss: 0.9366, Val Acc: 0.4933, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 176, Loss: 0.9308, Val Acc: 0.5111, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 177, Loss: 0.9217, Val Acc: 0.5200, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 178, Loss: 0.9258, Val Acc: 0.5067, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 179, Loss: 0.9270, Val Acc: 0.4978, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 180, Loss: 0.9249, Val Acc: 0.4978, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 181, Loss: 0.9218, Val Acc: 0.4889, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 182, Loss: 0.9233, Val Acc: 0.4889, Test Acc: 0.5378\n",
                        "Seed: 44, Epoch: 183, Loss: 0.9323, Val Acc: 0.4800, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 184, Loss: 0.9328, Val Acc: 0.4711, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 185, Loss: 0.9366, Val Acc: 0.4711, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 186, Loss: 0.9307, Val Acc: 0.4667, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 187, Loss: 0.9306, Val Acc: 0.4800, Test Acc: 0.5378\n",
                        "Seed: 44, Epoch: 188, Loss: 0.9220, Val Acc: 0.4844, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 189, Loss: 0.9223, Val Acc: 0.5022, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 190, Loss: 0.9204, Val Acc: 0.5022, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 191, Loss: 0.9213, Val Acc: 0.4978, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 192, Loss: 0.9240, Val Acc: 0.4978, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 193, Loss: 0.9203, Val Acc: 0.4978, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 194, Loss: 0.9188, Val Acc: 0.4844, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 195, Loss: 0.9229, Val Acc: 0.4756, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 196, Loss: 0.9246, Val Acc: 0.4756, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 197, Loss: 0.9235, Val Acc: 0.4933, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 198, Loss: 0.9194, Val Acc: 0.5022, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 199, Loss: 0.9156, Val Acc: 0.5111, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 200, Loss: 0.9172, Val Acc: 0.5156, Test Acc: 0.5156\n",
                        "Average Time: 81.64 seconds\n",
                        "Var Time: 16.09 seconds\n",
                        "Average Memory: 50.00 MB\n",
                        "Average Best Val Acc: 0.5096\n",
                        "Std Best Test Acc: 0.0219\n",
                        "Average Test Acc: 0.4681\n"
                    ]
                }
            ],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nmax_nodes = 500\ndata_path = \"/data1/Pooling\"\ndataset_sparse = TUDataset(root=data_path, name=\"IMDB-MULTI\", transform=T.Compose([T.OneHotDegree(88)]), use_node_attr=True)\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_geometric.nn import BatchNorm\nclass HierarchicalGCN_HGPSL(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_HGPSL, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = HGPSLPool(hidden_channels, ratio=0.7, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = HGPSLPool(hidden_channels, ratio=0.7, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv3 = GCNConv(hidden_channels, out_channels)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 32)\n        self.lin2 = torch.nn.Linear(32, num_classes)\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        edge_attr = None\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool1(x, edge_index, edge_attr, batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool2(x, edge_index, edge_attr, batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\ndef train():\n    model.train()\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        data = data.to(device)\n        out = model(data)\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n    return correct / len(loader.dataset)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 150\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    train_ratio = 0.7\n    val_ratio = 0.15\n    val_ratio = 0.15\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n    model = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 201):\n        loss = train()\n        val_acc = test(valid_loader)\n        test_acc = test(test_loader)\n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\nprint(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### COLLAB"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 1.1104, Val Acc: 0.3213, Test Acc: 0.3320\n",
                        "Seed: 42, Epoch: 002, Loss: 1.1003, Val Acc: 0.3213, Test Acc: 0.3320\n",
                        "Seed: 42, Epoch: 003, Loss: 1.0890, Val Acc: 0.3213, Test Acc: 0.3320\n",
                        "Seed: 42, Epoch: 004, Loss: 1.0746, Val Acc: 0.5880, Test Acc: 0.5800\n",
                        "Seed: 42, Epoch: 005, Loss: 1.0556, Val Acc: 0.6080, Test Acc: 0.6227\n",
                        "Seed: 42, Epoch: 006, Loss: 1.0261, Val Acc: 0.6173, Test Acc: 0.6280\n",
                        "Seed: 42, Epoch: 007, Loss: 0.9842, Val Acc: 0.6133, Test Acc: 0.6280\n",
                        "Seed: 42, Epoch: 008, Loss: 0.9317, Val Acc: 0.6173, Test Acc: 0.6280\n",
                        "Seed: 42, Epoch: 009, Loss: 0.8763, Val Acc: 0.6187, Test Acc: 0.6280\n",
                        "Seed: 42, Epoch: 010, Loss: 0.8179, Val Acc: 0.6320, Test Acc: 0.6533\n",
                        "Seed: 42, Epoch: 011, Loss: 0.7563, Val Acc: 0.6493, Test Acc: 0.6587\n",
                        "Seed: 42, Epoch: 012, Loss: 0.7014, Val Acc: 0.6653, Test Acc: 0.6653\n",
                        "Seed: 42, Epoch: 013, Loss: 0.6582, Val Acc: 0.6707, Test Acc: 0.6693\n",
                        "Seed: 42, Epoch: 014, Loss: 0.6282, Val Acc: 0.6787, Test Acc: 0.6773\n",
                        "Seed: 42, Epoch: 015, Loss: 0.6087, Val Acc: 0.6960, Test Acc: 0.6773\n",
                        "Seed: 42, Epoch: 016, Loss: 0.5905, Val Acc: 0.7000, Test Acc: 0.6867\n",
                        "Seed: 42, Epoch: 017, Loss: 0.5774, Val Acc: 0.7040, Test Acc: 0.6840\n",
                        "Seed: 42, Epoch: 018, Loss: 0.5653, Val Acc: 0.7067, Test Acc: 0.6920\n",
                        "Seed: 42, Epoch: 019, Loss: 0.5539, Val Acc: 0.7027, Test Acc: 0.6853\n",
                        "Seed: 42, Epoch: 020, Loss: 0.5485, Val Acc: 0.7067, Test Acc: 0.6920\n",
                        "Seed: 42, Epoch: 021, Loss: 0.5405, Val Acc: 0.7013, Test Acc: 0.6853\n",
                        "Seed: 42, Epoch: 022, Loss: 0.5349, Val Acc: 0.7093, Test Acc: 0.6920\n",
                        "Seed: 42, Epoch: 023, Loss: 0.5312, Val Acc: 0.7080, Test Acc: 0.6907\n",
                        "Seed: 42, Epoch: 024, Loss: 0.5233, Val Acc: 0.7093, Test Acc: 0.6947\n",
                        "Seed: 42, Epoch: 025, Loss: 0.5183, Val Acc: 0.7093, Test Acc: 0.6933\n",
                        "Seed: 42, Epoch: 026, Loss: 0.5127, Val Acc: 0.7107, Test Acc: 0.6893\n",
                        "Seed: 42, Epoch: 027, Loss: 0.5078, Val Acc: 0.7187, Test Acc: 0.7027\n",
                        "Seed: 42, Epoch: 028, Loss: 0.5046, Val Acc: 0.7173, Test Acc: 0.7173\n",
                        "Seed: 42, Epoch: 029, Loss: 0.4995, Val Acc: 0.7533, Test Acc: 0.7240\n",
                        "Seed: 42, Epoch: 030, Loss: 0.4957, Val Acc: 0.7533, Test Acc: 0.7240\n",
                        "Seed: 42, Epoch: 031, Loss: 0.4919, Val Acc: 0.7600, Test Acc: 0.7187\n",
                        "Seed: 42, Epoch: 032, Loss: 0.4869, Val Acc: 0.7613, Test Acc: 0.7227\n",
                        "Seed: 42, Epoch: 033, Loss: 0.4830, Val Acc: 0.7587, Test Acc: 0.7200\n",
                        "Seed: 42, Epoch: 034, Loss: 0.4827, Val Acc: 0.7653, Test Acc: 0.7200\n",
                        "Seed: 42, Epoch: 035, Loss: 0.4818, Val Acc: 0.7453, Test Acc: 0.7187\n",
                        "Seed: 42, Epoch: 036, Loss: 0.4810, Val Acc: 0.7787, Test Acc: 0.7213\n",
                        "Seed: 42, Epoch: 037, Loss: 0.4771, Val Acc: 0.7680, Test Acc: 0.7267\n",
                        "Seed: 42, Epoch: 038, Loss: 0.4744, Val Acc: 0.7693, Test Acc: 0.7267\n",
                        "Seed: 42, Epoch: 039, Loss: 0.4735, Val Acc: 0.7787, Test Acc: 0.7267\n",
                        "Seed: 42, Epoch: 040, Loss: 0.4694, Val Acc: 0.7640, Test Acc: 0.7253\n",
                        "Seed: 42, Epoch: 041, Loss: 0.4679, Val Acc: 0.7640, Test Acc: 0.7280\n",
                        "Seed: 42, Epoch: 042, Loss: 0.4700, Val Acc: 0.7747, Test Acc: 0.7307\n",
                        "Seed: 42, Epoch: 043, Loss: 0.4681, Val Acc: 0.7600, Test Acc: 0.7213\n",
                        "Seed: 42, Epoch: 044, Loss: 0.4660, Val Acc: 0.7667, Test Acc: 0.7240\n",
                        "Seed: 42, Epoch: 045, Loss: 0.4625, Val Acc: 0.7707, Test Acc: 0.7320\n",
                        "Seed: 42, Epoch: 046, Loss: 0.4608, Val Acc: 0.7640, Test Acc: 0.7307\n",
                        "Seed: 42, Epoch: 047, Loss: 0.4604, Val Acc: 0.7707, Test Acc: 0.7307\n",
                        "Seed: 42, Epoch: 048, Loss: 0.4574, Val Acc: 0.7653, Test Acc: 0.7280\n",
                        "Seed: 42, Epoch: 049, Loss: 0.4567, Val Acc: 0.7680, Test Acc: 0.7320\n",
                        "Seed: 42, Epoch: 050, Loss: 0.4563, Val Acc: 0.7720, Test Acc: 0.7307\n",
                        "Seed: 42, Epoch: 051, Loss: 0.4527, Val Acc: 0.7707, Test Acc: 0.7307\n",
                        "Seed: 42, Epoch: 052, Loss: 0.4520, Val Acc: 0.7747, Test Acc: 0.7360\n",
                        "Seed: 42, Epoch: 053, Loss: 0.4506, Val Acc: 0.7720, Test Acc: 0.7360\n",
                        "Seed: 42, Epoch: 054, Loss: 0.4494, Val Acc: 0.7680, Test Acc: 0.7333\n",
                        "Seed: 42, Epoch: 055, Loss: 0.4484, Val Acc: 0.7733, Test Acc: 0.7400\n",
                        "Seed: 42, Epoch: 056, Loss: 0.4471, Val Acc: 0.7587, Test Acc: 0.7307\n",
                        "Seed: 42, Epoch: 057, Loss: 0.4486, Val Acc: 0.7693, Test Acc: 0.7413\n",
                        "Seed: 42, Epoch: 058, Loss: 0.4481, Val Acc: 0.7773, Test Acc: 0.7400\n",
                        "Seed: 42, Epoch: 059, Loss: 0.4433, Val Acc: 0.7733, Test Acc: 0.7413\n",
                        "Seed: 42, Epoch: 060, Loss: 0.4433, Val Acc: 0.7773, Test Acc: 0.7427\n",
                        "Seed: 42, Epoch: 061, Loss: 0.4430, Val Acc: 0.7720, Test Acc: 0.7413\n",
                        "Seed: 42, Epoch: 062, Loss: 0.4415, Val Acc: 0.7747, Test Acc: 0.7440\n",
                        "Seed: 42, Epoch: 063, Loss: 0.4381, Val Acc: 0.7693, Test Acc: 0.7413\n",
                        "Seed: 42, Epoch: 064, Loss: 0.4395, Val Acc: 0.7760, Test Acc: 0.7440\n",
                        "Seed: 42, Epoch: 065, Loss: 0.4366, Val Acc: 0.7747, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 066, Loss: 0.4351, Val Acc: 0.7747, Test Acc: 0.7413\n",
                        "Seed: 42, Epoch: 067, Loss: 0.4359, Val Acc: 0.7707, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 068, Loss: 0.4351, Val Acc: 0.7707, Test Acc: 0.7480\n",
                        "Seed: 42, Epoch: 069, Loss: 0.4354, Val Acc: 0.7760, Test Acc: 0.7387\n",
                        "Seed: 42, Epoch: 070, Loss: 0.4335, Val Acc: 0.7720, Test Acc: 0.7493\n",
                        "Seed: 42, Epoch: 071, Loss: 0.4308, Val Acc: 0.7747, Test Acc: 0.7413\n",
                        "Seed: 42, Epoch: 072, Loss: 0.4305, Val Acc: 0.7693, Test Acc: 0.7480\n",
                        "Seed: 42, Epoch: 073, Loss: 0.4286, Val Acc: 0.7733, Test Acc: 0.7413\n",
                        "Seed: 42, Epoch: 074, Loss: 0.4273, Val Acc: 0.7733, Test Acc: 0.7400\n",
                        "Seed: 42, Epoch: 075, Loss: 0.4275, Val Acc: 0.7693, Test Acc: 0.7440\n",
                        "Seed: 42, Epoch: 076, Loss: 0.4270, Val Acc: 0.7707, Test Acc: 0.7440\n",
                        "Seed: 42, Epoch: 077, Loss: 0.4256, Val Acc: 0.7693, Test Acc: 0.7440\n",
                        "Seed: 42, Epoch: 078, Loss: 0.4239, Val Acc: 0.7707, Test Acc: 0.7440\n",
                        "Seed: 42, Epoch: 079, Loss: 0.4229, Val Acc: 0.7693, Test Acc: 0.7480\n",
                        "Seed: 42, Epoch: 080, Loss: 0.4224, Val Acc: 0.7707, Test Acc: 0.7507\n",
                        "Seed: 42, Epoch: 081, Loss: 0.4222, Val Acc: 0.7733, Test Acc: 0.7440\n",
                        "Seed: 42, Epoch: 082, Loss: 0.4206, Val Acc: 0.7720, Test Acc: 0.7427\n",
                        "Seed: 42, Epoch: 083, Loss: 0.4219, Val Acc: 0.7733, Test Acc: 0.7507\n",
                        "Seed: 42, Epoch: 084, Loss: 0.4217, Val Acc: 0.7800, Test Acc: 0.7493\n",
                        "Seed: 42, Epoch: 085, Loss: 0.4179, Val Acc: 0.7747, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 086, Loss: 0.4184, Val Acc: 0.7733, Test Acc: 0.7520\n",
                        "Seed: 42, Epoch: 087, Loss: 0.4168, Val Acc: 0.7840, Test Acc: 0.7520\n",
                        "Seed: 42, Epoch: 088, Loss: 0.4143, Val Acc: 0.7693, Test Acc: 0.7453\n",
                        "Seed: 42, Epoch: 089, Loss: 0.4153, Val Acc: 0.7707, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 090, Loss: 0.4145, Val Acc: 0.7720, Test Acc: 0.7493\n",
                        "Seed: 42, Epoch: 091, Loss: 0.4105, Val Acc: 0.7800, Test Acc: 0.7520\n",
                        "Seed: 42, Epoch: 092, Loss: 0.4104, Val Acc: 0.7760, Test Acc: 0.7547\n",
                        "Seed: 42, Epoch: 093, Loss: 0.4090, Val Acc: 0.7773, Test Acc: 0.7547\n",
                        "Seed: 42, Epoch: 094, Loss: 0.4077, Val Acc: 0.7733, Test Acc: 0.7493\n",
                        "Seed: 42, Epoch: 095, Loss: 0.4069, Val Acc: 0.7787, Test Acc: 0.7560\n",
                        "Seed: 42, Epoch: 096, Loss: 0.4060, Val Acc: 0.7800, Test Acc: 0.7480\n",
                        "Seed: 42, Epoch: 097, Loss: 0.4055, Val Acc: 0.7720, Test Acc: 0.7520\n",
                        "Seed: 42, Epoch: 098, Loss: 0.4039, Val Acc: 0.7787, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 099, Loss: 0.4042, Val Acc: 0.7840, Test Acc: 0.7507\n",
                        "Seed: 42, Epoch: 100, Loss: 0.4017, Val Acc: 0.7800, Test Acc: 0.7520\n",
                        "Seed: 42, Epoch: 101, Loss: 0.4013, Val Acc: 0.7827, Test Acc: 0.7480\n",
                        "Seed: 42, Epoch: 102, Loss: 0.4022, Val Acc: 0.7773, Test Acc: 0.7507\n",
                        "Seed: 42, Epoch: 103, Loss: 0.3980, Val Acc: 0.7840, Test Acc: 0.7493\n",
                        "Seed: 42, Epoch: 104, Loss: 0.3979, Val Acc: 0.7787, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 105, Loss: 0.3952, Val Acc: 0.7813, Test Acc: 0.7520\n",
                        "Seed: 42, Epoch: 106, Loss: 0.3935, Val Acc: 0.7867, Test Acc: 0.7480\n",
                        "Seed: 42, Epoch: 107, Loss: 0.3929, Val Acc: 0.7853, Test Acc: 0.7507\n",
                        "Seed: 42, Epoch: 108, Loss: 0.3942, Val Acc: 0.7880, Test Acc: 0.7480\n",
                        "Seed: 42, Epoch: 109, Loss: 0.3908, Val Acc: 0.7853, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 110, Loss: 0.3891, Val Acc: 0.7853, Test Acc: 0.7507\n",
                        "Seed: 42, Epoch: 111, Loss: 0.3884, Val Acc: 0.7880, Test Acc: 0.7440\n",
                        "Seed: 42, Epoch: 112, Loss: 0.3892, Val Acc: 0.7907, Test Acc: 0.7547\n",
                        "Seed: 42, Epoch: 113, Loss: 0.3858, Val Acc: 0.7920, Test Acc: 0.7453\n",
                        "Seed: 42, Epoch: 114, Loss: 0.3833, Val Acc: 0.7893, Test Acc: 0.7493\n",
                        "Seed: 42, Epoch: 115, Loss: 0.3828, Val Acc: 0.7853, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 116, Loss: 0.3846, Val Acc: 0.7880, Test Acc: 0.7493\n",
                        "Seed: 42, Epoch: 117, Loss: 0.3816, Val Acc: 0.7907, Test Acc: 0.7467\n",
                        "Seed: 42, Epoch: 118, Loss: 0.3819, Val Acc: 0.7853, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 119, Loss: 0.3773, Val Acc: 0.7893, Test Acc: 0.7520\n",
                        "Seed: 42, Epoch: 120, Loss: 0.3787, Val Acc: 0.7907, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 121, Loss: 0.3769, Val Acc: 0.7800, Test Acc: 0.7480\n",
                        "Seed: 42, Epoch: 122, Loss: 0.3755, Val Acc: 0.7907, Test Acc: 0.7573\n",
                        "Seed: 42, Epoch: 123, Loss: 0.3730, Val Acc: 0.7800, Test Acc: 0.7560\n",
                        "Seed: 42, Epoch: 124, Loss: 0.3759, Val Acc: 0.7840, Test Acc: 0.7520\n",
                        "Seed: 42, Epoch: 125, Loss: 0.3713, Val Acc: 0.7867, Test Acc: 0.7520\n",
                        "Seed: 42, Epoch: 126, Loss: 0.3685, Val Acc: 0.7867, Test Acc: 0.7547\n",
                        "Seed: 42, Epoch: 127, Loss: 0.3670, Val Acc: 0.7853, Test Acc: 0.7533\n",
                        "Seed: 42, Epoch: 128, Loss: 0.3653, Val Acc: 0.7787, Test Acc: 0.7600\n",
                        "Seed: 42, Epoch: 129, Loss: 0.3658, Val Acc: 0.7787, Test Acc: 0.7613\n",
                        "Seed: 42, Epoch: 130, Loss: 0.3694, Val Acc: 0.7840, Test Acc: 0.7560\n",
                        "Seed: 42, Epoch: 131, Loss: 0.3631, Val Acc: 0.7853, Test Acc: 0.7573\n",
                        "Seed: 42, Epoch: 132, Loss: 0.3660, Val Acc: 0.7813, Test Acc: 0.7573\n",
                        "Seed: 42, Epoch: 133, Loss: 0.3623, Val Acc: 0.7787, Test Acc: 0.7707\n",
                        "Seed: 42, Epoch: 134, Loss: 0.3670, Val Acc: 0.7893, Test Acc: 0.7587\n",
                        "Seed: 42, Epoch: 135, Loss: 0.3659, Val Acc: 0.7827, Test Acc: 0.7613\n",
                        "Seed: 42, Epoch: 136, Loss: 0.3629, Val Acc: 0.7800, Test Acc: 0.7613\n",
                        "Seed: 42, Epoch: 137, Loss: 0.3546, Val Acc: 0.7840, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 138, Loss: 0.3531, Val Acc: 0.7840, Test Acc: 0.7613\n",
                        "Seed: 42, Epoch: 139, Loss: 0.3527, Val Acc: 0.7813, Test Acc: 0.7640\n",
                        "Seed: 42, Epoch: 140, Loss: 0.3511, Val Acc: 0.7867, Test Acc: 0.7680\n",
                        "Seed: 42, Epoch: 141, Loss: 0.3487, Val Acc: 0.7813, Test Acc: 0.7707\n",
                        "Seed: 42, Epoch: 142, Loss: 0.3481, Val Acc: 0.7907, Test Acc: 0.7693\n",
                        "Seed: 42, Epoch: 143, Loss: 0.3474, Val Acc: 0.7840, Test Acc: 0.7693\n",
                        "Seed: 42, Epoch: 144, Loss: 0.3450, Val Acc: 0.7933, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 145, Loss: 0.3432, Val Acc: 0.7920, Test Acc: 0.7747\n",
                        "Seed: 42, Epoch: 146, Loss: 0.3420, Val Acc: 0.7973, Test Acc: 0.7693\n",
                        "Seed: 42, Epoch: 147, Loss: 0.3427, Val Acc: 0.7867, Test Acc: 0.7720\n",
                        "Seed: 42, Epoch: 148, Loss: 0.3409, Val Acc: 0.7947, Test Acc: 0.7720\n",
                        "Seed: 42, Epoch: 149, Loss: 0.3377, Val Acc: 0.7947, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 150, Loss: 0.3378, Val Acc: 0.7907, Test Acc: 0.7720\n",
                        "Seed: 42, Epoch: 151, Loss: 0.3386, Val Acc: 0.7920, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 152, Loss: 0.3387, Val Acc: 0.7987, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 153, Loss: 0.3380, Val Acc: 0.7933, Test Acc: 0.7773\n",
                        "Seed: 42, Epoch: 154, Loss: 0.3374, Val Acc: 0.7920, Test Acc: 0.7787\n",
                        "Seed: 42, Epoch: 155, Loss: 0.3336, Val Acc: 0.7920, Test Acc: 0.7760\n",
                        "Seed: 42, Epoch: 156, Loss: 0.3339, Val Acc: 0.7867, Test Acc: 0.7787\n",
                        "Seed: 42, Epoch: 157, Loss: 0.3319, Val Acc: 0.7907, Test Acc: 0.7720\n",
                        "Seed: 42, Epoch: 158, Loss: 0.3315, Val Acc: 0.7920, Test Acc: 0.7747\n",
                        "Seed: 42, Epoch: 159, Loss: 0.3306, Val Acc: 0.7933, Test Acc: 0.7867\n",
                        "Seed: 42, Epoch: 160, Loss: 0.3271, Val Acc: 0.7987, Test Acc: 0.7787\n",
                        "Seed: 42, Epoch: 161, Loss: 0.3275, Val Acc: 0.8000, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 162, Loss: 0.3246, Val Acc: 0.8000, Test Acc: 0.7760\n",
                        "Seed: 42, Epoch: 163, Loss: 0.3269, Val Acc: 0.7947, Test Acc: 0.7693\n",
                        "Seed: 42, Epoch: 164, Loss: 0.3271, Val Acc: 0.7973, Test Acc: 0.7707\n",
                        "Seed: 42, Epoch: 165, Loss: 0.3227, Val Acc: 0.8000, Test Acc: 0.7773\n",
                        "Seed: 42, Epoch: 166, Loss: 0.3265, Val Acc: 0.7987, Test Acc: 0.7720\n",
                        "Seed: 42, Epoch: 167, Loss: 0.3254, Val Acc: 0.8027, Test Acc: 0.7773\n",
                        "Seed: 42, Epoch: 168, Loss: 0.3214, Val Acc: 0.7960, Test Acc: 0.7893\n",
                        "Seed: 42, Epoch: 169, Loss: 0.3190, Val Acc: 0.7947, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 170, Loss: 0.3188, Val Acc: 0.8013, Test Acc: 0.7853\n",
                        "Seed: 42, Epoch: 171, Loss: 0.3161, Val Acc: 0.7933, Test Acc: 0.7747\n",
                        "Seed: 42, Epoch: 172, Loss: 0.3153, Val Acc: 0.8000, Test Acc: 0.7827\n",
                        "Seed: 42, Epoch: 173, Loss: 0.3145, Val Acc: 0.7960, Test Acc: 0.7773\n",
                        "Seed: 42, Epoch: 174, Loss: 0.3160, Val Acc: 0.7960, Test Acc: 0.7907\n",
                        "Seed: 42, Epoch: 175, Loss: 0.3155, Val Acc: 0.7973, Test Acc: 0.7760\n",
                        "Seed: 42, Epoch: 176, Loss: 0.3177, Val Acc: 0.8040, Test Acc: 0.7880\n",
                        "Seed: 42, Epoch: 177, Loss: 0.3114, Val Acc: 0.7973, Test Acc: 0.7853\n",
                        "Seed: 42, Epoch: 178, Loss: 0.3112, Val Acc: 0.8027, Test Acc: 0.7813\n",
                        "Seed: 42, Epoch: 179, Loss: 0.3096, Val Acc: 0.8080, Test Acc: 0.7813\n",
                        "Seed: 42, Epoch: 180, Loss: 0.3087, Val Acc: 0.7987, Test Acc: 0.7667\n",
                        "Seed: 42, Epoch: 181, Loss: 0.3075, Val Acc: 0.8000, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 182, Loss: 0.3052, Val Acc: 0.8013, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 183, Loss: 0.3067, Val Acc: 0.7880, Test Acc: 0.7920\n",
                        "Seed: 42, Epoch: 184, Loss: 0.3098, Val Acc: 0.7987, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 185, Loss: 0.3059, Val Acc: 0.8000, Test Acc: 0.7827\n",
                        "Seed: 42, Epoch: 186, Loss: 0.3051, Val Acc: 0.8013, Test Acc: 0.7733\n",
                        "Seed: 42, Epoch: 187, Loss: 0.3037, Val Acc: 0.8000, Test Acc: 0.7813\n",
                        "Seed: 42, Epoch: 188, Loss: 0.3042, Val Acc: 0.8027, Test Acc: 0.7800\n",
                        "Seed: 42, Epoch: 189, Loss: 0.3015, Val Acc: 0.8053, Test Acc: 0.7773\n",
                        "Seed: 42, Epoch: 190, Loss: 0.3011, Val Acc: 0.8000, Test Acc: 0.7920\n",
                        "Seed: 42, Epoch: 191, Loss: 0.2978, Val Acc: 0.7987, Test Acc: 0.8000\n",
                        "Seed: 42, Epoch: 192, Loss: 0.2983, Val Acc: 0.8027, Test Acc: 0.7907\n",
                        "Seed: 42, Epoch: 193, Loss: 0.2990, Val Acc: 0.8040, Test Acc: 0.7827\n",
                        "Seed: 42, Epoch: 194, Loss: 0.2944, Val Acc: 0.8053, Test Acc: 0.7920\n",
                        "Seed: 42, Epoch: 195, Loss: 0.2955, Val Acc: 0.8013, Test Acc: 0.7907\n",
                        "Seed: 42, Epoch: 196, Loss: 0.2947, Val Acc: 0.7960, Test Acc: 0.7960\n",
                        "Seed: 42, Epoch: 197, Loss: 0.2914, Val Acc: 0.8080, Test Acc: 0.7933\n",
                        "Seed: 42, Epoch: 198, Loss: 0.2915, Val Acc: 0.7933, Test Acc: 0.8013\n",
                        "Seed: 42, Epoch: 199, Loss: 0.2920, Val Acc: 0.7960, Test Acc: 0.7907\n",
                        "Seed: 42, Epoch: 200, Loss: 0.2945, Val Acc: 0.7987, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 001, Loss: 1.0718, Val Acc: 0.3440, Test Acc: 0.3307\n",
                        "Seed: 43, Epoch: 002, Loss: 1.0631, Val Acc: 0.3440, Test Acc: 0.3307\n",
                        "Seed: 43, Epoch: 003, Loss: 1.0531, Val Acc: 0.6600, Test Acc: 0.6533\n",
                        "Seed: 43, Epoch: 004, Loss: 1.0411, Val Acc: 0.6320, Test Acc: 0.6240\n",
                        "Seed: 43, Epoch: 005, Loss: 1.0231, Val Acc: 0.6467, Test Acc: 0.6307\n",
                        "Seed: 43, Epoch: 006, Loss: 0.9939, Val Acc: 0.6680, Test Acc: 0.6573\n",
                        "Seed: 43, Epoch: 007, Loss: 0.9474, Val Acc: 0.6853, Test Acc: 0.6760\n",
                        "Seed: 43, Epoch: 008, Loss: 0.8833, Val Acc: 0.6973, Test Acc: 0.6840\n",
                        "Seed: 43, Epoch: 009, Loss: 0.8192, Val Acc: 0.6973, Test Acc: 0.6867\n",
                        "Seed: 43, Epoch: 010, Loss: 0.7555, Val Acc: 0.7067, Test Acc: 0.7080\n",
                        "Seed: 43, Epoch: 011, Loss: 0.7079, Val Acc: 0.7067, Test Acc: 0.7147\n",
                        "Seed: 43, Epoch: 012, Loss: 0.6767, Val Acc: 0.7067, Test Acc: 0.7147\n",
                        "Seed: 43, Epoch: 013, Loss: 0.6562, Val Acc: 0.7080, Test Acc: 0.7187\n",
                        "Seed: 43, Epoch: 014, Loss: 0.6386, Val Acc: 0.7200, Test Acc: 0.7227\n",
                        "Seed: 43, Epoch: 015, Loss: 0.6240, Val Acc: 0.7173, Test Acc: 0.7307\n",
                        "Seed: 43, Epoch: 016, Loss: 0.6086, Val Acc: 0.7173, Test Acc: 0.7227\n",
                        "Seed: 43, Epoch: 017, Loss: 0.5962, Val Acc: 0.7200, Test Acc: 0.7240\n",
                        "Seed: 43, Epoch: 018, Loss: 0.5842, Val Acc: 0.7200, Test Acc: 0.7227\n",
                        "Seed: 43, Epoch: 019, Loss: 0.5747, Val Acc: 0.7173, Test Acc: 0.7240\n",
                        "Seed: 43, Epoch: 020, Loss: 0.5639, Val Acc: 0.7187, Test Acc: 0.7227\n",
                        "Seed: 43, Epoch: 021, Loss: 0.5545, Val Acc: 0.7187, Test Acc: 0.7187\n",
                        "Seed: 43, Epoch: 022, Loss: 0.5477, Val Acc: 0.7200, Test Acc: 0.7280\n",
                        "Seed: 43, Epoch: 023, Loss: 0.5392, Val Acc: 0.7200, Test Acc: 0.7293\n",
                        "Seed: 43, Epoch: 024, Loss: 0.5307, Val Acc: 0.7227, Test Acc: 0.7253\n",
                        "Seed: 43, Epoch: 025, Loss: 0.5251, Val Acc: 0.7213, Test Acc: 0.7307\n",
                        "Seed: 43, Epoch: 026, Loss: 0.5157, Val Acc: 0.7213, Test Acc: 0.7253\n",
                        "Seed: 43, Epoch: 027, Loss: 0.5113, Val Acc: 0.7253, Test Acc: 0.7213\n",
                        "Seed: 43, Epoch: 028, Loss: 0.5041, Val Acc: 0.7520, Test Acc: 0.7427\n",
                        "Seed: 43, Epoch: 029, Loss: 0.5004, Val Acc: 0.7627, Test Acc: 0.7520\n",
                        "Seed: 43, Epoch: 030, Loss: 0.4975, Val Acc: 0.7627, Test Acc: 0.7520\n",
                        "Seed: 43, Epoch: 031, Loss: 0.4919, Val Acc: 0.7693, Test Acc: 0.7440\n",
                        "Seed: 43, Epoch: 032, Loss: 0.4921, Val Acc: 0.7747, Test Acc: 0.7480\n",
                        "Seed: 43, Epoch: 033, Loss: 0.4862, Val Acc: 0.7653, Test Acc: 0.7587\n",
                        "Seed: 43, Epoch: 034, Loss: 0.4855, Val Acc: 0.7720, Test Acc: 0.7493\n",
                        "Seed: 43, Epoch: 035, Loss: 0.4817, Val Acc: 0.7680, Test Acc: 0.7627\n",
                        "Seed: 43, Epoch: 036, Loss: 0.4832, Val Acc: 0.7787, Test Acc: 0.7587\n",
                        "Seed: 43, Epoch: 037, Loss: 0.4799, Val Acc: 0.7800, Test Acc: 0.7560\n",
                        "Seed: 43, Epoch: 038, Loss: 0.4770, Val Acc: 0.7787, Test Acc: 0.7533\n",
                        "Seed: 43, Epoch: 039, Loss: 0.4733, Val Acc: 0.7800, Test Acc: 0.7520\n",
                        "Seed: 43, Epoch: 040, Loss: 0.4721, Val Acc: 0.7853, Test Acc: 0.7627\n",
                        "Seed: 43, Epoch: 041, Loss: 0.4708, Val Acc: 0.7827, Test Acc: 0.7640\n",
                        "Seed: 43, Epoch: 042, Loss: 0.4683, Val Acc: 0.7853, Test Acc: 0.7587\n",
                        "Seed: 43, Epoch: 043, Loss: 0.4689, Val Acc: 0.7813, Test Acc: 0.7613\n",
                        "Seed: 43, Epoch: 044, Loss: 0.4691, Val Acc: 0.7907, Test Acc: 0.7680\n",
                        "Seed: 43, Epoch: 045, Loss: 0.4648, Val Acc: 0.7853, Test Acc: 0.7613\n",
                        "Seed: 43, Epoch: 046, Loss: 0.4626, Val Acc: 0.7880, Test Acc: 0.7613\n",
                        "Seed: 43, Epoch: 047, Loss: 0.4616, Val Acc: 0.7813, Test Acc: 0.7573\n",
                        "Seed: 43, Epoch: 048, Loss: 0.4625, Val Acc: 0.7827, Test Acc: 0.7613\n",
                        "Seed: 43, Epoch: 049, Loss: 0.4604, Val Acc: 0.7893, Test Acc: 0.7627\n",
                        "Seed: 43, Epoch: 050, Loss: 0.4579, Val Acc: 0.7893, Test Acc: 0.7627\n",
                        "Seed: 43, Epoch: 051, Loss: 0.4576, Val Acc: 0.7907, Test Acc: 0.7613\n",
                        "Seed: 43, Epoch: 052, Loss: 0.4559, Val Acc: 0.7867, Test Acc: 0.7613\n",
                        "Seed: 43, Epoch: 053, Loss: 0.4570, Val Acc: 0.7880, Test Acc: 0.7667\n",
                        "Seed: 43, Epoch: 054, Loss: 0.4532, Val Acc: 0.7880, Test Acc: 0.7627\n",
                        "Seed: 43, Epoch: 055, Loss: 0.4526, Val Acc: 0.7867, Test Acc: 0.7667\n",
                        "Seed: 43, Epoch: 056, Loss: 0.4531, Val Acc: 0.7867, Test Acc: 0.7587\n",
                        "Seed: 43, Epoch: 057, Loss: 0.4550, Val Acc: 0.7867, Test Acc: 0.7653\n",
                        "Seed: 43, Epoch: 058, Loss: 0.4502, Val Acc: 0.7880, Test Acc: 0.7613\n",
                        "Seed: 43, Epoch: 059, Loss: 0.4482, Val Acc: 0.7893, Test Acc: 0.7627\n",
                        "Seed: 43, Epoch: 060, Loss: 0.4458, Val Acc: 0.7840, Test Acc: 0.7653\n",
                        "Seed: 43, Epoch: 061, Loss: 0.4454, Val Acc: 0.7880, Test Acc: 0.7627\n",
                        "Seed: 43, Epoch: 062, Loss: 0.4449, Val Acc: 0.7867, Test Acc: 0.7693\n",
                        "Seed: 43, Epoch: 063, Loss: 0.4441, Val Acc: 0.7920, Test Acc: 0.7653\n",
                        "Seed: 43, Epoch: 064, Loss: 0.4411, Val Acc: 0.7867, Test Acc: 0.7707\n",
                        "Seed: 43, Epoch: 065, Loss: 0.4425, Val Acc: 0.7907, Test Acc: 0.7667\n",
                        "Seed: 43, Epoch: 066, Loss: 0.4405, Val Acc: 0.7893, Test Acc: 0.7653\n",
                        "Seed: 43, Epoch: 067, Loss: 0.4404, Val Acc: 0.7880, Test Acc: 0.7733\n",
                        "Seed: 43, Epoch: 068, Loss: 0.4394, Val Acc: 0.7853, Test Acc: 0.7707\n",
                        "Seed: 43, Epoch: 069, Loss: 0.4389, Val Acc: 0.7893, Test Acc: 0.7653\n",
                        "Seed: 43, Epoch: 070, Loss: 0.4375, Val Acc: 0.7813, Test Acc: 0.7720\n",
                        "Seed: 43, Epoch: 071, Loss: 0.4370, Val Acc: 0.7853, Test Acc: 0.7640\n",
                        "Seed: 43, Epoch: 072, Loss: 0.4351, Val Acc: 0.7920, Test Acc: 0.7773\n",
                        "Seed: 43, Epoch: 073, Loss: 0.4317, Val Acc: 0.7893, Test Acc: 0.7747\n",
                        "Seed: 43, Epoch: 074, Loss: 0.4319, Val Acc: 0.7840, Test Acc: 0.7680\n",
                        "Seed: 43, Epoch: 075, Loss: 0.4290, Val Acc: 0.7907, Test Acc: 0.7720\n",
                        "Seed: 43, Epoch: 076, Loss: 0.4325, Val Acc: 0.7920, Test Acc: 0.7733\n",
                        "Seed: 43, Epoch: 077, Loss: 0.4268, Val Acc: 0.7853, Test Acc: 0.7707\n",
                        "Seed: 43, Epoch: 078, Loss: 0.4282, Val Acc: 0.7867, Test Acc: 0.7680\n",
                        "Seed: 43, Epoch: 079, Loss: 0.4239, Val Acc: 0.7920, Test Acc: 0.7747\n",
                        "Seed: 43, Epoch: 080, Loss: 0.4242, Val Acc: 0.7840, Test Acc: 0.7680\n",
                        "Seed: 43, Epoch: 081, Loss: 0.4233, Val Acc: 0.7920, Test Acc: 0.7707\n",
                        "Seed: 43, Epoch: 082, Loss: 0.4229, Val Acc: 0.7880, Test Acc: 0.7693\n",
                        "Seed: 43, Epoch: 083, Loss: 0.4223, Val Acc: 0.7907, Test Acc: 0.7787\n",
                        "Seed: 43, Epoch: 084, Loss: 0.4211, Val Acc: 0.7933, Test Acc: 0.7693\n",
                        "Seed: 43, Epoch: 085, Loss: 0.4193, Val Acc: 0.7907, Test Acc: 0.7720\n",
                        "Seed: 43, Epoch: 086, Loss: 0.4172, Val Acc: 0.7973, Test Acc: 0.7800\n",
                        "Seed: 43, Epoch: 087, Loss: 0.4155, Val Acc: 0.7947, Test Acc: 0.7733\n",
                        "Seed: 43, Epoch: 088, Loss: 0.4127, Val Acc: 0.7920, Test Acc: 0.7800\n",
                        "Seed: 43, Epoch: 089, Loss: 0.4147, Val Acc: 0.7907, Test Acc: 0.7787\n",
                        "Seed: 43, Epoch: 090, Loss: 0.4108, Val Acc: 0.7947, Test Acc: 0.7773\n",
                        "Seed: 43, Epoch: 091, Loss: 0.4100, Val Acc: 0.7920, Test Acc: 0.7773\n",
                        "Seed: 43, Epoch: 092, Loss: 0.4099, Val Acc: 0.7907, Test Acc: 0.7800\n",
                        "Seed: 43, Epoch: 093, Loss: 0.4076, Val Acc: 0.7920, Test Acc: 0.7813\n",
                        "Seed: 43, Epoch: 094, Loss: 0.4080, Val Acc: 0.7920, Test Acc: 0.7827\n",
                        "Seed: 43, Epoch: 095, Loss: 0.4059, Val Acc: 0.7973, Test Acc: 0.7827\n",
                        "Seed: 43, Epoch: 096, Loss: 0.4054, Val Acc: 0.7960, Test Acc: 0.7800\n",
                        "Seed: 43, Epoch: 097, Loss: 0.4022, Val Acc: 0.7907, Test Acc: 0.7827\n",
                        "Seed: 43, Epoch: 098, Loss: 0.4030, Val Acc: 0.7947, Test Acc: 0.7800\n",
                        "Seed: 43, Epoch: 099, Loss: 0.4046, Val Acc: 0.7867, Test Acc: 0.7800\n",
                        "Seed: 43, Epoch: 100, Loss: 0.4036, Val Acc: 0.7947, Test Acc: 0.7867\n",
                        "Seed: 43, Epoch: 101, Loss: 0.3990, Val Acc: 0.7920, Test Acc: 0.7827\n",
                        "Seed: 43, Epoch: 102, Loss: 0.4010, Val Acc: 0.7933, Test Acc: 0.7840\n",
                        "Seed: 43, Epoch: 103, Loss: 0.3963, Val Acc: 0.7987, Test Acc: 0.7853\n",
                        "Seed: 43, Epoch: 104, Loss: 0.3935, Val Acc: 0.7920, Test Acc: 0.7800\n",
                        "Seed: 43, Epoch: 105, Loss: 0.3956, Val Acc: 0.7973, Test Acc: 0.7813\n",
                        "Seed: 43, Epoch: 106, Loss: 0.3924, Val Acc: 0.7893, Test Acc: 0.7893\n",
                        "Seed: 43, Epoch: 107, Loss: 0.3911, Val Acc: 0.8013, Test Acc: 0.7853\n",
                        "Seed: 43, Epoch: 108, Loss: 0.3912, Val Acc: 0.7920, Test Acc: 0.7893\n",
                        "Seed: 43, Epoch: 109, Loss: 0.3887, Val Acc: 0.8013, Test Acc: 0.7880\n",
                        "Seed: 43, Epoch: 110, Loss: 0.3890, Val Acc: 0.7933, Test Acc: 0.7880\n",
                        "Seed: 43, Epoch: 111, Loss: 0.3904, Val Acc: 0.7987, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 112, Loss: 0.3880, Val Acc: 0.7960, Test Acc: 0.7733\n",
                        "Seed: 43, Epoch: 113, Loss: 0.3911, Val Acc: 0.7973, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 114, Loss: 0.3848, Val Acc: 0.8027, Test Acc: 0.7920\n",
                        "Seed: 43, Epoch: 115, Loss: 0.3836, Val Acc: 0.8000, Test Acc: 0.7867\n",
                        "Seed: 43, Epoch: 116, Loss: 0.3822, Val Acc: 0.7973, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 117, Loss: 0.3831, Val Acc: 0.7960, Test Acc: 0.7827\n",
                        "Seed: 43, Epoch: 118, Loss: 0.3818, Val Acc: 0.8067, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 119, Loss: 0.3803, Val Acc: 0.8027, Test Acc: 0.7893\n",
                        "Seed: 43, Epoch: 120, Loss: 0.3769, Val Acc: 0.7973, Test Acc: 0.7880\n",
                        "Seed: 43, Epoch: 121, Loss: 0.3798, Val Acc: 0.7987, Test Acc: 0.7853\n",
                        "Seed: 43, Epoch: 122, Loss: 0.3766, Val Acc: 0.8053, Test Acc: 0.7880\n",
                        "Seed: 43, Epoch: 123, Loss: 0.3760, Val Acc: 0.8013, Test Acc: 0.7920\n",
                        "Seed: 43, Epoch: 124, Loss: 0.3741, Val Acc: 0.8027, Test Acc: 0.7880\n",
                        "Seed: 43, Epoch: 125, Loss: 0.3720, Val Acc: 0.8000, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 126, Loss: 0.3721, Val Acc: 0.7973, Test Acc: 0.7840\n",
                        "Seed: 43, Epoch: 127, Loss: 0.3720, Val Acc: 0.8027, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 128, Loss: 0.3699, Val Acc: 0.8040, Test Acc: 0.7893\n",
                        "Seed: 43, Epoch: 129, Loss: 0.3682, Val Acc: 0.7987, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 130, Loss: 0.3686, Val Acc: 0.8040, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 131, Loss: 0.3670, Val Acc: 0.8053, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 132, Loss: 0.3659, Val Acc: 0.7987, Test Acc: 0.7880\n",
                        "Seed: 43, Epoch: 133, Loss: 0.3659, Val Acc: 0.8013, Test Acc: 0.7880\n",
                        "Seed: 43, Epoch: 134, Loss: 0.3642, Val Acc: 0.8053, Test Acc: 0.7880\n",
                        "Seed: 43, Epoch: 135, Loss: 0.3639, Val Acc: 0.8067, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 136, Loss: 0.3650, Val Acc: 0.8000, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 137, Loss: 0.3643, Val Acc: 0.8000, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 138, Loss: 0.3643, Val Acc: 0.8040, Test Acc: 0.7800\n",
                        "Seed: 43, Epoch: 139, Loss: 0.3664, Val Acc: 0.8027, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 140, Loss: 0.3632, Val Acc: 0.8013, Test Acc: 0.7973\n",
                        "Seed: 43, Epoch: 141, Loss: 0.3631, Val Acc: 0.8067, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 142, Loss: 0.3575, Val Acc: 0.8107, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 143, Loss: 0.3564, Val Acc: 0.8040, Test Acc: 0.7880\n",
                        "Seed: 43, Epoch: 144, Loss: 0.3557, Val Acc: 0.8093, Test Acc: 0.7880\n",
                        "Seed: 43, Epoch: 145, Loss: 0.3544, Val Acc: 0.8080, Test Acc: 0.7987\n",
                        "Seed: 43, Epoch: 146, Loss: 0.3530, Val Acc: 0.8120, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 147, Loss: 0.3546, Val Acc: 0.8053, Test Acc: 0.7840\n",
                        "Seed: 43, Epoch: 148, Loss: 0.3539, Val Acc: 0.8093, Test Acc: 0.7813\n",
                        "Seed: 43, Epoch: 149, Loss: 0.3534, Val Acc: 0.8067, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 150, Loss: 0.3498, Val Acc: 0.8093, Test Acc: 0.7960\n",
                        "Seed: 43, Epoch: 151, Loss: 0.3478, Val Acc: 0.8053, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 152, Loss: 0.3471, Val Acc: 0.8120, Test Acc: 0.7920\n",
                        "Seed: 43, Epoch: 153, Loss: 0.3458, Val Acc: 0.8133, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 154, Loss: 0.3482, Val Acc: 0.8080, Test Acc: 0.7960\n",
                        "Seed: 43, Epoch: 155, Loss: 0.3462, Val Acc: 0.8107, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 156, Loss: 0.3431, Val Acc: 0.8120, Test Acc: 0.7960\n",
                        "Seed: 43, Epoch: 157, Loss: 0.3414, Val Acc: 0.8147, Test Acc: 0.7960\n",
                        "Seed: 43, Epoch: 158, Loss: 0.3435, Val Acc: 0.8093, Test Acc: 0.7973\n",
                        "Seed: 43, Epoch: 159, Loss: 0.3412, Val Acc: 0.8120, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 160, Loss: 0.3388, Val Acc: 0.8120, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 161, Loss: 0.3416, Val Acc: 0.8120, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 162, Loss: 0.3387, Val Acc: 0.8133, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 163, Loss: 0.3383, Val Acc: 0.8173, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 164, Loss: 0.3369, Val Acc: 0.8147, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 165, Loss: 0.3358, Val Acc: 0.8093, Test Acc: 0.7920\n",
                        "Seed: 43, Epoch: 166, Loss: 0.3360, Val Acc: 0.8147, Test Acc: 0.7893\n",
                        "Seed: 43, Epoch: 167, Loss: 0.3351, Val Acc: 0.8147, Test Acc: 0.7973\n",
                        "Seed: 43, Epoch: 168, Loss: 0.3321, Val Acc: 0.8080, Test Acc: 0.7867\n",
                        "Seed: 43, Epoch: 169, Loss: 0.3319, Val Acc: 0.8107, Test Acc: 0.7867\n",
                        "Seed: 43, Epoch: 170, Loss: 0.3345, Val Acc: 0.8173, Test Acc: 0.7867\n",
                        "Seed: 43, Epoch: 171, Loss: 0.3293, Val Acc: 0.8173, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 172, Loss: 0.3290, Val Acc: 0.8173, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 173, Loss: 0.3268, Val Acc: 0.8173, Test Acc: 0.7960\n",
                        "Seed: 43, Epoch: 174, Loss: 0.3274, Val Acc: 0.8187, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 175, Loss: 0.3274, Val Acc: 0.8120, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 176, Loss: 0.3272, Val Acc: 0.8213, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 177, Loss: 0.3236, Val Acc: 0.8213, Test Acc: 0.7960\n",
                        "Seed: 43, Epoch: 178, Loss: 0.3288, Val Acc: 0.8187, Test Acc: 0.7920\n",
                        "Seed: 43, Epoch: 179, Loss: 0.3258, Val Acc: 0.8173, Test Acc: 0.7893\n",
                        "Seed: 43, Epoch: 180, Loss: 0.3253, Val Acc: 0.8173, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 181, Loss: 0.3258, Val Acc: 0.8133, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 182, Loss: 0.3217, Val Acc: 0.8133, Test Acc: 0.8040\n",
                        "Seed: 43, Epoch: 183, Loss: 0.3225, Val Acc: 0.8160, Test Acc: 0.8000\n",
                        "Seed: 43, Epoch: 184, Loss: 0.3190, Val Acc: 0.8213, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 185, Loss: 0.3195, Val Acc: 0.8147, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 186, Loss: 0.3166, Val Acc: 0.8187, Test Acc: 0.7947\n",
                        "Seed: 43, Epoch: 187, Loss: 0.3171, Val Acc: 0.8227, Test Acc: 0.7960\n",
                        "Seed: 43, Epoch: 188, Loss: 0.3160, Val Acc: 0.8213, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 189, Loss: 0.3179, Val Acc: 0.8053, Test Acc: 0.7813\n",
                        "Seed: 43, Epoch: 190, Loss: 0.3156, Val Acc: 0.8147, Test Acc: 0.7840\n",
                        "Seed: 43, Epoch: 191, Loss: 0.3190, Val Acc: 0.8133, Test Acc: 0.7840\n",
                        "Seed: 43, Epoch: 192, Loss: 0.3113, Val Acc: 0.8187, Test Acc: 0.7920\n",
                        "Seed: 43, Epoch: 193, Loss: 0.3106, Val Acc: 0.8133, Test Acc: 0.8000\n",
                        "Seed: 43, Epoch: 194, Loss: 0.3102, Val Acc: 0.8187, Test Acc: 0.7987\n",
                        "Seed: 43, Epoch: 195, Loss: 0.3074, Val Acc: 0.8160, Test Acc: 0.8027\n",
                        "Seed: 43, Epoch: 196, Loss: 0.3101, Val Acc: 0.8173, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 197, Loss: 0.3096, Val Acc: 0.8213, Test Acc: 0.8000\n",
                        "Seed: 43, Epoch: 198, Loss: 0.3093, Val Acc: 0.8120, Test Acc: 0.7907\n",
                        "Seed: 43, Epoch: 199, Loss: 0.3115, Val Acc: 0.8173, Test Acc: 0.7933\n",
                        "Seed: 43, Epoch: 200, Loss: 0.3061, Val Acc: 0.8120, Test Acc: 0.7840\n",
                        "Seed: 44, Epoch: 001, Loss: 1.1136, Val Acc: 0.3587, Test Acc: 0.3320\n",
                        "Seed: 44, Epoch: 002, Loss: 1.1023, Val Acc: 0.3587, Test Acc: 0.3320\n",
                        "Seed: 44, Epoch: 003, Loss: 1.0904, Val Acc: 0.3587, Test Acc: 0.3320\n",
                        "Seed: 44, Epoch: 004, Loss: 1.0763, Val Acc: 0.3600, Test Acc: 0.3320\n",
                        "Seed: 44, Epoch: 005, Loss: 1.0592, Val Acc: 0.3867, Test Acc: 0.3507\n",
                        "Seed: 44, Epoch: 006, Loss: 1.0372, Val Acc: 0.4933, Test Acc: 0.5213\n",
                        "Seed: 44, Epoch: 007, Loss: 1.0059, Val Acc: 0.4947, Test Acc: 0.5080\n",
                        "Seed: 44, Epoch: 008, Loss: 0.9695, Val Acc: 0.4947, Test Acc: 0.5080\n",
                        "Seed: 44, Epoch: 009, Loss: 0.9356, Val Acc: 0.5173, Test Acc: 0.5267\n",
                        "Seed: 44, Epoch: 010, Loss: 0.9006, Val Acc: 0.6160, Test Acc: 0.6307\n",
                        "Seed: 44, Epoch: 011, Loss: 0.8542, Val Acc: 0.6453, Test Acc: 0.6560\n",
                        "Seed: 44, Epoch: 012, Loss: 0.8036, Val Acc: 0.6600, Test Acc: 0.6653\n",
                        "Seed: 44, Epoch: 013, Loss: 0.7472, Val Acc: 0.6667, Test Acc: 0.6707\n",
                        "Seed: 44, Epoch: 014, Loss: 0.7039, Val Acc: 0.7000, Test Acc: 0.6853\n",
                        "Seed: 44, Epoch: 015, Loss: 0.6683, Val Acc: 0.7187, Test Acc: 0.6920\n",
                        "Seed: 44, Epoch: 016, Loss: 0.6440, Val Acc: 0.7227, Test Acc: 0.7120\n",
                        "Seed: 44, Epoch: 017, Loss: 0.6255, Val Acc: 0.7227, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 018, Loss: 0.6106, Val Acc: 0.7187, Test Acc: 0.7067\n",
                        "Seed: 44, Epoch: 019, Loss: 0.5965, Val Acc: 0.7240, Test Acc: 0.7027\n",
                        "Seed: 44, Epoch: 020, Loss: 0.5882, Val Acc: 0.7173, Test Acc: 0.7013\n",
                        "Seed: 44, Epoch: 021, Loss: 0.5787, Val Acc: 0.7173, Test Acc: 0.7027\n",
                        "Seed: 44, Epoch: 022, Loss: 0.5711, Val Acc: 0.7187, Test Acc: 0.6947\n",
                        "Seed: 44, Epoch: 023, Loss: 0.5610, Val Acc: 0.7147, Test Acc: 0.6960\n",
                        "Seed: 44, Epoch: 024, Loss: 0.5533, Val Acc: 0.7147, Test Acc: 0.6960\n",
                        "Seed: 44, Epoch: 025, Loss: 0.5474, Val Acc: 0.7147, Test Acc: 0.6960\n",
                        "Seed: 44, Epoch: 026, Loss: 0.5408, Val Acc: 0.7133, Test Acc: 0.6960\n",
                        "Seed: 44, Epoch: 027, Loss: 0.5326, Val Acc: 0.7213, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 028, Loss: 0.5260, Val Acc: 0.7160, Test Acc: 0.6973\n",
                        "Seed: 44, Epoch: 029, Loss: 0.5208, Val Acc: 0.7240, Test Acc: 0.7000\n",
                        "Seed: 44, Epoch: 030, Loss: 0.5137, Val Acc: 0.7400, Test Acc: 0.7200\n",
                        "Seed: 44, Epoch: 031, Loss: 0.5096, Val Acc: 0.7440, Test Acc: 0.7320\n",
                        "Seed: 44, Epoch: 032, Loss: 0.5064, Val Acc: 0.7427, Test Acc: 0.7400\n",
                        "Seed: 44, Epoch: 033, Loss: 0.4984, Val Acc: 0.7400, Test Acc: 0.7333\n",
                        "Seed: 44, Epoch: 034, Loss: 0.4969, Val Acc: 0.7453, Test Acc: 0.7413\n",
                        "Seed: 44, Epoch: 035, Loss: 0.4930, Val Acc: 0.7373, Test Acc: 0.7373\n",
                        "Seed: 44, Epoch: 036, Loss: 0.4917, Val Acc: 0.7427, Test Acc: 0.7440\n",
                        "Seed: 44, Epoch: 037, Loss: 0.4894, Val Acc: 0.7427, Test Acc: 0.7440\n",
                        "Seed: 44, Epoch: 038, Loss: 0.4878, Val Acc: 0.7453, Test Acc: 0.7480\n",
                        "Seed: 44, Epoch: 039, Loss: 0.4851, Val Acc: 0.7467, Test Acc: 0.7533\n",
                        "Seed: 44, Epoch: 040, Loss: 0.4825, Val Acc: 0.7467, Test Acc: 0.7520\n",
                        "Seed: 44, Epoch: 041, Loss: 0.4818, Val Acc: 0.7440, Test Acc: 0.7493\n",
                        "Seed: 44, Epoch: 042, Loss: 0.4809, Val Acc: 0.7507, Test Acc: 0.7547\n",
                        "Seed: 44, Epoch: 043, Loss: 0.4784, Val Acc: 0.7493, Test Acc: 0.7507\n",
                        "Seed: 44, Epoch: 044, Loss: 0.4758, Val Acc: 0.7507, Test Acc: 0.7533\n",
                        "Seed: 44, Epoch: 045, Loss: 0.4755, Val Acc: 0.7520, Test Acc: 0.7587\n",
                        "Seed: 44, Epoch: 046, Loss: 0.4741, Val Acc: 0.7480, Test Acc: 0.7547\n",
                        "Seed: 44, Epoch: 047, Loss: 0.4734, Val Acc: 0.7467, Test Acc: 0.7560\n",
                        "Seed: 44, Epoch: 048, Loss: 0.4702, Val Acc: 0.7533, Test Acc: 0.7640\n",
                        "Seed: 44, Epoch: 049, Loss: 0.4684, Val Acc: 0.7507, Test Acc: 0.7547\n",
                        "Seed: 44, Epoch: 050, Loss: 0.4686, Val Acc: 0.7493, Test Acc: 0.7587\n",
                        "Seed: 44, Epoch: 051, Loss: 0.4704, Val Acc: 0.7493, Test Acc: 0.7653\n",
                        "Seed: 44, Epoch: 052, Loss: 0.4693, Val Acc: 0.7533, Test Acc: 0.7547\n",
                        "Seed: 44, Epoch: 053, Loss: 0.4692, Val Acc: 0.7480, Test Acc: 0.7720\n",
                        "Seed: 44, Epoch: 054, Loss: 0.4622, Val Acc: 0.7573, Test Acc: 0.7613\n",
                        "Seed: 44, Epoch: 055, Loss: 0.4595, Val Acc: 0.7547, Test Acc: 0.7680\n",
                        "Seed: 44, Epoch: 056, Loss: 0.4597, Val Acc: 0.7533, Test Acc: 0.7720\n",
                        "Seed: 44, Epoch: 057, Loss: 0.4590, Val Acc: 0.7560, Test Acc: 0.7707\n",
                        "Seed: 44, Epoch: 058, Loss: 0.4564, Val Acc: 0.7560, Test Acc: 0.7680\n",
                        "Seed: 44, Epoch: 059, Loss: 0.4549, Val Acc: 0.7560, Test Acc: 0.7680\n",
                        "Seed: 44, Epoch: 060, Loss: 0.4537, Val Acc: 0.7520, Test Acc: 0.7733\n",
                        "Seed: 44, Epoch: 061, Loss: 0.4533, Val Acc: 0.7533, Test Acc: 0.7680\n",
                        "Seed: 44, Epoch: 062, Loss: 0.4514, Val Acc: 0.7573, Test Acc: 0.7707\n",
                        "Seed: 44, Epoch: 063, Loss: 0.4516, Val Acc: 0.7547, Test Acc: 0.7707\n",
                        "Seed: 44, Epoch: 064, Loss: 0.4499, Val Acc: 0.7560, Test Acc: 0.7720\n",
                        "Seed: 44, Epoch: 065, Loss: 0.4474, Val Acc: 0.7507, Test Acc: 0.7787\n",
                        "Seed: 44, Epoch: 066, Loss: 0.4471, Val Acc: 0.7493, Test Acc: 0.7747\n",
                        "Seed: 44, Epoch: 067, Loss: 0.4463, Val Acc: 0.7573, Test Acc: 0.7747\n",
                        "Seed: 44, Epoch: 068, Loss: 0.4445, Val Acc: 0.7520, Test Acc: 0.7733\n",
                        "Seed: 44, Epoch: 069, Loss: 0.4452, Val Acc: 0.7520, Test Acc: 0.7800\n",
                        "Seed: 44, Epoch: 070, Loss: 0.4440, Val Acc: 0.7533, Test Acc: 0.7773\n",
                        "Seed: 44, Epoch: 071, Loss: 0.4403, Val Acc: 0.7547, Test Acc: 0.7773\n",
                        "Seed: 44, Epoch: 072, Loss: 0.4393, Val Acc: 0.7613, Test Acc: 0.7800\n",
                        "Seed: 44, Epoch: 073, Loss: 0.4395, Val Acc: 0.7520, Test Acc: 0.7707\n",
                        "Seed: 44, Epoch: 074, Loss: 0.4393, Val Acc: 0.7587, Test Acc: 0.7747\n",
                        "Seed: 44, Epoch: 075, Loss: 0.4411, Val Acc: 0.7627, Test Acc: 0.7787\n",
                        "Seed: 44, Epoch: 076, Loss: 0.4368, Val Acc: 0.7520, Test Acc: 0.7747\n",
                        "Seed: 44, Epoch: 077, Loss: 0.4348, Val Acc: 0.7587, Test Acc: 0.7787\n",
                        "Seed: 44, Epoch: 078, Loss: 0.4341, Val Acc: 0.7560, Test Acc: 0.7813\n",
                        "Seed: 44, Epoch: 079, Loss: 0.4334, Val Acc: 0.7653, Test Acc: 0.7787\n",
                        "Seed: 44, Epoch: 080, Loss: 0.4317, Val Acc: 0.7653, Test Acc: 0.7813\n",
                        "Seed: 44, Epoch: 081, Loss: 0.4295, Val Acc: 0.7520, Test Acc: 0.7787\n",
                        "Seed: 44, Epoch: 082, Loss: 0.4293, Val Acc: 0.7573, Test Acc: 0.7827\n",
                        "Seed: 44, Epoch: 083, Loss: 0.4276, Val Acc: 0.7600, Test Acc: 0.7853\n",
                        "Seed: 44, Epoch: 084, Loss: 0.4263, Val Acc: 0.7627, Test Acc: 0.7880\n",
                        "Seed: 44, Epoch: 085, Loss: 0.4246, Val Acc: 0.7573, Test Acc: 0.7827\n",
                        "Seed: 44, Epoch: 086, Loss: 0.4236, Val Acc: 0.7653, Test Acc: 0.7853\n",
                        "Seed: 44, Epoch: 087, Loss: 0.4258, Val Acc: 0.7587, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 088, Loss: 0.4249, Val Acc: 0.7600, Test Acc: 0.7867\n",
                        "Seed: 44, Epoch: 089, Loss: 0.4233, Val Acc: 0.7640, Test Acc: 0.7853\n",
                        "Seed: 44, Epoch: 090, Loss: 0.4224, Val Acc: 0.7600, Test Acc: 0.7760\n",
                        "Seed: 44, Epoch: 091, Loss: 0.4225, Val Acc: 0.7627, Test Acc: 0.7893\n",
                        "Seed: 44, Epoch: 092, Loss: 0.4184, Val Acc: 0.7600, Test Acc: 0.7907\n",
                        "Seed: 44, Epoch: 093, Loss: 0.4173, Val Acc: 0.7573, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 094, Loss: 0.4172, Val Acc: 0.7600, Test Acc: 0.7893\n",
                        "Seed: 44, Epoch: 095, Loss: 0.4153, Val Acc: 0.7600, Test Acc: 0.7907\n",
                        "Seed: 44, Epoch: 096, Loss: 0.4134, Val Acc: 0.7627, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 097, Loss: 0.4147, Val Acc: 0.7587, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 098, Loss: 0.4120, Val Acc: 0.7587, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 099, Loss: 0.4113, Val Acc: 0.7640, Test Acc: 0.7907\n",
                        "Seed: 44, Epoch: 100, Loss: 0.4119, Val Acc: 0.7627, Test Acc: 0.7907\n",
                        "Seed: 44, Epoch: 101, Loss: 0.4088, Val Acc: 0.7667, Test Acc: 0.7880\n",
                        "Seed: 44, Epoch: 102, Loss: 0.4076, Val Acc: 0.7653, Test Acc: 0.7960\n",
                        "Seed: 44, Epoch: 103, Loss: 0.4080, Val Acc: 0.7613, Test Acc: 0.7907\n",
                        "Seed: 44, Epoch: 104, Loss: 0.4071, Val Acc: 0.7667, Test Acc: 0.7867\n",
                        "Seed: 44, Epoch: 105, Loss: 0.4042, Val Acc: 0.7653, Test Acc: 0.7973\n",
                        "Seed: 44, Epoch: 106, Loss: 0.4038, Val Acc: 0.7613, Test Acc: 0.7893\n",
                        "Seed: 44, Epoch: 107, Loss: 0.4041, Val Acc: 0.7680, Test Acc: 0.7987\n",
                        "Seed: 44, Epoch: 108, Loss: 0.4034, Val Acc: 0.7627, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 109, Loss: 0.4000, Val Acc: 0.7640, Test Acc: 0.7973\n",
                        "Seed: 44, Epoch: 110, Loss: 0.3982, Val Acc: 0.7640, Test Acc: 0.7893\n",
                        "Seed: 44, Epoch: 111, Loss: 0.3976, Val Acc: 0.7667, Test Acc: 0.7907\n",
                        "Seed: 44, Epoch: 112, Loss: 0.3969, Val Acc: 0.7653, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 113, Loss: 0.3950, Val Acc: 0.7667, Test Acc: 0.7893\n",
                        "Seed: 44, Epoch: 114, Loss: 0.3956, Val Acc: 0.7667, Test Acc: 0.7987\n",
                        "Seed: 44, Epoch: 115, Loss: 0.3918, Val Acc: 0.7693, Test Acc: 0.7933\n",
                        "Seed: 44, Epoch: 116, Loss: 0.3945, Val Acc: 0.7693, Test Acc: 0.7853\n",
                        "Seed: 44, Epoch: 117, Loss: 0.3936, Val Acc: 0.7693, Test Acc: 0.7907\n",
                        "Seed: 44, Epoch: 118, Loss: 0.3892, Val Acc: 0.7720, Test Acc: 0.7933\n",
                        "Seed: 44, Epoch: 119, Loss: 0.3880, Val Acc: 0.7693, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 120, Loss: 0.3881, Val Acc: 0.7693, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 121, Loss: 0.3864, Val Acc: 0.7680, Test Acc: 0.7907\n",
                        "Seed: 44, Epoch: 122, Loss: 0.3842, Val Acc: 0.7693, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 123, Loss: 0.3823, Val Acc: 0.7707, Test Acc: 0.7893\n",
                        "Seed: 44, Epoch: 124, Loss: 0.3809, Val Acc: 0.7747, Test Acc: 0.8040\n",
                        "Seed: 44, Epoch: 125, Loss: 0.3795, Val Acc: 0.7693, Test Acc: 0.7907\n",
                        "Seed: 44, Epoch: 126, Loss: 0.3795, Val Acc: 0.7760, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 127, Loss: 0.3751, Val Acc: 0.7733, Test Acc: 0.7933\n",
                        "Seed: 44, Epoch: 128, Loss: 0.3750, Val Acc: 0.7693, Test Acc: 0.7893\n",
                        "Seed: 44, Epoch: 129, Loss: 0.3731, Val Acc: 0.7733, Test Acc: 0.7933\n",
                        "Seed: 44, Epoch: 130, Loss: 0.3725, Val Acc: 0.7760, Test Acc: 0.7880\n",
                        "Seed: 44, Epoch: 131, Loss: 0.3698, Val Acc: 0.7760, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 132, Loss: 0.3676, Val Acc: 0.7733, Test Acc: 0.7907\n",
                        "Seed: 44, Epoch: 133, Loss: 0.3680, Val Acc: 0.7813, Test Acc: 0.7880\n",
                        "Seed: 44, Epoch: 134, Loss: 0.3678, Val Acc: 0.7747, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 135, Loss: 0.3645, Val Acc: 0.7813, Test Acc: 0.8000\n",
                        "Seed: 44, Epoch: 136, Loss: 0.3656, Val Acc: 0.7720, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 137, Loss: 0.3632, Val Acc: 0.7787, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 138, Loss: 0.3623, Val Acc: 0.7760, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 139, Loss: 0.3570, Val Acc: 0.7773, Test Acc: 0.7987\n",
                        "Seed: 44, Epoch: 140, Loss: 0.3571, Val Acc: 0.7747, Test Acc: 0.7973\n",
                        "Seed: 44, Epoch: 141, Loss: 0.3558, Val Acc: 0.7693, Test Acc: 0.7973\n",
                        "Seed: 44, Epoch: 142, Loss: 0.3558, Val Acc: 0.7800, Test Acc: 0.8000\n",
                        "Seed: 44, Epoch: 143, Loss: 0.3523, Val Acc: 0.7733, Test Acc: 0.7973\n",
                        "Seed: 44, Epoch: 144, Loss: 0.3494, Val Acc: 0.7840, Test Acc: 0.8000\n",
                        "Seed: 44, Epoch: 145, Loss: 0.3487, Val Acc: 0.7733, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 146, Loss: 0.3457, Val Acc: 0.7747, Test Acc: 0.8013\n",
                        "Seed: 44, Epoch: 147, Loss: 0.3442, Val Acc: 0.7680, Test Acc: 0.8000\n",
                        "Seed: 44, Epoch: 148, Loss: 0.3430, Val Acc: 0.7800, Test Acc: 0.8040\n",
                        "Seed: 44, Epoch: 149, Loss: 0.3421, Val Acc: 0.7787, Test Acc: 0.8000\n",
                        "Seed: 44, Epoch: 150, Loss: 0.3393, Val Acc: 0.7813, Test Acc: 0.8053\n",
                        "Seed: 44, Epoch: 151, Loss: 0.3388, Val Acc: 0.7773, Test Acc: 0.7987\n",
                        "Seed: 44, Epoch: 152, Loss: 0.3373, Val Acc: 0.7827, Test Acc: 0.8013\n",
                        "Seed: 44, Epoch: 153, Loss: 0.3334, Val Acc: 0.7787, Test Acc: 0.8013\n",
                        "Seed: 44, Epoch: 154, Loss: 0.3321, Val Acc: 0.7747, Test Acc: 0.8000\n",
                        "Seed: 44, Epoch: 155, Loss: 0.3322, Val Acc: 0.7760, Test Acc: 0.8027\n",
                        "Seed: 44, Epoch: 156, Loss: 0.3320, Val Acc: 0.7720, Test Acc: 0.8040\n",
                        "Seed: 44, Epoch: 157, Loss: 0.3293, Val Acc: 0.7760, Test Acc: 0.7960\n",
                        "Seed: 44, Epoch: 158, Loss: 0.3277, Val Acc: 0.7787, Test Acc: 0.8013\n",
                        "Seed: 44, Epoch: 159, Loss: 0.3259, Val Acc: 0.7720, Test Acc: 0.8027\n",
                        "Seed: 44, Epoch: 160, Loss: 0.3256, Val Acc: 0.7653, Test Acc: 0.8000\n",
                        "Seed: 44, Epoch: 161, Loss: 0.3243, Val Acc: 0.7680, Test Acc: 0.7987\n",
                        "Seed: 44, Epoch: 162, Loss: 0.3235, Val Acc: 0.7733, Test Acc: 0.7987\n",
                        "Seed: 44, Epoch: 163, Loss: 0.3200, Val Acc: 0.7760, Test Acc: 0.8013\n",
                        "Seed: 44, Epoch: 164, Loss: 0.3202, Val Acc: 0.7693, Test Acc: 0.8013\n",
                        "Seed: 44, Epoch: 165, Loss: 0.3184, Val Acc: 0.7747, Test Acc: 0.7920\n",
                        "Seed: 44, Epoch: 166, Loss: 0.3168, Val Acc: 0.7693, Test Acc: 0.8027\n",
                        "Seed: 44, Epoch: 167, Loss: 0.3127, Val Acc: 0.7760, Test Acc: 0.8027\n",
                        "Seed: 44, Epoch: 168, Loss: 0.3129, Val Acc: 0.7853, Test Acc: 0.8000\n",
                        "Seed: 44, Epoch: 169, Loss: 0.3154, Val Acc: 0.7827, Test Acc: 0.8040\n",
                        "Seed: 44, Epoch: 170, Loss: 0.3090, Val Acc: 0.7787, Test Acc: 0.8013\n",
                        "Seed: 44, Epoch: 171, Loss: 0.3059, Val Acc: 0.7720, Test Acc: 0.8013\n",
                        "Seed: 44, Epoch: 172, Loss: 0.3078, Val Acc: 0.7720, Test Acc: 0.7973\n",
                        "Seed: 44, Epoch: 173, Loss: 0.3108, Val Acc: 0.7760, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 174, Loss: 0.3032, Val Acc: 0.7747, Test Acc: 0.7973\n",
                        "Seed: 44, Epoch: 175, Loss: 0.3040, Val Acc: 0.7827, Test Acc: 0.8080\n",
                        "Seed: 44, Epoch: 176, Loss: 0.3112, Val Acc: 0.7773, Test Acc: 0.8040\n",
                        "Seed: 44, Epoch: 177, Loss: 0.3106, Val Acc: 0.7827, Test Acc: 0.7960\n",
                        "Seed: 44, Epoch: 178, Loss: 0.2963, Val Acc: 0.7800, Test Acc: 0.7973\n",
                        "Seed: 44, Epoch: 179, Loss: 0.2971, Val Acc: 0.7840, Test Acc: 0.8013\n",
                        "Seed: 44, Epoch: 180, Loss: 0.2984, Val Acc: 0.7853, Test Acc: 0.8067\n",
                        "Seed: 44, Epoch: 181, Loss: 0.2955, Val Acc: 0.7907, Test Acc: 0.8040\n",
                        "Seed: 44, Epoch: 182, Loss: 0.2967, Val Acc: 0.7893, Test Acc: 0.8080\n",
                        "Seed: 44, Epoch: 183, Loss: 0.2958, Val Acc: 0.7867, Test Acc: 0.8040\n",
                        "Seed: 44, Epoch: 184, Loss: 0.2933, Val Acc: 0.7853, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 185, Loss: 0.2908, Val Acc: 0.7800, Test Acc: 0.7987\n",
                        "Seed: 44, Epoch: 186, Loss: 0.2879, Val Acc: 0.7853, Test Acc: 0.7987\n",
                        "Seed: 44, Epoch: 187, Loss: 0.2857, Val Acc: 0.7960, Test Acc: 0.8000\n",
                        "Seed: 44, Epoch: 188, Loss: 0.2867, Val Acc: 0.7827, Test Acc: 0.8080\n",
                        "Seed: 44, Epoch: 189, Loss: 0.2836, Val Acc: 0.7840, Test Acc: 0.7960\n",
                        "Seed: 44, Epoch: 190, Loss: 0.2849, Val Acc: 0.7853, Test Acc: 0.8040\n",
                        "Seed: 44, Epoch: 191, Loss: 0.2829, Val Acc: 0.7840, Test Acc: 0.8040\n",
                        "Seed: 44, Epoch: 192, Loss: 0.2841, Val Acc: 0.7907, Test Acc: 0.8000\n",
                        "Seed: 44, Epoch: 193, Loss: 0.2820, Val Acc: 0.7827, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 194, Loss: 0.2811, Val Acc: 0.7920, Test Acc: 0.8013\n",
                        "Seed: 44, Epoch: 195, Loss: 0.2792, Val Acc: 0.7907, Test Acc: 0.8027\n",
                        "Seed: 44, Epoch: 196, Loss: 0.2748, Val Acc: 0.7787, Test Acc: 0.7947\n",
                        "Seed: 44, Epoch: 197, Loss: 0.2766, Val Acc: 0.7813, Test Acc: 0.7933\n",
                        "Seed: 44, Epoch: 198, Loss: 0.2797, Val Acc: 0.7813, Test Acc: 0.7880\n",
                        "Seed: 44, Epoch: 199, Loss: 0.2782, Val Acc: 0.7787, Test Acc: 0.7973\n",
                        "Seed: 44, Epoch: 200, Loss: 0.2789, Val Acc: 0.7893, Test Acc: 0.7987\n",
                        "Average Time: 755.68 seconds\n",
                        "Var Time: 57.20 seconds\n",
                        "Average Memory: 3684.00 MB\n",
                        "Average Best Val Acc: 0.8089\n",
                        "Std Best Test Acc: 0.0080\n",
                        "Average Test Acc: 0.7924\n"
                    ]
                }
            ],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\ndata_path = \"/data1/Pooling\"\ndataset_sparse = TUDataset(root=data_path, name=\"COLLAB\", transform=T.Compose([T.OneHotDegree(491)]), use_node_attr=True)\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_geometric.nn import BatchNorm\nclass HierarchicalGCN_HGPSL(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_HGPSL, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = HGPSLPool(hidden_channels, ratio=0.9, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = HGPSLPool(hidden_channels, ratio=0.9, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv3 = GCNConv(hidden_channels, out_channels)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 32)\n        self.lin2 = torch.nn.Linear(32, num_classes)\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        edge_attr = None\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool1(x, edge_index, edge_attr, batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool2(x, edge_index, edge_attr, batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\ndef train():\n    model.train()\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        data = data.to(device)\n        out = model(data)\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n    return correct / len(loader.dataset)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 150\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    train_ratio = 0.7\n    val_ratio = 0.15\n    val_ratio = 0.15\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n    model = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 201):\n        loss = train()\n        val_acc = test(valid_loader)\n        test_acc = test(test_loader)\n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\nprint(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Graph Regression"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### QM7"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/500MAE=1549.8635 MAE=1547.9595 MAE=1545.5062 MAE=1543.0140 MAE=1540.4630 MAE=1537.4766 MAE=1534.4363 MAE=1530.2174 MAE=1526.0850 Epoch: 10/500MAE=1521.9048 MAE=1517.2203 MAE=1511.4885 MAE=1506.5045 MAE=1501.0889 MAE=1494.7964 MAE=1488.3619 MAE=1481.3239 MAE=1473.1382 MAE=1465.1357 Epoch: 20/500MAE=1457.5217 MAE=1448.8146 MAE=1439.3210 MAE=1430.4114 MAE=1421.4143 MAE=1411.2148 MAE=1399.9261 MAE=1390.0613 MAE=1379.6327 MAE=1367.7371 Epoch: 30/500MAE=1357.7585 MAE=1345.2727 MAE=1334.4143 MAE=1322.5632 MAE=1307.4254 MAE=1294.1014 MAE=1282.1926 MAE=1269.1797 MAE=1258.5542 MAE=1240.6697 Epoch: 40/500MAE=1222.6736 MAE=1205.5114 MAE=1181.1680 MAE=1157.3481 MAE=1142.2969 MAE=1125.6968 MAE=1104.4193 MAE=1088.2207 MAE=1069.4326 MAE=1048.0422 Epoch: 50/500MAE=1029.2899 MAE=1009.1296 MAE=989.2792 MAE=968.0049 MAE=942.0250 MAE=923.2470 MAE=894.6740 MAE=879.9742 MAE=857.5229 MAE=829.1937 Epoch: 60/500MAE=807.6415 MAE=786.4309 MAE=752.1161 MAE=733.8427 MAE=711.0385 MAE=685.1681 MAE=673.7532 MAE=642.5768 MAE=617.2737 MAE=595.4249 Epoch: 70/500MAE=569.5669 MAE=549.6869 MAE=523.5189 MAE=502.0266 MAE=488.9053 MAE=470.8131 MAE=447.6087 MAE=415.8950 MAE=405.8915 MAE=403.8670 Epoch: 80/500MAE=349.9788 MAE=336.0854 MAE=291.7623 MAE=299.2717 MAE=284.4951 MAE=256.8955 MAE=240.2211 MAE=223.6518 MAE=215.8065 MAE=202.3109 Epoch: 90/500MAE=186.5519 MAE=188.4811 MAE=173.5156 MAE=164.2535 MAE=169.0439 MAE=159.2765 MAE=155.6689 MAE=154.9515 MAE=156.7802 MAE=147.5186 Epoch: 100/500MAE=147.0608 MAE=144.3703 MAE=145.3898 MAE=145.1623 MAE=146.3766 MAE=146.5401 MAE=144.4267 MAE=144.1161 MAE=144.8028 MAE=143.1928 Epoch: 110/500MAE=145.1661 MAE=144.1738 MAE=143.2495 MAE=143.2128 MAE=143.6238 MAE=143.9898 MAE=144.2323 MAE=143.6447 MAE=143.5688 MAE=143.4456 Epoch: 120/500MAE=143.2573 MAE=143.7017 MAE=143.5165 MAE=143.4678 MAE=143.6953 MAE=143.7767 MAE=143.8512 MAE=143.4703 MAE=143.4781 MAE=143.2446 Epoch: 130/500MAE=143.2995 MAE=143.4018 MAE=143.2468 MAE=143.3997 MAE=143.2549 MAE=143.3097 MAE=143.3498 MAE=143.2494 MAE=143.3582 MAE=143.3792 Epoch: 140/500MAE=143.3762 MAE=143.2885 MAE=143.2919 MAE=143.3971 MAE=143.4034 MAE=143.2921 MAE=143.3977 MAE=143.3968 MAE=143.2964 MAE=143.3971 Epoch: 150/500MAE=143.3944 MAE=143.3979 MAE=143.4296 MAE=143.4060 MAE=143.5354 MAE=143.5333 MAE=143.5310 MAE=143.5346 MAE=143.4078 MAE=143.5293 Epoch: 160/500MAE=143.5352 MAE=143.3034 MAE=143.5381 MAE=143.5369 MAE=143.4130 MAE=143.3088 MAE=143.4143 MAE=143.5388 MAE=143.5367 MAE=143.5307 Epoch: 170/500MAE=143.5268 MAE=143.5214 MAE=143.4238 MAE=143.5289 MAE=143.5316 MAE=143.5322 MAE=143.5340 MAE=143.4283 MAE=143.4339 MAE=143.5343 Epoch: 180/500MAE=143.4319 MAE=143.5333 MAE=143.4083 MAE=143.4060 MAE=143.4068 MAE=143.4107 MAE=143.4087 MAE=143.4109 MAE=143.4108 MAE=143.4080 Epoch: 190/500MAE=143.3131 MAE=143.4097 MAE=143.4072 MAE=143.4008 MAE=143.4099 MAE=143.4106 MAE=143.4046 MAE=143.4124 MAE=143.4137 MAE=143.3123 Epoch: 200/500MAE=143.4174 MAE=143.3143 MAE=143.4158 MAE=143.4146 MAE=143.4134 MAE=143.4133 MAE=143.4144 MAE=143.4226 MAE=143.4169 MAE=143.3162 Epoch: 210/500MAE=143.4154 MAE=143.4140 MAE=143.4159 MAE=143.3143 MAE=143.4189 MAE=143.3235 MAE=143.4216 MAE=143.3242 MAE=143.4257 MAE=143.4242 Epoch: 220/500MAE=143.4165 MAE=143.4187 MAE=143.4146 MAE=143.4163 MAE=143.4146 MAE=143.3161 MAE=143.3159 MAE=143.4127 MAE=143.4062 MAE=143.4106 Epoch: 230/500MAE=143.3068 MAE=143.4082 MAE=143.4066 MAE=143.4086 MAE=143.3030 MAE=143.4088 MAE=143.3063 MAE=143.4093 MAE=143.4119 MAE=143.3068 Epoch: 240/500MAE=143.4140 MAE=143.4141 MAE=143.4112 MAE=143.4131 MAE=143.4134 MAE=143.4051 MAE=143.3997 MAE=143.3969 MAE=143.3992 MAE=143.3978 Epoch: 250/500MAE=143.3038 MAE=143.4049 MAE=143.4024 MAE=143.4089 MAE=143.3105 MAE=143.3021 MAE=143.3948 MAE=143.3958 MAE=143.3987 MAE=143.3973 MAE=146.9083 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 146.908 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.9668 MAE=1547.8208 MAE=1545.1115 MAE=1542.6342 MAE=1539.8080 MAE=1536.4661 MAE=1533.2703 MAE=1529.7069 MAE=1526.4075 Epoch: 10/500MAE=1522.3674 MAE=1517.1733 MAE=1512.5620 MAE=1507.8643 MAE=1502.2681 MAE=1495.7338 MAE=1488.2742 MAE=1480.7280 MAE=1473.5723 MAE=1466.7515 Epoch: 20/500MAE=1458.4962 MAE=1449.8472 MAE=1440.2288 MAE=1432.0637 MAE=1422.6743 MAE=1411.7504 MAE=1403.8999 MAE=1391.6956 MAE=1380.8784 MAE=1371.9083 Epoch: 30/500MAE=1358.7557 MAE=1348.5216 MAE=1334.9713 MAE=1325.1248 MAE=1310.7761 MAE=1299.4661 MAE=1283.6600 MAE=1269.7633 MAE=1254.3682 MAE=1242.2228 Epoch: 40/500MAE=1227.6450 MAE=1210.8180 MAE=1193.4713 MAE=1178.5814 MAE=1158.7041 MAE=1125.1656 MAE=1100.9431 MAE=1087.9532 MAE=1074.9629 MAE=1050.0959 Epoch: 50/500MAE=1040.1252 MAE=1014.4616 MAE=995.8673 MAE=980.2297 MAE=955.8477 MAE=932.0801 MAE=910.7953 MAE=891.9787 MAE=866.1770 MAE=840.2076 Epoch: 60/500MAE=826.1038 MAE=799.2698 MAE=773.3002 MAE=751.5482 MAE=727.3602 MAE=704.0006 MAE=679.6435 MAE=663.1760 MAE=633.4949 MAE=597.0607 Epoch: 70/500MAE=581.3578 MAE=551.6602 MAE=540.0566 MAE=507.5862 MAE=497.9782 MAE=466.3432 MAE=454.2224 MAE=413.2325 MAE=407.5628 MAE=400.3265 Epoch: 80/500MAE=377.5235 MAE=374.8596 MAE=331.9953 MAE=321.4239 MAE=276.2776 MAE=262.0026 MAE=243.7956 MAE=230.5443 MAE=231.8353 MAE=215.6627 Epoch: 90/500MAE=210.7896 MAE=193.1585 MAE=190.7332 MAE=178.4158 MAE=175.9860 MAE=169.7022 MAE=164.1027 MAE=158.1668 MAE=176.3223 MAE=156.8632 Epoch: 100/500MAE=150.4130 MAE=153.1671 MAE=149.9915 MAE=147.3987 MAE=146.4159 MAE=146.3256 MAE=145.6448 MAE=143.5848 MAE=142.8719 MAE=143.6787 Epoch: 110/500MAE=143.1254 MAE=143.8640 MAE=145.2490 MAE=142.9379 MAE=142.6572 MAE=144.0211 MAE=144.0182 MAE=144.5462 MAE=142.7450 MAE=142.9290 Epoch: 120/500MAE=143.9810 MAE=143.8071 MAE=144.2982 MAE=143.4990 MAE=143.0141 MAE=143.8753 MAE=143.3429 MAE=143.1599 MAE=143.4418 MAE=143.4281 Epoch: 130/500MAE=143.6031 MAE=143.5077 MAE=143.5295 MAE=143.6514 MAE=143.5633 MAE=143.6030 MAE=143.5820 MAE=143.3683 MAE=143.3365 MAE=143.3690 Epoch: 140/500MAE=143.6441 MAE=143.6805 MAE=143.6723 MAE=143.6512 MAE=143.6299 MAE=143.6399 MAE=143.6417 MAE=143.5798 MAE=143.6492 MAE=143.5817 Epoch: 150/500MAE=143.5765 MAE=143.5782 MAE=143.5834 MAE=143.5856 MAE=143.5847 MAE=143.5847 MAE=143.5863 MAE=143.5867 MAE=143.5847 MAE=143.5869 Epoch: 160/500MAE=143.5869 MAE=143.5730 MAE=143.5754 MAE=143.5797 MAE=143.5677 MAE=143.5556 MAE=143.5624 MAE=143.5530 MAE=143.5566 MAE=143.5538 Epoch: 170/500MAE=143.5514 MAE=143.5594 MAE=143.5582 MAE=143.5659 MAE=143.5549 MAE=143.5470 MAE=143.5456 MAE=143.5396 MAE=143.5298 MAE=143.5331 Epoch: 180/500MAE=143.5263 MAE=143.5227 MAE=143.5233 MAE=143.5208 MAE=143.5247 MAE=143.5208 MAE=143.5153 MAE=143.5070 MAE=143.5007 MAE=143.4903 Epoch: 190/500MAE=143.4886 MAE=143.4911 MAE=143.4914 MAE=143.4989 MAE=143.4991 MAE=143.5014 MAE=143.4970 MAE=143.5023 MAE=143.5007 MAE=143.4979 Epoch: 200/500MAE=143.4994 MAE=143.4971 MAE=143.5005 MAE=143.5027 MAE=143.5044 MAE=143.5011 MAE=143.4962 MAE=143.4923 MAE=143.4911 MAE=143.4899 Epoch: 210/500MAE=143.4878 MAE=143.4885 MAE=143.4916 MAE=143.4898 MAE=143.4912 MAE=143.4910 MAE=143.4956 MAE=143.4943 MAE=143.4892 MAE=143.4915 Epoch: 220/500MAE=143.4931 MAE=143.4908 MAE=143.4921 MAE=143.4933 MAE=143.4906 MAE=143.4974 MAE=143.5005 MAE=143.5054 MAE=143.5074 MAE=143.5030 Epoch: 230/500MAE=143.4991 MAE=143.5010 MAE=143.5020 MAE=143.4991 MAE=143.4965 MAE=143.4964 MAE=143.4970 MAE=143.5019 MAE=143.5033 MAE=143.5021 Epoch: 240/500MAE=143.5024 MAE=143.5027 MAE=143.4947 MAE=143.4992 MAE=143.5036 MAE=143.5045 MAE=143.4958 MAE=143.4910 MAE=143.4899 MAE=143.4885 Epoch: 250/500MAE=143.4890 MAE=143.4915 MAE=143.4924 MAE=143.4914 MAE=143.4916 MAE=143.4933 MAE=143.4912 MAE=143.4896 MAE=143.4902 MAE=143.4930 Epoch: 260/500MAE=143.4918 MAE=143.4898 MAE=143.4897 MAE=143.4899 MAE=143.4830 MAE=144.6675 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 145.788 +/- 1.120\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.9138 MAE=1547.8518 MAE=1545.0142 MAE=1542.6521 MAE=1539.8901 MAE=1536.9999 MAE=1533.7499 MAE=1530.3464 MAE=1526.7886 Epoch: 10/500MAE=1522.4031 MAE=1518.0859 MAE=1513.6299 MAE=1508.7078 MAE=1502.3092 MAE=1495.5796 MAE=1488.2976 MAE=1482.4287 MAE=1475.0944 MAE=1467.6060 Epoch: 20/500MAE=1461.7225 MAE=1452.3951 MAE=1443.6567 MAE=1436.2830 MAE=1425.5527 MAE=1415.2600 MAE=1402.4388 MAE=1389.1743 MAE=1378.1030 MAE=1366.5176 Epoch: 30/500MAE=1356.1691 MAE=1341.9615 MAE=1328.8618 MAE=1318.1501 MAE=1303.4435 MAE=1289.2424 MAE=1275.8031 MAE=1263.4949 MAE=1247.3982 MAE=1230.8798 Epoch: 40/500MAE=1214.8583 MAE=1199.4254 MAE=1181.5432 MAE=1166.0409 MAE=1150.6656 MAE=1134.7249 MAE=1106.2170 MAE=1093.7544 MAE=1080.0265 MAE=1062.4907 Epoch: 50/500MAE=1042.3894 MAE=1013.7333 MAE=996.8285 MAE=975.8815 MAE=958.0312 MAE=933.9623 MAE=913.5536 MAE=893.7929 MAE=870.8329 MAE=856.1359 Epoch: 60/500MAE=819.5809 MAE=803.8287 MAE=780.7656 MAE=762.2483 MAE=746.3378 MAE=716.7480 MAE=698.9473 MAE=682.9396 MAE=654.1049 MAE=615.9612 Epoch: 70/500MAE=592.8448 MAE=569.1771 MAE=546.3953 MAE=516.8934 MAE=497.7696 MAE=484.9335 MAE=473.2288 MAE=444.3484 MAE=424.9572 MAE=400.9818 Epoch: 80/500MAE=389.6666 MAE=376.6601 MAE=347.3740 MAE=336.1967 MAE=332.5688 MAE=310.9323 MAE=295.8979 MAE=290.3278 MAE=255.0884 MAE=268.7979 Epoch: 90/500MAE=237.6301 MAE=242.5367 MAE=188.2399 MAE=172.8308 MAE=168.8398 MAE=172.1092 MAE=163.0673 MAE=161.0602 MAE=154.1140 MAE=152.0510 Epoch: 100/500MAE=153.7728 MAE=151.9893 MAE=146.8319 MAE=146.6473 MAE=146.8865 MAE=145.7177 MAE=144.8531 MAE=146.6911 MAE=143.5893 MAE=145.1247 Epoch: 110/500MAE=143.1673 MAE=143.1238 MAE=144.0076 MAE=145.1993 MAE=145.8774 MAE=144.2213 MAE=142.1059 MAE=142.5082 MAE=142.8478 MAE=142.7802 Epoch: 120/500MAE=143.0038 MAE=142.2396 MAE=141.7681 MAE=142.8996 MAE=142.4931 MAE=142.6653 MAE=142.7930 MAE=142.9987 MAE=142.6310 MAE=142.7068 Epoch: 130/500MAE=142.7104 MAE=142.9397 MAE=142.9299 MAE=142.5621 MAE=142.6967 MAE=142.8039 MAE=142.9868 MAE=142.9258 MAE=142.7154 MAE=142.7203 Epoch: 140/500MAE=143.0258 MAE=143.0865 MAE=142.7510 MAE=142.5940 MAE=142.5776 MAE=142.8266 MAE=142.9359 MAE=142.7777 MAE=142.6823 MAE=142.6735 Epoch: 150/500MAE=142.5953 MAE=142.6502 MAE=142.6684 MAE=142.5915 MAE=142.6643 MAE=142.6443 MAE=142.6467 MAE=142.5952 MAE=142.5910 MAE=142.5885 Epoch: 160/500MAE=142.6592 MAE=142.6593 MAE=142.6613 MAE=142.5873 MAE=142.5893 MAE=142.5867 MAE=142.6341 MAE=142.5827 MAE=142.5831 MAE=142.5132 Epoch: 170/500MAE=142.5137 MAE=142.5630 MAE=142.5068 MAE=142.5060 MAE=142.5753 MAE=142.5732 MAE=142.4984 MAE=142.5564 MAE=142.5645 MAE=142.5841 Epoch: 180/500MAE=142.5094 MAE=142.5088 MAE=142.5090 MAE=142.5101 MAE=142.5120 MAE=142.5113 MAE=142.5120 MAE=142.5619 MAE=142.5103 MAE=142.5103 Epoch: 190/500MAE=142.5102 MAE=142.5090 MAE=142.5802 MAE=142.5617 MAE=142.5077 MAE=142.5060 MAE=142.5080 MAE=142.5012 MAE=142.4982 MAE=142.5714 Epoch: 200/500MAE=142.5955 MAE=142.6691 MAE=142.6697 MAE=142.5773 MAE=142.5786 MAE=142.5772 MAE=142.5063 MAE=142.5838 MAE=142.5820 MAE=142.5837 Epoch: 210/500MAE=142.6317 MAE=142.5760 MAE=142.6242 MAE=142.5671 MAE=142.6177 MAE=142.5648 MAE=142.6151 MAE=142.6129 MAE=142.6156 MAE=142.5637 Epoch: 220/500MAE=142.6593 MAE=142.5621 MAE=142.5661 MAE=142.5679 MAE=142.5688 MAE=142.5699 MAE=142.5655 MAE=142.5704 MAE=142.4965 MAE=142.5719 Epoch: 230/500MAE=142.5699 MAE=142.5709 MAE=142.5011 MAE=142.5711 MAE=142.5721 MAE=142.5742 MAE=142.5782 MAE=142.5789 MAE=142.5725 MAE=142.6270 Epoch: 240/500MAE=142.5569 MAE=142.5000 MAE=142.6267 MAE=142.5521 MAE=142.4954 MAE=142.4929 MAE=142.4889 MAE=142.4927 MAE=142.4855 MAE=142.4830 Epoch: 250/500MAE=142.5578 MAE=142.4874 MAE=142.4861 MAE=142.4907 MAE=142.5451 MAE=142.4922 MAE=142.6201 MAE=142.4959 MAE=142.4956 MAE=142.4986 Epoch: 260/500MAE=142.4975 MAE=142.5543 MAE=142.4995 MAE=142.4992 MAE=142.4990 MAE=142.5034 MAE=142.5027 MAE=142.5721 MAE=142.5752 MAE=142.6248 Epoch: 270/500MAE=142.4967 MAE=142.4949 MAE=142.4959 MAE=146.9927 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 146.189 +/- 1.077\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.6725 MAE=1547.7017 MAE=1544.7715 MAE=1542.0786 MAE=1539.5242 MAE=1536.5022 MAE=1533.2051 MAE=1529.6461 MAE=1525.2958 Epoch: 10/500MAE=1521.0381 MAE=1516.2517 MAE=1511.6245 MAE=1506.9568 MAE=1500.3260 MAE=1494.1117 MAE=1486.9600 MAE=1479.8738 MAE=1471.0935 MAE=1462.1593 Epoch: 20/500MAE=1455.2018 MAE=1445.1105 MAE=1436.5686 MAE=1426.4851 MAE=1419.9203 MAE=1408.3447 MAE=1396.4148 MAE=1384.2296 MAE=1373.7285 MAE=1363.4150 Epoch: 30/500MAE=1348.6687 MAE=1338.5303 MAE=1324.6398 MAE=1314.2161 MAE=1299.1851 MAE=1280.6293 MAE=1268.1425 MAE=1252.3960 MAE=1237.3604 MAE=1215.9988 Epoch: 40/500MAE=1199.1248 MAE=1183.8671 MAE=1169.0537 MAE=1153.1354 MAE=1130.2690 MAE=1112.0247 MAE=1095.0651 MAE=1072.2983 MAE=1056.3536 MAE=1036.7607 Epoch: 50/500MAE=1016.7693 MAE=998.7756 MAE=979.5234 MAE=953.7726 MAE=935.6860 MAE=908.8910 MAE=895.2578 MAE=865.9794 MAE=842.7262 MAE=828.0005 Epoch: 60/500MAE=800.6052 MAE=777.9552 MAE=754.6523 MAE=731.5101 MAE=715.7272 MAE=689.8414 MAE=664.4452 MAE=643.4106 MAE=629.2057 MAE=611.1785 Epoch: 70/500MAE=594.6670 MAE=557.3999 MAE=547.8931 MAE=536.8704 MAE=516.2664 MAE=484.7093 MAE=452.0087 MAE=430.4377 MAE=408.7826 MAE=392.5240 Epoch: 80/500MAE=368.2551 MAE=351.9404 MAE=337.8307 MAE=302.9946 MAE=282.0692 MAE=279.9655 MAE=259.3994 MAE=253.9973 MAE=228.0984 MAE=214.0157 Epoch: 90/500MAE=215.0458 MAE=206.1734 MAE=190.8674 MAE=190.1771 MAE=181.8103 MAE=178.4986 MAE=180.9545 MAE=160.8220 MAE=158.7068 MAE=165.1079 Epoch: 100/500MAE=155.6945 MAE=154.3108 MAE=150.8867 MAE=151.7171 MAE=147.7744 MAE=145.1186 MAE=148.8158 MAE=146.3643 MAE=145.2867 MAE=151.5500 Epoch: 110/500MAE=146.9659 MAE=147.8221 MAE=146.1071 MAE=146.4108 MAE=146.9462 MAE=146.9969 MAE=146.6915 MAE=147.1005 MAE=146.8895 MAE=147.5493 Epoch: 120/500MAE=146.8550 MAE=146.8454 MAE=146.7979 MAE=146.9786 MAE=146.5730 MAE=146.7339 MAE=146.7550 MAE=146.8237 MAE=146.8855 MAE=146.7781 Epoch: 130/500MAE=146.6379 MAE=146.7773 MAE=146.6380 MAE=146.6617 MAE=146.6320 MAE=146.6707 MAE=146.7055 MAE=146.6456 MAE=146.8212 MAE=146.6728 Epoch: 140/500MAE=146.7223 MAE=146.7977 MAE=146.7967 MAE=146.6539 MAE=146.7999 MAE=146.7338 MAE=146.6645 MAE=146.6673 MAE=146.7311 MAE=146.6660 Epoch: 150/500MAE=146.6692 MAE=146.7371 MAE=146.7398 MAE=146.6720 MAE=146.7395 MAE=146.6715 MAE=146.8148 MAE=146.6708 MAE=146.8177 MAE=146.6629 Epoch: 160/500MAE=146.8133 MAE=146.7327 MAE=146.8112 MAE=146.6648 MAE=146.6618 MAE=146.7421 MAE=146.6600 MAE=146.7237 MAE=146.6573 MAE=146.6584 Epoch: 170/500MAE=146.7263 MAE=146.6579 MAE=146.7204 MAE=146.6545 MAE=146.7184 MAE=146.6551 MAE=146.6584 MAE=146.6566 MAE=146.6537 MAE=146.6536 Epoch: 180/500MAE=146.7223 MAE=146.7176 MAE=146.7143 MAE=146.8034 MAE=146.6450 MAE=146.6448 MAE=146.7225 MAE=146.7138 MAE=146.7136 MAE=146.7091 Epoch: 190/500MAE=146.7889 MAE=146.7878 MAE=146.7219 MAE=146.6443 MAE=146.7925 MAE=146.7110 MAE=146.6383 MAE=146.6386 MAE=146.6382 MAE=146.6411 Epoch: 200/500MAE=146.6496 MAE=146.7986 MAE=146.6425 MAE=146.6412 MAE=146.7157 MAE=146.6494 MAE=146.7968 MAE=146.7173 MAE=146.7926 MAE=146.6482 Epoch: 210/500MAE=146.7975 MAE=146.6488 MAE=146.7970 MAE=146.7196 MAE=146.7183 MAE=146.6481 MAE=146.7148 MAE=146.7141 MAE=146.6478 MAE=146.6460 Epoch: 220/500MAE=146.6468 MAE=146.7994 MAE=146.6455 MAE=146.7995 MAE=146.6444 MAE=146.6432 MAE=146.7248 MAE=146.6438 MAE=146.6441 MAE=146.6457 Epoch: 230/500MAE=146.6435 MAE=146.6466 MAE=146.7129 MAE=146.7956 MAE=146.7144 MAE=146.6422 MAE=146.7222 MAE=146.6419 MAE=146.7040 MAE=146.6358 Epoch: 240/500MAE=146.6335 MAE=146.6357 MAE=146.7048 MAE=146.7032 MAE=146.6326 MAE=146.7015 MAE=146.7083 MAE=146.7069 MAE=146.6375 MAE=146.7000 Epoch: 250/500MAE=146.6316 MAE=146.7017 MAE=146.6393 MAE=146.7068 MAE=146.6409 MAE=146.7121 MAE=147.4844 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 146.513 +/- 1.088\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.6597 MAE=1547.6189 MAE=1545.3894 MAE=1543.0171 MAE=1540.3457 MAE=1537.6130 MAE=1534.3514 MAE=1530.6661 MAE=1526.5879 Epoch: 10/500MAE=1521.9928 MAE=1518.0818 MAE=1513.2250 MAE=1507.8809 MAE=1501.8997 MAE=1496.3425 MAE=1489.4094 MAE=1482.2433 MAE=1476.3108 MAE=1467.6277 Epoch: 20/500MAE=1459.6359 MAE=1451.6362 MAE=1441.7966 MAE=1431.8755 MAE=1424.0085 MAE=1413.0037 MAE=1400.5447 MAE=1390.7520 MAE=1380.0807 MAE=1369.6404 Epoch: 30/500MAE=1357.7913 MAE=1343.5854 MAE=1331.1656 MAE=1319.0900 MAE=1306.3545 MAE=1292.0861 MAE=1276.6459 MAE=1262.6106 MAE=1246.7224 MAE=1222.3732 Epoch: 40/500MAE=1209.6527 MAE=1197.8521 MAE=1182.1327 MAE=1162.2285 MAE=1143.4557 MAE=1119.2393 MAE=1107.7793 MAE=1084.7726 MAE=1068.8657 MAE=1046.8386 Epoch: 50/500MAE=1020.1961 MAE=1003.7770 MAE=983.7183 MAE=959.8005 MAE=937.2889 MAE=916.9593 MAE=891.0723 MAE=873.8250 MAE=854.5501 MAE=829.3828 Epoch: 60/500MAE=808.5653 MAE=778.9899 MAE=756.5432 MAE=730.9534 MAE=709.5838 MAE=686.3301 MAE=656.4869 MAE=634.2529 MAE=617.1717 MAE=585.6183 Epoch: 70/500MAE=569.3551 MAE=542.1151 MAE=526.0543 MAE=507.6790 MAE=477.6490 MAE=456.5734 MAE=434.3333 MAE=415.9960 MAE=393.2002 MAE=390.3540 Epoch: 80/500MAE=365.9034 MAE=327.6873 MAE=318.1814 MAE=299.3533 MAE=284.8786 MAE=263.8975 MAE=258.6449 MAE=245.2772 MAE=234.9541 MAE=224.4203 Epoch: 90/500MAE=231.5576 MAE=194.4422 MAE=195.5031 MAE=181.8282 MAE=181.2897 MAE=168.4438 MAE=164.2927 MAE=155.6930 MAE=160.2081 MAE=156.6016 Epoch: 100/500MAE=152.7653 MAE=150.6909 MAE=150.2382 MAE=152.4890 MAE=151.2269 MAE=147.2685 MAE=145.0155 MAE=148.4783 MAE=146.4043 MAE=145.9749 Epoch: 110/500MAE=143.1732 MAE=147.3986 MAE=147.2446 MAE=145.2017 MAE=147.6445 MAE=143.8365 MAE=143.8354 MAE=144.6818 MAE=143.7873 MAE=143.9772 Epoch: 120/500MAE=144.8556 MAE=144.0843 MAE=143.1335 MAE=143.7817 MAE=143.4493 MAE=143.3602 MAE=144.2313 MAE=143.9986 MAE=143.9140 MAE=143.9774 Epoch: 130/500MAE=144.4090 MAE=144.0896 MAE=144.2775 MAE=143.8875 MAE=144.0379 MAE=144.2279 MAE=144.3033 MAE=144.2578 MAE=144.2155 MAE=144.2638 Epoch: 140/500MAE=144.2420 MAE=144.1566 MAE=144.1041 MAE=144.1351 MAE=144.1438 MAE=144.1675 MAE=144.1824 MAE=144.1890 MAE=144.1765 MAE=144.1760 Epoch: 150/500MAE=144.1697 MAE=144.1701 MAE=144.1717 MAE=144.1813 MAE=144.1899 MAE=144.1905 MAE=144.1879 MAE=144.1917 MAE=144.1921 MAE=144.1937 Epoch: 160/500MAE=144.1985 MAE=144.1954 MAE=144.1957 MAE=144.1948 MAE=144.1990 MAE=144.1927 MAE=144.1909 MAE=144.1897 MAE=144.1927 MAE=144.2001 Epoch: 170/500MAE=144.2062 MAE=144.2102 MAE=144.2114 MAE=144.2115 MAE=144.2127 MAE=144.2938 MAE=144.2928 MAE=144.2081 MAE=144.2095 MAE=144.2130 Epoch: 180/500MAE=144.3020 MAE=144.3051 MAE=144.2229 MAE=144.2290 MAE=144.2310 MAE=144.2343 MAE=144.3220 MAE=144.3209 MAE=144.2405 MAE=144.3219 Epoch: 190/500MAE=144.3209 MAE=144.2399 MAE=144.2425 MAE=144.2448 MAE=144.2410 MAE=144.3248 MAE=144.2428 MAE=144.3222 MAE=144.3135 MAE=144.3121 Epoch: 200/500MAE=144.3137 MAE=144.2287 MAE=144.3108 MAE=144.2343 MAE=144.3116 MAE=144.3143 MAE=144.3219 MAE=144.3185 MAE=144.2412 MAE=144.3216 Epoch: 210/500MAE=144.3167 MAE=144.3202 MAE=144.3301 MAE=144.3324 MAE=144.3342 MAE=144.3283 MAE=144.3271 MAE=144.3306 MAE=144.3298 MAE=144.3248 Epoch: 220/500MAE=144.3213 MAE=144.3210 MAE=144.3217 MAE=144.2368 MAE=144.3151 MAE=144.2986 MAE=144.3122 MAE=144.3155 MAE=144.3176 MAE=144.3239 Epoch: 230/500MAE=144.3278 MAE=144.3169 MAE=144.3119 MAE=144.3177 MAE=144.3151 MAE=144.3178 MAE=144.3129 MAE=144.3165 MAE=144.3340 MAE=144.3194 Epoch: 240/500MAE=144.3242 MAE=144.3478 MAE=144.3471 MAE=144.3200 MAE=144.3200 MAE=144.3221 MAE=144.3179 MAE=144.3145 MAE=144.3431 MAE=144.3445 Epoch: 250/500MAE=144.3155 MAE=144.3472 MAE=144.3237 MAE=144.3231 MAE=144.3209 MAE=144.3211 MAE=144.3436 MAE=144.3433 MAE=144.3445 MAE=144.3452 Epoch: 260/500MAE=144.3185 MAE=144.3172 MAE=144.3186 MAE=144.3188 MAE=144.2947 MAE=144.3199 MAE=144.3227 MAE=144.3262 MAE=144.3463 MAE=144.3481 Epoch: 270/500MAE=144.3275 MAE=144.3313 MAE=144.3379 MAE=148.9408 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 146.999 +/- 1.375\n",
                        "\n",
                        "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/500MAE=1549.7041 MAE=1547.6503 MAE=1544.9675 MAE=1542.5037 MAE=1539.7524 MAE=1537.2094 MAE=1534.0073 MAE=1530.2415 MAE=1526.0391 Epoch: 10/500MAE=1521.5889 MAE=1517.1984 MAE=1512.3088 MAE=1507.6580 MAE=1499.2712 MAE=1492.8860 MAE=1485.0657 MAE=1475.2930 MAE=1471.7419 MAE=1461.3563 Epoch: 20/500MAE=1450.4430 MAE=1439.3501 MAE=1435.0869 MAE=1424.1578 MAE=1409.7035 MAE=1403.8911 MAE=1391.3324 MAE=1383.4353 MAE=1363.4930 MAE=1358.4717 Epoch: 30/500MAE=1348.1567 MAE=1335.1973 MAE=1324.8070 MAE=1311.4805 MAE=1292.4440 MAE=1275.6575 MAE=1268.0627 MAE=1248.1978 MAE=1233.5419 MAE=1227.9683 Epoch: 40/500MAE=1198.7960 MAE=1187.7836 MAE=1165.6233 MAE=1163.5176 MAE=1128.6357 MAE=1105.3425 MAE=1089.9443 MAE=1080.5354 MAE=1062.9449 MAE=1029.2859 Epoch: 50/500MAE=1012.5382 MAE=996.1334 MAE=989.2988 MAE=950.0574 MAE=932.3923 MAE=907.8840 MAE=894.9083 MAE=874.3600 MAE=835.0623 MAE=825.1254 Epoch: 60/500MAE=801.9864 MAE=768.0751 MAE=751.4254 MAE=725.8554 MAE=692.6243 MAE=687.7693 MAE=653.4218 MAE=599.4812 MAE=604.5106 MAE=563.2019 Epoch: 70/500MAE=552.2093 MAE=494.7734 MAE=514.0097 MAE=467.6218 MAE=348.5341 MAE=395.6891 MAE=399.4294 MAE=407.1846 MAE=350.0846 MAE=335.2494 Epoch: 80/500MAE=319.7251 MAE=301.7011 MAE=294.2070 MAE=287.3998 MAE=264.2914 MAE=275.6633 MAE=249.9785 MAE=247.6322 MAE=244.0633 MAE=226.0952 Epoch: 90/500MAE=242.3236 MAE=197.3733 MAE=214.1351 MAE=181.3567 MAE=170.6054 MAE=185.1692 MAE=183.3485 MAE=153.1647 MAE=163.3418 MAE=150.9417 Epoch: 100/500MAE=169.6827 MAE=136.6969 MAE=147.3563 MAE=138.2810 MAE=127.7088 MAE=128.3290 MAE=139.4665 MAE=126.6187 MAE=125.5029 MAE=119.3977 Epoch: 110/500MAE=117.7894 MAE=124.4750 MAE=117.9380 MAE=119.1067 MAE=113.7676 MAE=118.3846 MAE=114.0329 MAE=119.2151 MAE=116.8276 MAE=115.8955 Epoch: 120/500MAE=112.7312 MAE=115.8645 MAE=115.0202 MAE=113.0521 MAE=115.9836 MAE=115.6043 MAE=114.4448 MAE=114.6769 MAE=115.4387 MAE=114.4157 Epoch: 130/500MAE=113.8212 MAE=114.2515 MAE=113.7456 MAE=113.7211 MAE=113.8038 MAE=114.0448 MAE=114.0732 MAE=114.1217 MAE=113.8998 MAE=113.9947 Epoch: 140/500MAE=114.1515 MAE=113.9892 MAE=113.8186 MAE=113.9052 MAE=113.8737 MAE=113.9793 MAE=113.7767 MAE=113.8877 MAE=113.7207 MAE=113.7700 Epoch: 150/500MAE=113.6914 MAE=113.8138 MAE=114.0319 MAE=113.8901 MAE=113.9949 MAE=113.8033 MAE=113.8767 MAE=113.8997 MAE=113.8548 MAE=113.8071 Epoch: 160/500MAE=113.9082 MAE=113.8467 MAE=113.8605 MAE=113.9137 MAE=113.8375 MAE=113.8762 MAE=114.0014 MAE=113.9538 MAE=113.9247 MAE=113.8892 Epoch: 170/500MAE=113.9337 MAE=114.0706 MAE=113.8868 MAE=113.8952 MAE=113.9284 MAE=113.8109 MAE=113.8335 MAE=113.8348 MAE=113.9505 MAE=113.9075 Epoch: 180/500MAE=113.8659 MAE=113.9590 MAE=113.8296 MAE=114.0233 MAE=113.8857 MAE=113.8354 MAE=113.9091 MAE=113.7868 MAE=113.8563 MAE=113.9307 Epoch: 190/500MAE=113.9654 MAE=113.7906 MAE=113.9985 MAE=113.8728 MAE=113.8782 MAE=113.7721 MAE=113.8509 MAE=113.7438 MAE=113.8751 MAE=113.8415 Epoch: 200/500MAE=113.7267 MAE=113.7479 MAE=113.8916 MAE=113.8224 MAE=113.7229 MAE=113.7433 MAE=113.7520 MAE=113.7803 MAE=113.8206 MAE=113.7094 Epoch: 210/500MAE=113.7299 MAE=113.7202 MAE=113.7982 MAE=113.5798 MAE=113.5893 MAE=113.7059 MAE=113.6512 MAE=113.6957 MAE=113.6885 MAE=113.6236 Epoch: 220/500MAE=113.6288 MAE=113.6676 MAE=113.6337 MAE=113.6756 MAE=113.6942 MAE=113.6853 MAE=113.6033 MAE=113.6554 MAE=113.6427 MAE=113.6635 Epoch: 230/500MAE=113.7342 MAE=113.5746 MAE=113.5908 MAE=113.7109 MAE=113.6679 MAE=113.6864 MAE=113.6718 MAE=113.5754 MAE=113.6202 MAE=113.5648 Epoch: 240/500MAE=113.5860 MAE=113.6700 MAE=113.7298 MAE=113.7539 MAE=113.5933 MAE=113.6319 MAE=113.6928 MAE=113.5776 MAE=113.5401 MAE=113.5752 Epoch: 250/500MAE=113.5281 MAE=113.5465 MAE=113.6294 MAE=113.5298 MAE=113.5537 MAE=113.5095 MAE=113.5769 MAE=113.5135 MAE=113.8184 MAE=113.5335 Epoch: 260/500MAE=113.5726 MAE=113.5417 MAE=113.5962 MAE=113.5599 MAE=113.5116 MAE=113.5605 MAE=113.5903 MAE=113.4784 MAE=113.6532 MAE=113.4990 Epoch: 270/500MAE=113.7050 MAE=114.8809 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 114.881 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.6558 MAE=1547.4688 MAE=1544.7986 MAE=1542.5686 MAE=1539.9631 MAE=1537.3328 MAE=1534.6145 MAE=1531.1399 MAE=1526.9121 Epoch: 10/500MAE=1522.2859 MAE=1517.6748 MAE=1511.9062 MAE=1506.9092 MAE=1500.1119 MAE=1492.5054 MAE=1486.0510 MAE=1479.4027 MAE=1472.4528 MAE=1461.5833 Epoch: 20/500MAE=1454.0375 MAE=1445.7183 MAE=1434.7382 MAE=1421.5183 MAE=1415.8717 MAE=1406.4434 MAE=1392.6796 MAE=1380.5046 MAE=1373.2317 MAE=1353.0792 Epoch: 30/500MAE=1346.6323 MAE=1332.3687 MAE=1314.9354 MAE=1317.0608 MAE=1287.8778 MAE=1280.7878 MAE=1260.6194 MAE=1250.1526 MAE=1229.5205 MAE=1213.4497 Epoch: 40/500MAE=1199.4641 MAE=1173.6523 MAE=1159.3239 MAE=1148.8794 MAE=1140.6332 MAE=1120.0378 MAE=1091.1281 MAE=1077.6277 MAE=1061.5679 MAE=1030.6228 Epoch: 50/500MAE=1016.1840 MAE=1003.5781 MAE=976.5583 MAE=945.1150 MAE=934.3611 MAE=909.0179 MAE=892.9720 MAE=861.8372 MAE=830.2152 MAE=811.2584 Epoch: 60/500MAE=788.1943 MAE=771.9344 MAE=751.7896 MAE=724.7853 MAE=683.1545 MAE=684.9942 MAE=655.0032 MAE=633.9648 MAE=600.5098 MAE=588.5960 Epoch: 70/500MAE=542.6094 MAE=533.0312 MAE=520.0273 MAE=489.2560 MAE=471.1777 MAE=448.5920 MAE=377.1956 MAE=385.9592 MAE=379.2917 MAE=307.3336 Epoch: 80/500MAE=367.7877 MAE=272.3665 MAE=241.1332 MAE=254.6907 MAE=248.3801 MAE=264.8165 MAE=184.9402 MAE=200.5281 MAE=146.2506 MAE=143.4832 Epoch: 90/500MAE=153.1579 MAE=139.6844 MAE=131.6155 MAE=150.8385 MAE=124.4199 MAE=123.6466 MAE=123.8307 MAE=120.6901 MAE=120.5673 MAE=125.5207 Epoch: 100/500MAE=117.4791 MAE=113.8672 MAE=113.9072 MAE=119.1212 MAE=116.0624 MAE=117.4147 MAE=115.2278 MAE=115.7739 MAE=114.0455 MAE=113.0296 Epoch: 110/500MAE=112.8136 MAE=113.3226 MAE=114.6134 MAE=112.5529 MAE=116.6529 MAE=114.2439 MAE=113.7932 MAE=113.1578 MAE=112.5836 MAE=113.8467 Epoch: 120/500MAE=113.3780 MAE=113.9229 MAE=113.0549 MAE=113.5354 MAE=113.2100 MAE=112.7969 MAE=113.1424 MAE=113.5205 MAE=112.9658 MAE=112.6224 Epoch: 130/500MAE=113.2347 MAE=113.3286 MAE=113.3480 MAE=113.3072 MAE=113.3920 MAE=113.4514 MAE=113.3624 MAE=113.2772 MAE=113.2525 MAE=113.2744 Epoch: 140/500MAE=113.2996 MAE=113.2093 MAE=113.2768 MAE=113.2960 MAE=113.2803 MAE=113.2996 MAE=113.3899 MAE=113.2757 MAE=113.2788 MAE=113.2912 Epoch: 150/500MAE=113.2872 MAE=113.2936 MAE=113.3359 MAE=113.2999 MAE=113.3889 MAE=113.3015 MAE=113.2785 MAE=113.2955 MAE=113.3006 MAE=113.2932 Epoch: 160/500MAE=113.2908 MAE=113.2861 MAE=113.2632 MAE=113.2542 MAE=113.2445 MAE=113.2808 MAE=113.2741 MAE=113.2570 MAE=113.2236 MAE=113.2647 Epoch: 170/500MAE=113.3228 MAE=113.3086 MAE=113.3079 MAE=113.3156 MAE=113.1966 MAE=113.2826 MAE=113.3165 MAE=113.2441 MAE=113.2397 MAE=113.2365 Epoch: 180/500MAE=113.2320 MAE=113.2373 MAE=113.1992 MAE=113.3150 MAE=113.2148 MAE=113.2400 MAE=113.2379 MAE=113.2351 MAE=113.2312 MAE=113.2397 Epoch: 190/500MAE=113.2379 MAE=113.2388 MAE=113.2366 MAE=113.2455 MAE=113.2402 MAE=113.2530 MAE=113.2564 MAE=113.2540 MAE=113.2567 MAE=113.2547 Epoch: 200/500MAE=113.2498 MAE=113.2540 MAE=113.2253 MAE=113.2522 MAE=113.3116 MAE=113.3046 MAE=113.3141 MAE=113.3221 MAE=113.3189 MAE=113.3146 Epoch: 210/500MAE=113.3655 MAE=113.2434 MAE=113.2458 MAE=113.3179 MAE=113.2524 MAE=113.3186 MAE=113.3137 MAE=113.3165 MAE=113.3161 MAE=113.3158 Epoch: 220/500MAE=113.2465 MAE=113.3159 MAE=113.2217 MAE=113.3143 MAE=113.3047 MAE=113.3022 MAE=113.2997 MAE=113.2771 MAE=113.2012 MAE=113.2534 Epoch: 230/500MAE=113.2515 MAE=113.2255 MAE=113.2295 MAE=113.2466 MAE=113.2160 MAE=113.2962 MAE=113.2976 MAE=113.2480 MAE=113.2769 MAE=113.2833 Epoch: 240/500MAE=113.2504 MAE=113.2828 MAE=113.2009 MAE=113.2404 MAE=113.2688 MAE=113.2635 MAE=113.2377 MAE=113.2754 MAE=113.2250 MAE=113.2464 Epoch: 250/500MAE=113.2468 MAE=113.2351 MAE=113.2439 MAE=113.2617 MAE=113.2248 MAE=113.2669 MAE=113.2656 MAE=113.2389 MAE=113.2347 MAE=113.1690 Epoch: 260/500MAE=113.2112 MAE=113.2402 MAE=113.2367 MAE=113.2407 MAE=109.0923 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 111.987 +/- 2.894\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.8406 MAE=1548.0737 MAE=1545.5950 MAE=1542.9023 MAE=1540.3223 MAE=1537.4077 MAE=1534.2792 MAE=1530.8903 MAE=1526.7889 Epoch: 10/500MAE=1522.3042 MAE=1517.0688 MAE=1512.5294 MAE=1505.0024 MAE=1500.9133 MAE=1493.7719 MAE=1484.9207 MAE=1474.8920 MAE=1467.3975 MAE=1461.5115 Epoch: 20/500MAE=1452.7229 MAE=1438.9751 MAE=1434.9355 MAE=1429.0225 MAE=1415.3560 MAE=1403.7524 MAE=1397.4034 MAE=1389.6268 MAE=1376.5731 MAE=1356.1255 Epoch: 30/500MAE=1351.6876 MAE=1331.9407 MAE=1318.0542 MAE=1307.3042 MAE=1295.8740 MAE=1264.7278 MAE=1265.6956 MAE=1254.7388 MAE=1238.7683 MAE=1206.4670 Epoch: 40/500MAE=1199.1335 MAE=1192.4525 MAE=1180.1738 MAE=1143.8350 MAE=1133.7238 MAE=1117.5123 MAE=1091.4092 MAE=1077.4475 MAE=1054.2190 MAE=1037.3677 Epoch: 50/500MAE=1015.8882 MAE=1007.9313 MAE=970.7637 MAE=949.8817 MAE=938.5264 MAE=905.6559 MAE=788.3160 MAE=897.2838 MAE=837.3462 MAE=821.6913 Epoch: 60/500MAE=794.0996 MAE=771.7682 MAE=769.7131 MAE=782.1340 MAE=745.4504 MAE=734.7701 MAE=724.0087 MAE=712.1592 MAE=697.2660 MAE=688.7369 Epoch: 70/500MAE=672.5443 MAE=653.4169 MAE=674.1593 MAE=656.9949 MAE=615.2325 MAE=620.6307 MAE=608.2702 MAE=578.1824 MAE=579.8177 MAE=580.5807 Epoch: 80/500MAE=565.2281 MAE=543.9960 MAE=529.2994 MAE=528.9668 MAE=504.2791 MAE=527.8226 MAE=472.5324 MAE=473.5626 MAE=473.0760 MAE=455.9766 Epoch: 90/500MAE=441.9273 MAE=435.0328 MAE=416.7512 MAE=403.0287 MAE=401.6547 MAE=381.5661 MAE=404.9488 MAE=370.2306 MAE=324.5685 MAE=354.7119 Epoch: 100/500MAE=337.9361 MAE=336.5558 MAE=326.3884 MAE=308.8422 MAE=300.7785 MAE=306.8972 MAE=309.9620 MAE=295.2518 MAE=298.2165 MAE=282.9089 Epoch: 110/500MAE=277.0662 MAE=291.8593 MAE=272.5228 MAE=270.5766 MAE=279.3262 MAE=254.4019 MAE=246.6728 MAE=249.3302 MAE=236.8238 MAE=238.1154 Epoch: 120/500MAE=240.2471 MAE=232.6004 MAE=238.0508 MAE=227.4512 MAE=219.3090 MAE=219.5558 MAE=215.4595 MAE=214.8329 MAE=204.6532 MAE=200.1953 Epoch: 130/500MAE=200.2315 MAE=204.0685 MAE=197.3458 MAE=196.9233 MAE=184.7962 MAE=181.9709 MAE=179.3663 MAE=183.6259 MAE=173.2735 MAE=166.9599 Epoch: 140/500MAE=169.1663 MAE=164.9511 MAE=169.1309 MAE=153.4421 MAE=156.6957 MAE=155.8330 MAE=155.4286 MAE=151.2024 MAE=150.0230 MAE=147.5744 Epoch: 150/500MAE=148.7097 MAE=145.5166 MAE=140.3521 MAE=136.1919 MAE=143.5726 MAE=133.6320 MAE=129.4200 MAE=133.0730 MAE=127.8710 MAE=134.3746 Epoch: 160/500MAE=125.0685 MAE=132.7107 MAE=124.3970 MAE=125.6017 MAE=124.7412 MAE=121.7878 MAE=118.7142 MAE=123.8566 MAE=117.5062 MAE=121.5964 Epoch: 170/500MAE=119.7371 MAE=118.0989 MAE=118.4540 MAE=120.5014 MAE=121.8193 MAE=120.4689 MAE=117.6848 MAE=119.2098 MAE=119.7892 MAE=119.2627 Epoch: 180/500MAE=120.2537 MAE=119.0425 MAE=119.2251 MAE=119.6201 MAE=119.4350 MAE=118.9670 MAE=119.1448 MAE=119.2569 MAE=118.8209 MAE=118.8597 Epoch: 190/500MAE=119.1283 MAE=119.2788 MAE=119.1846 MAE=119.1216 MAE=118.9243 MAE=119.0807 MAE=119.2261 MAE=119.0874 MAE=119.1827 MAE=119.0669 Epoch: 200/500MAE=119.0878 MAE=119.1199 MAE=119.0667 MAE=118.9503 MAE=118.8819 MAE=119.0555 MAE=119.0936 MAE=119.1148 MAE=119.1113 MAE=119.1382 Epoch: 210/500MAE=119.0284 MAE=119.1325 MAE=119.0708 MAE=119.0667 MAE=119.0359 MAE=118.9357 MAE=118.9292 MAE=118.8433 MAE=119.1536 MAE=118.8844 Epoch: 220/500MAE=118.9971 MAE=118.9547 MAE=118.8384 MAE=119.0692 MAE=119.0603 MAE=119.1863 MAE=118.8929 MAE=118.9183 MAE=119.0695 MAE=119.1470 Epoch: 230/500MAE=119.1474 MAE=119.1649 MAE=118.8900 MAE=118.9912 MAE=119.0400 MAE=119.0866 MAE=118.9225 MAE=119.0309 MAE=119.0304 MAE=119.0068 Epoch: 240/500MAE=119.0327 MAE=119.0838 MAE=118.9632 MAE=118.6426 MAE=118.8386 MAE=118.9560 MAE=118.8778 MAE=118.9308 MAE=119.0623 MAE=118.7713 Epoch: 250/500MAE=118.9782 MAE=118.9295 MAE=119.0772 MAE=119.0361 MAE=118.8979 MAE=119.0516 MAE=119.0029 MAE=119.1042 MAE=118.9569 MAE=119.0457 Epoch: 260/500MAE=119.0860 MAE=119.0798 MAE=118.9753 MAE=118.9607 MAE=118.9809 MAE=119.1537 MAE=118.9778 MAE=119.0703 MAE=119.0775 MAE=119.0687 Epoch: 270/500MAE=118.9718 MAE=119.0514 MAE=119.0464 MAE=119.0300 MAE=118.8651 MAE=118.9436 MAE=119.0564 MAE=118.9396 MAE=119.0327 MAE=118.9382 Epoch: 280/500MAE=118.9240 MAE=118.9336 MAE=118.9180 MAE=118.8597 MAE=118.9259 MAE=119.0134 MAE=118.9446 MAE=119.0862 MAE=118.8397 MAE=118.7435 Epoch: 290/500MAE=118.9235 MAE=118.9432 MAE=119.0286 MAE=118.9221 MAE=119.0315 MAE=119.0276 MAE=118.9910 MAE=118.9947 MAE=118.8959 MAE=118.7942 Epoch: 300/500MAE=118.8695 MAE=118.8737 MAE=118.8297 MAE=118.7066 MAE=118.7800 MAE=118.7196 MAE=118.8250 MAE=118.7312 MAE=118.8003 MAE=118.8317 Epoch: 310/500MAE=118.5323 MAE=118.9095 MAE=118.8960 MAE=118.6854 MAE=118.8315 MAE=118.7725 MAE=118.6666 MAE=118.8577 MAE=118.7644 MAE=122.4413 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 115.471 +/- 5.466\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.5839 MAE=1547.3682 MAE=1544.5076 MAE=1541.9727 MAE=1539.3242 MAE=1536.4585 MAE=1532.9080 MAE=1529.7458 MAE=1526.0012 Epoch: 10/500MAE=1521.5837 MAE=1517.3712 MAE=1511.7974 MAE=1505.4338 MAE=1498.9408 MAE=1492.6655 MAE=1485.7618 MAE=1475.5967 MAE=1466.2377 MAE=1459.9509 Epoch: 20/500MAE=1453.0366 MAE=1444.1296 MAE=1434.3850 MAE=1426.0886 MAE=1414.7644 MAE=1404.9885 MAE=1388.3401 MAE=1381.2640 MAE=1365.1346 MAE=1355.5778 Epoch: 30/500MAE=1351.0240 MAE=1326.4338 MAE=1321.0515 MAE=1307.9277 MAE=1291.4843 MAE=1270.8430 MAE=1251.3232 MAE=1240.8242 MAE=1223.9822 MAE=1216.7728 Epoch: 40/500MAE=1193.1333 MAE=1169.8676 MAE=1156.8014 MAE=1135.4849 MAE=1118.2670 MAE=1105.4203 MAE=1083.3439 MAE=1061.1724 MAE=1043.1698 MAE=1026.5828 Epoch: 50/500MAE=1000.6963 MAE=979.2222 MAE=966.1021 MAE=943.9895 MAE=921.6985 MAE=895.1174 MAE=869.5118 MAE=849.3670 MAE=833.9036 MAE=807.6094 Epoch: 60/500MAE=783.5878 MAE=754.5497 MAE=723.3439 MAE=720.3723 MAE=694.7008 MAE=671.8236 MAE=662.2769 MAE=616.4933 MAE=574.9828 MAE=577.2172 Epoch: 70/500MAE=541.4070 MAE=522.4876 MAE=479.3148 MAE=473.3484 MAE=466.3835 MAE=411.7960 MAE=378.8111 MAE=363.1763 MAE=330.8072 MAE=327.2021 Epoch: 80/500MAE=285.5408 MAE=257.4202 MAE=246.3329 MAE=226.6890 MAE=213.7625 MAE=246.2411 MAE=210.0418 MAE=209.0471 MAE=170.5315 MAE=151.3587 Epoch: 90/500MAE=217.0971 MAE=131.8137 MAE=124.5206 MAE=125.9900 MAE=124.1566 MAE=120.4033 MAE=121.6531 MAE=120.9576 MAE=116.4023 MAE=119.8653 Epoch: 100/500MAE=115.2103 MAE=114.3638 MAE=113.0209 MAE=114.2727 MAE=113.8610 MAE=117.6601 MAE=115.4864 MAE=113.7496 MAE=112.4206 MAE=112.0730 Epoch: 110/500MAE=111.2211 MAE=112.4521 MAE=112.5321 MAE=111.7401 MAE=112.1779 MAE=111.4506 MAE=112.1558 MAE=110.3106 MAE=110.3524 MAE=111.2276 Epoch: 120/500MAE=112.1163 MAE=112.6256 MAE=111.7677 MAE=111.8362 MAE=111.4627 MAE=112.0817 MAE=111.6095 MAE=111.3771 MAE=112.0451 MAE=111.7295 Epoch: 130/500MAE=111.4790 MAE=111.7043 MAE=111.4862 MAE=111.3477 MAE=111.2698 MAE=111.2988 MAE=111.4143 MAE=111.3636 MAE=111.3438 MAE=111.3620 Epoch: 140/500MAE=111.4880 MAE=111.4894 MAE=111.4967 MAE=111.5007 MAE=111.5049 MAE=111.5085 MAE=111.5015 MAE=111.5062 MAE=111.4419 MAE=111.5466 Epoch: 150/500MAE=111.4858 MAE=111.4841 MAE=111.4868 MAE=111.4313 MAE=111.4383 MAE=111.3738 MAE=111.4883 MAE=111.4243 MAE=111.3712 MAE=111.3775 Epoch: 160/500MAE=111.3808 MAE=111.4553 MAE=111.4553 MAE=111.4930 MAE=111.4387 MAE=111.4422 MAE=111.4384 MAE=111.4379 MAE=111.4317 MAE=111.4324 Epoch: 170/500MAE=111.4024 MAE=111.4252 MAE=111.4328 MAE=111.4347 MAE=111.4348 MAE=111.3125 MAE=111.4117 MAE=111.3160 MAE=111.3858 MAE=111.4361 Epoch: 180/500MAE=111.4330 MAE=111.4372 MAE=111.4426 MAE=111.3931 MAE=111.4296 MAE=111.4359 MAE=111.5154 MAE=111.4094 MAE=111.4071 MAE=111.3965 Epoch: 190/500MAE=111.4027 MAE=111.3997 MAE=111.4267 MAE=111.4298 MAE=111.4296 MAE=111.4257 MAE=111.4725 MAE=111.4245 MAE=111.4664 MAE=111.4246 Epoch: 200/500MAE=111.4206 MAE=111.3604 MAE=111.4015 MAE=111.4773 MAE=111.4782 MAE=111.3999 MAE=111.4365 MAE=111.4361 MAE=111.4273 MAE=111.4464 Epoch: 210/500MAE=111.2826 MAE=111.4287 MAE=111.4224 MAE=111.3932 MAE=111.4321 MAE=111.3971 MAE=111.4798 MAE=111.3915 MAE=111.3898 MAE=111.3904 Epoch: 220/500MAE=111.4269 MAE=111.4313 MAE=111.3879 MAE=111.3900 MAE=111.3938 MAE=111.3894 MAE=111.3885 MAE=111.3897 MAE=111.3959 MAE=111.3921 Epoch: 230/500MAE=111.4563 MAE=111.4223 MAE=111.4265 MAE=111.4656 MAE=111.3873 MAE=111.4829 MAE=111.4989 MAE=111.4143 MAE=111.3936 MAE=111.4374 Epoch: 240/500MAE=111.3854 MAE=111.4534 MAE=111.4194 MAE=111.3840 MAE=111.4067 MAE=111.2840 MAE=111.3292 MAE=111.3937 MAE=111.4180 MAE=111.4696 Epoch: 250/500MAE=111.4352 MAE=111.4362 MAE=111.4331 MAE=111.4361 MAE=111.4325 MAE=111.4030 MAE=111.4357 MAE=111.2996 MAE=111.3793 MAE=111.2871 Epoch: 260/500MAE=111.3999 MAE=111.3143 MAE=111.3192 MAE=111.3162 MAE=111.3176 MAE=111.3145 MAE=111.3189 MAE=111.3150 MAE=109.0158 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 113.858 +/- 5.497\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.2771 MAE=1547.2683 MAE=1545.0400 MAE=1542.5952 MAE=1539.7277 MAE=1537.0544 MAE=1533.9675 MAE=1530.3431 MAE=1526.2986 Epoch: 10/500MAE=1521.5784 MAE=1516.9138 MAE=1511.6675 MAE=1505.6564 MAE=1500.4915 MAE=1494.5037 MAE=1486.9664 MAE=1479.8213 MAE=1470.6277 MAE=1458.0020 Epoch: 20/500MAE=1452.5217 MAE=1440.9508 MAE=1439.5990 MAE=1431.1117 MAE=1421.3088 MAE=1408.7136 MAE=1399.3887 MAE=1382.0457 MAE=1379.4374 MAE=1359.5624 Epoch: 30/500MAE=1345.6333 MAE=1332.2058 MAE=1320.4609 MAE=1306.6648 MAE=1290.9559 MAE=1277.3596 MAE=1261.4277 MAE=1244.3638 MAE=1226.1516 MAE=1207.7860 Epoch: 40/500MAE=1199.4453 MAE=1190.7742 MAE=1175.0758 MAE=1138.8481 MAE=1127.8025 MAE=1108.3499 MAE=1091.4336 MAE=1069.8333 MAE=1057.6493 MAE=1047.9458 Epoch: 50/500MAE=1020.6678 MAE=1008.1283 MAE=977.8011 MAE=959.3918 MAE=927.9084 MAE=924.9966 MAE=886.9872 MAE=873.8297 MAE=840.9128 MAE=826.5515 Epoch: 60/500MAE=783.9451 MAE=766.0240 MAE=743.3470 MAE=720.9484 MAE=703.9158 MAE=663.7333 MAE=666.5891 MAE=592.0078 MAE=612.8925 MAE=580.3700 Epoch: 70/500MAE=551.1375 MAE=522.9034 MAE=477.9237 MAE=436.8160 MAE=416.1858 MAE=410.3714 MAE=378.8097 MAE=350.9616 MAE=310.7261 MAE=294.5062 Epoch: 80/500MAE=268.3268 MAE=249.2805 MAE=239.2417 MAE=216.0585 MAE=209.5330 MAE=175.6260 MAE=190.4962 MAE=162.6355 MAE=144.4782 MAE=165.6622 Epoch: 90/500MAE=138.0793 MAE=123.6908 MAE=136.1236 MAE=124.7218 MAE=118.0586 MAE=115.5713 MAE=118.9072 MAE=115.4635 MAE=118.7214 MAE=112.3638 Epoch: 100/500MAE=113.2805 MAE=110.7522 MAE=114.7983 MAE=113.9064 MAE=118.7253 MAE=111.2432 MAE=110.8495 MAE=109.9733 MAE=108.9552 MAE=109.2702 Epoch: 110/500MAE=109.8510 MAE=110.6073 MAE=107.7465 MAE=110.5446 MAE=109.3485 MAE=111.5563 MAE=109.9012 MAE=109.9494 MAE=109.1394 MAE=109.7170 Epoch: 120/500MAE=109.6146 MAE=109.4531 MAE=110.5516 MAE=110.1813 MAE=109.3217 MAE=109.3649 MAE=109.7088 MAE=109.3655 MAE=109.2202 MAE=109.3988 Epoch: 130/500MAE=109.5801 MAE=109.3312 MAE=109.3458 MAE=109.4381 MAE=109.3487 MAE=109.3403 MAE=109.3707 MAE=109.3789 MAE=109.4111 MAE=109.3930 Epoch: 140/500MAE=109.4639 MAE=109.4469 MAE=109.3978 MAE=109.4996 MAE=109.5137 MAE=109.5105 MAE=109.5344 MAE=109.5423 MAE=109.5238 MAE=109.5663 Epoch: 150/500MAE=109.5251 MAE=109.5205 MAE=109.5290 MAE=109.5134 MAE=109.5747 MAE=109.5245 MAE=109.5305 MAE=109.5751 MAE=109.5310 MAE=109.5445 Epoch: 160/500MAE=109.5366 MAE=109.5645 MAE=109.5614 MAE=109.5607 MAE=109.4582 MAE=109.3907 MAE=109.5148 MAE=109.5894 MAE=109.4031 MAE=109.4586 Epoch: 170/500MAE=109.4581 MAE=109.5744 MAE=109.5896 MAE=109.6403 MAE=109.5979 MAE=109.5302 MAE=109.5935 MAE=109.5750 MAE=109.5759 MAE=109.6186 Epoch: 180/500MAE=109.5940 MAE=109.4743 MAE=109.5615 MAE=109.5658 MAE=109.5294 MAE=109.5297 MAE=109.5671 MAE=109.5665 MAE=109.5702 MAE=109.5634 Epoch: 190/500MAE=109.5721 MAE=109.5421 MAE=109.5673 MAE=109.5278 MAE=109.5299 MAE=109.4647 MAE=109.5662 MAE=109.5665 MAE=109.5625 MAE=109.6144 Epoch: 200/500MAE=109.5550 MAE=109.5170 MAE=109.4518 MAE=109.4828 MAE=109.4908 MAE=109.4642 MAE=109.4899 MAE=109.4658 MAE=109.5387 MAE=109.5239 Epoch: 210/500MAE=109.5392 MAE=109.6533 MAE=109.5225 MAE=109.6174 MAE=109.5667 MAE=109.5246 MAE=109.5312 MAE=109.5936 MAE=109.4258 MAE=109.3910 Epoch: 220/500MAE=109.3895 MAE=109.4874 MAE=109.4850 MAE=109.5180 MAE=109.4376 MAE=109.5034 MAE=109.4378 MAE=109.4860 MAE=109.4020 MAE=109.4184 Epoch: 230/500MAE=109.4914 MAE=109.4732 MAE=109.4838 MAE=109.4843 MAE=109.5157 MAE=109.4986 MAE=109.4931 MAE=109.5303 MAE=109.4845 MAE=109.4931 Epoch: 240/500MAE=109.5519 MAE=109.5079 MAE=109.5024 MAE=109.4947 MAE=109.4582 MAE=109.5034 MAE=109.5080 MAE=109.5145 MAE=109.5106 MAE=109.5052 Epoch: 250/500MAE=109.5592 MAE=109.5063 MAE=109.5631 MAE=109.5132 MAE=109.5077 MAE=109.5671 MAE=109.5106 MAE=109.5159 MAE=109.5123 MAE=109.5068 Epoch: 260/500MAE=109.5120 MAE=109.5138 MAE=109.5061 MAE=110.7269 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 113.231 +/- 5.074\n",
                        "\n",
                        "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/500MAE=1549.2435 MAE=1547.5469 MAE=1545.2173 MAE=1542.4419 MAE=1539.9993 MAE=1537.7761 MAE=1534.1943 MAE=1530.4446 MAE=1526.7681 Epoch: 10/500MAE=1521.7867 MAE=1516.9563 MAE=1512.1973 MAE=1504.7629 MAE=1501.4690 MAE=1494.2510 MAE=1487.6837 MAE=1479.2162 MAE=1471.3292 MAE=1461.6600 Epoch: 20/500MAE=1452.9401 MAE=1445.8669 MAE=1434.8235 MAE=1424.4875 MAE=1414.3782 MAE=1403.2456 MAE=1392.1711 MAE=1381.5699 MAE=1369.4666 MAE=1356.6798 Epoch: 30/500MAE=1344.7472 MAE=1327.6416 MAE=1322.1909 MAE=1310.1250 MAE=1287.1202 MAE=1278.9860 MAE=1256.7476 MAE=1246.1250 MAE=1228.8718 MAE=1211.7678 Epoch: 40/500MAE=1198.5232 MAE=1179.2872 MAE=1171.1614 MAE=1146.5618 MAE=1131.2201 MAE=1117.3075 MAE=1088.9272 MAE=1071.4495 MAE=1040.8790 MAE=1064.4707 Epoch: 50/500MAE=1020.0479 MAE=990.1522 MAE=985.2731 MAE=956.7378 MAE=932.1387 MAE=911.2842 MAE=884.8031 MAE=883.2629 MAE=843.8314 MAE=839.0093 Epoch: 60/500MAE=815.8380 MAE=769.9561 MAE=748.1021 MAE=730.8373 MAE=721.2566 MAE=679.0197 MAE=656.3359 MAE=642.9686 MAE=634.3546 MAE=579.7433 Epoch: 70/500MAE=560.2947 MAE=530.3109 MAE=496.0108 MAE=456.2385 MAE=474.1974 MAE=416.0627 MAE=381.0308 MAE=360.3991 MAE=355.3697 MAE=302.6099 Epoch: 80/500MAE=275.8973 MAE=310.9415 MAE=255.0740 MAE=285.0674 MAE=220.0577 MAE=208.1384 MAE=214.5291 MAE=159.2660 MAE=163.7387 MAE=119.7619 Epoch: 90/500MAE=121.2462 MAE=127.0764 MAE=102.5220 MAE=108.4158 MAE=137.9656 MAE=99.6562 MAE=100.8519 MAE=104.3200 MAE=105.8058 MAE=101.5261 Epoch: 100/500MAE=98.9741 MAE=101.7042 MAE=96.3272 MAE=97.6465 MAE=96.2876 MAE=95.6310 MAE=94.8506 MAE=99.0610 MAE=94.2566 MAE=94.7114 Epoch: 110/500MAE=93.5419 MAE=96.8491 MAE=94.2867 MAE=94.6918 MAE=95.0430 MAE=95.8092 MAE=94.3223 MAE=95.6091 MAE=94.5961 MAE=93.6682 Epoch: 120/500MAE=93.2444 MAE=93.3157 MAE=92.7264 MAE=93.0544 MAE=93.9304 MAE=93.0826 MAE=92.8456 MAE=93.5012 MAE=93.1424 MAE=92.7648 Epoch: 130/500MAE=92.9761 MAE=92.8630 MAE=93.0913 MAE=93.1462 MAE=92.9717 MAE=93.1503 MAE=93.1056 MAE=93.0184 MAE=92.9770 MAE=93.0200 Epoch: 140/500MAE=92.8132 MAE=92.9726 MAE=92.7926 MAE=92.8618 MAE=92.8007 MAE=92.9100 MAE=92.8904 MAE=92.7937 MAE=92.7380 MAE=92.7197 Epoch: 150/500MAE=92.8309 MAE=92.8105 MAE=92.8625 MAE=92.7300 MAE=92.7145 MAE=92.7805 MAE=92.8200 MAE=92.9181 MAE=92.8792 MAE=92.8280 Epoch: 160/500MAE=92.8660 MAE=92.8941 MAE=92.8325 MAE=92.8910 MAE=92.8178 MAE=92.7749 MAE=92.7917 MAE=92.7853 MAE=92.7192 MAE=92.8617 Epoch: 170/500MAE=92.7477 MAE=92.8956 MAE=92.8606 MAE=92.7416 MAE=92.8059 MAE=92.8549 MAE=92.8529 MAE=92.8181 MAE=92.7709 MAE=92.7811 Epoch: 180/500MAE=92.8372 MAE=92.6902 MAE=92.7732 MAE=92.8926 MAE=92.6771 MAE=92.7489 MAE=92.7261 MAE=92.9248 MAE=92.7799 MAE=92.6605 Epoch: 190/500MAE=92.7375 MAE=92.8484 MAE=92.7567 MAE=92.8464 MAE=92.8266 MAE=92.8761 MAE=92.6807 MAE=92.7935 MAE=92.8568 MAE=92.8158 Epoch: 200/500MAE=92.8887 MAE=92.8208 MAE=92.7849 MAE=92.7779 MAE=92.6702 MAE=92.7158 MAE=92.6978 MAE=92.7120 MAE=92.7556 MAE=92.7492 Epoch: 210/500MAE=92.8161 MAE=92.7835 MAE=92.7449 MAE=92.8994 MAE=92.6816 MAE=92.7562 MAE=92.7762 MAE=92.6834 MAE=92.9160 MAE=92.7208 Epoch: 220/500MAE=92.8349 MAE=92.8065 MAE=92.7852 MAE=92.8307 MAE=92.7437 MAE=92.8076 MAE=92.7902 MAE=92.8191 MAE=92.9048 MAE=92.7764 Epoch: 230/500MAE=92.8229 MAE=92.8604 MAE=92.7528 MAE=92.7252 MAE=92.7975 MAE=92.7332 MAE=92.7576 MAE=92.7675 MAE=92.8611 MAE=92.7761 Epoch: 240/500MAE=92.7579 MAE=92.9279 MAE=92.8783 MAE=92.7626 MAE=92.6908 MAE=92.8008 MAE=92.7753 MAE=92.7675 MAE=92.8856 MAE=92.8670 Epoch: 250/500MAE=92.7571 MAE=92.7313 MAE=92.8299 MAE=92.7548 MAE=92.7414 MAE=92.7858 MAE=92.8249 MAE=92.7595 MAE=92.8096 MAE=92.8987 Epoch: 260/500MAE=92.7572 MAE=92.8831 MAE=92.7357 MAE=92.8255 MAE=92.9061 MAE=92.7506 MAE=92.8691 MAE=92.8736 MAE=92.7661 MAE=92.7644 Epoch: 270/500MAE=92.8051 MAE=92.8268 MAE=92.8428 MAE=92.7382 MAE=92.8501 MAE=92.8984 MAE=92.8149 MAE=92.8179 MAE=92.7266 MAE=92.9429 Epoch: 280/500MAE=92.7595 MAE=92.8396 MAE=92.6902 MAE=92.7407 MAE=92.6977 MAE=92.7737 MAE=92.6637 MAE=92.7988 MAE=92.7487 MAE=92.7768 Epoch: 290/500MAE=92.7031 MAE=92.8015 MAE=92.8402 MAE=92.7114 MAE=92.7998 MAE=92.7369 MAE=92.7757 MAE=92.6897 MAE=92.8552 MAE=92.8123 Epoch: 300/500MAE=92.8584 MAE=92.8321 MAE=92.8396 MAE=92.7043 MAE=92.7421 MAE=92.8285 MAE=92.7523 MAE=92.8147 MAE=92.7829 MAE=92.7736 Epoch: 310/500MAE=92.7256 MAE=92.6955 MAE=92.8056 MAE=92.7882 MAE=92.8345 MAE=92.7907 MAE=92.8510 MAE=92.7092 MAE=92.7336 MAE=92.7163 Epoch: 320/500MAE=92.8896 MAE=92.7022 MAE=92.8912 MAE=92.7271 MAE=92.7640 MAE=92.7546 MAE=92.7347 MAE=92.7902 MAE=92.8395 MAE=92.8963 Epoch: 330/500MAE=92.8770 MAE=92.7425 MAE=92.6537 MAE=92.7809 MAE=92.7410 MAE=92.7812 MAE=92.7993 MAE=92.9002 MAE=92.8690 MAE=92.9214 Epoch: 340/500MAE=92.8694 MAE=92.8860 MAE=92.7110 MAE=92.7172 MAE=92.7958 MAE=92.8077 MAE=92.6936 MAE=92.7309 MAE=92.8771 MAE=92.9565 Epoch: 350/500MAE=92.9255 MAE=92.8894 MAE=92.8176 MAE=92.9095 MAE=92.7992 MAE=92.8275 MAE=92.7035 MAE=92.8742 MAE=92.8456 MAE=92.7596 Epoch: 360/500MAE=92.7731 MAE=92.7609 MAE=92.8594 MAE=92.7911 MAE=92.7152 MAE=92.6411 MAE=92.7817 MAE=92.8894 MAE=92.8930 MAE=92.7638 Epoch: 370/500MAE=92.8268 MAE=92.8288 MAE=92.7347 MAE=92.7769 MAE=92.8276 MAE=92.7873 MAE=92.6784 MAE=92.7915 MAE=92.7409 MAE=92.8463 Epoch: 380/500MAE=92.7353 MAE=92.7247 MAE=92.8630 MAE=92.7473 MAE=92.8688 MAE=92.8611 MAE=92.7601 MAE=92.8663 MAE=92.7638 MAE=92.8024 Epoch: 390/500MAE=92.7417 MAE=92.8008 MAE=92.7858 MAE=92.6919 MAE=92.7606 MAE=92.6368 MAE=92.8348 MAE=92.7381 MAE=92.7348 MAE=92.7379 Epoch: 400/500MAE=92.7975 MAE=92.8563 MAE=92.7881 MAE=92.7276 MAE=92.6285 MAE=92.6060 MAE=92.7157 MAE=92.7017 MAE=92.6862 MAE=92.6841 Epoch: 410/500MAE=92.6763 MAE=92.7874 MAE=92.7227 MAE=92.7925 MAE=92.7166 MAE=92.7801 MAE=92.6463 MAE=92.6924 MAE=92.7983 MAE=92.8446 Epoch: 420/500MAE=92.7915 MAE=92.7805 MAE=92.7033 MAE=92.8342 MAE=92.8727 MAE=92.7183 MAE=92.7809 MAE=92.7685 MAE=92.7078 MAE=92.6012 Epoch: 430/500MAE=92.7071 MAE=92.7379 MAE=92.6273 MAE=92.7536 MAE=92.7076 MAE=92.7604 MAE=92.7472 MAE=92.7354 MAE=92.8804 MAE=92.8310 Epoch: 440/500MAE=92.6465 MAE=92.6783 MAE=92.7662 MAE=92.7711 MAE=92.8829 MAE=92.7915 MAE=92.6962 MAE=92.7961 MAE=92.8473 MAE=92.7524 Epoch: 450/500MAE=92.7131 MAE=92.8125 MAE=92.7603 MAE=92.7730 MAE=92.6503 MAE=92.7118 MAE=92.6606 MAE=92.8296 MAE=92.7878 MAE=92.9047 Epoch: 460/500MAE=92.8350 MAE=92.7469 MAE=92.8465 MAE=92.7423 MAE=92.7996 MAE=92.7848 MAE=92.6906 MAE=92.7621 MAE=92.7346 MAE=92.8705 Epoch: 470/500MAE=92.8380 MAE=92.8348 MAE=92.7617 MAE=92.7286 MAE=92.8580 MAE=92.7296 MAE=92.8134 MAE=92.7118 MAE=92.8296 MAE=92.8381 Epoch: 480/500MAE=92.7618 MAE=92.7422 MAE=92.6499 MAE=92.8373 MAE=92.7825 MAE=92.7114 MAE=92.7477 MAE=92.7497 MAE=92.6669 MAE=92.6885 Epoch: 490/500MAE=92.7517 MAE=92.6557 MAE=92.6625 MAE=92.7751 MAE=92.7054 MAE=92.6419 MAE=92.7827 MAE=92.6830 MAE=92.8174 MAE=92.7473 Epoch: 500/500MAE=92.7605 MAE=92.2716 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 92.272 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.8474 MAE=1547.5745 MAE=1544.2084 MAE=1541.6405 MAE=1539.3448 MAE=1536.7098 MAE=1534.0770 MAE=1529.7971 MAE=1526.1050 Epoch: 10/500MAE=1520.9866 MAE=1516.5620 MAE=1511.5773 MAE=1505.5596 MAE=1500.1754 MAE=1494.9456 MAE=1484.2754 MAE=1477.0710 MAE=1470.7191 MAE=1460.8411 Epoch: 20/500MAE=1449.9421 MAE=1439.0337 MAE=1433.0342 MAE=1424.2556 MAE=1410.5702 MAE=1401.9094 MAE=1390.5240 MAE=1377.4261 MAE=1364.6782 MAE=1355.2957 Epoch: 30/500MAE=1339.6243 MAE=1335.3162 MAE=1318.5879 MAE=1305.0112 MAE=1287.7817 MAE=1274.9094 MAE=1266.5247 MAE=1242.3063 MAE=1236.7036 MAE=1213.4576 Epoch: 40/500MAE=1196.7373 MAE=1186.2815 MAE=1164.3750 MAE=1155.6708 MAE=1132.3755 MAE=1113.4109 MAE=1089.5134 MAE=1080.4819 MAE=1041.6515 MAE=1002.3482 Epoch: 50/500MAE=1007.8385 MAE=998.2841 MAE=967.9774 MAE=980.7404 MAE=941.1786 MAE=939.0054 MAE=877.3088 MAE=884.6014 MAE=854.6519 MAE=848.5399 Epoch: 60/500MAE=816.4188 MAE=773.9442 MAE=753.5968 MAE=719.4340 MAE=710.8347 MAE=676.2062 MAE=661.1427 MAE=632.9840 MAE=606.4906 MAE=606.3020 Epoch: 70/500MAE=569.1985 MAE=541.5350 MAE=486.3244 MAE=463.8761 MAE=451.3504 MAE=431.8023 MAE=385.0918 MAE=375.8629 MAE=334.9954 MAE=314.0438 Epoch: 80/500MAE=300.1280 MAE=284.3837 MAE=250.9337 MAE=270.0522 MAE=282.8162 MAE=271.5417 MAE=155.3006 MAE=165.5555 MAE=148.5409 MAE=144.1842 Epoch: 90/500MAE=116.6644 MAE=115.5436 MAE=120.5627 MAE=108.9579 MAE=102.9710 MAE=120.1093 MAE=110.6605 MAE=99.3155 MAE=98.6249 MAE=95.9183 Epoch: 100/500MAE=103.0041 MAE=101.1828 MAE=100.3002 MAE=96.2503 MAE=93.5947 MAE=93.7801 MAE=92.9812 MAE=94.5225 MAE=93.5290 MAE=93.8698 Epoch: 110/500MAE=92.4266 MAE=92.7696 MAE=93.7678 MAE=93.2797 MAE=96.0047 MAE=91.8894 MAE=93.1755 MAE=92.1013 MAE=92.3417 MAE=92.3547 Epoch: 120/500MAE=91.1653 MAE=90.9987 MAE=92.0289 MAE=90.8671 MAE=91.3586 MAE=91.3052 MAE=92.3663 MAE=92.1168 MAE=90.9556 MAE=90.5200 Epoch: 130/500MAE=91.0284 MAE=91.4280 MAE=91.1845 MAE=91.4023 MAE=91.1938 MAE=91.0366 MAE=91.1118 MAE=90.9357 MAE=90.9541 MAE=90.7091 Epoch: 140/500MAE=90.8589 MAE=90.8102 MAE=90.7499 MAE=90.4043 MAE=90.6773 MAE=90.7504 MAE=90.8215 MAE=90.7933 MAE=90.6075 MAE=90.7665 Epoch: 150/500MAE=90.5140 MAE=90.6704 MAE=90.6895 MAE=90.6985 MAE=90.9105 MAE=90.7004 MAE=90.6084 MAE=90.6470 MAE=90.7457 MAE=90.8086 Epoch: 160/500MAE=90.8013 MAE=90.5745 MAE=90.6220 MAE=90.7719 MAE=90.7564 MAE=90.7244 MAE=90.7007 MAE=90.6996 MAE=90.9383 MAE=90.6985 Epoch: 170/500MAE=90.7678 MAE=90.7454 MAE=90.7473 MAE=90.5989 MAE=90.9511 MAE=90.7420 MAE=90.6803 MAE=90.7911 MAE=90.7033 MAE=90.8725 Epoch: 180/500MAE=90.8205 MAE=90.6884 MAE=90.6742 MAE=90.8125 MAE=90.6467 MAE=90.6381 MAE=90.8387 MAE=90.7702 MAE=90.7487 MAE=90.8762 Epoch: 190/500MAE=90.9056 MAE=90.6583 MAE=90.9362 MAE=90.8862 MAE=90.8173 MAE=90.6739 MAE=90.6090 MAE=90.7877 MAE=90.7298 MAE=90.6512 Epoch: 200/500MAE=91.0087 MAE=90.5786 MAE=90.7801 MAE=90.7641 MAE=90.8309 MAE=90.7693 MAE=90.6934 MAE=90.6220 MAE=90.9616 MAE=90.6323 Epoch: 210/500MAE=91.0153 MAE=90.8384 MAE=90.6524 MAE=90.6497 MAE=90.7460 MAE=90.6226 MAE=90.7512 MAE=90.6050 MAE=90.8418 MAE=90.6018 Epoch: 220/500MAE=90.7819 MAE=90.5718 MAE=90.7995 MAE=90.7116 MAE=90.7519 MAE=90.7915 MAE=90.6287 MAE=90.7446 MAE=90.5874 MAE=90.7787 Epoch: 230/500MAE=90.6204 MAE=90.8255 MAE=90.6155 MAE=90.4872 MAE=90.7212 MAE=90.5672 MAE=90.7565 MAE=90.7898 MAE=90.5053 MAE=90.5222 Epoch: 240/500MAE=90.4916 MAE=90.7096 MAE=90.5281 MAE=90.7481 MAE=90.8868 MAE=90.5637 MAE=90.6979 MAE=90.7579 MAE=90.7567 MAE=90.6599 Epoch: 250/500MAE=90.4329 MAE=90.7644 MAE=90.7192 MAE=90.7107 MAE=90.5751 MAE=90.7470 MAE=90.6602 MAE=90.7728 MAE=90.8876 MAE=90.6982 Epoch: 260/500MAE=90.6668 MAE=90.7908 MAE=90.5556 MAE=90.7327 MAE=90.6960 MAE=90.7718 MAE=90.7875 MAE=90.7935 MAE=90.7588 MAE=90.6905 Epoch: 270/500MAE=90.7352 MAE=90.8668 MAE=90.9483 MAE=90.6127 MAE=90.7727 MAE=90.8070 MAE=90.7437 MAE=90.7859 MAE=90.7047 MAE=90.8574 Epoch: 280/500MAE=90.6073 MAE=90.8183 MAE=90.7609 MAE=90.8437 MAE=90.8216 MAE=90.5782 MAE=90.7939 MAE=90.8195 MAE=90.6617 MAE=90.9563 Epoch: 290/500MAE=90.7102 MAE=90.8624 MAE=90.9030 MAE=90.7228 MAE=89.3200 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 90.796 +/- 1.476\n",
                        "\n",
                        "Epoch: 1/500MAE=1550.1019 MAE=1548.1646 MAE=1545.5751 MAE=1543.0159 MAE=1540.1431 MAE=1537.6731 MAE=1534.4948 MAE=1531.3535 MAE=1527.1594 Epoch: 10/500MAE=1522.9783 MAE=1517.8003 MAE=1512.8987 MAE=1507.5173 MAE=1501.3434 MAE=1492.2218 MAE=1483.6396 MAE=1477.0077 MAE=1469.0314 MAE=1459.1379 Epoch: 20/500MAE=1450.6406 MAE=1441.8723 MAE=1431.9249 MAE=1424.2560 MAE=1413.6377 MAE=1400.7146 MAE=1391.6321 MAE=1381.2076 MAE=1370.1479 MAE=1354.8788 Epoch: 30/500MAE=1343.3929 MAE=1332.0421 MAE=1319.8871 MAE=1307.7427 MAE=1293.9988 MAE=1274.7390 MAE=1266.8876 MAE=1247.3506 MAE=1238.4666 MAE=1219.0564 Epoch: 40/500MAE=1202.0712 MAE=1193.6304 MAE=1162.5361 MAE=1155.0186 MAE=1135.3728 MAE=1120.1503 MAE=1110.5576 MAE=1080.0515 MAE=1058.4707 MAE=1045.8544 Epoch: 50/500MAE=1022.1277 MAE=994.7943 MAE=983.8396 MAE=959.6226 MAE=943.9385 MAE=930.0668 MAE=895.5823 MAE=870.0101 MAE=861.2675 MAE=843.9758 Epoch: 60/500MAE=791.6591 MAE=808.4337 MAE=807.8680 MAE=740.4233 MAE=729.5575 MAE=699.4851 MAE=656.8523 MAE=638.9944 MAE=590.4597 MAE=550.3645 Epoch: 70/500MAE=444.5427 MAE=480.9098 MAE=473.5335 MAE=460.9512 MAE=402.3338 MAE=472.0337 MAE=418.1196 MAE=369.5558 MAE=388.9073 MAE=325.3120 Epoch: 80/500MAE=249.6406 MAE=277.0349 MAE=247.2441 MAE=211.6637 MAE=264.0042 MAE=222.0425 MAE=188.2399 MAE=172.5239 MAE=128.2065 MAE=137.3880 Epoch: 90/500MAE=132.5871 MAE=130.3906 MAE=122.9238 MAE=109.8988 MAE=106.9135 MAE=115.3668 MAE=105.9657 MAE=119.3262 MAE=126.0507 MAE=105.0562 Epoch: 100/500MAE=99.2568 MAE=104.3151 MAE=101.7811 MAE=102.7417 MAE=103.9816 MAE=96.2137 MAE=94.7126 MAE=93.7344 MAE=95.0652 MAE=99.0794 Epoch: 110/500MAE=96.2588 MAE=95.0748 MAE=93.5763 MAE=94.5526 MAE=94.2607 MAE=96.4211 MAE=95.0375 MAE=95.1540 MAE=94.3014 MAE=94.4666 Epoch: 120/500MAE=94.8406 MAE=94.4304 MAE=94.5993 MAE=94.1702 MAE=94.8173 MAE=94.7238 MAE=94.6092 MAE=94.5820 MAE=94.6980 MAE=94.8141 Epoch: 130/500MAE=94.6480 MAE=94.5450 MAE=94.7076 MAE=94.9211 MAE=94.9264 MAE=94.8470 MAE=95.1259 MAE=94.6647 MAE=94.8023 MAE=94.8382 Epoch: 140/500MAE=94.9653 MAE=94.8482 MAE=95.0886 MAE=95.0210 MAE=94.6022 MAE=94.7374 MAE=94.9248 MAE=95.1977 MAE=95.1848 MAE=94.8069 Epoch: 150/500MAE=94.7859 MAE=95.2419 MAE=94.9695 MAE=94.5909 MAE=94.7759 MAE=94.8312 MAE=94.9938 MAE=94.8631 MAE=95.0328 MAE=95.2233 Epoch: 160/500MAE=94.9253 MAE=94.6777 MAE=94.9040 MAE=94.5130 MAE=95.1547 MAE=94.8031 MAE=94.7866 MAE=95.0352 MAE=94.8831 MAE=94.9578 Epoch: 170/500MAE=94.9315 MAE=94.9563 MAE=94.8307 MAE=94.8684 MAE=94.7457 MAE=94.7696 MAE=94.8638 MAE=94.9072 MAE=95.0903 MAE=94.6699 Epoch: 180/500MAE=94.9159 MAE=94.5312 MAE=94.9270 MAE=94.6417 MAE=94.6453 MAE=94.6315 MAE=95.0594 MAE=94.4398 MAE=94.8136 MAE=94.8031 Epoch: 190/500MAE=94.8766 MAE=94.9130 MAE=94.7562 MAE=95.0175 MAE=94.7368 MAE=94.9389 MAE=94.8439 MAE=94.7150 MAE=94.6975 MAE=94.6818 Epoch: 200/500MAE=94.8487 MAE=95.0890 MAE=94.7203 MAE=94.8300 MAE=94.8803 MAE=94.4823 MAE=94.8790 MAE=95.1164 MAE=94.4910 MAE=94.7972 Epoch: 210/500MAE=94.6786 MAE=94.9131 MAE=94.5356 MAE=94.8073 MAE=95.1711 MAE=94.7366 MAE=95.0626 MAE=94.9660 MAE=94.6521 MAE=94.5802 Epoch: 220/500MAE=94.9094 MAE=94.8175 MAE=95.0175 MAE=94.9900 MAE=95.1665 MAE=94.7313 MAE=94.7214 MAE=94.6678 MAE=94.9665 MAE=94.8735 Epoch: 230/500MAE=94.8185 MAE=95.0254 MAE=94.4236 MAE=95.0783 MAE=94.7620 MAE=94.4520 MAE=94.5868 MAE=94.9877 MAE=95.0959 MAE=94.8379 Epoch: 240/500MAE=94.8478 MAE=94.6715 MAE=95.0169 MAE=94.9932 MAE=94.8057 MAE=94.7683 MAE=94.8530 MAE=94.7214 MAE=94.7827 MAE=95.0245 Epoch: 250/500MAE=94.8824 MAE=94.9381 MAE=94.8591 MAE=94.7914 MAE=95.0467 MAE=94.4126 MAE=94.8977 MAE=94.8518 MAE=94.8170 MAE=94.8525 Epoch: 260/500MAE=94.7959 MAE=94.8716 MAE=94.7274 MAE=93.3870 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 91.660 +/- 1.716\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.2993 MAE=1547.4990 MAE=1544.9539 MAE=1542.5781 MAE=1539.7881 MAE=1537.2188 MAE=1534.4241 MAE=1530.8212 MAE=1526.2734 Epoch: 10/500MAE=1522.6018 MAE=1517.5068 MAE=1512.7180 MAE=1506.9064 MAE=1499.2148 MAE=1494.5605 MAE=1485.3904 MAE=1479.6587 MAE=1468.3181 MAE=1460.5374 Epoch: 20/500MAE=1453.9253 MAE=1445.8767 MAE=1430.5620 MAE=1420.4771 MAE=1412.7587 MAE=1400.9508 MAE=1391.0879 MAE=1381.9938 MAE=1366.8965 MAE=1354.2150 Epoch: 30/500MAE=1345.4678 MAE=1329.1481 MAE=1320.0043 MAE=1305.7346 MAE=1286.7303 MAE=1271.7124 MAE=1261.5227 MAE=1243.1478 MAE=1229.7236 MAE=1210.7251 Epoch: 40/500MAE=1187.9863 MAE=1177.5861 MAE=1167.2549 MAE=1150.4302 MAE=1128.2131 MAE=1099.8679 MAE=1097.5388 MAE=1065.2871 MAE=1049.1357 MAE=1026.6857 Epoch: 50/500MAE=1013.7994 MAE=991.6235 MAE=978.6901 MAE=942.8297 MAE=953.9299 MAE=915.4731 MAE=899.7487 MAE=860.9810 MAE=847.7803 MAE=816.9445 Epoch: 60/500MAE=790.2038 MAE=765.9506 MAE=763.3934 MAE=717.3779 MAE=692.1296 MAE=683.4924 MAE=643.8580 MAE=605.9538 MAE=605.3545 MAE=572.5580 Epoch: 70/500MAE=551.8567 MAE=521.6792 MAE=491.6418 MAE=487.3218 MAE=429.8706 MAE=443.6902 MAE=396.6700 MAE=416.5994 MAE=408.8993 MAE=322.7144 Epoch: 80/500MAE=334.6871 MAE=258.7396 MAE=275.2760 MAE=271.4637 MAE=237.1139 MAE=189.2356 MAE=158.5000 MAE=181.2309 MAE=153.0774 MAE=145.2016 Epoch: 90/500MAE=154.3963 MAE=129.8511 MAE=104.6733 MAE=110.9703 MAE=101.8296 MAE=102.0814 MAE=95.5497 MAE=98.8259 MAE=112.5604 MAE=93.9736 Epoch: 100/500MAE=100.5746 MAE=94.3905 MAE=101.0272 MAE=95.5229 MAE=88.8974 MAE=89.9253 MAE=88.4882 MAE=88.5889 MAE=88.9647 MAE=88.4083 Epoch: 110/500MAE=88.5345 MAE=90.5547 MAE=90.0732 MAE=89.2851 MAE=89.5834 MAE=88.7847 MAE=88.2000 MAE=88.6132 MAE=88.4971 MAE=89.1767 Epoch: 120/500MAE=86.9636 MAE=88.5742 MAE=89.2994 MAE=89.7830 MAE=88.4525 MAE=88.0411 MAE=88.2233 MAE=87.4346 MAE=87.7736 MAE=87.3453 Epoch: 130/500MAE=87.3400 MAE=87.6891 MAE=87.2971 MAE=87.7106 MAE=87.5428 MAE=87.3607 MAE=87.6964 MAE=87.5930 MAE=87.1488 MAE=87.5282 Epoch: 140/500MAE=87.2885 MAE=87.5260 MAE=87.4525 MAE=87.1897 MAE=87.5999 MAE=87.4303 MAE=87.4169 MAE=87.3717 MAE=87.1978 MAE=87.0415 Epoch: 150/500MAE=87.2589 MAE=87.3479 MAE=87.4642 MAE=87.1640 MAE=87.4081 MAE=87.2444 MAE=87.1659 MAE=87.1528 MAE=87.4353 MAE=87.2527 Epoch: 160/500MAE=87.1771 MAE=87.3373 MAE=87.1803 MAE=86.9025 MAE=87.0922 MAE=87.1384 MAE=87.1275 MAE=87.3189 MAE=87.0267 MAE=87.2970 Epoch: 170/500MAE=87.3337 MAE=87.1234 MAE=87.3303 MAE=87.3500 MAE=87.1846 MAE=87.3992 MAE=87.4439 MAE=87.4750 MAE=87.1644 MAE=87.3536 Epoch: 180/500MAE=87.2334 MAE=87.2075 MAE=87.2840 MAE=87.2703 MAE=87.2511 MAE=87.2954 MAE=87.2006 MAE=87.5986 MAE=87.0511 MAE=87.4529 Epoch: 190/500MAE=87.3850 MAE=87.3656 MAE=87.2959 MAE=87.3046 MAE=87.3450 MAE=87.0918 MAE=87.3103 MAE=87.4174 MAE=87.5679 MAE=87.3331 Epoch: 200/500MAE=87.3463 MAE=87.5362 MAE=87.1823 MAE=87.4712 MAE=87.3312 MAE=87.4809 MAE=87.3996 MAE=87.2226 MAE=87.1237 MAE=87.4236 Epoch: 210/500MAE=87.4924 MAE=87.3723 MAE=87.3743 MAE=87.3473 MAE=87.4529 MAE=87.2627 MAE=87.2320 MAE=87.5001 MAE=87.2910 MAE=87.0014 Epoch: 220/500MAE=87.1263 MAE=87.1251 MAE=87.3505 MAE=87.1654 MAE=87.3469 MAE=87.4216 MAE=87.2087 MAE=87.3263 MAE=87.2375 MAE=87.2992 Epoch: 230/500MAE=87.0056 MAE=87.4211 MAE=87.2297 MAE=87.2645 MAE=87.2172 MAE=87.1718 MAE=87.4325 MAE=87.1656 MAE=87.1603 MAE=87.3665 Epoch: 240/500MAE=87.3166 MAE=87.3759 MAE=87.1748 MAE=87.2714 MAE=87.2180 MAE=87.5789 MAE=87.0686 MAE=87.0810 MAE=87.3249 MAE=87.3374 Epoch: 250/500MAE=87.2361 MAE=87.4247 MAE=87.1721 MAE=87.3395 MAE=87.0573 MAE=86.9603 MAE=87.1323 MAE=87.2573 MAE=87.2713 MAE=87.2980 Epoch: 260/500MAE=87.3089 MAE=87.0332 MAE=86.9634 MAE=87.3571 MAE=87.1489 MAE=87.2463 MAE=87.0692 MAE=87.2107 MAE=87.1481 MAE=87.1178 Epoch: 270/500MAE=87.0243 MAE=87.2226 MAE=87.0295 MAE=86.9844 MAE=87.0366 MAE=87.0188 MAE=87.1837 MAE=87.2395 MAE=87.2775 MAE=87.3797 Epoch: 280/500MAE=86.9442 MAE=86.9480 MAE=87.1173 MAE=86.9794 MAE=87.2834 MAE=87.2260 MAE=87.1736 MAE=87.1637 MAE=87.1191 MAE=87.0216 Epoch: 290/500MAE=87.0746 MAE=87.1539 MAE=87.1200 MAE=87.0291 MAE=87.3856 MAE=87.0991 MAE=86.8391 MAE=87.0021 MAE=87.1163 MAE=87.1299 Epoch: 300/500MAE=87.1204 MAE=87.1505 MAE=87.0581 MAE=87.2255 MAE=87.1862 MAE=87.1340 MAE=87.0285 MAE=87.0623 MAE=87.1644 MAE=86.9879 Epoch: 310/500MAE=87.2041 MAE=87.3274 MAE=87.2369 MAE=87.2876 MAE=87.3430 MAE=87.1748 MAE=86.9910 MAE=87.3779 MAE=87.0591 MAE=87.3449 Epoch: 320/500MAE=87.2247 MAE=87.2535 MAE=87.3427 MAE=87.3044 MAE=87.2162 MAE=87.2485 MAE=87.2357 MAE=87.3317 MAE=87.2611 MAE=87.0546 Epoch: 330/500MAE=87.4048 MAE=87.3055 MAE=87.1617 MAE=87.1396 MAE=87.0694 MAE=87.1162 MAE=87.1912 MAE=87.4143 MAE=86.9459 MAE=86.9928 Epoch: 340/500MAE=87.2643 MAE=87.2746 MAE=87.0915 MAE=87.1290 MAE=87.1887 MAE=87.2258 MAE=87.0656 MAE=86.9573 MAE=87.1728 MAE=87.3476 Epoch: 350/500MAE=87.0273 MAE=87.1344 MAE=86.9843 MAE=87.0138 MAE=87.1897 MAE=87.1088 MAE=86.9434 MAE=86.9101 MAE=87.1671 MAE=87.0698 Epoch: 360/500MAE=87.0117 MAE=87.1298 MAE=87.1109 MAE=87.3081 MAE=87.0603 MAE=87.0225 MAE=87.1599 MAE=87.2935 MAE=87.3605 MAE=86.8012 Epoch: 370/500MAE=87.0344 MAE=87.1844 MAE=87.0299 MAE=86.9568 MAE=87.2061 MAE=87.0603 MAE=87.0403 MAE=87.0042 MAE=87.0402 MAE=87.3152 Epoch: 380/500MAE=86.9603 MAE=87.2273 MAE=87.1828 MAE=87.1823 MAE=87.1839 MAE=87.3408 MAE=87.0083 MAE=87.1342 MAE=87.0779 MAE=87.2679 Epoch: 390/500MAE=87.0263 MAE=87.1401 MAE=87.0839 MAE=87.0325 MAE=86.9102 MAE=87.0047 MAE=87.2067 MAE=87.2195 MAE=87.1094 MAE=87.1637 Epoch: 400/500MAE=87.0684 MAE=87.0901 MAE=86.9294 MAE=87.2412 MAE=87.0835 MAE=86.9176 MAE=87.1789 MAE=86.9978 MAE=87.2501 MAE=87.1725 Epoch: 410/500MAE=87.0276 MAE=86.9407 MAE=87.3088 MAE=86.9823 MAE=87.2255 MAE=87.2640 MAE=87.2937 MAE=87.0477 MAE=86.9872 MAE=87.1014 Epoch: 420/500MAE=87.1584 MAE=87.0698 MAE=87.2870 MAE=87.3404 MAE=87.1898 MAE=87.4775 MAE=87.3685 MAE=86.9433 MAE=87.1457 MAE=87.3031 Epoch: 430/500MAE=87.3297 MAE=87.1326 MAE=87.2203 MAE=87.1085 MAE=87.1180 MAE=86.9717 MAE=87.2091 MAE=87.1757 MAE=87.1127 MAE=87.2249 Epoch: 440/500MAE=87.0314 MAE=87.1498 MAE=87.2088 MAE=87.2530 MAE=87.2706 MAE=87.3296 MAE=87.1795 MAE=87.3676 MAE=87.3431 MAE=87.0859 Epoch: 450/500MAE=87.2655 MAE=87.0152 MAE=87.2548 MAE=87.1554 MAE=87.1578 MAE=87.2543 MAE=87.0803 MAE=87.1228 MAE=87.2927 MAE=87.2880 Epoch: 460/500MAE=87.2507 MAE=87.4037 MAE=87.1427 MAE=87.3173 MAE=87.0881 MAE=87.5886 MAE=86.9208 MAE=87.2740 MAE=87.3010 MAE=87.4349 Epoch: 470/500MAE=87.3428 MAE=87.2589 MAE=87.2809 MAE=87.1950 MAE=87.3190 MAE=87.3288 MAE=87.1164 MAE=87.4281 MAE=87.3323 MAE=87.2645 Epoch: 480/500MAE=87.2283 MAE=87.2879 MAE=87.1713 MAE=87.2987 MAE=87.4328 MAE=87.1538 MAE=87.4455 MAE=87.2896 MAE=87.2956 MAE=87.2409 Epoch: 490/500MAE=87.4588 MAE=87.2587 MAE=87.1426 MAE=87.4823 MAE=87.3671 MAE=87.3109 MAE=87.4641 MAE=87.2414 MAE=87.1878 MAE=87.5647 Epoch: 500/500MAE=87.1740 MAE=90.9198 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 91.475 +/- 1.520\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.3269 MAE=1547.0527 MAE=1544.4155 MAE=1541.7761 MAE=1539.1484 MAE=1536.1061 MAE=1533.4407 MAE=1530.3047 MAE=1525.5665 Epoch: 10/500MAE=1521.2592 MAE=1516.0422 MAE=1511.0220 MAE=1503.9922 MAE=1496.9454 MAE=1491.6208 MAE=1487.3953 MAE=1479.1819 MAE=1469.5244 MAE=1462.8953 Epoch: 20/500MAE=1452.2723 MAE=1445.4501 MAE=1442.5763 MAE=1425.7114 MAE=1417.3073 MAE=1407.1663 MAE=1395.6853 MAE=1381.0996 MAE=1366.9258 MAE=1359.7786 Epoch: 30/500MAE=1345.3015 MAE=1334.6975 MAE=1329.4672 MAE=1302.3083 MAE=1296.6107 MAE=1278.8917 MAE=1257.4906 MAE=1249.5609 MAE=1240.9286 MAE=1213.2880 Epoch: 40/500MAE=1204.0332 MAE=1187.6700 MAE=1191.7429 MAE=1153.8479 MAE=1139.3014 MAE=1116.2981 MAE=1105.1520 MAE=1077.3811 MAE=1123.2681 MAE=1028.5876 Epoch: 50/500MAE=1010.1096 MAE=983.1319 MAE=964.0963 MAE=953.8424 MAE=936.1939 MAE=910.3254 MAE=874.0840 MAE=849.2797 MAE=820.5371 MAE=794.3102 Epoch: 60/500MAE=795.0350 MAE=810.3972 MAE=733.4836 MAE=722.2117 MAE=699.8837 MAE=677.1433 MAE=685.0701 MAE=622.8700 MAE=594.1187 MAE=601.1940 Epoch: 70/500MAE=536.8571 MAE=510.5311 MAE=481.1171 MAE=503.4367 MAE=472.9853 MAE=430.2282 MAE=372.5710 MAE=379.5818 MAE=308.6464 MAE=317.9465 Epoch: 80/500MAE=280.2046 MAE=253.0965 MAE=238.7951 MAE=208.2708 MAE=227.9170 MAE=167.5278 MAE=176.3048 MAE=138.8628 MAE=135.8453 MAE=121.3453 Epoch: 90/500MAE=133.1578 MAE=128.4920 MAE=108.9320 MAE=122.4043 MAE=135.0620 MAE=102.6431 MAE=107.1267 MAE=113.8346 MAE=100.9189 MAE=113.8953 Epoch: 100/500MAE=103.9971 MAE=97.3360 MAE=99.7650 MAE=101.2327 MAE=96.5598 MAE=94.6198 MAE=103.6650 MAE=96.5759 MAE=92.0519 MAE=96.1120 Epoch: 110/500MAE=94.5458 MAE=92.6172 MAE=96.2624 MAE=92.1285 MAE=91.6403 MAE=92.9443 MAE=91.8983 MAE=91.4507 MAE=95.2196 MAE=90.6538 Epoch: 120/500MAE=90.5903 MAE=95.5646 MAE=93.2910 MAE=91.2812 MAE=93.4593 MAE=90.9623 MAE=92.5898 MAE=91.7313 MAE=92.4542 MAE=90.6889 Epoch: 130/500MAE=91.1696 MAE=91.8337 MAE=91.6839 MAE=90.6366 MAE=91.2347 MAE=90.6572 MAE=91.1632 MAE=91.3317 MAE=91.2072 MAE=90.9235 Epoch: 140/500MAE=90.4688 MAE=91.1363 MAE=91.3570 MAE=91.1428 MAE=91.1960 MAE=91.2102 MAE=91.4585 MAE=91.5029 MAE=91.0937 MAE=90.9018 Epoch: 150/500MAE=91.0699 MAE=90.9936 MAE=91.1179 MAE=91.1136 MAE=90.9784 MAE=91.0681 MAE=90.9946 MAE=91.1738 MAE=91.0203 MAE=90.8922 Epoch: 160/500MAE=90.9343 MAE=90.9914 MAE=91.0239 MAE=91.0223 MAE=90.9290 MAE=91.1510 MAE=91.1811 MAE=91.1284 MAE=91.2553 MAE=91.1743 Epoch: 170/500MAE=91.2098 MAE=90.9995 MAE=91.2109 MAE=91.2031 MAE=91.0730 MAE=90.9063 MAE=91.1913 MAE=90.9020 MAE=91.0671 MAE=90.9970 Epoch: 180/500MAE=90.9429 MAE=90.9921 MAE=91.1028 MAE=91.0095 MAE=91.0006 MAE=91.1305 MAE=90.8699 MAE=91.0196 MAE=90.9829 MAE=91.0421 Epoch: 190/500MAE=91.0585 MAE=90.9842 MAE=91.2881 MAE=91.0616 MAE=90.8550 MAE=91.1292 MAE=91.1861 MAE=91.2867 MAE=91.3311 MAE=90.9601 Epoch: 200/500MAE=91.1522 MAE=91.1118 MAE=91.1624 MAE=91.3079 MAE=90.8520 MAE=90.8963 MAE=91.0444 MAE=90.9144 MAE=90.9571 MAE=91.1221 Epoch: 210/500MAE=90.9769 MAE=91.0453 MAE=91.1051 MAE=91.1531 MAE=91.2189 MAE=90.9281 MAE=91.2108 MAE=91.1627 MAE=91.2122 MAE=90.8543 Epoch: 220/500MAE=91.2180 MAE=90.9877 MAE=90.9703 MAE=91.1325 MAE=91.0134 MAE=91.1469 MAE=91.3334 MAE=91.0387 MAE=90.9893 MAE=90.9109 Epoch: 230/500MAE=91.2421 MAE=91.0407 MAE=91.2736 MAE=91.3203 MAE=91.0697 MAE=90.9340 MAE=91.1657 MAE=91.1768 MAE=91.0182 MAE=91.2757 Epoch: 240/500MAE=91.0765 MAE=91.0654 MAE=91.2328 MAE=91.2217 MAE=91.1581 MAE=90.9051 MAE=91.1547 MAE=91.2821 MAE=91.2796 MAE=91.1766 Epoch: 250/500MAE=91.1509 MAE=91.2853 MAE=91.1542 MAE=91.1075 MAE=90.9389 MAE=91.0690 MAE=90.9936 MAE=91.1900 MAE=91.1620 MAE=91.1531 Epoch: 260/500MAE=91.2849 MAE=91.0733 MAE=91.2957 MAE=91.1060 MAE=91.2758 MAE=91.1516 MAE=91.0085 MAE=90.9041 MAE=91.1087 MAE=91.1033 Epoch: 270/500MAE=91.2191 MAE=91.1611 MAE=91.0903 MAE=91.1089 MAE=90.8749 MAE=91.3412 MAE=91.2486 MAE=91.0810 MAE=91.0830 MAE=91.2538 Epoch: 280/500MAE=91.1206 MAE=91.0564 MAE=91.1585 MAE=91.2138 MAE=91.2042 MAE=91.2892 MAE=91.1496 MAE=91.1389 MAE=91.2267 MAE=90.9373 Epoch: 290/500MAE=91.1010 MAE=92.4081 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 91.661 +/- 1.410\n",
                        "\n",
                        "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/500MAE=1549.6150 MAE=1547.6302 MAE=1544.7346 MAE=1542.3718 MAE=1539.9789 MAE=1536.9453 MAE=1533.6816 MAE=1530.3635 MAE=1526.2855 Epoch: 10/500MAE=1521.2844 MAE=1514.2882 MAE=1509.8376 MAE=1504.8848 MAE=1498.7700 MAE=1492.9932 MAE=1483.3271 MAE=1473.4554 MAE=1470.0295 MAE=1457.2673 Epoch: 20/500MAE=1451.5530 MAE=1438.4215 MAE=1428.4360 MAE=1418.6868 MAE=1405.7878 MAE=1399.3987 MAE=1387.5334 MAE=1376.2693 MAE=1364.0481 MAE=1352.9546 Epoch: 30/500MAE=1339.6423 MAE=1326.0823 MAE=1316.2917 MAE=1301.4392 MAE=1288.4017 MAE=1271.6943 MAE=1260.8635 MAE=1243.3359 MAE=1226.6089 MAE=1209.4075 Epoch: 40/500MAE=1186.2549 MAE=1175.6979 MAE=1161.1423 MAE=1152.1337 MAE=1137.1630 MAE=1093.9165 MAE=1069.7490 MAE=1085.5748 MAE=1051.9088 MAE=1039.0298 Epoch: 50/500MAE=1010.8568 MAE=1000.1766 MAE=982.7783 MAE=971.7221 MAE=947.0696 MAE=922.3444 MAE=889.7389 MAE=873.5607 MAE=842.4465 MAE=818.5090 Epoch: 60/500MAE=799.9646 MAE=779.2515 MAE=744.8649 MAE=709.2362 MAE=705.1268 MAE=665.5605 MAE=392.3831 MAE=660.9072 MAE=600.2480 MAE=586.6929 Epoch: 70/500MAE=543.3602 MAE=514.8193 MAE=510.7490 MAE=536.8003 MAE=491.9420 MAE=494.4818 MAE=497.8795 MAE=480.4832 MAE=471.3356 MAE=468.5094 Epoch: 80/500MAE=468.9391 MAE=463.6009 MAE=468.0884 MAE=456.3526 MAE=459.4018 MAE=456.5927 MAE=455.1038 MAE=451.9470 MAE=455.5365 MAE=452.4090 Epoch: 90/500MAE=451.5394 MAE=451.0751 MAE=451.3223 MAE=450.6417 MAE=450.5850 MAE=450.4368 MAE=450.3443 MAE=450.0763 MAE=449.9698 MAE=449.5471 Epoch: 100/500MAE=449.3804 MAE=449.5941 MAE=449.6372 MAE=449.5083 MAE=449.4596 MAE=449.5326 MAE=449.3775 MAE=449.4003 MAE=449.3085 MAE=449.3447 Epoch: 110/500MAE=449.2788 MAE=449.1880 MAE=449.1732 MAE=449.1902 MAE=449.1720 MAE=449.1508 MAE=449.1257 MAE=449.1276 MAE=449.1044 MAE=449.0723 Epoch: 120/500MAE=449.0661 MAE=449.0516 MAE=448.9596 MAE=448.9468 MAE=448.9464 MAE=448.9827 MAE=448.9283 MAE=448.8963 MAE=448.9737 MAE=448.9061 Epoch: 130/500MAE=448.9280 MAE=448.9312 MAE=448.8964 MAE=448.8605 MAE=448.7671 MAE=448.7510 MAE=448.7674 MAE=448.7337 MAE=448.7392 MAE=448.6892 Epoch: 140/500MAE=448.6147 MAE=448.6422 MAE=448.5993 MAE=448.5790 MAE=448.6360 MAE=448.5773 MAE=448.5565 MAE=448.5864 MAE=448.5289 MAE=448.5034 Epoch: 150/500MAE=448.5807 MAE=448.4983 MAE=448.4636 MAE=448.4614 MAE=448.3996 MAE=448.4533 MAE=448.4329 MAE=448.3226 MAE=448.3185 MAE=448.3032 Epoch: 160/500MAE=448.3117 MAE=448.3033 MAE=448.3599 MAE=448.2489 MAE=448.2219 MAE=448.2375 MAE=448.1554 MAE=448.0721 MAE=448.1173 MAE=448.1741 Epoch: 170/500MAE=448.1844 MAE=448.1164 MAE=448.0439 MAE=448.0355 MAE=448.0343 MAE=448.0251 MAE=448.0280 MAE=447.9469 MAE=447.9594 MAE=447.9041 Epoch: 180/500MAE=447.9185 MAE=447.8948 MAE=447.9099 MAE=447.8214 MAE=447.7999 MAE=447.7599 MAE=447.7324 MAE=447.7378 MAE=447.7498 MAE=447.7914 Epoch: 190/500MAE=447.6986 MAE=447.6416 MAE=447.5792 MAE=447.6636 MAE=447.6111 MAE=447.5855 MAE=447.5771 MAE=447.6318 MAE=447.5951 MAE=447.5613 Epoch: 200/500MAE=447.5010 MAE=447.4393 MAE=447.4741 MAE=447.4404 MAE=447.3879 MAE=447.4151 MAE=447.4210 MAE=447.4397 MAE=447.3810 MAE=447.3318 Epoch: 210/500MAE=447.3651 MAE=447.3167 MAE=447.3068 MAE=447.3069 MAE=447.2974 MAE=447.2831 MAE=447.2054 MAE=399.1884 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 399.188 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.6947 MAE=1547.6172 MAE=1544.9921 MAE=1542.4275 MAE=1540.1807 MAE=1537.0778 MAE=1533.9388 MAE=1530.8375 MAE=1526.6021 Epoch: 10/500MAE=1521.6625 MAE=1515.8008 MAE=1511.5286 MAE=1505.3774 MAE=1498.8044 MAE=1493.3604 MAE=1486.5780 MAE=1478.9075 MAE=1472.9603 MAE=1464.5494 Epoch: 20/500MAE=1453.2620 MAE=1445.4315 MAE=1434.9862 MAE=1425.1185 MAE=1407.6595 MAE=1399.1367 MAE=1391.0544 MAE=1385.0321 MAE=1367.9791 MAE=1365.5691 Epoch: 30/500MAE=1348.6211 MAE=1329.8944 MAE=1319.2185 MAE=1304.5862 MAE=1292.6409 MAE=1274.4923 MAE=1253.3591 MAE=1247.4514 MAE=1230.8196 MAE=1208.3628 Epoch: 40/500MAE=1192.9711 MAE=1175.5576 MAE=1172.4148 MAE=1144.4631 MAE=1129.2153 MAE=1111.7595 MAE=1087.1331 MAE=1076.6438 MAE=1057.8464 MAE=1033.0664 Epoch: 50/500MAE=1017.9512 MAE=1011.0927 MAE=990.7607 MAE=945.3225 MAE=906.9955 MAE=899.4634 MAE=917.3724 MAE=859.4799 MAE=840.1337 MAE=801.5006 Epoch: 60/500MAE=784.5906 MAE=766.1519 MAE=761.1332 MAE=772.8596 MAE=667.7228 MAE=672.2607 MAE=641.5887 MAE=600.6126 MAE=606.8813 MAE=535.3848 Epoch: 70/500MAE=563.6802 MAE=533.4447 MAE=477.8014 MAE=480.9654 MAE=461.9792 MAE=445.5940 MAE=411.3769 MAE=411.8030 MAE=340.2209 MAE=320.6182 Epoch: 80/500MAE=338.2710 MAE=347.4197 MAE=241.7905 MAE=254.6601 MAE=215.0243 MAE=221.6302 MAE=171.5465 MAE=156.3418 MAE=167.2140 MAE=149.2470 Epoch: 90/500MAE=146.6544 MAE=146.0131 MAE=131.4293 MAE=124.5782 MAE=117.8653 MAE=117.2988 MAE=126.9100 MAE=139.7699 MAE=122.6119 MAE=108.6827 Epoch: 100/500MAE=111.6225 MAE=119.5588 MAE=100.8369 MAE=112.0279 MAE=106.9481 MAE=105.0700 MAE=108.1607 MAE=101.9049 MAE=98.3635 MAE=101.2605 Epoch: 110/500MAE=98.8026 MAE=101.3134 MAE=100.2006 MAE=98.5145 MAE=99.6870 MAE=98.7312 MAE=99.4827 MAE=98.2465 MAE=99.0476 MAE=98.3432 Epoch: 120/500MAE=99.0578 MAE=98.2341 MAE=98.3660 MAE=97.7884 MAE=98.0227 MAE=98.2432 MAE=99.3374 MAE=98.6495 MAE=97.7638 MAE=98.4339 Epoch: 130/500MAE=98.4167 MAE=98.4005 MAE=97.6164 MAE=97.9086 MAE=97.9481 MAE=98.3079 MAE=98.0339 MAE=97.9914 MAE=98.2551 MAE=97.9266 Epoch: 140/500MAE=97.7626 MAE=97.8321 MAE=97.7763 MAE=97.7472 MAE=97.8944 MAE=98.1570 MAE=98.1027 MAE=98.1844 MAE=98.2468 MAE=98.2526 Epoch: 150/500MAE=98.2513 MAE=98.2658 MAE=98.2750 MAE=98.2425 MAE=98.2330 MAE=98.2290 MAE=98.2326 MAE=98.1404 MAE=98.2352 MAE=98.2316 Epoch: 160/500MAE=98.2478 MAE=98.2680 MAE=98.2421 MAE=98.2591 MAE=98.2497 MAE=98.2806 MAE=98.2829 MAE=98.2470 MAE=98.2510 MAE=98.2603 Epoch: 170/500MAE=98.2268 MAE=98.2516 MAE=98.2447 MAE=98.2829 MAE=98.2935 MAE=98.2417 MAE=98.2323 MAE=98.2767 MAE=98.2245 MAE=98.2850 Epoch: 180/500MAE=98.2835 MAE=98.2319 MAE=98.2814 MAE=98.2920 MAE=98.2748 MAE=98.2748 MAE=98.3113 MAE=98.2943 MAE=98.2873 MAE=98.2725 Epoch: 190/500MAE=98.2800 MAE=98.2816 MAE=98.2880 MAE=98.3071 MAE=98.2635 MAE=98.2740 MAE=98.2796 MAE=98.2549 MAE=98.2467 MAE=98.2576 Epoch: 200/500MAE=98.2426 MAE=98.2433 MAE=98.2576 MAE=98.2643 MAE=98.2677 MAE=98.2517 MAE=98.2622 MAE=98.2582 MAE=98.2477 MAE=98.2458 Epoch: 210/500MAE=98.2238 MAE=98.2450 MAE=98.2270 MAE=98.2544 MAE=98.2405 MAE=98.2349 MAE=98.2144 MAE=98.2421 MAE=98.2345 MAE=98.2290 Epoch: 220/500MAE=98.2228 MAE=98.2222 MAE=98.2529 MAE=98.2474 MAE=98.2186 MAE=98.2474 MAE=98.2371 MAE=98.2393 MAE=98.2293 MAE=98.2261 Epoch: 230/500MAE=98.2455 MAE=98.2232 MAE=98.1863 MAE=98.1982 MAE=98.1888 MAE=98.2425 MAE=98.2164 MAE=98.2262 MAE=98.2515 MAE=98.2409 Epoch: 240/500MAE=98.2320 MAE=98.2308 MAE=98.2691 MAE=98.2401 MAE=98.2119 MAE=98.2027 MAE=98.2118 MAE=98.1958 MAE=98.2005 MAE=98.1909 Epoch: 250/500MAE=98.2257 MAE=98.1976 MAE=98.2155 MAE=98.2160 MAE=98.2043 MAE=98.2114 MAE=98.2003 MAE=98.2320 MAE=98.2179 MAE=98.1813 Epoch: 260/500MAE=98.1597 MAE=98.1754 MAE=98.1626 MAE=98.1607 MAE=98.1967 MAE=98.2124 MAE=98.1855 MAE=98.1948 MAE=98.1655 MAE=98.1661 Epoch: 270/500MAE=98.1483 MAE=98.1646 MAE=98.1514 MAE=98.1555 MAE=98.1946 MAE=98.1679 MAE=98.2162 MAE=98.2081 MAE=98.1594 MAE=98.1921 Epoch: 280/500MAE=98.2226 MAE=98.1560 MAE=98.1870 MAE=96.8991 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 248.044 +/- 151.145\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.8346 MAE=1547.8005 MAE=1545.0952 MAE=1542.2424 MAE=1539.4373 MAE=1537.2207 MAE=1533.7358 MAE=1530.8104 MAE=1526.2502 Epoch: 10/500MAE=1522.1958 MAE=1517.1644 MAE=1510.9921 MAE=1505.8412 MAE=1499.9452 MAE=1494.2551 MAE=1487.2903 MAE=1479.5977 MAE=1470.8557 MAE=1463.1941 Epoch: 20/500MAE=1454.2329 MAE=1446.1431 MAE=1442.1783 MAE=1431.0731 MAE=1418.4330 MAE=1403.7373 MAE=1396.7594 MAE=1377.9475 MAE=1374.9722 MAE=1358.5764 Epoch: 30/500MAE=1346.7483 MAE=1332.4567 MAE=1320.3740 MAE=1309.0977 MAE=1287.0188 MAE=1266.5789 MAE=1270.1497 MAE=1243.4049 MAE=1231.1464 MAE=1216.9396 Epoch: 40/500MAE=1198.8918 MAE=1179.2504 MAE=1160.3652 MAE=1143.6262 MAE=1136.0374 MAE=1120.2417 MAE=1090.5687 MAE=1086.6443 MAE=1063.2919 MAE=1039.1401 Epoch: 50/500MAE=1020.4456 MAE=990.0697 MAE=997.5293 MAE=934.5623 MAE=932.6998 MAE=984.3231 MAE=898.8510 MAE=887.3536 MAE=864.6166 MAE=840.9911 Epoch: 60/500MAE=793.7649 MAE=804.0853 MAE=770.7209 MAE=750.9659 MAE=720.9889 MAE=648.7050 MAE=468.3692 MAE=611.8524 MAE=594.9429 MAE=609.7750 Epoch: 70/500MAE=557.8392 MAE=557.5000 MAE=522.2467 MAE=524.1964 MAE=497.7676 MAE=487.9048 MAE=493.1401 MAE=474.8061 MAE=480.1779 MAE=472.1566 Epoch: 80/500MAE=459.1775 MAE=464.3793 MAE=461.6004 MAE=453.4124 MAE=453.8707 MAE=447.4492 MAE=446.6574 MAE=446.1199 MAE=442.9326 MAE=433.3190 Epoch: 90/500MAE=435.9160 MAE=433.8284 MAE=429.7707 MAE=427.3317 MAE=424.6331 MAE=418.6285 MAE=415.6028 MAE=414.9262 MAE=413.0374 MAE=408.6390 Epoch: 100/500MAE=405.9452 MAE=407.0631 MAE=399.2636 MAE=400.5371 MAE=397.9237 MAE=397.4212 MAE=390.2472 MAE=391.5049 MAE=384.0574 MAE=381.4755 Epoch: 110/500MAE=382.3450 MAE=380.6204 MAE=376.1420 MAE=371.0399 MAE=374.0895 MAE=365.5298 MAE=364.8008 MAE=364.6000 MAE=361.4753 MAE=352.7061 Epoch: 120/500MAE=356.9822 MAE=351.0490 MAE=351.5058 MAE=351.1511 MAE=346.3469 MAE=343.7235 MAE=339.3786 MAE=337.5546 MAE=335.2084 MAE=331.2053 Epoch: 130/500MAE=328.5349 MAE=328.1573 MAE=324.7861 MAE=325.4978 MAE=320.2982 MAE=322.3794 MAE=315.5142 MAE=313.2818 MAE=307.0115 MAE=312.3715 Epoch: 140/500MAE=308.7427 MAE=305.9645 MAE=302.0599 MAE=302.6850 MAE=300.2274 MAE=298.0265 MAE=295.5303 MAE=286.0428 MAE=282.5814 MAE=284.7825 Epoch: 150/500MAE=287.3430 MAE=284.3711 MAE=277.1342 MAE=279.7713 MAE=275.6873 MAE=269.0551 MAE=265.1492 MAE=272.4574 MAE=267.7215 MAE=262.9310 Epoch: 160/500MAE=256.3540 MAE=258.7314 MAE=253.5537 MAE=251.4588 MAE=250.0223 MAE=248.4610 MAE=243.4842 MAE=237.7169 MAE=241.5961 MAE=240.2518 Epoch: 170/500MAE=238.6105 MAE=235.3826 MAE=239.3888 MAE=233.8663 MAE=225.2752 MAE=228.1157 MAE=225.6426 MAE=220.8118 MAE=217.6466 MAE=223.9113 Epoch: 180/500MAE=228.4275 MAE=211.3274 MAE=215.1134 MAE=215.0894 MAE=209.8220 MAE=209.2429 MAE=202.9179 MAE=203.7341 MAE=202.1355 MAE=206.7377 Epoch: 190/500MAE=206.3816 MAE=196.5374 MAE=189.9071 MAE=193.7073 MAE=193.5377 MAE=189.2033 MAE=187.4830 MAE=189.3009 MAE=187.2929 MAE=184.6854 Epoch: 200/500MAE=176.8347 MAE=178.1779 MAE=179.1370 MAE=175.5650 MAE=177.3296 MAE=180.9213 MAE=169.1195 MAE=165.6740 MAE=168.9632 MAE=168.8813 Epoch: 210/500MAE=167.3239 MAE=167.1437 MAE=162.2444 MAE=163.5672 MAE=160.4859 MAE=159.1411 MAE=159.3214 MAE=156.7072 MAE=159.3746 MAE=159.5914 Epoch: 220/500MAE=156.6432 MAE=157.0890 MAE=156.5170 MAE=153.1397 MAE=157.1075 MAE=156.2871 MAE=152.8166 MAE=151.2349 MAE=152.1918 MAE=149.8261 Epoch: 230/500MAE=150.0973 MAE=150.6437 MAE=149.2484 MAE=149.7590 MAE=150.8246 MAE=148.1672 MAE=146.6571 MAE=144.3989 MAE=144.9994 MAE=146.9426 Epoch: 240/500MAE=145.6585 MAE=145.0991 MAE=144.0018 MAE=143.9719 MAE=142.2327 MAE=142.9735 MAE=141.9647 MAE=141.0868 MAE=142.1005 MAE=142.3226 Epoch: 250/500MAE=141.3870 MAE=140.7164 MAE=139.8679 MAE=141.1725 MAE=139.5440 MAE=139.9399 MAE=139.2044 MAE=138.6519 MAE=137.4014 MAE=137.6395 Epoch: 260/500MAE=138.3365 MAE=138.1009 MAE=138.0320 MAE=137.4872 MAE=137.0316 MAE=136.8488 MAE=136.9344 MAE=136.4132 MAE=136.7220 MAE=137.4261 Epoch: 270/500MAE=137.0135 MAE=137.0197 MAE=136.2509 MAE=135.8326 MAE=136.2128 MAE=136.3378 MAE=135.4278 MAE=135.1864 MAE=135.6493 MAE=135.6151 Epoch: 280/500MAE=135.7783 MAE=135.4209 MAE=135.6978 MAE=135.5847 MAE=135.4143 MAE=135.3800 MAE=135.5438 MAE=135.5157 MAE=135.4056 MAE=135.3960 Epoch: 290/500MAE=135.3560 MAE=135.3549 MAE=135.3846 MAE=135.3728 MAE=135.2514 MAE=135.2973 MAE=135.3171 MAE=135.1206 MAE=135.2648 MAE=135.2763 Epoch: 300/500MAE=135.2326 MAE=135.3101 MAE=135.2945 MAE=135.2003 MAE=135.2159 MAE=135.1504 MAE=135.2097 MAE=135.1086 MAE=135.2208 MAE=135.1127 Epoch: 310/500MAE=135.2492 MAE=135.1788 MAE=135.0313 MAE=135.1501 MAE=135.1379 MAE=135.0321 MAE=135.1527 MAE=135.1259 MAE=134.9938 MAE=135.1130 Epoch: 320/500MAE=135.1077 MAE=135.0822 MAE=135.0354 MAE=135.1195 MAE=135.0610 MAE=135.2236 MAE=135.2421 MAE=135.0506 MAE=135.1159 MAE=135.1135 Epoch: 330/500MAE=134.9916 MAE=135.0245 MAE=134.9409 MAE=135.0557 MAE=135.0331 MAE=135.0098 MAE=134.9724 MAE=134.9999 MAE=134.9240 MAE=134.9212 Epoch: 340/500MAE=134.9646 MAE=134.9493 MAE=134.9431 MAE=134.9343 MAE=134.9099 MAE=134.9117 MAE=134.8966 MAE=134.8734 MAE=134.8695 MAE=134.8827 Epoch: 350/500MAE=134.9264 MAE=134.8256 MAE=134.7970 MAE=134.8990 MAE=134.8506 MAE=134.7317 MAE=134.7278 MAE=134.6169 MAE=134.7204 MAE=134.7411 Epoch: 360/500MAE=134.7058 MAE=134.7332 MAE=134.7481 MAE=134.6876 MAE=134.6980 MAE=134.6394 MAE=134.6402 MAE=134.6823 MAE=134.6332 MAE=134.5602 Epoch: 370/500MAE=134.5948 MAE=134.6185 MAE=134.6234 MAE=134.6242 MAE=134.6115 MAE=134.5892 MAE=134.6057 MAE=134.5907 MAE=134.6095 MAE=134.4861 Epoch: 380/500MAE=134.5702 MAE=134.5355 MAE=134.4955 MAE=134.3973 MAE=134.5071 MAE=134.4457 MAE=134.4507 MAE=134.3793 MAE=134.4521 MAE=134.3984 Epoch: 390/500MAE=134.4272 MAE=134.3836 MAE=134.4041 MAE=134.4236 MAE=134.3899 MAE=134.4141 MAE=134.3262 MAE=134.3758 MAE=134.3133 MAE=134.4172 Epoch: 400/500MAE=134.2981 MAE=134.3155 MAE=134.2787 MAE=134.1612 MAE=134.2087 MAE=134.1862 MAE=134.1783 MAE=134.0998 MAE=134.1653 MAE=134.0351 Epoch: 410/500MAE=134.0915 MAE=134.1364 MAE=134.1436 MAE=134.0552 MAE=133.9856 MAE=134.0486 MAE=133.9717 MAE=133.9539 MAE=134.0587 MAE=134.0267 Epoch: 420/500MAE=133.9836 MAE=133.9838 MAE=134.0556 MAE=134.0293 MAE=133.9816 MAE=133.9574 MAE=133.8953 MAE=133.8694 MAE=133.9274 MAE=133.8825 Epoch: 430/500MAE=133.9084 MAE=133.9224 MAE=133.8346 MAE=133.7982 MAE=133.5876 MAE=133.6650 MAE=133.7612 MAE=133.8000 MAE=133.7424 MAE=133.7335 Epoch: 440/500MAE=133.7200 MAE=133.6923 MAE=133.7626 MAE=133.7034 MAE=133.6810 MAE=133.6938 MAE=133.6889 MAE=133.6780 MAE=133.7402 MAE=133.6514 Epoch: 450/500MAE=133.6856 MAE=133.6584 MAE=133.6095 MAE=133.6057 MAE=133.5950 MAE=133.5793 MAE=133.5399 MAE=133.5871 MAE=133.5326 MAE=133.6161 Epoch: 460/500MAE=133.4247 MAE=133.5617 MAE=133.4324 MAE=133.3647 MAE=133.3739 MAE=133.3612 MAE=133.4131 MAE=133.3255 MAE=133.3585 MAE=133.3947 Epoch: 470/500MAE=133.3560 MAE=133.3082 MAE=133.3651 MAE=133.2969 MAE=133.2708 MAE=133.3036 MAE=133.2956 MAE=133.2978 MAE=133.2674 MAE=133.2374 Epoch: 480/500MAE=133.1609 MAE=133.1461 MAE=133.2423 MAE=133.3000 MAE=133.2155 MAE=133.1938 MAE=133.1865 MAE=133.1920 MAE=133.1984 MAE=133.1774 Epoch: 490/500MAE=133.1180 MAE=133.1410 MAE=133.1020 MAE=133.2297 MAE=133.1543 MAE=133.0982 MAE=133.1208 MAE=133.1096 MAE=133.1186 MAE=133.0439 Epoch: 500/500MAE=133.0350 MAE=135.5066 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 210.531 +/- 134.329\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.7745 MAE=1548.1001 MAE=1545.7732 MAE=1543.7595 MAE=1541.4083 MAE=1538.5277 MAE=1535.0441 MAE=1531.5669 MAE=1527.5427 Epoch: 10/500MAE=1523.2314 MAE=1518.7517 MAE=1513.0027 MAE=1507.5181 MAE=1501.6084 MAE=1493.3201 MAE=1491.0144 MAE=1475.9172 MAE=1475.3174 MAE=1464.1782 Epoch: 20/500MAE=1451.9597 MAE=1441.8290 MAE=1441.7720 MAE=1425.1316 MAE=1412.3989 MAE=1412.4883 MAE=1393.7039 MAE=1385.2969 MAE=1376.3452 MAE=1361.2838 Epoch: 30/500MAE=1337.0657 MAE=1324.2778 MAE=1319.2865 MAE=1306.8712 MAE=1293.5935 MAE=1278.3121 MAE=1260.4181 MAE=1242.0162 MAE=1224.3734 MAE=1225.5710 Epoch: 40/500MAE=1211.4307 MAE=1172.1741 MAE=1173.3989 MAE=1135.8193 MAE=1131.6562 MAE=1128.4902 MAE=1102.7832 MAE=1093.6646 MAE=1063.4188 MAE=1056.5010 Epoch: 50/500MAE=1025.9287 MAE=1018.6683 MAE=970.5842 MAE=950.7797 MAE=957.7524 MAE=929.3264 MAE=876.7589 MAE=878.5382 MAE=818.3503 MAE=843.5558 Epoch: 60/500MAE=830.9531 MAE=816.3555 MAE=763.8676 MAE=713.4150 MAE=711.9448 MAE=669.8607 MAE=689.7310 MAE=619.8175 MAE=604.5901 MAE=588.7543 Epoch: 70/500MAE=549.8692 MAE=507.7747 MAE=567.3622 MAE=472.8159 MAE=495.3335 MAE=437.9888 MAE=429.6032 MAE=374.9061 MAE=368.4820 MAE=371.4820 Epoch: 80/500MAE=328.7238 MAE=314.7172 MAE=314.6356 MAE=292.2406 MAE=223.5808 MAE=244.4505 MAE=293.8949 MAE=201.9633 MAE=176.8973 MAE=209.9860 Epoch: 90/500MAE=160.3203 MAE=156.6130 MAE=159.9136 MAE=138.7835 MAE=145.5314 MAE=146.1558 MAE=118.1930 MAE=141.8436 MAE=108.9249 MAE=117.5486 Epoch: 100/500MAE=107.9115 MAE=107.7318 MAE=106.5868 MAE=105.2525 MAE=108.6012 MAE=109.8380 MAE=104.2740 MAE=107.8925 MAE=102.7223 MAE=104.0348 Epoch: 110/500MAE=108.5188 MAE=104.7757 MAE=101.0472 MAE=101.4373 MAE=105.9886 MAE=99.0712 MAE=105.4413 MAE=107.3537 MAE=98.6666 MAE=103.5993 Epoch: 120/500MAE=105.1835 MAE=106.3799 MAE=101.4523 MAE=101.3056 MAE=100.3997 MAE=100.2349 MAE=99.2875 MAE=99.0063 MAE=99.9065 MAE=99.7874 Epoch: 130/500MAE=97.8919 MAE=99.1802 MAE=101.3439 MAE=98.7397 MAE=97.7169 MAE=97.1213 MAE=100.9057 MAE=97.0728 MAE=97.8181 MAE=99.6139 Epoch: 140/500MAE=98.3582 MAE=98.2980 MAE=99.1090 MAE=98.8947 MAE=99.1427 MAE=99.2151 MAE=99.0858 MAE=98.5100 MAE=99.2795 MAE=99.0233 Epoch: 150/500MAE=99.4054 MAE=98.9594 MAE=98.6364 MAE=99.0450 MAE=98.9945 MAE=98.9465 MAE=98.8735 MAE=98.9173 MAE=98.9034 MAE=98.8739 Epoch: 160/500MAE=98.8931 MAE=99.0217 MAE=98.9972 MAE=98.9667 MAE=98.9330 MAE=98.9329 MAE=98.9840 MAE=98.9672 MAE=98.9473 MAE=99.0173 Epoch: 170/500MAE=98.9769 MAE=98.9987 MAE=98.9650 MAE=98.9856 MAE=99.0080 MAE=99.0141 MAE=99.0248 MAE=99.0054 MAE=98.9883 MAE=98.9954 Epoch: 180/500MAE=98.9717 MAE=98.9771 MAE=98.9900 MAE=98.9697 MAE=98.9781 MAE=98.9798 MAE=98.9857 MAE=98.9736 MAE=98.9730 MAE=98.9646 Epoch: 190/500MAE=98.9954 MAE=98.9371 MAE=98.9288 MAE=98.9733 MAE=98.9485 MAE=98.9912 MAE=98.9177 MAE=98.9510 MAE=98.9247 MAE=98.9339 Epoch: 200/500MAE=98.9482 MAE=98.9019 MAE=98.9111 MAE=98.9187 MAE=98.9150 MAE=98.9445 MAE=98.9309 MAE=98.9147 MAE=98.9963 MAE=98.9340 Epoch: 210/500MAE=98.9307 MAE=98.9434 MAE=98.9818 MAE=98.9268 MAE=98.9634 MAE=98.9212 MAE=98.9117 MAE=98.9075 MAE=98.9020 MAE=98.8908 Epoch: 220/500MAE=98.8887 MAE=98.8840 MAE=98.9255 MAE=98.8879 MAE=98.9068 MAE=98.9052 MAE=98.9100 MAE=98.9299 MAE=98.9263 MAE=98.8976 Epoch: 230/500MAE=98.9242 MAE=98.9036 MAE=98.8927 MAE=98.8812 MAE=98.9168 MAE=98.8763 MAE=98.8972 MAE=98.9073 MAE=98.8972 MAE=98.8567 Epoch: 240/500MAE=98.9102 MAE=98.9223 MAE=98.8995 MAE=98.8864 MAE=98.9071 MAE=98.9187 MAE=98.9230 MAE=98.9415 MAE=98.9407 MAE=98.9388 Epoch: 250/500MAE=98.9031 MAE=98.9258 MAE=98.9082 MAE=98.8819 MAE=98.8686 MAE=98.8750 MAE=98.8805 MAE=98.8914 MAE=98.8744 MAE=98.9123 Epoch: 260/500MAE=98.8708 MAE=98.8624 MAE=98.8406 MAE=98.8495 MAE=98.8622 MAE=98.8777 MAE=98.8477 MAE=98.8436 MAE=98.8348 MAE=98.8489 Epoch: 270/500MAE=98.8455 MAE=98.8508 MAE=98.8380 MAE=98.8909 MAE=98.8310 MAE=98.8204 MAE=98.8966 MAE=98.8727 MAE=98.8088 MAE=98.8926 Epoch: 280/500MAE=98.9572 MAE=98.9664 MAE=98.8556 MAE=98.9599 MAE=98.9007 MAE=98.8743 MAE=98.8979 MAE=98.9629 MAE=98.3040 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 182.475 +/- 126.074\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.3806 MAE=1547.2412 MAE=1544.7053 MAE=1542.5286 MAE=1539.9795 MAE=1537.8289 MAE=1534.4240 MAE=1530.6750 MAE=1526.7167 Epoch: 10/500MAE=1522.0057 MAE=1516.4739 MAE=1510.1028 MAE=1502.3203 MAE=1496.7281 MAE=1489.9438 MAE=1482.9155 MAE=1478.8806 MAE=1470.0134 MAE=1460.5913 Epoch: 20/500MAE=1454.1925 MAE=1439.4175 MAE=1432.4692 MAE=1427.1710 MAE=1411.1582 MAE=1400.0693 MAE=1390.2678 MAE=1382.8033 MAE=1372.4331 MAE=1355.6609 Epoch: 30/500MAE=1356.5859 MAE=1328.7850 MAE=1319.1299 MAE=1298.8696 MAE=1285.0187 MAE=1272.2249 MAE=1264.1149 MAE=1236.6511 MAE=1232.0908 MAE=1214.7256 Epoch: 40/500MAE=1182.9825 MAE=1177.4607 MAE=1160.3517 MAE=1144.4600 MAE=1123.7305 MAE=1106.0286 MAE=1086.8311 MAE=1068.9934 MAE=1048.4956 MAE=1019.1400 Epoch: 50/500MAE=1011.4852 MAE=1005.5060 MAE=962.0674 MAE=947.1110 MAE=934.2147 MAE=884.7502 MAE=899.1692 MAE=859.9905 MAE=831.9679 MAE=845.3816 Epoch: 60/500MAE=776.7664 MAE=762.8982 MAE=754.2229 MAE=772.4304 MAE=739.8996 MAE=666.6400 MAE=673.5842 MAE=640.9362 MAE=619.4087 MAE=600.0981 Epoch: 70/500MAE=550.6371 MAE=525.8920 MAE=488.0091 MAE=567.2250 MAE=469.3535 MAE=488.3845 MAE=391.1512 MAE=373.8184 MAE=383.6026 MAE=301.6657 Epoch: 80/500MAE=315.6578 MAE=255.5364 MAE=243.3982 MAE=217.4434 MAE=195.0081 MAE=231.7378 MAE=188.5977 MAE=177.5530 MAE=130.3033 MAE=163.2375 Epoch: 90/500MAE=132.1892 MAE=139.9127 MAE=142.7945 MAE=121.0090 MAE=122.4436 MAE=120.4147 MAE=105.7356 MAE=124.3037 MAE=106.4194 MAE=110.6514 Epoch: 100/500MAE=107.6390 MAE=103.4968 MAE=110.9027 MAE=104.2970 MAE=103.7723 MAE=105.3386 MAE=104.4336 MAE=102.4087 MAE=104.2938 MAE=102.9351 Epoch: 110/500MAE=102.4096 MAE=101.8502 MAE=100.5797 MAE=100.8730 MAE=104.3204 MAE=102.0830 MAE=100.1806 MAE=101.2511 MAE=101.9495 MAE=100.9852 Epoch: 120/500MAE=99.3008 MAE=101.4486 MAE=101.1080 MAE=98.0664 MAE=100.3094 MAE=100.8362 MAE=103.4797 MAE=98.3336 MAE=100.2474 MAE=99.8257 Epoch: 130/500MAE=99.8119 MAE=100.4942 MAE=99.5086 MAE=99.8392 MAE=100.0227 MAE=100.2015 MAE=99.9097 MAE=99.8149 MAE=99.8641 MAE=99.5293 Epoch: 140/500MAE=99.5882 MAE=99.4316 MAE=99.2628 MAE=99.4022 MAE=99.4037 MAE=99.4239 MAE=99.4423 MAE=99.4722 MAE=99.4484 MAE=99.4041 Epoch: 150/500MAE=99.3460 MAE=99.3997 MAE=99.3426 MAE=99.4127 MAE=99.3429 MAE=99.3642 MAE=99.3657 MAE=99.3463 MAE=99.3895 MAE=99.3556 Epoch: 160/500MAE=99.4402 MAE=99.3949 MAE=99.4623 MAE=99.4250 MAE=99.4611 MAE=99.3815 MAE=99.4033 MAE=99.3716 MAE=99.3486 MAE=99.3503 Epoch: 170/500MAE=99.3364 MAE=99.4233 MAE=99.3736 MAE=99.4099 MAE=99.3577 MAE=99.4081 MAE=99.4042 MAE=99.3861 MAE=99.3861 MAE=99.3795 Epoch: 180/500MAE=99.3725 MAE=99.3963 MAE=99.4131 MAE=99.3630 MAE=99.3378 MAE=99.3806 MAE=99.3786 MAE=99.3248 MAE=99.3160 MAE=99.3146 Epoch: 190/500MAE=99.3429 MAE=99.2878 MAE=99.2669 MAE=99.3121 MAE=99.2589 MAE=99.3169 MAE=99.3750 MAE=99.3277 MAE=99.3193 MAE=99.2824 Epoch: 200/500MAE=99.2724 MAE=99.2569 MAE=99.2468 MAE=99.2676 MAE=99.2583 MAE=99.2601 MAE=99.2583 MAE=99.3340 MAE=99.2723 MAE=99.2413 Epoch: 210/500MAE=99.3013 MAE=99.2885 MAE=99.3083 MAE=99.2973 MAE=99.2799 MAE=99.2641 MAE=99.3013 MAE=99.2544 MAE=99.2921 MAE=99.2708 Epoch: 220/500MAE=99.2313 MAE=99.2525 MAE=99.2988 MAE=99.2977 MAE=99.2255 MAE=99.2885 MAE=99.2377 MAE=99.2929 MAE=99.2494 MAE=99.2242 Epoch: 230/500MAE=99.2248 MAE=99.1777 MAE=99.2984 MAE=99.2538 MAE=99.2321 MAE=99.2487 MAE=99.3087 MAE=99.3273 MAE=99.2337 MAE=99.1820 Epoch: 240/500MAE=99.2041 MAE=99.2019 MAE=99.2022 MAE=99.1809 MAE=99.2443 MAE=99.2488 MAE=99.1358 MAE=99.2522 MAE=99.2088 MAE=99.1932 Epoch: 250/500MAE=99.1919 MAE=99.1600 MAE=99.1643 MAE=99.1504 MAE=99.1422 MAE=99.1121 MAE=99.1118 MAE=99.1142 MAE=99.1246 MAE=99.1423 Epoch: 260/500MAE=99.1296 MAE=99.1046 MAE=99.0796 MAE=99.1431 MAE=99.1302 MAE=99.0683 MAE=99.1270 MAE=99.1050 MAE=99.0464 MAE=99.1546 Epoch: 270/500MAE=99.1088 MAE=99.2167 MAE=99.2335 MAE=99.1956 MAE=102.9388 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 166.567 +/- 117.166\n",
                        "\n",
                        "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/500MAE=1549.6572 MAE=1547.6973 MAE=1545.2377 MAE=1543.1116 MAE=1540.9504 MAE=1538.1926 MAE=1535.1061 MAE=1531.9686 MAE=1527.7889 Epoch: 10/500MAE=1523.4015 MAE=1515.4363 MAE=1507.7097 MAE=1500.9128 MAE=1494.7717 MAE=1487.6423 MAE=1483.1072 MAE=1471.7354 MAE=1466.5488 MAE=1460.6624 Epoch: 20/500MAE=1447.3804 MAE=1446.2354 MAE=1436.5443 MAE=1426.1136 MAE=1411.7291 MAE=1400.6548 MAE=1389.9321 MAE=1374.6973 MAE=1372.3422 MAE=1362.7061 Epoch: 30/500MAE=1350.3481 MAE=1334.8448 MAE=1311.4297 MAE=1302.8726 MAE=1295.0610 MAE=1285.1205 MAE=1258.1165 MAE=1240.8634 MAE=1232.1609 MAE=1225.3494 Epoch: 40/500MAE=1199.9984 MAE=1186.3773 MAE=1165.2529 MAE=1150.7161 MAE=1130.0668 MAE=1115.9307 MAE=1098.7896 MAE=1089.2207 MAE=1064.9766 MAE=1036.9899 Epoch: 50/500MAE=1037.8007 MAE=1001.7024 MAE=1002.1243 MAE=965.1636 MAE=937.7885 MAE=917.2433 MAE=899.3974 MAE=878.3220 MAE=860.3746 MAE=830.6730 Epoch: 60/500MAE=820.6469 MAE=796.2928 MAE=761.5148 MAE=740.0946 MAE=723.0585 MAE=669.3820 MAE=637.9417 MAE=618.6134 MAE=603.8188 MAE=595.9412 Epoch: 70/500MAE=563.6207 MAE=543.6824 MAE=510.6570 MAE=538.1465 MAE=450.9413 MAE=412.6882 MAE=472.3735 MAE=331.6718 MAE=413.4397 MAE=355.6819 Epoch: 80/500MAE=305.2731 MAE=301.4408 MAE=228.9078 MAE=284.5290 MAE=231.5743 MAE=234.0920 MAE=224.0164 MAE=141.5666 MAE=158.4309 MAE=134.4095 Epoch: 90/500MAE=172.3482 MAE=157.0256 MAE=151.6567 MAE=114.6197 MAE=71.4837 MAE=115.1631 MAE=68.6305 MAE=96.5124 MAE=77.7012 MAE=76.7674 Epoch: 100/500MAE=61.4525 MAE=64.4989 MAE=68.1335 MAE=77.0614 MAE=75.1510 MAE=55.0315 MAE=52.7613 MAE=57.9269 MAE=51.2331 MAE=55.8601 Epoch: 110/500MAE=52.0488 MAE=56.7916 MAE=52.0190 MAE=49.0270 MAE=49.6933 MAE=50.1850 MAE=49.4658 MAE=50.4559 MAE=47.6579 MAE=47.0116 Epoch: 120/500MAE=48.7177 MAE=47.9423 MAE=46.9460 MAE=49.5836 MAE=49.1483 MAE=47.8692 MAE=49.2296 MAE=47.2130 MAE=46.9666 MAE=47.1197 Epoch: 130/500MAE=48.1366 MAE=48.0874 MAE=47.8033 MAE=47.8273 MAE=48.2867 MAE=48.0882 MAE=47.6565 MAE=47.5480 MAE=47.6275 MAE=47.6247 Epoch: 140/500MAE=47.7017 MAE=47.5961 MAE=47.6174 MAE=47.6093 MAE=47.6052 MAE=47.5925 MAE=47.6346 MAE=47.6075 MAE=47.6060 MAE=47.6119 Epoch: 150/500MAE=47.5968 MAE=47.5884 MAE=47.6000 MAE=47.5761 MAE=47.5669 MAE=47.5596 MAE=47.5554 MAE=47.5541 MAE=47.5547 MAE=47.5542 Epoch: 160/500MAE=47.5344 MAE=47.5378 MAE=47.5474 MAE=47.5527 MAE=47.5395 MAE=47.5459 MAE=47.5271 MAE=47.5476 MAE=47.5424 MAE=47.5394 Epoch: 170/500MAE=47.5128 MAE=47.5192 MAE=47.5180 MAE=47.5123 MAE=47.5254 MAE=47.5046 MAE=47.5158 MAE=47.5246 MAE=47.5170 MAE=47.5236 Epoch: 180/500MAE=47.5132 MAE=47.4974 MAE=47.5122 MAE=47.5113 MAE=47.5083 MAE=47.4988 MAE=47.4885 MAE=47.4999 MAE=47.4975 MAE=47.5068 Epoch: 190/500MAE=47.5181 MAE=47.5097 MAE=47.5123 MAE=47.5083 MAE=47.4874 MAE=47.4911 MAE=47.4981 MAE=47.4833 MAE=47.4896 MAE=47.4805 Epoch: 200/500MAE=47.4775 MAE=47.4923 MAE=47.4858 MAE=47.4922 MAE=47.5040 MAE=47.4845 MAE=47.4897 MAE=47.4877 MAE=47.4884 MAE=47.4766 Epoch: 210/500MAE=47.4783 MAE=47.4523 MAE=47.4716 MAE=47.4745 MAE=47.4752 MAE=47.4828 MAE=47.5026 MAE=47.4888 MAE=47.5079 MAE=47.4816 Epoch: 220/500MAE=47.4882 MAE=47.4793 MAE=47.4741 MAE=47.4958 MAE=47.4730 MAE=47.4878 MAE=47.4801 MAE=47.4605 MAE=47.4417 MAE=47.4594 Epoch: 230/500MAE=47.4851 MAE=47.4737 MAE=47.4804 MAE=47.4841 MAE=47.4629 MAE=47.4701 MAE=47.4719 MAE=47.4864 MAE=47.5058 MAE=47.4873 Epoch: 240/500MAE=47.4789 MAE=47.4971 MAE=47.4916 MAE=47.4966 MAE=47.4809 MAE=47.4785 MAE=47.4827 MAE=47.4916 MAE=47.4978 MAE=47.4948 Epoch: 250/500MAE=47.4921 MAE=47.5016 MAE=47.4901 MAE=47.4836 MAE=47.5167 MAE=47.5034 MAE=47.4930 MAE=47.4983 MAE=47.4992 MAE=47.5153 Epoch: 260/500MAE=47.5254 MAE=47.5130 MAE=47.5014 MAE=47.4914 MAE=47.4791 MAE=47.4725 MAE=47.4731 MAE=47.4942 MAE=47.4970 MAE=47.4841 Epoch: 270/500MAE=47.4827 MAE=47.4634 MAE=47.4789 MAE=48.2915 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 48.292 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.6880 MAE=1547.7793 MAE=1545.4280 MAE=1543.3787 MAE=1540.9280 MAE=1538.2024 MAE=1535.8728 MAE=1531.3096 MAE=1527.8628 Epoch: 10/500MAE=1522.9215 MAE=1518.3433 MAE=1513.3097 MAE=1506.8748 MAE=1501.0873 MAE=1493.5747 MAE=1489.3109 MAE=1481.1812 MAE=1473.2251 MAE=1465.2646 Epoch: 20/500MAE=1458.3665 MAE=1449.5034 MAE=1440.0687 MAE=1429.2921 MAE=1418.2251 MAE=1406.0911 MAE=1399.6682 MAE=1391.7427 MAE=1373.9756 MAE=1359.4601 Epoch: 30/500MAE=1341.9062 MAE=1348.3899 MAE=1320.4043 MAE=1306.9023 MAE=1300.8169 MAE=1282.2480 MAE=1262.9409 MAE=1253.2463 MAE=1249.0488 MAE=1217.7271 Epoch: 40/500MAE=1181.2156 MAE=1212.2036 MAE=1184.9697 MAE=1176.9464 MAE=1148.2139 MAE=1120.9695 MAE=1102.9363 MAE=1082.9460 MAE=1069.6230 MAE=1045.1515 Epoch: 50/500MAE=1021.5995 MAE=1009.2980 MAE=977.4266 MAE=950.5757 MAE=937.8535 MAE=930.5608 MAE=886.1164 MAE=886.1514 MAE=854.7512 MAE=826.4774 Epoch: 60/500MAE=784.7909 MAE=764.7142 MAE=772.1329 MAE=738.6120 MAE=704.3424 MAE=704.8208 MAE=634.1454 MAE=584.0527 MAE=606.5370 MAE=555.5858 Epoch: 70/500MAE=564.2167 MAE=547.8644 MAE=520.4909 MAE=463.2197 MAE=428.5614 MAE=464.9052 MAE=485.0530 MAE=404.9394 MAE=382.1252 MAE=338.1039 Epoch: 80/500MAE=275.5001 MAE=302.9530 MAE=306.4919 MAE=189.3769 MAE=200.2174 MAE=237.6998 MAE=207.0989 MAE=188.8507 MAE=167.2311 MAE=166.4704 Epoch: 90/500MAE=155.6918 MAE=147.7750 MAE=124.8585 MAE=80.8405 MAE=81.3750 MAE=64.4916 MAE=64.5949 MAE=97.5240 MAE=60.7187 MAE=77.7525 Epoch: 100/500MAE=76.1355 MAE=64.9475 MAE=67.6139 MAE=56.8217 MAE=54.8439 MAE=58.8850 MAE=55.7135 MAE=53.0788 MAE=61.2780 MAE=52.6794 Epoch: 110/500MAE=52.0976 MAE=56.1606 MAE=50.9737 MAE=52.6714 MAE=64.1651 MAE=50.2467 MAE=56.7312 MAE=59.7609 MAE=49.3095 MAE=50.8363 Epoch: 120/500MAE=48.8207 MAE=49.0311 MAE=50.6782 MAE=51.9770 MAE=49.6894 MAE=52.8131 MAE=50.7031 MAE=47.9119 MAE=47.0156 MAE=47.3853 Epoch: 130/500MAE=47.0977 MAE=49.1359 MAE=48.5745 MAE=50.0282 MAE=48.6101 MAE=47.9191 MAE=53.5012 MAE=47.9376 MAE=48.2485 MAE=49.5743 Epoch: 140/500MAE=50.0840 MAE=49.5896 MAE=49.2791 MAE=48.5688 MAE=48.4741 MAE=48.8295 MAE=48.6602 MAE=48.7424 MAE=49.1287 MAE=49.0043 Epoch: 150/500MAE=48.8305 MAE=48.6979 MAE=48.6752 MAE=48.7141 MAE=48.6007 MAE=48.7336 MAE=48.8053 MAE=48.7279 MAE=48.6433 MAE=48.6940 Epoch: 160/500MAE=48.7300 MAE=48.7123 MAE=48.7171 MAE=48.7057 MAE=48.7268 MAE=48.7327 MAE=48.7150 MAE=48.7064 MAE=48.6901 MAE=48.7201 Epoch: 170/500MAE=48.7334 MAE=48.7676 MAE=48.7795 MAE=48.7885 MAE=48.7917 MAE=48.7980 MAE=48.7688 MAE=48.7554 MAE=48.7688 MAE=48.7811 Epoch: 180/500MAE=48.7534 MAE=48.7734 MAE=48.7915 MAE=48.7524 MAE=48.7469 MAE=48.7351 MAE=48.7356 MAE=48.7665 MAE=48.7776 MAE=48.7400 Epoch: 190/500MAE=48.7000 MAE=48.7422 MAE=48.7575 MAE=48.7255 MAE=48.7496 MAE=48.7295 MAE=48.7303 MAE=48.7376 MAE=48.7462 MAE=48.7337 Epoch: 200/500MAE=48.7128 MAE=48.7204 MAE=48.7277 MAE=48.7103 MAE=48.7085 MAE=48.7084 MAE=48.7153 MAE=48.7278 MAE=48.7205 MAE=48.7268 Epoch: 210/500MAE=48.7170 MAE=48.7269 MAE=48.7274 MAE=48.7440 MAE=48.7481 MAE=48.7579 MAE=48.7466 MAE=48.7838 MAE=48.7530 MAE=48.7278 Epoch: 220/500MAE=48.6928 MAE=48.7709 MAE=48.7326 MAE=48.7401 MAE=48.7187 MAE=48.7499 MAE=48.7559 MAE=48.7517 MAE=48.7337 MAE=48.7311 Epoch: 230/500MAE=48.7124 MAE=48.7018 MAE=48.7057 MAE=48.7178 MAE=48.7102 MAE=48.7305 MAE=48.7058 MAE=48.7031 MAE=48.6691 MAE=48.6793 Epoch: 240/500MAE=48.6603 MAE=48.6723 MAE=48.6846 MAE=48.6913 MAE=48.6978 MAE=48.6923 MAE=48.7176 MAE=48.6864 MAE=48.6501 MAE=48.6756 Epoch: 250/500MAE=48.6701 MAE=48.6743 MAE=48.6807 MAE=48.6725 MAE=48.6820 MAE=48.6577 MAE=48.6625 MAE=48.6709 MAE=48.6872 MAE=48.7016 Epoch: 260/500MAE=48.6456 MAE=48.6537 MAE=48.6894 MAE=48.6150 MAE=48.6616 MAE=48.6769 MAE=48.6993 MAE=48.6688 MAE=48.6830 MAE=48.6741 Epoch: 270/500MAE=48.6997 MAE=48.7107 MAE=48.7215 MAE=48.7187 MAE=48.7233 MAE=48.7309 MAE=48.6934 MAE=48.7139 MAE=48.7244 MAE=47.4484 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 47.870 +/- 0.422\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.6670 MAE=1547.7087 MAE=1545.1984 MAE=1543.1617 MAE=1541.0383 MAE=1538.8870 MAE=1535.9277 MAE=1532.4509 MAE=1528.0190 Epoch: 10/500MAE=1523.8130 MAE=1517.5056 MAE=1510.7893 MAE=1506.7666 MAE=1499.5006 MAE=1492.9722 MAE=1485.8254 MAE=1473.8647 MAE=1476.3239 MAE=1463.9395 Epoch: 20/500MAE=1457.1064 MAE=1438.4174 MAE=1430.6567 MAE=1430.4351 MAE=1415.4446 MAE=1403.6233 MAE=1396.4749 MAE=1383.7156 MAE=1380.5894 MAE=1352.0421 Epoch: 30/500MAE=1347.1110 MAE=1323.5608 MAE=1324.5377 MAE=1292.7792 MAE=1290.3960 MAE=1276.0732 MAE=1267.2935 MAE=1245.6472 MAE=1230.6897 MAE=1232.6550 Epoch: 40/500MAE=1191.5691 MAE=1186.3589 MAE=1155.4219 MAE=1148.9565 MAE=1122.5808 MAE=1117.6934 MAE=1103.3862 MAE=1085.6011 MAE=1071.2527 MAE=1042.4329 Epoch: 50/500MAE=991.3527 MAE=1007.5271 MAE=997.7380 MAE=972.9420 MAE=909.8193 MAE=906.4854 MAE=851.2036 MAE=870.7411 MAE=865.8150 MAE=827.4138 Epoch: 60/500MAE=778.9417 MAE=754.4590 MAE=758.9732 MAE=761.0486 MAE=723.9731 MAE=638.6218 MAE=663.0746 MAE=673.4211 MAE=592.0931 MAE=564.3920 Epoch: 70/500MAE=531.7182 MAE=508.7959 MAE=506.0600 MAE=465.9424 MAE=472.2941 MAE=460.1130 MAE=417.4327 MAE=354.4812 MAE=378.6589 MAE=315.3369 Epoch: 80/500MAE=268.3103 MAE=309.7956 MAE=246.0819 MAE=255.9037 MAE=221.1624 MAE=211.5848 MAE=194.1957 MAE=174.9517 MAE=147.9850 MAE=165.3332 Epoch: 90/500MAE=162.8965 MAE=146.2998 MAE=132.0028 MAE=106.1492 MAE=91.9785 MAE=72.5287 MAE=77.9243 MAE=90.2343 MAE=103.5146 MAE=86.9388 Epoch: 100/500MAE=66.7415 MAE=58.1421 MAE=58.5265 MAE=67.0677 MAE=60.0310 MAE=54.2516 MAE=51.6477 MAE=54.9394 MAE=55.2846 MAE=54.8178 Epoch: 110/500MAE=53.7806 MAE=49.5943 MAE=50.5659 MAE=49.3502 MAE=49.2351 MAE=51.7854 MAE=55.9583 MAE=47.2610 MAE=52.6613 MAE=52.9684 Epoch: 120/500MAE=47.9304 MAE=49.7741 MAE=48.2577 MAE=48.3255 MAE=46.0047 MAE=46.6464 MAE=48.9301 MAE=46.8638 MAE=46.1758 MAE=47.4846 Epoch: 130/500MAE=47.0784 MAE=47.4363 MAE=47.6853 MAE=47.6371 MAE=48.2507 MAE=47.9136 MAE=47.2621 MAE=47.3763 MAE=47.4501 MAE=47.1748 Epoch: 140/500MAE=47.2000 MAE=47.1419 MAE=47.0998 MAE=47.0176 MAE=47.0102 MAE=47.0658 MAE=47.0237 MAE=47.0708 MAE=46.9953 MAE=47.0192 Epoch: 150/500MAE=47.0256 MAE=47.0620 MAE=47.0489 MAE=47.0490 MAE=47.0633 MAE=47.0680 MAE=47.0545 MAE=47.0613 MAE=47.0226 MAE=47.0491 Epoch: 160/500MAE=47.0630 MAE=47.0623 MAE=47.0581 MAE=47.0480 MAE=47.0471 MAE=47.0440 MAE=47.0405 MAE=47.0490 MAE=47.0589 MAE=47.0439 Epoch: 170/500MAE=47.0525 MAE=47.0254 MAE=47.0399 MAE=47.0474 MAE=47.0523 MAE=47.0428 MAE=47.0498 MAE=47.0308 MAE=47.0717 MAE=47.0417 Epoch: 180/500MAE=47.0005 MAE=47.0258 MAE=47.0174 MAE=47.0247 MAE=47.0149 MAE=47.0481 MAE=47.0401 MAE=47.0440 MAE=47.0384 MAE=47.0301 Epoch: 190/500MAE=47.0255 MAE=47.0203 MAE=47.0070 MAE=47.0069 MAE=46.9981 MAE=46.9978 MAE=46.9833 MAE=47.0034 MAE=46.9981 MAE=47.0005 Epoch: 200/500MAE=46.9991 MAE=46.9846 MAE=46.9804 MAE=46.9901 MAE=47.0152 MAE=46.9862 MAE=46.9828 MAE=46.9808 MAE=46.9953 MAE=46.9893 Epoch: 210/500MAE=46.9998 MAE=46.9879 MAE=46.9838 MAE=46.9760 MAE=46.9812 MAE=46.9818 MAE=46.9928 MAE=46.9911 MAE=47.0045 MAE=46.9743 Epoch: 220/500MAE=46.9908 MAE=47.0057 MAE=46.9821 MAE=46.9958 MAE=46.9714 MAE=46.9757 MAE=46.9671 MAE=46.9565 MAE=46.9992 MAE=46.9932 Epoch: 230/500MAE=46.9921 MAE=46.9697 MAE=46.9884 MAE=46.9732 MAE=46.9824 MAE=46.9984 MAE=46.9987 MAE=46.9916 MAE=47.0009 MAE=46.9734 Epoch: 240/500MAE=46.9627 MAE=46.9641 MAE=46.9551 MAE=46.9641 MAE=46.9556 MAE=46.9516 MAE=46.9546 MAE=46.9571 MAE=46.9524 MAE=46.9538 Epoch: 250/500MAE=46.9529 MAE=46.9397 MAE=46.9322 MAE=46.9523 MAE=46.9443 MAE=46.9330 MAE=46.9465 MAE=46.9679 MAE=46.9548 MAE=46.9353 Epoch: 260/500MAE=46.9307 MAE=46.9398 MAE=46.9385 MAE=46.9332 MAE=46.9350 MAE=46.9563 MAE=46.9656 MAE=46.9597 MAE=46.9469 MAE=46.9721 Epoch: 270/500MAE=46.9540 MAE=46.9409 MAE=46.9413 MAE=46.9427 MAE=46.9222 MAE=47.3075 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 47.682 +/- 0.434\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.6077 MAE=1547.2782 MAE=1544.7966 MAE=1542.3772 MAE=1539.6438 MAE=1537.2721 MAE=1534.4590 MAE=1530.9501 MAE=1527.1732 Epoch: 10/500MAE=1520.6677 MAE=1513.3820 MAE=1510.5867 MAE=1500.4708 MAE=1499.4958 MAE=1488.6162 MAE=1481.8501 MAE=1467.4548 MAE=1470.8521 MAE=1455.3221 Epoch: 20/500MAE=1459.3867 MAE=1441.9287 MAE=1438.5168 MAE=1426.3899 MAE=1419.4827 MAE=1411.7944 MAE=1387.9641 MAE=1388.9614 MAE=1370.0087 MAE=1358.8601 Epoch: 30/500MAE=1346.2839 MAE=1342.2739 MAE=1307.0370 MAE=1300.8647 MAE=1305.0962 MAE=1255.3794 MAE=1252.2677 MAE=1241.1992 MAE=1232.0356 MAE=1199.6179 Epoch: 40/500MAE=1203.1580 MAE=1205.6392 MAE=1161.9902 MAE=1156.3623 MAE=1107.7643 MAE=1108.4423 MAE=1105.0161 MAE=1045.6841 MAE=1070.3705 MAE=1050.4646 Epoch: 50/500MAE=1023.4524 MAE=996.8342 MAE=991.5749 MAE=956.0928 MAE=932.5861 MAE=902.1030 MAE=904.8564 MAE=869.7671 MAE=871.5922 MAE=852.8962 Epoch: 60/500MAE=841.3734 MAE=794.2242 MAE=758.7130 MAE=707.7750 MAE=721.6886 MAE=679.7230 MAE=647.4657 MAE=581.8727 MAE=603.2638 MAE=579.0888 Epoch: 70/500MAE=552.4917 MAE=518.1172 MAE=480.2737 MAE=488.4919 MAE=489.6505 MAE=446.0877 MAE=391.0448 MAE=369.5604 MAE=395.8815 MAE=345.3406 Epoch: 80/500MAE=307.2187 MAE=283.7762 MAE=286.1404 MAE=221.6904 MAE=189.2202 MAE=253.1551 MAE=153.5608 MAE=217.8299 MAE=136.5387 MAE=126.2744 Epoch: 90/500MAE=136.6917 MAE=97.0478 MAE=145.5801 MAE=116.4501 MAE=90.1415 MAE=104.0481 MAE=109.1340 MAE=79.3242 MAE=81.9798 MAE=62.2782 Epoch: 100/500MAE=79.8557 MAE=64.7245 MAE=79.6568 MAE=61.1851 MAE=64.9342 MAE=58.2682 MAE=59.9545 MAE=68.1722 MAE=68.8494 MAE=53.8721 Epoch: 110/500MAE=52.8557 MAE=51.2483 MAE=57.0384 MAE=49.3522 MAE=58.3417 MAE=49.5977 MAE=58.2475 MAE=61.7076 MAE=50.4161 MAE=51.1223 Epoch: 120/500MAE=51.2322 MAE=49.1472 MAE=55.0218 MAE=51.1456 MAE=49.6295 MAE=47.0698 MAE=50.2924 MAE=49.4789 MAE=49.9304 MAE=55.3630 Epoch: 130/500MAE=46.6241 MAE=48.7999 MAE=50.3913 MAE=53.6706 MAE=48.6140 MAE=49.2240 MAE=49.3819 MAE=49.9821 MAE=46.4635 MAE=47.0631 Epoch: 140/500MAE=47.5335 MAE=50.0016 MAE=47.4474 MAE=48.7006 MAE=48.7776 MAE=48.8594 MAE=48.7952 MAE=47.3123 MAE=47.2154 MAE=47.4464 Epoch: 150/500MAE=47.6190 MAE=47.7139 MAE=48.2924 MAE=48.5366 MAE=48.2519 MAE=48.0060 MAE=48.0301 MAE=48.1567 MAE=47.8447 MAE=47.8281 Epoch: 160/500MAE=47.8092 MAE=48.1107 MAE=48.0794 MAE=48.0214 MAE=47.9689 MAE=47.9497 MAE=47.9733 MAE=47.9738 MAE=47.9569 MAE=47.9808 Epoch: 170/500MAE=47.9528 MAE=47.9401 MAE=47.9392 MAE=47.9284 MAE=47.9557 MAE=47.9437 MAE=47.9173 MAE=47.9212 MAE=47.9210 MAE=47.9076 Epoch: 180/500MAE=47.9106 MAE=47.9165 MAE=47.9345 MAE=47.9338 MAE=47.9257 MAE=47.9060 MAE=47.8856 MAE=47.8762 MAE=47.9060 MAE=47.8996 Epoch: 190/500MAE=47.8505 MAE=47.8756 MAE=47.8733 MAE=47.8862 MAE=47.8883 MAE=47.8654 MAE=47.8743 MAE=47.8841 MAE=47.8638 MAE=47.8553 Epoch: 200/500MAE=47.8614 MAE=47.8295 MAE=47.8375 MAE=47.8324 MAE=47.8475 MAE=47.8505 MAE=47.8577 MAE=47.8601 MAE=47.8738 MAE=47.8639 Epoch: 210/500MAE=47.8668 MAE=47.8539 MAE=47.8601 MAE=47.8537 MAE=47.8630 MAE=47.8679 MAE=47.8537 MAE=47.8493 MAE=47.8694 MAE=47.8700 Epoch: 220/500MAE=47.8858 MAE=47.8829 MAE=47.8884 MAE=47.9055 MAE=47.8781 MAE=47.9035 MAE=47.9048 MAE=47.8872 MAE=47.8820 MAE=47.8936 Epoch: 230/500MAE=47.9066 MAE=47.9201 MAE=47.9365 MAE=47.9085 MAE=47.9200 MAE=47.8948 MAE=47.8766 MAE=47.9029 MAE=47.8956 MAE=47.9020 Epoch: 240/500MAE=47.8933 MAE=47.9231 MAE=47.8971 MAE=47.9041 MAE=47.9262 MAE=47.9233 MAE=47.9340 MAE=47.9243 MAE=47.9033 MAE=47.9149 Epoch: 250/500MAE=47.8975 MAE=47.8695 MAE=47.8739 MAE=47.8917 MAE=47.8848 MAE=47.8985 MAE=47.8955 MAE=47.8750 MAE=47.8608 MAE=47.8670 Epoch: 260/500MAE=47.8795 MAE=47.8825 MAE=47.8781 MAE=47.8695 MAE=47.8896 MAE=47.8582 MAE=47.8726 MAE=47.8775 MAE=47.8791 MAE=47.8749 Epoch: 270/500MAE=47.8661 MAE=47.8691 MAE=47.8403 MAE=47.8624 MAE=47.8727 MAE=47.8459 MAE=47.8592 MAE=47.8611 MAE=47.8463 MAE=47.8402 Epoch: 280/500MAE=47.8449 MAE=47.8759 MAE=47.8712 MAE=47.8500 MAE=47.8527 MAE=47.8690 MAE=47.8463 MAE=47.8442 MAE=47.8515 MAE=47.0422 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 47.522 +/- 0.467\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.5308 MAE=1547.6421 MAE=1545.2371 MAE=1543.2014 MAE=1540.4358 MAE=1537.8527 MAE=1534.6653 MAE=1530.7668 MAE=1527.0789 Epoch: 10/500MAE=1522.7039 MAE=1517.9360 MAE=1512.3757 MAE=1506.7222 MAE=1501.7366 MAE=1494.2664 MAE=1488.1774 MAE=1479.8610 MAE=1474.1514 MAE=1465.5671 Epoch: 20/500MAE=1459.6191 MAE=1450.3052 MAE=1441.7830 MAE=1431.0251 MAE=1418.5442 MAE=1408.9282 MAE=1396.9374 MAE=1385.3706 MAE=1381.5730 MAE=1363.6594 Epoch: 30/500MAE=1356.3278 MAE=1336.8538 MAE=1340.6648 MAE=1310.1694 MAE=1288.2338 MAE=1273.6658 MAE=1270.1411 MAE=1238.3152 MAE=1236.2018 MAE=1222.7832 Epoch: 40/500MAE=1199.3220 MAE=1162.1953 MAE=1161.7145 MAE=1147.0237 MAE=1122.0853 MAE=1099.8184 MAE=1085.1526 MAE=1075.0874 MAE=1053.2587 MAE=1021.3278 Epoch: 50/500MAE=1005.9983 MAE=986.7887 MAE=961.0793 MAE=922.5877 MAE=933.5897 MAE=921.5849 MAE=877.8853 MAE=861.3206 MAE=825.4370 MAE=840.7434 Epoch: 60/500MAE=800.7736 MAE=772.8699 MAE=694.7733 MAE=684.9521 MAE=738.9980 MAE=651.6654 MAE=689.9603 MAE=559.0607 MAE=605.3148 MAE=541.3389 Epoch: 70/500MAE=545.2495 MAE=499.2662 MAE=517.1158 MAE=448.8375 MAE=457.8255 MAE=428.6847 MAE=424.3890 MAE=365.2421 MAE=342.5900 MAE=348.6541 Epoch: 80/500MAE=334.0977 MAE=301.8452 MAE=226.2430 MAE=254.7815 MAE=211.0826 MAE=170.6185 MAE=161.2181 MAE=192.8658 MAE=156.3784 MAE=154.5642 Epoch: 90/500MAE=131.6940 MAE=122.0943 MAE=96.1244 MAE=92.6509 MAE=79.3285 MAE=90.2090 MAE=77.0014 MAE=90.4811 MAE=84.6361 MAE=93.5498 Epoch: 100/500MAE=53.8569 MAE=67.8202 MAE=66.2280 MAE=78.4552 MAE=85.4553 MAE=62.5261 MAE=50.0739 MAE=50.5876 MAE=53.7841 MAE=52.8535 Epoch: 110/500MAE=49.3227 MAE=50.7905 MAE=51.5150 MAE=52.2302 MAE=50.2441 MAE=49.7681 MAE=53.9737 MAE=49.0216 MAE=52.0036 MAE=50.4655 Epoch: 120/500MAE=49.2197 MAE=49.3615 MAE=49.8215 MAE=49.7939 MAE=48.5583 MAE=52.5832 MAE=50.7035 MAE=49.6655 MAE=50.2764 MAE=49.1824 Epoch: 130/500MAE=49.4427 MAE=50.4026 MAE=50.4261 MAE=49.5337 MAE=49.6421 MAE=50.1303 MAE=49.2543 MAE=49.7168 MAE=49.8438 MAE=49.8962 Epoch: 140/500MAE=49.7865 MAE=49.8133 MAE=49.9938 MAE=50.0340 MAE=49.9092 MAE=49.9324 MAE=49.8587 MAE=49.8796 MAE=49.9287 MAE=49.9113 Epoch: 150/500MAE=49.9159 MAE=49.9753 MAE=50.0065 MAE=49.9775 MAE=49.9803 MAE=49.9520 MAE=49.9497 MAE=49.9317 MAE=49.9361 MAE=49.9293 Epoch: 160/500MAE=49.9201 MAE=49.9291 MAE=49.9120 MAE=49.8994 MAE=49.9143 MAE=49.8948 MAE=49.9131 MAE=49.9068 MAE=49.9017 MAE=49.8819 Epoch: 170/500MAE=49.8927 MAE=49.8969 MAE=49.8984 MAE=49.8870 MAE=49.8880 MAE=49.8818 MAE=49.8797 MAE=49.9038 MAE=49.9052 MAE=49.9141 Epoch: 180/500MAE=49.9060 MAE=49.9176 MAE=49.8917 MAE=49.9265 MAE=49.8963 MAE=49.8780 MAE=49.8666 MAE=49.8994 MAE=49.9004 MAE=49.9268 Epoch: 190/500MAE=49.9093 MAE=49.9221 MAE=49.9251 MAE=49.9231 MAE=49.9329 MAE=49.9284 MAE=49.9185 MAE=49.8731 MAE=49.8881 MAE=49.8665 Epoch: 200/500MAE=49.9047 MAE=49.9198 MAE=49.9092 MAE=49.9400 MAE=49.9381 MAE=49.9047 MAE=49.9071 MAE=49.9230 MAE=49.8927 MAE=49.8770 Epoch: 210/500MAE=49.8487 MAE=49.8455 MAE=49.8599 MAE=49.8162 MAE=49.8590 MAE=49.8869 MAE=49.8983 MAE=49.8802 MAE=49.9027 MAE=49.8932 Epoch: 220/500MAE=49.8827 MAE=49.8782 MAE=49.8900 MAE=49.8701 MAE=49.8572 MAE=49.8747 MAE=49.8999 MAE=49.8612 MAE=49.8688 MAE=49.8688 Epoch: 230/500MAE=49.8736 MAE=49.8743 MAE=49.8598 MAE=49.8745 MAE=49.9085 MAE=49.8786 MAE=49.8905 MAE=49.8847 MAE=49.8690 MAE=49.8597 Epoch: 240/500MAE=49.8906 MAE=49.9070 MAE=49.9044 MAE=49.8892 MAE=49.8775 MAE=49.8898 MAE=49.8900 MAE=49.8837 MAE=49.8942 MAE=49.9209 Epoch: 250/500MAE=49.9034 MAE=49.8750 MAE=49.8730 MAE=49.8876 MAE=49.8997 MAE=49.8965 MAE=49.8860 MAE=49.8823 MAE=49.8836 MAE=49.8744 Epoch: 260/500MAE=49.8737 MAE=49.8628 MAE=49.8742 MAE=49.8598 MAE=49.8725 MAE=49.8604 MAE=49.8551 MAE=49.8561 MAE=49.8783 MAE=49.8824 Epoch: 270/500MAE=49.8634 MAE=49.8843 MAE=49.8807 MAE=49.8624 MAE=49.8782 MAE=49.3046 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 47.879 +/- 0.826\n",
                        "\n"
                    ]
                }
            ],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --hgpsl_ratio=0.1 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --hgpsl_ratio=0.3 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --hgpsl_ratio=0.5 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --hgpsl_ratio=0.7 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --hgpsl_ratio=0.9 --pooling='HGPSL'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### QM8"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150MAE=0.1300 MAE=0.1249 MAE=0.1030 MAE=0.0756 MAE=0.0593 MAE=0.0502 MAE=0.0453 MAE=0.0426 MAE=0.0376 Epoch: 10/150MAE=0.0353 MAE=0.0332 MAE=0.0318 MAE=0.0317 MAE=0.0302 MAE=0.0287 MAE=0.0288 MAE=0.0278 MAE=0.0283 MAE=0.0278 Epoch: 20/150MAE=0.0279 MAE=0.0270 MAE=0.0270 MAE=0.0267 MAE=0.0264 MAE=0.0262 MAE=0.0262 MAE=0.0264 MAE=0.0264 MAE=0.0269 Epoch: 30/150MAE=0.0260 MAE=0.0259 MAE=0.0259 MAE=0.0264 MAE=0.0259 MAE=0.0257 MAE=0.0260 MAE=0.0258 MAE=0.0259 MAE=0.0258 Epoch: 40/150MAE=0.0252 MAE=0.0253 MAE=0.0251 MAE=0.0253 MAE=0.0250 MAE=0.0250 MAE=0.0250 MAE=0.0248 MAE=0.0251 MAE=0.0248 Epoch: 50/150MAE=0.0249 MAE=0.0251 MAE=0.0246 MAE=0.0247 MAE=0.0245 MAE=0.0245 MAE=0.0246 MAE=0.0245 MAE=0.0246 MAE=0.0246 Epoch: 60/150MAE=0.0247 MAE=0.0245 MAE=0.0245 MAE=0.0245 MAE=0.0244 MAE=0.0244 MAE=0.0245 MAE=0.0244 MAE=0.0244 MAE=0.0244 Epoch: 70/150MAE=0.0244 MAE=0.0244 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 Epoch: 80/150MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 Epoch: 90/150MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 Epoch: 100/150MAE=0.0243 MAE=0.0243 MAE=0.0250 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 0.025 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1404 MAE=0.1211 MAE=0.1015 MAE=0.0784 MAE=0.0640 MAE=0.0548 MAE=0.0459 MAE=0.0417 MAE=0.0377 Epoch: 10/150MAE=0.0360 MAE=0.0327 MAE=0.0320 MAE=0.0311 MAE=0.0299 MAE=0.0291 MAE=0.0284 MAE=0.0279 MAE=0.0279 MAE=0.0271 Epoch: 20/150MAE=0.0277 MAE=0.0271 MAE=0.0269 MAE=0.0268 MAE=0.0270 MAE=0.0266 MAE=0.0259 MAE=0.0260 MAE=0.0257 MAE=0.0259 Epoch: 30/150MAE=0.0257 MAE=0.0255 MAE=0.0257 MAE=0.0252 MAE=0.0252 MAE=0.0254 MAE=0.0253 MAE=0.0251 MAE=0.0252 MAE=0.0249 Epoch: 40/150MAE=0.0251 MAE=0.0250 MAE=0.0250 MAE=0.0251 MAE=0.0247 MAE=0.0246 MAE=0.0245 MAE=0.0245 MAE=0.0245 MAE=0.0244 Epoch: 50/150MAE=0.0245 MAE=0.0245 MAE=0.0245 MAE=0.0244 MAE=0.0245 MAE=0.0245 MAE=0.0244 MAE=0.0245 MAE=0.0243 MAE=0.0244 Epoch: 60/150MAE=0.0244 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 Epoch: 70/150MAE=0.0243 MAE=0.0242 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 Epoch: 80/150MAE=0.0243 MAE=0.0242 MAE=0.0249 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 0.025 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1316 MAE=0.1118 MAE=0.0871 MAE=0.0713 MAE=0.0620 MAE=0.0511 MAE=0.0467 MAE=0.0412 MAE=0.0387 Epoch: 10/150MAE=0.0350 MAE=0.0333 MAE=0.0334 MAE=0.0305 MAE=0.0302 MAE=0.0297 MAE=0.0306 MAE=0.0291 MAE=0.0297 MAE=0.0294 Epoch: 20/150MAE=0.0287 MAE=0.0281 MAE=0.0278 MAE=0.0283 MAE=0.0283 MAE=0.0285 MAE=0.0276 MAE=0.0271 MAE=0.0270 MAE=0.0275 Epoch: 30/150MAE=0.0275 MAE=0.0270 MAE=0.0267 MAE=0.0269 MAE=0.0275 MAE=0.0272 MAE=0.0270 MAE=0.0263 MAE=0.0262 MAE=0.0260 Epoch: 40/150MAE=0.0261 MAE=0.0261 MAE=0.0263 MAE=0.0261 MAE=0.0257 MAE=0.0256 MAE=0.0255 MAE=0.0254 MAE=0.0255 MAE=0.0255 Epoch: 50/150MAE=0.0255 MAE=0.0256 MAE=0.0254 MAE=0.0253 MAE=0.0254 MAE=0.0255 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0254 Epoch: 60/150MAE=0.0254 MAE=0.0254 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0252 MAE=0.0253 MAE=0.0254 MAE=0.0252 Epoch: 70/150MAE=0.0253 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 Epoch: 80/150MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 Epoch: 90/150MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 Epoch: 100/150MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0259 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 0.025 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1408 MAE=0.1298 MAE=0.1076 MAE=0.0848 MAE=0.0648 MAE=0.0529 MAE=0.0444 MAE=0.0406 MAE=0.0375 Epoch: 10/150MAE=0.0353 MAE=0.0341 MAE=0.0317 MAE=0.0301 MAE=0.0296 MAE=0.0285 MAE=0.0283 MAE=0.0275 MAE=0.0272 MAE=0.0269 Epoch: 20/150MAE=0.0271 MAE=0.0266 MAE=0.0263 MAE=0.0258 MAE=0.0259 MAE=0.0257 MAE=0.0256 MAE=0.0257 MAE=0.0257 MAE=0.0258 Epoch: 30/150MAE=0.0256 MAE=0.0257 MAE=0.0255 MAE=0.0256 MAE=0.0255 MAE=0.0254 MAE=0.0256 MAE=0.0254 MAE=0.0255 MAE=0.0257 Epoch: 40/150MAE=0.0251 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0250 MAE=0.0249 Epoch: 50/150MAE=0.0248 MAE=0.0248 MAE=0.0249 MAE=0.0248 MAE=0.0248 MAE=0.0248 MAE=0.0249 MAE=0.0250 MAE=0.0248 MAE=0.0248 Epoch: 60/150MAE=0.0248 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0248 Epoch: 70/150MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 Epoch: 80/150MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0248 MAE=0.0247 MAE=0.0248 MAE=0.0248 MAE=0.0248 MAE=0.0248 MAE=0.0248 Epoch: 90/150MAE=0.0248 MAE=0.0248 MAE=0.0254 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 0.025 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1457 MAE=0.1287 MAE=0.1067 MAE=0.0805 MAE=0.0636 MAE=0.0531 MAE=0.0486 MAE=0.0444 MAE=0.0430 Epoch: 10/150MAE=0.0388 MAE=0.0387 MAE=0.0363 MAE=0.0345 MAE=0.0326 MAE=0.0300 MAE=0.0291 MAE=0.0292 MAE=0.0281 MAE=0.0284 Epoch: 20/150MAE=0.0273 MAE=0.0278 MAE=0.0274 MAE=0.0263 MAE=0.0268 MAE=0.0261 MAE=0.0268 MAE=0.0262 MAE=0.0258 MAE=0.0258 Epoch: 30/150MAE=0.0258 MAE=0.0261 MAE=0.0258 MAE=0.0258 MAE=0.0253 MAE=0.0252 MAE=0.0252 MAE=0.0253 MAE=0.0252 MAE=0.0251 Epoch: 40/150MAE=0.0252 MAE=0.0250 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0251 MAE=0.0248 MAE=0.0248 MAE=0.0249 MAE=0.0250 Epoch: 50/150MAE=0.0248 MAE=0.0248 MAE=0.0248 MAE=0.0249 MAE=0.0248 MAE=0.0246 MAE=0.0246 MAE=0.0247 MAE=0.0247 MAE=0.0246 Epoch: 60/150MAE=0.0246 MAE=0.0246 MAE=0.0245 MAE=0.0246 MAE=0.0244 MAE=0.0244 MAE=0.0245 MAE=0.0245 MAE=0.0245 MAE=0.0244 Epoch: 70/150MAE=0.0245 MAE=0.0244 MAE=0.0245 MAE=0.0245 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 Epoch: 80/150MAE=0.0245 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 Epoch: 90/150MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 Epoch: 100/150MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 Epoch: 110/150MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0244 MAE=0.0250 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 0.025 +/- 0.000\n",
                        "\n",
                        "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150MAE=0.1605 MAE=0.1409 MAE=0.1179 MAE=0.0888 MAE=0.0716 MAE=0.0613 MAE=0.0527 MAE=0.0483 MAE=0.0444 Epoch: 10/150MAE=0.0410 MAE=0.0376 MAE=0.0353 MAE=0.0338 MAE=0.0328 MAE=0.0314 MAE=0.0297 MAE=0.0292 MAE=0.0283 MAE=0.0275 Epoch: 20/150MAE=0.0281 MAE=0.0270 MAE=0.0264 MAE=0.0266 MAE=0.0259 MAE=0.0264 MAE=0.0256 MAE=0.0250 MAE=0.0251 MAE=0.0257 Epoch: 30/150MAE=0.0252 MAE=0.0251 MAE=0.0245 MAE=0.0245 MAE=0.0240 MAE=0.0242 MAE=0.0243 MAE=0.0238 MAE=0.0239 MAE=0.0238 Epoch: 40/150MAE=0.0239 MAE=0.0239 MAE=0.0239 MAE=0.0239 MAE=0.0234 MAE=0.0234 MAE=0.0234 MAE=0.0231 MAE=0.0232 MAE=0.0231 Epoch: 50/150MAE=0.0233 MAE=0.0231 MAE=0.0229 MAE=0.0230 MAE=0.0229 MAE=0.0230 MAE=0.0230 MAE=0.0231 MAE=0.0231 MAE=0.0231 Epoch: 60/150MAE=0.0230 MAE=0.0230 MAE=0.0229 MAE=0.0228 MAE=0.0227 MAE=0.0228 MAE=0.0228 MAE=0.0228 MAE=0.0227 MAE=0.0227 Epoch: 70/150MAE=0.0227 MAE=0.0227 MAE=0.0227 MAE=0.0227 MAE=0.0227 MAE=0.0227 MAE=0.0227 MAE=0.0227 MAE=0.0227 MAE=0.0227 Epoch: 80/150MAE=0.0227 MAE=0.0227 MAE=0.0230 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 0.023 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1335 MAE=0.1177 MAE=0.0900 MAE=0.0727 MAE=0.0583 MAE=0.0541 MAE=0.0453 MAE=0.0407 MAE=0.0386 Epoch: 10/150MAE=0.0347 MAE=0.0337 MAE=0.0306 MAE=0.0293 MAE=0.0286 MAE=0.0284 MAE=0.0285 MAE=0.0266 MAE=0.0262 MAE=0.0253 Epoch: 20/150MAE=0.0252 MAE=0.0251 MAE=0.0246 MAE=0.0243 MAE=0.0242 MAE=0.0242 MAE=0.0245 MAE=0.0242 MAE=0.0238 MAE=0.0233 Epoch: 30/150MAE=0.0235 MAE=0.0234 MAE=0.0230 MAE=0.0229 MAE=0.0231 MAE=0.0229 MAE=0.0228 MAE=0.0225 MAE=0.0226 MAE=0.0222 Epoch: 40/150MAE=0.0226 MAE=0.0227 MAE=0.0221 MAE=0.0224 MAE=0.0223 MAE=0.0223 MAE=0.0224 MAE=0.0217 MAE=0.0217 MAE=0.0217 Epoch: 50/150MAE=0.0217 MAE=0.0216 MAE=0.0217 MAE=0.0216 MAE=0.0214 MAE=0.0215 MAE=0.0217 MAE=0.0216 MAE=0.0214 MAE=0.0211 Epoch: 60/150MAE=0.0212 MAE=0.0212 MAE=0.0211 MAE=0.0209 MAE=0.0210 MAE=0.0211 MAE=0.0210 MAE=0.0211 MAE=0.0210 MAE=0.0208 Epoch: 70/150MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0208 Epoch: 80/150MAE=0.0208 MAE=0.0208 MAE=0.0207 MAE=0.0207 MAE=0.0207 MAE=0.0207 MAE=0.0207 MAE=0.0207 MAE=0.0207 MAE=0.0207 Epoch: 90/150MAE=0.0207 MAE=0.0206 MAE=0.0207 MAE=0.0207 MAE=0.0207 MAE=0.0207 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 Epoch: 100/150MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 Epoch: 110/150MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 Epoch: 120/150MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 Epoch: 130/150MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0211 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 0.022 +/- 0.001\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1407 MAE=0.1311 MAE=0.1007 MAE=0.0761 MAE=0.0602 MAE=0.0512 MAE=0.0456 MAE=0.0412 MAE=0.0381 Epoch: 10/150MAE=0.0345 MAE=0.0325 MAE=0.0310 MAE=0.0304 MAE=0.0281 MAE=0.0274 MAE=0.0269 MAE=0.0267 MAE=0.0260 MAE=0.0256 Epoch: 20/150MAE=0.0251 MAE=0.0250 MAE=0.0249 MAE=0.0243 MAE=0.0241 MAE=0.0238 MAE=0.0237 MAE=0.0233 MAE=0.0239 MAE=0.0236 Epoch: 30/150MAE=0.0229 MAE=0.0232 MAE=0.0231 MAE=0.0231 MAE=0.0232 MAE=0.0225 MAE=0.0225 MAE=0.0223 MAE=0.0221 MAE=0.0220 Epoch: 40/150MAE=0.0221 MAE=0.0221 MAE=0.0222 MAE=0.0219 MAE=0.0219 MAE=0.0217 MAE=0.0217 MAE=0.0217 MAE=0.0217 MAE=0.0217 Epoch: 50/150MAE=0.0220 MAE=0.0214 MAE=0.0214 MAE=0.0213 MAE=0.0213 MAE=0.0212 MAE=0.0212 MAE=0.0213 MAE=0.0212 MAE=0.0211 Epoch: 60/150MAE=0.0211 MAE=0.0212 MAE=0.0211 MAE=0.0212 MAE=0.0211 MAE=0.0211 MAE=0.0211 MAE=0.0210 MAE=0.0211 MAE=0.0211 Epoch: 70/150MAE=0.0210 MAE=0.0210 MAE=0.0211 MAE=0.0210 MAE=0.0209 MAE=0.0211 MAE=0.0209 MAE=0.0209 MAE=0.0209 MAE=0.0209 Epoch: 80/150MAE=0.0209 MAE=0.0208 MAE=0.0208 MAE=0.0209 MAE=0.0208 MAE=0.0207 MAE=0.0207 MAE=0.0207 MAE=0.0208 MAE=0.0208 Epoch: 90/150MAE=0.0209 MAE=0.0207 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0207 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 Epoch: 100/150MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 Epoch: 110/150MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 Epoch: 120/150MAE=0.0205 MAE=0.0204 MAE=0.0205 MAE=0.0205 MAE=0.0204 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 Epoch: 130/150MAE=0.0204 MAE=0.0204 MAE=0.0211 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 0.022 +/- 0.001\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1392 MAE=0.1263 MAE=0.0988 MAE=0.0730 MAE=0.0593 MAE=0.0524 MAE=0.0480 MAE=0.0403 MAE=0.0381 Epoch: 10/150MAE=0.0358 MAE=0.0341 MAE=0.0333 MAE=0.0306 MAE=0.0302 MAE=0.0282 MAE=0.0263 MAE=0.0257 MAE=0.0258 MAE=0.0254 Epoch: 20/150MAE=0.0247 MAE=0.0248 MAE=0.0254 MAE=0.0241 MAE=0.0239 MAE=0.0243 MAE=0.0237 MAE=0.0232 MAE=0.0235 MAE=0.0233 Epoch: 30/150MAE=0.0227 MAE=0.0233 MAE=0.0236 MAE=0.0227 MAE=0.0220 MAE=0.0226 MAE=0.0224 MAE=0.0224 MAE=0.0225 MAE=0.0219 Epoch: 40/150MAE=0.0217 MAE=0.0214 MAE=0.0215 MAE=0.0217 MAE=0.0217 MAE=0.0214 MAE=0.0215 MAE=0.0213 MAE=0.0212 MAE=0.0213 Epoch: 50/150MAE=0.0211 MAE=0.0211 MAE=0.0210 MAE=0.0214 MAE=0.0211 MAE=0.0211 MAE=0.0214 MAE=0.0208 MAE=0.0206 MAE=0.0207 Epoch: 60/150MAE=0.0205 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0205 MAE=0.0206 MAE=0.0207 MAE=0.0205 MAE=0.0206 MAE=0.0203 Epoch: 70/150MAE=0.0204 MAE=0.0204 MAE=0.0204 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0204 MAE=0.0203 MAE=0.0203 MAE=0.0203 Epoch: 80/150MAE=0.0203 MAE=0.0203 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0203 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 Epoch: 90/150MAE=0.0201 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 Epoch: 100/150MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0200 MAE=0.0200 Epoch: 110/150MAE=0.0200 MAE=0.0201 MAE=0.0201 MAE=0.0200 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0200 MAE=0.0200 Epoch: 120/150MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0205 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 0.021 +/- 0.001\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1248 MAE=0.1071 MAE=0.0914 MAE=0.0802 MAE=0.0657 MAE=0.0553 MAE=0.0480 MAE=0.0428 MAE=0.0388 Epoch: 10/150MAE=0.0350 MAE=0.0332 MAE=0.0316 MAE=0.0318 MAE=0.0285 MAE=0.0282 MAE=0.0290 MAE=0.0282 MAE=0.0271 MAE=0.0264 Epoch: 20/150MAE=0.0259 MAE=0.0257 MAE=0.0256 MAE=0.0254 MAE=0.0254 MAE=0.0255 MAE=0.0251 MAE=0.0250 MAE=0.0250 MAE=0.0248 Epoch: 30/150MAE=0.0250 MAE=0.0247 MAE=0.0238 MAE=0.0240 MAE=0.0239 MAE=0.0250 MAE=0.0239 MAE=0.0230 MAE=0.0233 MAE=0.0233 Epoch: 40/150MAE=0.0224 MAE=0.0223 MAE=0.0226 MAE=0.0229 MAE=0.0227 MAE=0.0226 MAE=0.0223 MAE=0.0219 MAE=0.0222 MAE=0.0222 Epoch: 50/150MAE=0.0221 MAE=0.0223 MAE=0.0222 MAE=0.0219 MAE=0.0220 MAE=0.0217 MAE=0.0219 MAE=0.0217 MAE=0.0217 MAE=0.0216 Epoch: 60/150MAE=0.0219 MAE=0.0219 MAE=0.0217 MAE=0.0219 MAE=0.0216 MAE=0.0216 MAE=0.0217 MAE=0.0216 MAE=0.0216 MAE=0.0215 Epoch: 70/150MAE=0.0216 MAE=0.0215 MAE=0.0214 MAE=0.0216 MAE=0.0214 MAE=0.0217 MAE=0.0215 MAE=0.0214 MAE=0.0214 MAE=0.0215 Epoch: 80/150MAE=0.0215 MAE=0.0214 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 Epoch: 90/150MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0218 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 0.022 +/- 0.001\n",
                        "\n",
                        "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150MAE=0.1479 MAE=0.1192 MAE=0.1027 MAE=0.0797 MAE=0.0627 MAE=0.0531 MAE=0.0464 MAE=0.0397 MAE=0.0381 Epoch: 10/150MAE=0.0354 MAE=0.0343 MAE=0.0320 MAE=0.0299 MAE=0.0297 MAE=0.0274 MAE=0.0264 MAE=0.0262 MAE=0.0248 MAE=0.0244 Epoch: 20/150MAE=0.0245 MAE=0.0236 MAE=0.0231 MAE=0.0232 MAE=0.0228 MAE=0.0221 MAE=0.0221 MAE=0.0216 MAE=0.0215 MAE=0.0214 Epoch: 30/150MAE=0.0212 MAE=0.0207 MAE=0.0207 MAE=0.0209 MAE=0.0205 MAE=0.0214 MAE=0.0207 MAE=0.0207 MAE=0.0208 MAE=0.0202 Epoch: 40/150MAE=0.0201 MAE=0.0199 MAE=0.0198 MAE=0.0197 MAE=0.0197 MAE=0.0199 MAE=0.0201 MAE=0.0198 MAE=0.0197 MAE=0.0192 Epoch: 50/150MAE=0.0192 MAE=0.0192 MAE=0.0191 MAE=0.0191 MAE=0.0190 MAE=0.0190 MAE=0.0191 MAE=0.0190 MAE=0.0189 MAE=0.0189 Epoch: 60/150MAE=0.0190 MAE=0.0190 MAE=0.0189 MAE=0.0190 MAE=0.0189 MAE=0.0190 MAE=0.0189 MAE=0.0188 MAE=0.0190 MAE=0.0189 Epoch: 70/150MAE=0.0188 MAE=0.0189 MAE=0.0188 MAE=0.0187 MAE=0.0188 MAE=0.0188 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 Epoch: 80/150MAE=0.0187 MAE=0.0186 MAE=0.0187 MAE=0.0186 MAE=0.0187 MAE=0.0187 MAE=0.0185 MAE=0.0185 MAE=0.0185 MAE=0.0185 Epoch: 90/150MAE=0.0185 MAE=0.0185 MAE=0.0185 MAE=0.0185 MAE=0.0185 MAE=0.0184 MAE=0.0185 MAE=0.0184 MAE=0.0184 MAE=0.0184 Epoch: 100/150MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 Epoch: 110/150MAE=0.0184 MAE=0.0184 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0183 Epoch: 120/150MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0182 MAE=0.0182 Epoch: 130/150MAE=0.0182 MAE=0.0182 MAE=0.0183 MAE=0.0183 MAE=0.0182 MAE=0.0182 MAE=0.0182 MAE=0.0181 MAE=0.0181 MAE=0.0182 Epoch: 140/150MAE=0.0181 MAE=0.0181 MAE=0.0182 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 Epoch: 150/150MAE=0.0181 MAE=0.0187 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 0.019 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1437 MAE=0.1403 MAE=0.1136 MAE=0.0834 MAE=0.0611 MAE=0.0496 MAE=0.0432 MAE=0.0369 MAE=0.0346 Epoch: 10/150MAE=0.0325 MAE=0.0583 MAE=0.0400 MAE=0.0580 MAE=0.0453 MAE=0.0487 MAE=0.0437 MAE=0.0398 MAE=0.0287 MAE=0.0275 Epoch: 20/150MAE=0.0265 MAE=0.0227 MAE=0.0246 MAE=0.0231 MAE=0.0230 MAE=0.0242 MAE=0.0211 MAE=0.0208 MAE=0.0209 MAE=0.0211 Epoch: 30/150MAE=0.0205 MAE=0.0210 MAE=0.0208 MAE=0.0205 MAE=0.0209 MAE=0.0205 MAE=0.0207 MAE=0.0203 MAE=0.0207 MAE=0.0203 Epoch: 40/150MAE=0.0204 MAE=0.0203 MAE=0.0207 MAE=0.0210 MAE=0.0203 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0199 MAE=0.0200 Epoch: 50/150MAE=0.0197 MAE=0.0199 MAE=0.0198 MAE=0.0198 MAE=0.0197 MAE=0.0196 MAE=0.0196 MAE=0.0195 MAE=0.0194 MAE=0.0194 Epoch: 60/150MAE=0.0194 MAE=0.0193 MAE=0.0195 MAE=0.0196 MAE=0.0193 MAE=0.0196 MAE=0.0194 MAE=0.0193 MAE=0.0192 MAE=0.0192 Epoch: 70/150MAE=0.0191 MAE=0.0192 MAE=0.0191 MAE=0.0190 MAE=0.0191 MAE=0.0191 MAE=0.0190 MAE=0.0190 MAE=0.0191 MAE=0.0191 Epoch: 80/150MAE=0.0191 MAE=0.0190 MAE=0.0190 MAE=0.0190 MAE=0.0190 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 Epoch: 90/150MAE=0.0190 MAE=0.0189 MAE=0.0190 MAE=0.0190 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0195 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 0.019 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1506 MAE=0.1294 MAE=0.1058 MAE=0.0802 MAE=0.0633 MAE=0.0548 MAE=0.0462 MAE=0.0410 MAE=0.0381 Epoch: 10/150MAE=0.0334 MAE=0.0292 MAE=0.0288 MAE=0.0271 MAE=0.0257 MAE=0.0260 MAE=0.0249 MAE=0.0264 MAE=0.0239 MAE=0.0237 Epoch: 20/150MAE=0.0236 MAE=0.0231 MAE=0.0228 MAE=0.0230 MAE=0.0232 MAE=0.0234 MAE=0.0219 MAE=0.0212 MAE=0.0213 MAE=0.0218 Epoch: 30/150MAE=0.0220 MAE=0.0208 MAE=0.0208 MAE=0.0206 MAE=0.0211 MAE=0.0211 MAE=0.0207 MAE=0.0206 MAE=0.0203 MAE=0.0203 Epoch: 40/150MAE=0.0200 MAE=0.0199 MAE=0.0203 MAE=0.0200 MAE=0.0197 MAE=0.0196 MAE=0.0195 MAE=0.0198 MAE=0.0199 MAE=0.0198 Epoch: 50/150MAE=0.0195 MAE=0.0191 MAE=0.0191 MAE=0.0190 MAE=0.0190 MAE=0.0190 MAE=0.0189 MAE=0.0189 MAE=0.0188 MAE=0.0189 Epoch: 60/150MAE=0.0189 MAE=0.0188 MAE=0.0188 MAE=0.0189 MAE=0.0187 MAE=0.0188 MAE=0.0188 MAE=0.0187 MAE=0.0188 MAE=0.0186 Epoch: 70/150MAE=0.0186 MAE=0.0186 MAE=0.0187 MAE=0.0186 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0183 MAE=0.0184 Epoch: 80/150MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0183 MAE=0.0183 MAE=0.0182 MAE=0.0183 MAE=0.0183 MAE=0.0183 MAE=0.0182 Epoch: 90/150MAE=0.0182 MAE=0.0182 MAE=0.0182 MAE=0.0182 MAE=0.0182 MAE=0.0182 MAE=0.0182 MAE=0.0181 MAE=0.0181 MAE=0.0181 Epoch: 100/150MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 Epoch: 110/150MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 Epoch: 120/150MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 Epoch: 130/150MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0186 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 0.019 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1427 MAE=0.1323 MAE=0.1065 MAE=0.0800 MAE=0.0644 MAE=0.0551 MAE=0.0484 MAE=0.0407 MAE=0.0365 Epoch: 10/150MAE=0.0352 MAE=0.0329 MAE=0.0315 MAE=0.0310 MAE=0.0299 MAE=0.0294 MAE=0.0286 MAE=0.0277 MAE=0.0270 MAE=0.0271 Epoch: 20/150MAE=0.0255 MAE=0.0253 MAE=0.0253 MAE=0.0255 MAE=0.0253 MAE=0.0246 MAE=0.0248 MAE=0.0242 MAE=0.0245 MAE=0.0241 Epoch: 30/150MAE=0.0238 MAE=0.0232 MAE=0.0233 MAE=0.0236 MAE=0.0232 MAE=0.0230 MAE=0.0229 MAE=0.0227 MAE=0.0228 MAE=0.0225 Epoch: 40/150MAE=0.0224 MAE=0.0223 MAE=0.0223 MAE=0.0223 MAE=0.0225 MAE=0.0225 MAE=0.0221 MAE=0.0222 MAE=0.0221 MAE=0.0219 Epoch: 50/150MAE=0.0221 MAE=0.0219 MAE=0.0215 MAE=0.0216 MAE=0.0217 MAE=0.0216 MAE=0.0220 MAE=0.0213 MAE=0.0212 MAE=0.0210 Epoch: 60/150MAE=0.0211 MAE=0.0209 MAE=0.0210 MAE=0.0209 MAE=0.0210 MAE=0.0207 MAE=0.0208 MAE=0.0208 MAE=0.0209 MAE=0.0208 Epoch: 70/150MAE=0.0207 MAE=0.0206 MAE=0.0206 MAE=0.0206 MAE=0.0205 MAE=0.0205 MAE=0.0206 MAE=0.0205 MAE=0.0205 MAE=0.0205 Epoch: 80/150MAE=0.0204 MAE=0.0204 MAE=0.0203 MAE=0.0204 MAE=0.0204 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 Epoch: 90/150MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 Epoch: 100/150MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0202 MAE=0.0202 MAE=0.0203 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 Epoch: 110/150MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 Epoch: 120/150MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0206 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 0.019 +/- 0.001\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1401 MAE=0.1061 MAE=0.0797 MAE=0.0651 MAE=0.0544 MAE=0.0490 MAE=0.0439 MAE=0.0403 MAE=0.0385 Epoch: 10/150MAE=0.0338 MAE=0.0334 MAE=0.0322 MAE=0.0306 MAE=0.0295 MAE=0.0305 MAE=0.0270 MAE=0.0363 MAE=0.0269 MAE=0.0254 Epoch: 20/150MAE=0.0360 MAE=0.0265 MAE=0.0295 MAE=0.0245 MAE=0.0271 MAE=0.0233 MAE=0.0356 MAE=0.0238 MAE=0.0224 MAE=0.0285 Epoch: 30/150MAE=0.0223 MAE=0.0213 MAE=0.0220 MAE=0.0221 MAE=0.0216 MAE=0.0209 MAE=0.0210 MAE=0.0219 MAE=0.0215 MAE=0.0212 Epoch: 40/150MAE=0.0204 MAE=0.0201 MAE=0.0203 MAE=0.0200 MAE=0.0199 MAE=0.0200 MAE=0.0199 MAE=0.0200 MAE=0.0200 MAE=0.0195 Epoch: 50/150MAE=0.0195 MAE=0.0195 MAE=0.0196 MAE=0.0195 MAE=0.0192 MAE=0.0192 MAE=0.0193 MAE=0.0192 MAE=0.0191 MAE=0.0191 Epoch: 60/150MAE=0.0190 MAE=0.0191 MAE=0.0190 MAE=0.0192 MAE=0.0191 MAE=0.0191 MAE=0.0190 MAE=0.0190 MAE=0.0190 MAE=0.0190 Epoch: 70/150MAE=0.0191 MAE=0.0190 MAE=0.0190 MAE=0.0190 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 Epoch: 80/150MAE=0.0189 MAE=0.0190 MAE=0.0188 MAE=0.0189 MAE=0.0188 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0188 MAE=0.0189 Epoch: 90/150MAE=0.0189 MAE=0.0189 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0189 MAE=0.0189 MAE=0.0188 Epoch: 100/150MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 Epoch: 110/150MAE=0.0188 MAE=0.0188 MAE=0.0192 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 0.019 +/- 0.001\n",
                        "\n",
                        "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150MAE=0.1211 MAE=0.1042 MAE=0.0847 MAE=0.0714 MAE=0.0583 MAE=0.0505 MAE=0.0461 MAE=0.0428 MAE=0.0400 Epoch: 10/150MAE=0.0384 MAE=0.0339 MAE=0.0330 MAE=0.0302 MAE=0.0280 MAE=0.0261 MAE=0.0255 MAE=0.0250 MAE=0.0239 MAE=0.0248 Epoch: 20/150MAE=0.0235 MAE=0.0223 MAE=0.0229 MAE=0.0227 MAE=0.0215 MAE=0.0218 MAE=0.0212 MAE=0.0206 MAE=0.0204 MAE=0.0203 Epoch: 30/150MAE=0.0203 MAE=0.0198 MAE=0.0194 MAE=0.0198 MAE=0.0196 MAE=0.0192 MAE=0.0193 MAE=0.0192 MAE=0.0194 MAE=0.0188 Epoch: 40/150MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0186 MAE=0.0186 MAE=0.0184 MAE=0.0186 MAE=0.0191 MAE=0.0182 MAE=0.0178 Epoch: 50/150MAE=0.0181 MAE=0.0180 MAE=0.0181 MAE=0.0178 MAE=0.0178 MAE=0.0178 MAE=0.0179 MAE=0.0180 MAE=0.0172 MAE=0.0173 Epoch: 60/150MAE=0.0172 MAE=0.0170 MAE=0.0171 MAE=0.0170 MAE=0.0169 MAE=0.0169 MAE=0.0169 MAE=0.0169 MAE=0.0169 MAE=0.0169 Epoch: 70/150MAE=0.0168 MAE=0.0168 MAE=0.0168 MAE=0.0167 MAE=0.0167 MAE=0.0166 MAE=0.0166 MAE=0.0167 MAE=0.0169 MAE=0.0167 Epoch: 80/150MAE=0.0165 MAE=0.0165 MAE=0.0165 MAE=0.0164 MAE=0.0164 MAE=0.0164 MAE=0.0164 MAE=0.0164 MAE=0.0164 MAE=0.0163 Epoch: 90/150MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0162 MAE=0.0162 Epoch: 100/150MAE=0.0162 MAE=0.0163 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 Epoch: 110/150MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 Epoch: 120/150MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 Epoch: 130/150MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 Epoch: 140/150MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0166 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 0.017 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1261 MAE=0.1141 MAE=0.0949 MAE=0.0814 MAE=0.0620 MAE=0.0510 MAE=0.0470 MAE=0.0439 MAE=0.0405 Epoch: 10/150MAE=0.0367 MAE=0.0338 MAE=0.0332 MAE=0.0293 MAE=0.0282 MAE=0.0271 MAE=0.0268 MAE=0.0248 MAE=0.0236 MAE=0.0248 Epoch: 20/150MAE=0.0236 MAE=0.0233 MAE=0.0221 MAE=0.0224 MAE=0.0221 MAE=0.0222 MAE=0.0211 MAE=0.0213 MAE=0.0213 MAE=0.0210 Epoch: 30/150MAE=0.0206 MAE=0.0210 MAE=0.0203 MAE=0.0205 MAE=0.0195 MAE=0.0208 MAE=0.0199 MAE=0.0197 MAE=0.0192 MAE=0.0200 Epoch: 40/150MAE=0.0194 MAE=0.0197 MAE=0.0196 MAE=0.0191 MAE=0.0181 MAE=0.0180 MAE=0.0183 MAE=0.0180 MAE=0.0178 MAE=0.0179 Epoch: 50/150MAE=0.0179 MAE=0.0177 MAE=0.0177 MAE=0.0175 MAE=0.0178 MAE=0.0174 MAE=0.0173 MAE=0.0175 MAE=0.0177 MAE=0.0176 Epoch: 60/150MAE=0.0174 MAE=0.0171 MAE=0.0170 MAE=0.0170 MAE=0.0170 MAE=0.0169 MAE=0.0168 MAE=0.0168 MAE=0.0167 MAE=0.0167 Epoch: 70/150MAE=0.0168 MAE=0.0168 MAE=0.0167 MAE=0.0168 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0165 MAE=0.0165 MAE=0.0166 Epoch: 80/150MAE=0.0165 MAE=0.0164 MAE=0.0165 MAE=0.0165 MAE=0.0165 MAE=0.0165 MAE=0.0164 MAE=0.0164 MAE=0.0164 MAE=0.0164 Epoch: 90/150MAE=0.0164 MAE=0.0164 MAE=0.0164 MAE=0.0164 MAE=0.0164 MAE=0.0164 MAE=0.0165 MAE=0.0164 MAE=0.0164 MAE=0.0164 Epoch: 100/150MAE=0.0164 MAE=0.0163 MAE=0.0163 MAE=0.0164 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 Epoch: 110/150MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 Epoch: 120/150MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 Epoch: 130/150MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0162 MAE=0.0163 MAE=0.0162 MAE=0.0162 MAE=0.0163 MAE=0.0162 Epoch: 140/150MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0167 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 0.017 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1274 MAE=0.0916 MAE=0.0671 MAE=0.0651 MAE=0.0551 MAE=0.0515 MAE=0.0441 MAE=0.0394 MAE=0.0369 Epoch: 10/150MAE=0.0342 MAE=0.0312 MAE=0.0300 MAE=0.0271 MAE=0.0272 MAE=0.0260 MAE=0.0254 MAE=0.0260 MAE=0.0242 MAE=0.0238 Epoch: 20/150MAE=0.0231 MAE=0.0230 MAE=0.0232 MAE=0.0223 MAE=0.0219 MAE=0.0219 MAE=0.0213 MAE=0.0210 MAE=0.0212 MAE=0.0210 Epoch: 30/150MAE=0.0203 MAE=0.0207 MAE=0.0201 MAE=0.0202 MAE=0.0201 MAE=0.0198 MAE=0.0203 MAE=0.0198 MAE=0.0195 MAE=0.0196 Epoch: 40/150MAE=0.0197 MAE=0.0193 MAE=0.0192 MAE=0.0191 MAE=0.0193 MAE=0.0188 MAE=0.0188 MAE=0.0192 MAE=0.0189 MAE=0.0189 Epoch: 50/150MAE=0.0187 MAE=0.0186 MAE=0.0183 MAE=0.0184 MAE=0.0183 MAE=0.0184 MAE=0.0183 MAE=0.0181 MAE=0.0181 MAE=0.0181 Epoch: 60/150MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0178 MAE=0.0179 Epoch: 70/150MAE=0.0178 MAE=0.0178 MAE=0.0178 MAE=0.0178 MAE=0.0178 MAE=0.0178 MAE=0.0178 MAE=0.0177 MAE=0.0178 MAE=0.0177 Epoch: 80/150MAE=0.0177 MAE=0.0177 MAE=0.0177 MAE=0.0177 MAE=0.0177 MAE=0.0177 MAE=0.0177 MAE=0.0177 MAE=0.0177 MAE=0.0177 Epoch: 90/150MAE=0.0176 MAE=0.0176 MAE=0.0177 MAE=0.0176 MAE=0.0176 MAE=0.0176 MAE=0.0176 MAE=0.0176 MAE=0.0176 MAE=0.0176 Epoch: 100/150MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 Epoch: 110/150MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0173 Epoch: 120/150MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0172 Epoch: 130/150MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0171 MAE=0.0172 MAE=0.0171 MAE=0.0171 Epoch: 140/150MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0170 MAE=0.0170 MAE=0.0170 MAE=0.0171 MAE=0.0170 MAE=0.0170 MAE=0.0170 Epoch: 150/150MAE=0.0169 MAE=0.0174 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 0.017 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1340 MAE=0.1165 MAE=0.0856 MAE=0.0729 MAE=0.0631 MAE=0.0575 MAE=0.0515 MAE=0.0439 MAE=0.0414 Epoch: 10/150MAE=0.0358 MAE=0.0334 MAE=0.0341 MAE=0.0309 MAE=0.0298 MAE=0.0283 MAE=0.0276 MAE=0.0271 MAE=0.0276 MAE=0.0269 Epoch: 20/150MAE=0.0259 MAE=0.0247 MAE=0.0255 MAE=0.0247 MAE=0.0237 MAE=0.0241 MAE=0.0237 MAE=0.0222 MAE=0.0221 MAE=0.0225 Epoch: 30/150MAE=0.0220 MAE=0.0216 MAE=0.0207 MAE=0.0215 MAE=0.0208 MAE=0.0205 MAE=0.0208 MAE=0.0199 MAE=0.0213 MAE=0.0212 Epoch: 40/150MAE=0.0203 MAE=0.0210 MAE=0.0201 MAE=0.0194 MAE=0.0193 MAE=0.0191 MAE=0.0190 MAE=0.0187 MAE=0.0188 MAE=0.0185 Epoch: 50/150MAE=0.0184 MAE=0.0188 MAE=0.0187 MAE=0.0182 MAE=0.0181 MAE=0.0182 MAE=0.0187 MAE=0.0184 MAE=0.0182 MAE=0.0178 Epoch: 60/150MAE=0.0177 MAE=0.0177 MAE=0.0177 MAE=0.0178 MAE=0.0178 MAE=0.0176 MAE=0.0176 MAE=0.0176 MAE=0.0176 MAE=0.0176 Epoch: 70/150MAE=0.0176 MAE=0.0175 MAE=0.0176 MAE=0.0175 MAE=0.0174 MAE=0.0175 MAE=0.0175 MAE=0.0174 MAE=0.0175 MAE=0.0174 Epoch: 80/150MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0173 MAE=0.0173 Epoch: 90/150MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 Epoch: 100/150MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 Epoch: 110/150MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 Epoch: 120/150MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 MAE=0.0173 Epoch: 130/150MAE=0.0173 MAE=0.0178 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 0.017 +/- 0.001\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1543 MAE=0.1072 MAE=0.0871 MAE=0.0663 MAE=0.0551 MAE=0.0473 MAE=0.0407 MAE=0.0386 MAE=0.0350 Epoch: 10/150MAE=0.0322 MAE=0.0323 MAE=0.0302 MAE=0.0311 MAE=0.0292 MAE=0.0277 MAE=0.0264 MAE=0.0257 MAE=0.0265 MAE=0.0244 Epoch: 20/150MAE=0.0241 MAE=0.0227 MAE=0.0238 MAE=0.0223 MAE=0.0227 MAE=0.0218 MAE=0.0223 MAE=0.0216 MAE=0.0209 MAE=0.0212 Epoch: 30/150MAE=0.0209 MAE=0.0208 MAE=0.0207 MAE=0.0213 MAE=0.0205 MAE=0.0208 MAE=0.0202 MAE=0.0202 MAE=0.0200 MAE=0.0200 Epoch: 40/150MAE=0.0199 MAE=0.0199 MAE=0.0201 MAE=0.0192 MAE=0.0194 MAE=0.0198 MAE=0.0197 MAE=0.0192 MAE=0.0186 MAE=0.0184 Epoch: 50/150MAE=0.0184 MAE=0.0183 MAE=0.0183 MAE=0.0184 MAE=0.0181 MAE=0.0181 MAE=0.0182 MAE=0.0181 MAE=0.0180 MAE=0.0181 Epoch: 60/150MAE=0.0182 MAE=0.0181 MAE=0.0180 MAE=0.0176 MAE=0.0178 MAE=0.0179 MAE=0.0177 MAE=0.0177 MAE=0.0173 MAE=0.0172 Epoch: 70/150MAE=0.0172 MAE=0.0173 MAE=0.0173 MAE=0.0171 MAE=0.0173 MAE=0.0173 MAE=0.0174 MAE=0.0172 MAE=0.0169 MAE=0.0169 Epoch: 80/150MAE=0.0169 MAE=0.0168 MAE=0.0168 MAE=0.0168 MAE=0.0168 MAE=0.0168 MAE=0.0167 MAE=0.0168 MAE=0.0168 MAE=0.0168 Epoch: 90/150MAE=0.0167 MAE=0.0168 MAE=0.0167 MAE=0.0167 MAE=0.0167 MAE=0.0167 MAE=0.0167 MAE=0.0167 MAE=0.0167 MAE=0.0167 Epoch: 100/150MAE=0.0167 MAE=0.0167 MAE=0.0167 MAE=0.0166 MAE=0.0167 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0167 MAE=0.0166 Epoch: 110/150MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 Epoch: 120/150MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 Epoch: 130/150MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 Epoch: 140/150MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 MAE=0.0166 Epoch: 150/150MAE=0.0166 MAE=0.0169 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 0.017 +/- 0.000\n",
                        "\n",
                        "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150MAE=0.1284 MAE=0.0998 MAE=0.0894 MAE=0.0747 MAE=0.0636 MAE=0.0518 MAE=0.0483 MAE=0.0439 MAE=0.0401 Epoch: 10/150MAE=0.0378 MAE=0.0346 MAE=0.0321 MAE=0.0301 MAE=0.0300 MAE=0.0280 MAE=0.0288 MAE=0.0295 MAE=0.0272 MAE=0.0244 Epoch: 20/150MAE=0.0244 MAE=0.0237 MAE=0.0222 MAE=0.0220 MAE=0.0222 MAE=0.0225 MAE=0.0217 MAE=0.0218 MAE=0.0207 MAE=0.0208 Epoch: 30/150MAE=0.0209 MAE=0.0206 MAE=0.0205 MAE=0.0202 MAE=0.0198 MAE=0.0206 MAE=0.0202 MAE=0.0203 MAE=0.0198 MAE=0.0188 Epoch: 40/150MAE=0.0188 MAE=0.0185 MAE=0.0184 MAE=0.0185 MAE=0.0184 MAE=0.0182 MAE=0.0182 MAE=0.0181 MAE=0.0180 MAE=0.0182 Epoch: 50/150MAE=0.0182 MAE=0.0181 MAE=0.0177 MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0176 MAE=0.0176 MAE=0.0176 MAE=0.0175 Epoch: 60/150MAE=0.0175 MAE=0.0175 MAE=0.0173 MAE=0.0174 MAE=0.0172 MAE=0.0173 MAE=0.0173 MAE=0.0174 MAE=0.0173 MAE=0.0169 Epoch: 70/150MAE=0.0169 MAE=0.0169 MAE=0.0169 MAE=0.0168 MAE=0.0167 MAE=0.0167 MAE=0.0168 MAE=0.0168 MAE=0.0166 MAE=0.0166 Epoch: 80/150MAE=0.0165 MAE=0.0165 MAE=0.0165 MAE=0.0165 MAE=0.0165 MAE=0.0165 MAE=0.0166 MAE=0.0164 MAE=0.0164 MAE=0.0164 Epoch: 90/150MAE=0.0163 MAE=0.0163 MAE=0.0164 MAE=0.0163 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0163 MAE=0.0161 MAE=0.0162 Epoch: 100/150MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0160 MAE=0.0159 MAE=0.0160 MAE=0.0159 MAE=0.0160 MAE=0.0159 MAE=0.0158 Epoch: 110/150MAE=0.0158 MAE=0.0159 MAE=0.0158 MAE=0.0158 MAE=0.0158 MAE=0.0158 MAE=0.0158 MAE=0.0157 MAE=0.0157 MAE=0.0159 Epoch: 120/150MAE=0.0157 MAE=0.0157 MAE=0.0157 MAE=0.0157 MAE=0.0156 MAE=0.0156 MAE=0.0156 MAE=0.0156 MAE=0.0156 MAE=0.0156 Epoch: 130/150MAE=0.0156 MAE=0.0156 MAE=0.0156 MAE=0.0156 MAE=0.0156 MAE=0.0156 MAE=0.0155 MAE=0.0155 MAE=0.0155 MAE=0.0155 Epoch: 140/150MAE=0.0154 MAE=0.0154 MAE=0.0154 MAE=0.0154 MAE=0.0154 MAE=0.0154 MAE=0.0154 MAE=0.0154 MAE=0.0154 MAE=0.0154 Epoch: 150/150MAE=0.0154 MAE=0.0159 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 0.016 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1277 MAE=0.1023 MAE=0.0862 MAE=0.0730 MAE=0.0633 MAE=0.0538 MAE=0.0491 MAE=0.0452 MAE=0.0419 Epoch: 10/150MAE=0.0408 MAE=0.0386 MAE=0.0380 MAE=0.0347 MAE=0.0324 MAE=0.0306 MAE=0.0280 MAE=0.0273 MAE=0.0278 MAE=0.0259 Epoch: 20/150MAE=0.0266 MAE=0.0237 MAE=0.0241 MAE=0.0228 MAE=0.0232 MAE=0.0222 MAE=0.0212 MAE=0.0211 MAE=0.0215 MAE=0.0208 Epoch: 30/150MAE=0.0207 MAE=0.0204 MAE=0.0201 MAE=0.0198 MAE=0.0197 MAE=0.0193 MAE=0.0193 MAE=0.0193 MAE=0.0191 MAE=0.0193 Epoch: 40/150MAE=0.0191 MAE=0.0189 MAE=0.0191 MAE=0.0187 MAE=0.0183 MAE=0.0188 MAE=0.0186 MAE=0.0182 MAE=0.0182 MAE=0.0187 Epoch: 50/150MAE=0.0180 MAE=0.0179 MAE=0.0178 MAE=0.0179 MAE=0.0176 MAE=0.0175 MAE=0.0180 MAE=0.0177 MAE=0.0184 MAE=0.0175 Epoch: 60/150MAE=0.0175 MAE=0.0178 MAE=0.0189 MAE=0.0173 MAE=0.0171 MAE=0.0170 MAE=0.0175 MAE=0.0168 MAE=0.0167 MAE=0.0174 Epoch: 70/150MAE=0.0167 MAE=0.0172 MAE=0.0175 MAE=0.0160 MAE=0.0158 MAE=0.0158 MAE=0.0158 MAE=0.0159 MAE=0.0156 MAE=0.0157 Epoch: 80/150MAE=0.0155 MAE=0.0155 MAE=0.0155 MAE=0.0157 MAE=0.0154 MAE=0.0154 MAE=0.0155 MAE=0.0154 MAE=0.0156 MAE=0.0154 Epoch: 90/150MAE=0.0154 MAE=0.0155 MAE=0.0152 MAE=0.0151 MAE=0.0152 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0149 MAE=0.0149 Epoch: 100/150MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0149 MAE=0.0150 MAE=0.0149 MAE=0.0148 MAE=0.0148 MAE=0.0149 MAE=0.0150 Epoch: 110/150MAE=0.0149 MAE=0.0149 MAE=0.0148 MAE=0.0147 MAE=0.0148 MAE=0.0147 MAE=0.0148 MAE=0.0148 MAE=0.0147 MAE=0.0147 Epoch: 120/150MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0146 MAE=0.0146 MAE=0.0147 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 Epoch: 130/150MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 Epoch: 140/150MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0146 MAE=0.0152 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 0.016 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1135 MAE=0.1054 MAE=0.0820 MAE=0.0661 MAE=0.0565 MAE=0.0492 MAE=0.0441 MAE=0.0426 MAE=0.0382 Epoch: 10/150MAE=0.0358 MAE=0.0336 MAE=0.0323 MAE=0.0304 MAE=0.0272 MAE=0.0257 MAE=0.0251 MAE=0.0239 MAE=0.0229 MAE=0.0226 Epoch: 20/150MAE=0.0217 MAE=0.0229 MAE=0.0215 MAE=0.0213 MAE=0.0214 MAE=0.0209 MAE=0.0200 MAE=0.0198 MAE=0.0198 MAE=0.0191 Epoch: 30/150MAE=0.0194 MAE=0.0194 MAE=0.0187 MAE=0.0189 MAE=0.0184 MAE=0.0186 MAE=0.0182 MAE=0.0181 MAE=0.0181 MAE=0.0177 Epoch: 40/150MAE=0.0178 MAE=0.0179 MAE=0.0178 MAE=0.0178 MAE=0.0172 MAE=0.0171 MAE=0.0171 MAE=0.0168 MAE=0.0167 MAE=0.0167 Epoch: 50/150MAE=0.0166 MAE=0.0165 MAE=0.0164 MAE=0.0165 MAE=0.0165 MAE=0.0163 MAE=0.0162 MAE=0.0162 MAE=0.0164 MAE=0.0162 Epoch: 60/150MAE=0.0161 MAE=0.0163 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0163 MAE=0.0161 MAE=0.0161 MAE=0.0159 MAE=0.0163 Epoch: 70/150MAE=0.0161 MAE=0.0159 MAE=0.0158 MAE=0.0160 MAE=0.0159 MAE=0.0157 MAE=0.0158 MAE=0.0157 MAE=0.0155 MAE=0.0156 Epoch: 80/150MAE=0.0156 MAE=0.0156 MAE=0.0155 MAE=0.0155 MAE=0.0155 MAE=0.0157 MAE=0.0157 MAE=0.0153 MAE=0.0152 MAE=0.0152 Epoch: 90/150MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0150 MAE=0.0150 Epoch: 100/150MAE=0.0149 MAE=0.0149 MAE=0.0151 MAE=0.0151 MAE=0.0150 MAE=0.0149 MAE=0.0149 MAE=0.0148 MAE=0.0149 MAE=0.0149 Epoch: 110/150MAE=0.0149 MAE=0.0149 MAE=0.0148 MAE=0.0148 MAE=0.0148 MAE=0.0148 MAE=0.0148 MAE=0.0148 MAE=0.0148 MAE=0.0148 Epoch: 120/150MAE=0.0148 MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0148 MAE=0.0148 MAE=0.0147 MAE=0.0148 MAE=0.0147 Epoch: 130/150MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0147 Epoch: 140/150MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0147 MAE=0.0153 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 0.015 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1215 MAE=0.1145 MAE=0.0984 MAE=0.0816 MAE=0.0628 MAE=0.0582 MAE=0.0477 MAE=0.0469 MAE=0.0407 Epoch: 10/150MAE=0.0403 MAE=0.0390 MAE=0.0364 MAE=0.0339 MAE=0.0315 MAE=0.0313 MAE=0.0283 MAE=0.0274 MAE=0.0265 MAE=0.0265 Epoch: 20/150MAE=0.0250 MAE=0.0244 MAE=0.0241 MAE=0.0235 MAE=0.0230 MAE=0.0220 MAE=0.0221 MAE=0.0216 MAE=0.0217 MAE=0.0205 Epoch: 30/150MAE=0.0205 MAE=0.0198 MAE=0.0201 MAE=0.0215 MAE=0.0198 MAE=0.0193 MAE=0.0204 MAE=0.0184 MAE=0.0198 MAE=0.0188 Epoch: 40/150MAE=0.0187 MAE=0.0184 MAE=0.0182 MAE=0.0183 MAE=0.0182 MAE=0.0177 MAE=0.0182 MAE=0.0175 MAE=0.0183 MAE=0.0177 Epoch: 50/150MAE=0.0181 MAE=0.0178 MAE=0.0168 MAE=0.0166 MAE=0.0164 MAE=0.0166 MAE=0.0164 MAE=0.0165 MAE=0.0166 MAE=0.0164 Epoch: 60/150MAE=0.0163 MAE=0.0162 MAE=0.0162 MAE=0.0163 MAE=0.0162 MAE=0.0161 MAE=0.0162 MAE=0.0160 MAE=0.0160 MAE=0.0161 Epoch: 70/150MAE=0.0162 MAE=0.0159 MAE=0.0161 MAE=0.0159 MAE=0.0160 MAE=0.0157 MAE=0.0157 MAE=0.0158 MAE=0.0158 MAE=0.0160 Epoch: 80/150MAE=0.0155 MAE=0.0155 MAE=0.0155 MAE=0.0154 MAE=0.0155 MAE=0.0154 MAE=0.0156 MAE=0.0155 MAE=0.0153 MAE=0.0153 Epoch: 90/150MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0151 MAE=0.0151 Epoch: 100/150MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 Epoch: 110/150MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 Epoch: 120/150MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 Epoch: 130/150MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 Epoch: 140/150MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0153 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 0.015 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1295 MAE=0.1123 MAE=0.0975 MAE=0.0819 MAE=0.0629 MAE=0.0508 MAE=0.0439 MAE=0.0417 MAE=0.0417 Epoch: 10/150MAE=0.0378 MAE=0.0361 MAE=0.0345 MAE=0.0299 MAE=0.0289 MAE=0.0270 MAE=0.0253 MAE=0.0255 MAE=0.0232 MAE=0.0232 Epoch: 20/150MAE=0.0223 MAE=0.0223 MAE=0.0223 MAE=0.0214 MAE=0.0211 MAE=0.0206 MAE=0.0212 MAE=0.0209 MAE=0.0207 MAE=0.0201 Epoch: 30/150MAE=0.0199 MAE=0.0196 MAE=0.0193 MAE=0.0191 MAE=0.0192 MAE=0.0189 MAE=0.0190 MAE=0.0189 MAE=0.0192 MAE=0.0191 Epoch: 40/150MAE=0.0185 MAE=0.0180 MAE=0.0178 MAE=0.0177 MAE=0.0176 MAE=0.0176 MAE=0.0177 MAE=0.0175 MAE=0.0175 MAE=0.0174 Epoch: 50/150MAE=0.0174 MAE=0.0174 MAE=0.0172 MAE=0.0170 MAE=0.0171 MAE=0.0171 MAE=0.0169 MAE=0.0170 MAE=0.0169 MAE=0.0169 Epoch: 60/150MAE=0.0169 MAE=0.0166 MAE=0.0167 MAE=0.0165 MAE=0.0168 MAE=0.0166 MAE=0.0165 MAE=0.0164 MAE=0.0163 MAE=0.0163 Epoch: 70/150MAE=0.0163 MAE=0.0167 MAE=0.0165 MAE=0.0161 MAE=0.0161 MAE=0.0160 MAE=0.0162 MAE=0.0161 MAE=0.0160 MAE=0.0160 Epoch: 80/150MAE=0.0159 MAE=0.0158 MAE=0.0160 MAE=0.0158 MAE=0.0162 MAE=0.0158 MAE=0.0157 MAE=0.0155 MAE=0.0154 MAE=0.0156 Epoch: 90/150MAE=0.0155 MAE=0.0155 MAE=0.0155 MAE=0.0154 MAE=0.0153 MAE=0.0153 MAE=0.0153 MAE=0.0153 MAE=0.0154 MAE=0.0153 Epoch: 100/150MAE=0.0152 MAE=0.0153 MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0152 MAE=0.0152 Epoch: 110/150MAE=0.0151 MAE=0.0151 MAE=0.0152 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0151 MAE=0.0150 MAE=0.0150 MAE=0.0150 Epoch: 120/150MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 Epoch: 130/150MAE=0.0150 MAE=0.0150 MAE=0.0149 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0150 MAE=0.0149 MAE=0.0149 MAE=0.0149 Epoch: 140/150MAE=0.0149 MAE=0.0149 MAE=0.0149 MAE=0.0149 MAE=0.0149 MAE=0.0149 MAE=0.0149 MAE=0.0149 MAE=0.0149 MAE=0.0149 Epoch: 150/150MAE=0.0149 MAE=0.0156 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 0.015 +/- 0.000\n",
                        "\n"
                    ]
                }
            ],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --hgpsl_ratio=0.1 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --hgpsl_ratio=0.3 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --hgpsl_ratio=0.5 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --hgpsl_ratio=0.7 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --hgpsl_ratio=0.9 --pooling='HGPSL'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### BACE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "++++++++++++++++++++++0.1++++++++++++++++++++++++\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
                        "********************1's fold 1's run over********************\n",
                        "AUROC: 0.839 +/- 0.000\n",
                        "AUPRC: 0.796 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
                        "********************1's fold 2's run over********************\n",
                        "AUROC: 0.830 +/- 0.009\n",
                        "AUPRC: 0.796 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 3's run over********************\n",
                        "AUROC: 0.829 +/- 0.008\n",
                        "AUPRC: 0.791 +/- 0.007\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
                        "********************1's fold 4's run over********************\n",
                        "AUROC: 0.838 +/- 0.017\n",
                        "AUPRC: 0.801 +/- 0.019\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
                        "********************1's fold 5's run over********************\n",
                        "AUROC: 0.841 +/- 0.016\n",
                        "AUPRC: 0.801 +/- 0.017\n",
                        "\n",
                        "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
                        "********************1's fold 1's run over********************\n",
                        "AUROC: 0.849 +/- 0.000\n",
                        "AUPRC: 0.839 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
                        "********************1's fold 2's run over********************\n",
                        "AUROC: 0.853 +/- 0.005\n",
                        "AUPRC: 0.831 +/- 0.009\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 3's run over********************\n",
                        "AUROC: 0.849 +/- 0.007\n",
                        "AUPRC: 0.832 +/- 0.007\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
                        "********************1's fold 4's run over********************\n",
                        "AUROC: 0.845 +/- 0.009\n",
                        "AUPRC: 0.823 +/- 0.016\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 5's run over********************\n",
                        "AUROC: 0.830 +/- 0.032\n",
                        "AUPRC: 0.806 +/- 0.037\n",
                        "\n",
                        "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
                        "********************1's fold 1's run over********************\n",
                        "AUROC: 0.831 +/- 0.000\n",
                        "AUPRC: 0.778 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 2's run over********************\n",
                        "AUROC: 0.832 +/- 0.001\n",
                        "AUPRC: 0.799 +/- 0.021\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 3's run over********************\n",
                        "AUROC: 0.825 +/- 0.010\n",
                        "AUPRC: 0.795 +/- 0.018\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 4's run over********************\n",
                        "AUROC: 0.836 +/- 0.022\n",
                        "AUPRC: 0.806 +/- 0.023\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 5's run over********************\n",
                        "AUROC: 0.832 +/- 0.021\n",
                        "AUPRC: 0.794 +/- 0.032\n",
                        "\n",
                        "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 1's run over********************\n",
                        "AUROC: 0.838 +/- 0.000\n",
                        "AUPRC: 0.836 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 2's run over********************\n",
                        "AUROC: 0.834 +/- 0.005\n",
                        "AUPRC: 0.802 +/- 0.034\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 3's run over********************\n",
                        "AUROC: 0.839 +/- 0.008\n",
                        "AUPRC: 0.807 +/- 0.029\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 4's run over********************\n",
                        "AUROC: 0.826 +/- 0.023\n",
                        "AUPRC: 0.798 +/- 0.030\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 5's run over********************\n",
                        "AUROC: 0.826 +/- 0.021\n",
                        "AUPRC: 0.791 +/- 0.030\n",
                        "\n",
                        "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 1's run over********************\n",
                        "AUROC: 0.838 +/- 0.000\n",
                        "AUPRC: 0.808 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 2's run over********************\n",
                        "AUROC: 0.840 +/- 0.002\n",
                        "AUPRC: 0.801 +/- 0.007\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 3's run over********************\n",
                        "AUROC: 0.838 +/- 0.004\n",
                        "AUPRC: 0.802 +/- 0.007\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 4's run over********************\n",
                        "AUROC: 0.836 +/- 0.005\n",
                        "AUPRC: 0.794 +/- 0.015\n",
                        "\n",
                        "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
                        "********************1's fold 5's run over********************\n",
                        "AUROC: 0.837 +/- 0.005\n",
                        "AUPRC: 0.801 +/- 0.019\n",
                        "\n"
                    ]
                }
            ],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.1 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.3 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.5 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.7 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.9 --pooling='HGPSL'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ESOL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=1.5778 RMSE=1.4261 RMSE=1.2931 RMSE=1.3573 RMSE=1.3921 RMSE=1.3780 RMSE=1.2969 RMSE=1.3914 RMSE=1.3335 Epoch: 10/150RMSE=1.3878 RMSE=1.3276 RMSE=1.3144 RMSE=1.3332 RMSE=1.3484 RMSE=1.3725 RMSE=1.3852 RMSE=1.3747 RMSE=1.4050 RMSE=1.4163 Epoch: 20/150RMSE=1.4126 RMSE=1.4114 RMSE=1.4043 RMSE=1.3772 RMSE=1.4000 RMSE=1.4171 RMSE=1.4196 RMSE=1.4236 RMSE=1.6002 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 1.600 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.6612 RMSE=1.3769 RMSE=1.4377 RMSE=1.3524 RMSE=1.3245 RMSE=1.4048 RMSE=1.4792 RMSE=1.3655 RMSE=1.4407 Epoch: 10/150RMSE=1.3751 RMSE=1.3638 RMSE=1.4074 RMSE=1.4074 RMSE=1.3561 RMSE=1.3588 RMSE=1.3978 RMSE=1.3469 RMSE=1.3454 RMSE=1.3460 Epoch: 20/150RMSE=1.3340 RMSE=1.3449 RMSE=1.3433 RMSE=1.3471 RMSE=1.3549 RMSE=1.3474 RMSE=1.5774 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 1.589 +/- 0.011\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.5498 RMSE=1.4866 RMSE=1.4087 RMSE=1.4431 RMSE=1.3913 RMSE=1.3324 RMSE=1.4288 RMSE=1.2968 RMSE=1.3715 Epoch: 10/150RMSE=1.3918 RMSE=1.5070 RMSE=1.4569 RMSE=1.5153 RMSE=1.5422 RMSE=1.4688 RMSE=1.4782 RMSE=1.3863 RMSE=1.4138 RMSE=1.4174 Epoch: 20/150RMSE=1.4299 RMSE=1.3716 RMSE=1.3967 RMSE=1.3919 RMSE=1.3950 RMSE=1.4052 RMSE=1.3954 RMSE=1.3903 RMSE=1.4054 RMSE=1.4842 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 1.554 +/- 0.050\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.4966 RMSE=1.3470 RMSE=1.3404 RMSE=1.3486 RMSE=1.3707 RMSE=1.4395 RMSE=1.3679 RMSE=1.3292 RMSE=1.3519 Epoch: 10/150RMSE=1.3567 RMSE=1.3418 RMSE=1.3608 RMSE=1.3682 RMSE=1.3590 RMSE=1.3401 RMSE=1.3724 RMSE=1.3646 RMSE=1.3436 RMSE=1.3646 Epoch: 20/150RMSE=1.3391 RMSE=1.3611 RMSE=1.3520 RMSE=1.3500 RMSE=1.3681 RMSE=1.3594 RMSE=1.3578 RMSE=1.3570 RMSE=1.3606 RMSE=1.5965 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 1.565 +/- 0.047\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.6667 RMSE=1.4774 RMSE=1.4242 RMSE=1.4979 RMSE=1.4718 RMSE=1.4566 RMSE=1.4638 RMSE=1.4346 RMSE=1.4826 Epoch: 10/150RMSE=1.4529 RMSE=1.4728 RMSE=1.4822 RMSE=1.3957 RMSE=1.4342 RMSE=1.4078 RMSE=1.4450 RMSE=1.4290 RMSE=1.3966 RMSE=1.4300 Epoch: 20/150RMSE=1.4326 RMSE=1.4411 RMSE=1.4464 RMSE=1.4434 RMSE=1.4558 RMSE=1.4195 RMSE=1.4359 RMSE=1.4289 RMSE=1.4343 RMSE=1.4282 Epoch: 30/150RMSE=1.4251 RMSE=1.4266 RMSE=1.4226 RMSE=1.4243 RMSE=1.4318 RMSE=1.4281 RMSE=1.4314 RMSE=1.4272 RMSE=1.4330 RMSE=1.5404 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 1.560 +/- 0.043\n",
                        "\n",
                        "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=1.5854 RMSE=1.3651 RMSE=1.2111 RMSE=1.2631 RMSE=1.2666 RMSE=1.1911 RMSE=1.1725 RMSE=1.2195 RMSE=1.2212 Epoch: 10/150RMSE=1.1904 RMSE=1.2535 RMSE=1.2459 RMSE=1.2169 RMSE=1.1269 RMSE=1.2795 RMSE=1.2839 RMSE=1.2469 RMSE=1.2092 RMSE=1.2196 Epoch: 20/150RMSE=1.1747 RMSE=1.1632 RMSE=1.2308 RMSE=1.2350 RMSE=1.1686 RMSE=1.1634 RMSE=1.1724 RMSE=1.1622 RMSE=1.1640 RMSE=1.1563 Epoch: 30/150RMSE=1.1526 RMSE=1.1614 RMSE=1.1713 RMSE=1.1602 RMSE=1.1557 RMSE=1.1722 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 1.172 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.6198 RMSE=1.4551 RMSE=1.2658 RMSE=1.2940 RMSE=1.3059 RMSE=1.2759 RMSE=1.3097 RMSE=1.1336 RMSE=1.1774 Epoch: 10/150RMSE=1.1957 RMSE=1.1705 RMSE=1.1421 RMSE=1.2250 RMSE=1.2208 RMSE=1.2124 RMSE=1.2020 RMSE=1.2196 RMSE=1.2126 RMSE=1.2047 Epoch: 20/150RMSE=1.2219 RMSE=1.1928 RMSE=1.2218 RMSE=1.2156 RMSE=1.2064 RMSE=1.2232 RMSE=1.2164 RMSE=1.2323 RMSE=1.2180 RMSE=1.2287 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 1.200 +/- 0.028\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.5522 RMSE=1.2509 RMSE=1.2621 RMSE=1.2720 RMSE=1.2302 RMSE=1.2365 RMSE=1.1743 RMSE=1.2091 RMSE=1.2290 Epoch: 10/150RMSE=1.2794 RMSE=1.2260 RMSE=1.1923 RMSE=1.1823 RMSE=1.2013 RMSE=1.1665 RMSE=1.2128 RMSE=1.2414 RMSE=1.1948 RMSE=1.2433 Epoch: 20/150RMSE=1.2353 RMSE=1.2103 RMSE=1.2085 RMSE=1.1932 RMSE=1.1978 RMSE=1.2006 RMSE=1.2147 RMSE=1.2189 RMSE=1.2034 RMSE=1.1927 Epoch: 30/150RMSE=1.2001 RMSE=1.2106 RMSE=1.2175 RMSE=1.2112 RMSE=1.2250 RMSE=1.2128 RMSE=1.2272 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 1.209 +/- 0.026\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.6937 RMSE=1.3691 RMSE=1.2016 RMSE=1.2852 RMSE=1.2560 RMSE=1.2419 RMSE=1.2413 RMSE=1.1801 RMSE=1.2456 Epoch: 10/150RMSE=1.2371 RMSE=1.2094 RMSE=1.1950 RMSE=1.1462 RMSE=1.1794 RMSE=1.1674 RMSE=1.1479 RMSE=1.1708 RMSE=1.1288 RMSE=1.1342 Epoch: 20/150RMSE=1.1419 RMSE=1.1557 RMSE=1.1671 RMSE=1.1724 RMSE=1.1689 RMSE=1.1521 RMSE=1.1509 RMSE=1.1578 RMSE=1.1635 RMSE=1.1717 Epoch: 30/150RMSE=1.1632 RMSE=1.1708 RMSE=1.1809 RMSE=1.1718 RMSE=1.1817 RMSE=1.1758 RMSE=1.1761 RMSE=1.1701 RMSE=1.1749 RMSE=1.1045 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 1.183 +/- 0.051\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.6570 RMSE=1.3965 RMSE=1.3642 RMSE=1.1982 RMSE=1.2430 RMSE=1.1692 RMSE=1.2436 RMSE=1.2763 RMSE=1.1552 Epoch: 10/150RMSE=1.1626 RMSE=1.1588 RMSE=1.1507 RMSE=1.1350 RMSE=1.1401 RMSE=1.1414 RMSE=1.1825 RMSE=1.1001 RMSE=1.0600 RMSE=1.1591 Epoch: 20/150RMSE=1.1296 RMSE=1.1155 RMSE=1.1396 RMSE=1.1846 RMSE=1.1158 RMSE=1.2095 RMSE=1.1603 RMSE=1.1246 RMSE=1.1069 RMSE=1.1267 Epoch: 30/150RMSE=1.1225 RMSE=1.1627 RMSE=1.1316 RMSE=1.1430 RMSE=1.1290 RMSE=1.1643 RMSE=1.1391 RMSE=1.1599 RMSE=1.1355 RMSE=1.1919 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 1.185 +/- 0.046\n",
                        "\n",
                        "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=1.3329 RMSE=1.3613 RMSE=1.2497 RMSE=1.1042 RMSE=1.0964 RMSE=1.0655 RMSE=1.1729 RMSE=1.0869 RMSE=1.1314 Epoch: 10/150RMSE=1.0381 RMSE=1.1062 RMSE=0.9791 RMSE=1.1018 RMSE=1.0564 RMSE=1.0705 RMSE=0.9499 RMSE=1.0033 RMSE=0.9447 RMSE=1.0122 Epoch: 20/150RMSE=1.0655 RMSE=0.9829 RMSE=1.0022 RMSE=0.9891 RMSE=0.9942 RMSE=0.9880 RMSE=0.9917 RMSE=1.0053 RMSE=0.9950 RMSE=0.9933 Epoch: 30/150RMSE=0.9848 RMSE=1.0039 RMSE=0.9898 RMSE=0.9728 RMSE=0.9823 RMSE=0.9903 RMSE=0.9763 RMSE=0.9717 RMSE=0.9678 RMSE=1.1296 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 1.130 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.5348 RMSE=1.0977 RMSE=1.1045 RMSE=1.0228 RMSE=1.1161 RMSE=1.2164 RMSE=1.1300 RMSE=1.0430 RMSE=1.0972 Epoch: 10/150RMSE=1.0226 RMSE=1.0878 RMSE=1.0269 RMSE=1.0207 RMSE=1.0026 RMSE=0.9921 RMSE=0.9963 RMSE=0.9876 RMSE=0.9895 RMSE=1.0014 Epoch: 20/150RMSE=1.0109 RMSE=1.0475 RMSE=1.0465 RMSE=1.0236 RMSE=1.0192 RMSE=1.0492 RMSE=1.0398 RMSE=1.0231 RMSE=1.0377 RMSE=1.0380 Epoch: 30/150RMSE=1.0352 RMSE=1.0363 RMSE=1.0501 RMSE=1.0561 RMSE=1.0537 RMSE=1.0495 RMSE=1.0493 RMSE=1.0493 RMSE=1.1656 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 1.148 +/- 0.018\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.4092 RMSE=1.1495 RMSE=1.1167 RMSE=1.2122 RMSE=1.0943 RMSE=1.1840 RMSE=1.2431 RMSE=1.1165 RMSE=1.1387 Epoch: 10/150RMSE=1.0906 RMSE=1.0736 RMSE=1.0737 RMSE=1.0717 RMSE=1.0697 RMSE=1.0873 RMSE=1.0612 RMSE=1.0820 RMSE=1.0812 RMSE=1.1070 Epoch: 20/150RMSE=1.0271 RMSE=1.0815 RMSE=1.0166 RMSE=1.0502 RMSE=0.9874 RMSE=1.0142 RMSE=1.0822 RMSE=1.0719 RMSE=1.0917 RMSE=1.0203 Epoch: 30/150RMSE=1.0239 RMSE=1.0508 RMSE=1.0909 RMSE=1.0145 RMSE=1.0174 RMSE=1.0407 RMSE=1.0172 RMSE=1.0193 RMSE=1.0176 RMSE=1.0215 Epoch: 40/150RMSE=1.0215 RMSE=1.0209 RMSE=1.0252 RMSE=1.0288 RMSE=1.0241 RMSE=1.0402 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 1.112 +/- 0.053\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.5730 RMSE=1.1050 RMSE=0.9980 RMSE=1.0221 RMSE=1.0247 RMSE=1.0321 RMSE=0.9924 RMSE=1.0413 RMSE=0.9183 Epoch: 10/150RMSE=0.9535 RMSE=0.9665 RMSE=0.8682 RMSE=0.9127 RMSE=0.9262 RMSE=0.9676 RMSE=0.7972 RMSE=0.9732 RMSE=0.9682 RMSE=0.9754 Epoch: 20/150RMSE=0.9083 RMSE=0.9175 RMSE=0.9045 RMSE=0.9227 RMSE=0.9086 RMSE=0.8749 RMSE=0.8471 RMSE=0.8595 RMSE=0.8901 RMSE=0.8477 Epoch: 30/150RMSE=0.8408 RMSE=0.8534 RMSE=0.8422 RMSE=0.8475 RMSE=0.8458 RMSE=0.8492 RMSE=0.8456 RMSE=0.9692 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 1.076 +/- 0.077\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.6865 RMSE=1.1752 RMSE=1.1974 RMSE=1.1073 RMSE=1.0735 RMSE=1.0411 RMSE=1.0920 RMSE=1.0313 RMSE=1.0799 Epoch: 10/150RMSE=1.0945 RMSE=1.1360 RMSE=1.0772 RMSE=1.0471 RMSE=1.0575 RMSE=0.9490 RMSE=0.9950 RMSE=1.0431 RMSE=0.9686 RMSE=1.0303 Epoch: 20/150RMSE=0.9915 RMSE=0.9857 RMSE=0.9893 RMSE=0.9730 RMSE=0.9671 RMSE=0.9913 RMSE=0.9840 RMSE=0.9945 RMSE=0.9961 RMSE=0.9882 Epoch: 30/150RMSE=1.0083 RMSE=0.9894 RMSE=0.9894 RMSE=0.9981 RMSE=0.9956 RMSE=0.9851 RMSE=1.1550 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 1.092 +/- 0.076\n",
                        "\n",
                        "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=1.4431 RMSE=1.2593 RMSE=1.1699 RMSE=1.0761 RMSE=1.0750 RMSE=1.0757 RMSE=1.0294 RMSE=0.9368 RMSE=0.9881 Epoch: 10/150RMSE=0.9734 RMSE=1.0242 RMSE=0.8615 RMSE=0.9240 RMSE=0.8778 RMSE=0.9124 RMSE=1.0174 RMSE=0.9555 RMSE=0.9433 RMSE=0.9439 Epoch: 20/150RMSE=0.9494 RMSE=0.9143 RMSE=0.9031 RMSE=0.9219 RMSE=0.9090 RMSE=0.9089 RMSE=0.8883 RMSE=0.9052 RMSE=0.8893 RMSE=0.9010 Epoch: 30/150RMSE=0.8937 RMSE=0.9022 RMSE=0.9019 RMSE=1.0065 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 1.007 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.5343 RMSE=1.2620 RMSE=1.0999 RMSE=1.0015 RMSE=0.9907 RMSE=0.9041 RMSE=0.9490 RMSE=0.9458 RMSE=0.9589 Epoch: 10/150RMSE=0.8848 RMSE=0.9732 RMSE=0.9274 RMSE=0.9360 RMSE=0.9799 RMSE=1.0269 RMSE=0.9482 RMSE=0.9582 RMSE=0.9681 RMSE=0.9585 Epoch: 20/150RMSE=0.8992 RMSE=0.8987 RMSE=0.9007 RMSE=0.8943 RMSE=0.8961 RMSE=0.8909 RMSE=0.9068 RMSE=0.9019 RMSE=0.9045 RMSE=0.9117 Epoch: 30/150RMSE=0.9171 RMSE=1.0204 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 1.013 +/- 0.007\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.5616 RMSE=1.1579 RMSE=1.1810 RMSE=1.0326 RMSE=0.9274 RMSE=0.9522 RMSE=0.9502 RMSE=0.9819 RMSE=0.9752 Epoch: 10/150RMSE=0.9102 RMSE=0.8986 RMSE=0.9216 RMSE=0.9103 RMSE=0.8890 RMSE=0.9053 RMSE=0.8993 RMSE=0.9087 RMSE=0.8932 RMSE=0.8975 Epoch: 20/150RMSE=0.9098 RMSE=0.8948 RMSE=0.8548 RMSE=0.8819 RMSE=0.8646 RMSE=0.8836 RMSE=0.9032 RMSE=0.8774 RMSE=0.8693 RMSE=0.8988 Epoch: 30/150RMSE=0.8820 RMSE=0.8800 RMSE=0.8790 RMSE=0.8788 RMSE=0.8856 RMSE=0.8775 RMSE=0.8808 RMSE=0.8791 RMSE=0.8808 RMSE=0.8800 Epoch: 40/150RMSE=0.8785 RMSE=0.8796 RMSE=0.8765 RMSE=1.0080 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 1.012 +/- 0.006\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.6373 RMSE=1.1295 RMSE=1.0695 RMSE=0.9843 RMSE=0.9686 RMSE=1.0726 RMSE=1.0516 RMSE=1.0110 RMSE=1.0923 Epoch: 10/150RMSE=1.0240 RMSE=1.0316 RMSE=1.0201 RMSE=1.0657 RMSE=1.0481 RMSE=1.0485 RMSE=1.0415 RMSE=1.0163 RMSE=1.0150 RMSE=1.0157 Epoch: 20/150RMSE=1.0013 RMSE=1.0274 RMSE=1.0106 RMSE=1.0210 RMSE=1.0318 RMSE=1.0277 RMSE=1.1072 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 1.036 +/- 0.042\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.4834 RMSE=1.0712 RMSE=1.0543 RMSE=1.0321 RMSE=1.0431 RMSE=1.0756 RMSE=1.0032 RMSE=0.9507 RMSE=0.9619 Epoch: 10/150RMSE=0.9070 RMSE=0.9398 RMSE=0.9682 RMSE=0.8655 RMSE=0.9105 RMSE=0.9700 RMSE=0.9237 RMSE=0.9111 RMSE=0.9244 RMSE=0.9535 Epoch: 20/150RMSE=0.8764 RMSE=0.9316 RMSE=0.9074 RMSE=0.8872 RMSE=0.8862 RMSE=0.9036 RMSE=0.8905 RMSE=0.8875 RMSE=0.8808 RMSE=0.8907 Epoch: 30/150RMSE=0.8753 RMSE=0.8832 RMSE=0.8823 RMSE=0.8835 RMSE=1.0009 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 1.029 +/- 0.040\n",
                        "\n",
                        "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=1.4856 RMSE=1.2448 RMSE=1.0346 RMSE=1.0292 RMSE=0.9968 RMSE=1.0087 RMSE=0.9789 RMSE=0.9667 RMSE=0.9315 Epoch: 10/150RMSE=0.9823 RMSE=0.9832 RMSE=0.9005 RMSE=0.9720 RMSE=0.9940 RMSE=0.9331 RMSE=0.9186 RMSE=0.8747 RMSE=0.8745 RMSE=0.8598 Epoch: 20/150RMSE=0.8801 RMSE=0.8548 RMSE=0.9000 RMSE=0.8957 RMSE=0.8747 RMSE=0.9036 RMSE=0.9288 RMSE=0.9191 RMSE=0.9163 RMSE=0.8842 Epoch: 30/150RMSE=0.8893 RMSE=0.8871 RMSE=0.9011 RMSE=0.8742 RMSE=0.8744 RMSE=0.8752 RMSE=0.8781 RMSE=0.8775 RMSE=0.8772 RMSE=0.8769 Epoch: 40/150RMSE=0.8763 RMSE=0.8751 RMSE=1.1315 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 1.132 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.5438 RMSE=1.1135 RMSE=1.0948 RMSE=0.9726 RMSE=1.0128 RMSE=0.9439 RMSE=0.9348 RMSE=1.0200 RMSE=0.8943 Epoch: 10/150RMSE=0.8309 RMSE=1.0268 RMSE=0.8395 RMSE=0.8693 RMSE=0.8864 RMSE=0.9663 RMSE=0.9899 RMSE=0.9472 RMSE=0.9202 RMSE=0.9638 Epoch: 20/150RMSE=0.9790 RMSE=0.9758 RMSE=0.9538 RMSE=0.9303 RMSE=0.9604 RMSE=0.9482 RMSE=0.9452 RMSE=0.9405 RMSE=0.9386 RMSE=0.9327 Epoch: 30/150RMSE=0.9399 RMSE=0.9800 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 1.056 +/- 0.076\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.6700 RMSE=1.3124 RMSE=1.1918 RMSE=1.1273 RMSE=1.0940 RMSE=0.9714 RMSE=0.9709 RMSE=1.0040 RMSE=0.9491 Epoch: 10/150RMSE=1.0713 RMSE=0.8902 RMSE=1.0821 RMSE=0.9386 RMSE=1.0487 RMSE=0.9307 RMSE=0.9336 RMSE=0.9112 RMSE=1.0106 RMSE=1.0022 Epoch: 20/150RMSE=0.9387 RMSE=0.9474 RMSE=0.9522 RMSE=0.9105 RMSE=0.9208 RMSE=0.9198 RMSE=0.9280 RMSE=0.9273 RMSE=0.9086 RMSE=0.8977 Epoch: 30/150RMSE=0.9063 RMSE=0.9127 RMSE=0.9560 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 1.022 +/- 0.078\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.6462 RMSE=1.2536 RMSE=1.1166 RMSE=1.0584 RMSE=1.0456 RMSE=1.0710 RMSE=1.0799 RMSE=1.1175 RMSE=1.0664 Epoch: 10/150RMSE=0.9821 RMSE=0.9329 RMSE=1.0137 RMSE=0.9942 RMSE=0.9503 RMSE=0.9314 RMSE=0.9645 RMSE=0.9714 RMSE=0.9631 RMSE=0.9408 Epoch: 20/150RMSE=0.9464 RMSE=0.9363 RMSE=0.9457 RMSE=0.9314 RMSE=0.9476 RMSE=0.9363 RMSE=0.9377 RMSE=0.9419 RMSE=0.9292 RMSE=0.9233 Epoch: 30/150RMSE=0.9164 RMSE=0.9359 RMSE=0.9196 RMSE=0.9323 RMSE=0.9272 RMSE=0.9252 RMSE=0.9343 RMSE=0.9434 RMSE=0.9441 RMSE=0.9501 Epoch: 40/150RMSE=0.9448 RMSE=0.9417 RMSE=0.9464 RMSE=0.9441 RMSE=0.9447 RMSE=0.9406 RMSE=0.9431 RMSE=0.9414 RMSE=0.9410 RMSE=0.9387 Epoch: 50/150RMSE=0.9384 RMSE=1.0183 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 1.021 +/- 0.067\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.5126 RMSE=1.1886 RMSE=1.0617 RMSE=1.0147 RMSE=1.0032 RMSE=0.9915 RMSE=0.9481 RMSE=0.9719 RMSE=0.9083 Epoch: 10/150RMSE=0.9736 RMSE=0.9110 RMSE=0.9538 RMSE=0.8810 RMSE=0.9406 RMSE=0.8950 RMSE=0.9470 RMSE=0.9737 RMSE=0.8862 RMSE=0.8644 Epoch: 20/150RMSE=0.9049 RMSE=0.9570 RMSE=0.8853 RMSE=0.9212 RMSE=0.8724 RMSE=0.8851 RMSE=0.8788 RMSE=0.8861 RMSE=0.8687 RMSE=0.8855 Epoch: 30/150RMSE=0.8578 RMSE=0.8763 RMSE=0.8649 RMSE=0.8686 RMSE=0.8715 RMSE=0.8646 RMSE=0.8650 RMSE=0.8593 RMSE=0.8607 RMSE=0.8598 Epoch: 40/150RMSE=0.8619 RMSE=0.8555 RMSE=0.8599 RMSE=0.8671 RMSE=0.8630 RMSE=0.8582 RMSE=0.8601 RMSE=0.8595 RMSE=0.8617 RMSE=0.8583 Epoch: 50/150RMSE=0.8591 RMSE=0.8599 RMSE=0.8624 RMSE=0.8614 RMSE=0.8615 RMSE=0.8622 RMSE=0.8619 RMSE=0.8614 RMSE=0.8618 RMSE=0.8622 Epoch: 60/150RMSE=0.8624 RMSE=0.8616 RMSE=1.0019 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 1.018 +/- 0.061\n",
                        "\n"
                    ]
                }
            ],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python -W ignore /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.1 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python -W ignore /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.3 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python -W ignore /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.5 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python -W ignore /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.7 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python -W ignore /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.9 --pooling='HGPSL'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Freesolv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=5.2482 RMSE=4.5708 RMSE=3.2658 RMSE=2.2828 RMSE=2.1757 RMSE=2.1363 RMSE=2.0819 RMSE=2.1122 RMSE=2.2811 Epoch: 10/150RMSE=2.0639 RMSE=2.1916 RMSE=2.2379 RMSE=2.4131 RMSE=1.9504 RMSE=1.8215 RMSE=2.0446 RMSE=2.0446 RMSE=2.0372 RMSE=1.9190 Epoch: 20/150RMSE=1.9083 RMSE=1.8567 RMSE=1.9782 RMSE=1.9589 RMSE=2.0076 RMSE=1.9407 RMSE=1.9879 RMSE=1.9908 RMSE=1.9344 RMSE=1.8616 Epoch: 30/150RMSE=1.8475 RMSE=1.9225 RMSE=1.8375 RMSE=1.8373 RMSE=1.9416 RMSE=1.8420 RMSE=3.0029 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 3.003 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.3558 RMSE=4.7592 RMSE=3.6294 RMSE=2.6478 RMSE=2.3354 RMSE=2.1562 RMSE=2.2082 RMSE=2.0188 RMSE=1.9637 Epoch: 10/150RMSE=1.9438 RMSE=2.0948 RMSE=1.8186 RMSE=2.0114 RMSE=1.9046 RMSE=1.8911 RMSE=2.1982 RMSE=1.7543 RMSE=1.8716 RMSE=1.8794 Epoch: 20/150RMSE=1.8558 RMSE=1.8424 RMSE=1.8884 RMSE=1.9405 RMSE=1.8021 RMSE=1.9898 RMSE=1.8269 RMSE=1.7934 RMSE=1.8089 RMSE=1.8170 Epoch: 30/150RMSE=1.7991 RMSE=1.8751 RMSE=1.8951 RMSE=1.7934 RMSE=1.8595 RMSE=1.7690 RMSE=1.7754 RMSE=1.7846 RMSE=2.5646 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 2.784 +/- 0.219\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.2308 RMSE=4.5781 RMSE=3.4636 RMSE=2.3420 RMSE=2.1013 RMSE=2.0688 RMSE=1.9929 RMSE=2.0895 RMSE=2.1019 Epoch: 10/150RMSE=1.9922 RMSE=1.8452 RMSE=2.1595 RMSE=2.1991 RMSE=2.1022 RMSE=1.9390 RMSE=1.8818 RMSE=2.0159 RMSE=1.8619 RMSE=1.9015 Epoch: 20/150RMSE=1.8538 RMSE=1.8736 RMSE=1.9347 RMSE=1.8370 RMSE=1.9192 RMSE=1.8579 RMSE=1.9633 RMSE=1.8347 RMSE=1.8450 RMSE=1.9292 Epoch: 30/150RMSE=1.8244 RMSE=1.8506 RMSE=1.8833 RMSE=1.9728 RMSE=1.9068 RMSE=1.8509 RMSE=1.8583 RMSE=1.8806 RMSE=1.8487 RMSE=1.8402 Epoch: 40/150RMSE=1.8393 RMSE=1.8597 RMSE=1.8597 RMSE=1.8601 RMSE=1.8466 RMSE=1.8989 RMSE=1.9058 RMSE=1.8490 RMSE=1.9043 RMSE=1.9049 Epoch: 50/150RMSE=1.8547 RMSE=2.8703 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 2.813 +/- 0.184\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.2480 RMSE=4.5568 RMSE=3.1532 RMSE=2.3685 RMSE=2.1467 RMSE=2.2613 RMSE=2.0461 RMSE=2.1384 RMSE=2.3642 Epoch: 10/150RMSE=2.0443 RMSE=2.3309 RMSE=2.0478 RMSE=2.2916 RMSE=2.1496 RMSE=2.3095 RMSE=1.8938 RMSE=1.9809 RMSE=1.9266 RMSE=1.9206 Epoch: 20/150RMSE=2.0724 RMSE=2.0835 RMSE=1.9946 RMSE=1.9591 RMSE=1.8658 RMSE=1.9590 RMSE=1.9746 RMSE=1.9837 RMSE=1.9334 RMSE=2.0099 Epoch: 30/150RMSE=2.0079 RMSE=2.0222 RMSE=1.9436 RMSE=2.0359 RMSE=2.0229 RMSE=2.0005 RMSE=1.9971 RMSE=2.0023 RMSE=2.0092 RMSE=2.0069 Epoch: 40/150RMSE=1.9351 RMSE=1.9379 RMSE=1.9320 RMSE=2.0202 RMSE=1.9397 RMSE=2.3804 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 2.705 +/- 0.246\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.2930 RMSE=4.7824 RMSE=3.5072 RMSE=2.2980 RMSE=2.0119 RMSE=2.0238 RMSE=2.0406 RMSE=1.9455 RMSE=1.9111 Epoch: 10/150RMSE=2.0621 RMSE=1.8467 RMSE=1.9868 RMSE=1.9930 RMSE=1.8467 RMSE=2.0968 RMSE=1.8737 RMSE=2.1027 RMSE=2.0956 RMSE=2.1345 Epoch: 20/150RMSE=2.1503 RMSE=2.0712 RMSE=2.0373 RMSE=2.1745 RMSE=2.0898 RMSE=2.1103 RMSE=2.1138 RMSE=2.0251 RMSE=2.0281 RMSE=2.0631 Epoch: 30/150RMSE=2.0642 RMSE=2.0646 RMSE=2.7616 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 2.716 +/- 0.221\n",
                        "\n",
                        "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=5.1081 RMSE=4.4382 RMSE=3.1370 RMSE=2.1836 RMSE=1.7659 RMSE=1.8347 RMSE=1.8780 RMSE=1.7178 RMSE=2.3365 Epoch: 10/150RMSE=1.8120 RMSE=1.9967 RMSE=1.7532 RMSE=1.7464 RMSE=1.7496 RMSE=2.0230 RMSE=1.8330 RMSE=1.7909 RMSE=2.0363 RMSE=1.8000 Epoch: 20/150RMSE=1.9969 RMSE=2.0316 RMSE=2.0587 RMSE=2.0015 RMSE=2.0782 RMSE=2.0636 RMSE=2.0802 RMSE=2.0750 RMSE=2.0846 RMSE=2.2292 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 2.229 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.2199 RMSE=4.5341 RMSE=3.2796 RMSE=2.4952 RMSE=2.0693 RMSE=1.8900 RMSE=1.7002 RMSE=1.9310 RMSE=2.2371 Epoch: 10/150RMSE=1.8256 RMSE=1.8484 RMSE=1.6267 RMSE=1.6645 RMSE=1.7763 RMSE=1.8030 RMSE=1.6747 RMSE=1.6361 RMSE=1.6376 RMSE=1.6518 Epoch: 20/150RMSE=1.7331 RMSE=1.6823 RMSE=1.6830 RMSE=1.7023 RMSE=1.6997 RMSE=1.7066 RMSE=1.7233 RMSE=1.7338 RMSE=1.7318 RMSE=1.7369 Epoch: 30/150RMSE=1.7346 RMSE=1.7363 RMSE=1.7356 RMSE=2.6787 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 2.454 +/- 0.225\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.1534 RMSE=4.4261 RMSE=3.0609 RMSE=2.3190 RMSE=1.8182 RMSE=1.7432 RMSE=1.6379 RMSE=1.8096 RMSE=1.8462 Epoch: 10/150RMSE=1.6792 RMSE=1.7240 RMSE=1.7323 RMSE=1.7097 RMSE=1.7401 RMSE=1.7155 RMSE=1.7168 RMSE=1.6815 RMSE=1.7329 RMSE=1.6750 Epoch: 20/150RMSE=1.6639 RMSE=1.6656 RMSE=1.6621 RMSE=1.6628 RMSE=1.6636 RMSE=1.6454 RMSE=1.6193 RMSE=1.6290 RMSE=1.6186 RMSE=1.6642 Epoch: 30/150RMSE=1.6343 RMSE=1.6307 RMSE=1.6560 RMSE=1.6281 RMSE=1.6253 RMSE=1.6252 RMSE=1.6229 RMSE=1.6538 RMSE=1.6366 RMSE=1.6212 Epoch: 40/150RMSE=1.6221 RMSE=1.6397 RMSE=1.6575 RMSE=1.6578 RMSE=1.6572 RMSE=1.6546 RMSE=1.6535 RMSE=1.6175 RMSE=1.6538 RMSE=1.6173 Epoch: 50/150RMSE=1.6526 RMSE=1.6527 RMSE=1.6536 RMSE=1.6536 RMSE=1.6531 RMSE=1.6525 RMSE=1.6524 RMSE=1.6174 RMSE=1.6528 RMSE=1.6522 Epoch: 60/150RMSE=1.6178 RMSE=1.6185 RMSE=1.6532 RMSE=1.6549 RMSE=1.6553 RMSE=1.6183 RMSE=1.6540 RMSE=1.6531 RMSE=1.6557 RMSE=1.6172 Epoch: 70/150RMSE=1.6528 RMSE=1.6527 RMSE=1.6165 RMSE=1.6161 RMSE=1.6546 RMSE=1.6526 RMSE=1.6527 RMSE=1.6163 RMSE=1.6524 RMSE=1.6525 Epoch: 80/150RMSE=1.6159 RMSE=1.6526 RMSE=1.6555 RMSE=1.6162 RMSE=1.6168 RMSE=1.6166 RMSE=1.6532 RMSE=1.6558 RMSE=1.6172 RMSE=1.6532 Epoch: 90/150RMSE=1.6542 RMSE=1.6538 RMSE=1.6179 RMSE=1.6344 RMSE=1.6175 RMSE=1.6553 RMSE=1.6534 RMSE=1.6536 RMSE=1.6541 RMSE=1.6172 Epoch: 100/150RMSE=1.6182 RMSE=2.0997 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 2.336 +/- 0.248\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.2545 RMSE=4.6976 RMSE=3.2783 RMSE=2.2040 RMSE=1.8937 RMSE=1.7194 RMSE=1.8966 RMSE=1.9273 RMSE=1.7741 Epoch: 10/150RMSE=1.5388 RMSE=1.5549 RMSE=1.8184 RMSE=1.5976 RMSE=1.6487 RMSE=1.6579 RMSE=1.5018 RMSE=1.5746 RMSE=1.7364 RMSE=1.5483 Epoch: 20/150RMSE=1.6713 RMSE=1.5486 RMSE=1.5042 RMSE=1.6114 RMSE=1.4414 RMSE=1.5104 RMSE=1.6990 RMSE=1.5781 RMSE=1.5312 RMSE=1.6661 Epoch: 30/150RMSE=1.5458 RMSE=1.5363 RMSE=1.5595 RMSE=1.6762 RMSE=1.5538 RMSE=1.5466 RMSE=1.6066 RMSE=1.5708 RMSE=1.7094 RMSE=1.5722 Epoch: 40/150RMSE=1.5656 RMSE=1.5656 RMSE=1.7013 RMSE=1.5742 RMSE=1.6311 RMSE=1.9371 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 2.236 +/- 0.276\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.1119 RMSE=4.2569 RMSE=3.0924 RMSE=2.1859 RMSE=1.8484 RMSE=1.5730 RMSE=1.6707 RMSE=1.6435 RMSE=1.6267 Epoch: 10/150RMSE=1.6175 RMSE=1.6027 RMSE=1.5532 RMSE=1.4846 RMSE=1.5429 RMSE=1.5827 RMSE=1.5600 RMSE=1.6368 RMSE=1.6015 RMSE=1.5566 Epoch: 20/150RMSE=1.6252 RMSE=1.5879 RMSE=1.5830 RMSE=1.5743 RMSE=1.5787 RMSE=1.5769 RMSE=1.6161 RMSE=1.5622 RMSE=1.5949 RMSE=1.6214 Epoch: 30/150RMSE=1.6054 RMSE=1.6107 RMSE=1.6148 RMSE=1.6086 RMSE=2.0729 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 2.204 +/- 0.255\n",
                        "\n",
                        "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=5.1778 RMSE=4.6405 RMSE=3.8646 RMSE=2.7425 RMSE=2.0317 RMSE=1.7225 RMSE=1.7749 RMSE=1.8243 RMSE=1.5686 Epoch: 10/150RMSE=1.6808 RMSE=1.8211 RMSE=1.6364 RMSE=1.6193 RMSE=1.5476 RMSE=1.4771 RMSE=1.5544 RMSE=1.5156 RMSE=1.5547 RMSE=1.5471 Epoch: 20/150RMSE=1.5352 RMSE=1.4743 RMSE=1.4778 RMSE=1.5158 RMSE=1.4392 RMSE=1.4977 RMSE=1.4551 RMSE=1.5272 RMSE=1.4590 RMSE=1.4669 Epoch: 30/150RMSE=1.4399 RMSE=1.4582 RMSE=1.4214 RMSE=1.4039 RMSE=1.4627 RMSE=1.4374 RMSE=1.3516 RMSE=1.3921 RMSE=1.4239 RMSE=1.4242 Epoch: 40/150RMSE=1.4108 RMSE=1.3801 RMSE=1.3891 RMSE=1.4072 RMSE=1.4032 RMSE=1.3972 RMSE=1.4090 RMSE=1.3999 RMSE=1.4038 RMSE=1.4016 Epoch: 50/150RMSE=1.4131 RMSE=1.4005 RMSE=1.3869 RMSE=1.3861 RMSE=1.3907 RMSE=1.3885 RMSE=1.3918 RMSE=2.1839 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 2.184 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=4.9978 RMSE=4.2464 RMSE=3.1476 RMSE=2.2558 RMSE=1.9953 RMSE=1.8407 RMSE=1.8531 RMSE=1.7350 RMSE=1.6304 Epoch: 10/150RMSE=1.7122 RMSE=1.5446 RMSE=1.4623 RMSE=1.8711 RMSE=1.9828 RMSE=1.8330 RMSE=1.5671 RMSE=1.5337 RMSE=1.4726 RMSE=1.5628 Epoch: 20/150RMSE=1.5409 RMSE=1.4987 RMSE=1.4587 RMSE=1.4868 RMSE=1.4852 RMSE=1.4819 RMSE=1.4566 RMSE=1.4722 RMSE=1.4747 RMSE=1.4491 Epoch: 30/150RMSE=1.4360 RMSE=1.4471 RMSE=1.4784 RMSE=1.4434 RMSE=1.4060 RMSE=1.4022 RMSE=1.4291 RMSE=1.4529 RMSE=1.4243 RMSE=1.3924 Epoch: 40/150RMSE=1.3856 RMSE=1.4160 RMSE=1.4364 RMSE=1.4247 RMSE=1.4285 RMSE=1.4345 RMSE=1.4474 RMSE=1.4311 RMSE=1.4261 RMSE=1.3894 Epoch: 50/150RMSE=1.3902 RMSE=1.4108 RMSE=1.4103 RMSE=1.4043 RMSE=1.4063 RMSE=1.4128 RMSE=1.4079 RMSE=1.4015 RMSE=1.4026 RMSE=1.4081 Epoch: 60/150RMSE=1.4041 RMSE=1.9062 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 2.045 +/- 0.139\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.1184 RMSE=4.3645 RMSE=3.4308 RMSE=2.8185 RMSE=2.1476 RMSE=2.0350 RMSE=1.9055 RMSE=1.8356 RMSE=1.8980 Epoch: 10/150RMSE=1.7139 RMSE=1.7687 RMSE=1.6293 RMSE=1.6368 RMSE=1.4513 RMSE=1.5221 RMSE=1.5231 RMSE=1.5404 RMSE=1.6426 RMSE=1.5519 Epoch: 20/150RMSE=1.5326 RMSE=1.5539 RMSE=1.5824 RMSE=1.4639 RMSE=1.5059 RMSE=1.5098 RMSE=1.4819 RMSE=1.4768 RMSE=1.4870 RMSE=1.5008 Epoch: 30/150RMSE=1.5272 RMSE=1.5062 RMSE=1.5040 RMSE=1.5237 RMSE=1.5013 RMSE=2.3570 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 2.149 +/- 0.186\n",
                        "\n",
                        "Epoch: 1/150RMSE=4.9911 RMSE=4.2113 RMSE=3.1687 RMSE=2.5036 RMSE=2.0169 RMSE=1.9201 RMSE=2.0079 RMSE=2.0230 RMSE=1.7156 Epoch: 10/150RMSE=1.9040 RMSE=1.8618 RMSE=1.5573 RMSE=1.9621 RMSE=2.3452 RMSE=1.8064 RMSE=1.5963 RMSE=1.5992 RMSE=1.5369 RMSE=1.5762 Epoch: 20/150RMSE=1.5653 RMSE=1.4624 RMSE=1.4873 RMSE=1.4154 RMSE=1.3930 RMSE=1.4425 RMSE=1.4926 RMSE=1.4968 RMSE=1.5053 RMSE=1.5290 Epoch: 30/150RMSE=1.5256 RMSE=1.5374 RMSE=1.4932 RMSE=1.4983 RMSE=1.5091 RMSE=1.5054 RMSE=1.5820 RMSE=1.5128 RMSE=1.5354 RMSE=1.5268 Epoch: 40/150RMSE=1.5132 RMSE=1.5222 RMSE=1.5043 RMSE=1.5055 RMSE=1.4959 RMSE=2.0088 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 2.114 +/- 0.172\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.0569 RMSE=4.2004 RMSE=3.1728 RMSE=2.6400 RMSE=2.1946 RMSE=1.9688 RMSE=1.9606 RMSE=1.9555 RMSE=1.8082 Epoch: 10/150RMSE=1.8237 RMSE=2.4856 RMSE=1.8021 RMSE=1.7768 RMSE=1.9250 RMSE=1.7794 RMSE=1.8829 RMSE=1.9615 RMSE=1.8960 RMSE=1.8700 Epoch: 20/150RMSE=1.7702 RMSE=1.8231 RMSE=1.8616 RMSE=1.9037 RMSE=1.8523 RMSE=1.8723 RMSE=1.8052 RMSE=1.8094 RMSE=1.8801 RMSE=1.8976 Epoch: 30/150RMSE=1.8987 RMSE=1.8462 RMSE=1.8912 RMSE=1.9127 RMSE=1.8908 RMSE=1.8899 RMSE=1.8860 RMSE=1.8868 RMSE=1.8817 RMSE=1.9084 Epoch: 40/150RMSE=1.8762 RMSE=2.2098 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 2.133 +/- 0.158\n",
                        "\n",
                        "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=4.9671 RMSE=4.2981 RMSE=3.3665 RMSE=2.3951 RMSE=2.0033 RMSE=1.6885 RMSE=1.5467 RMSE=2.0057 RMSE=1.8728 Epoch: 10/150RMSE=1.5846 RMSE=1.6104 RMSE=1.4857 RMSE=1.5426 RMSE=1.5289 RMSE=1.5413 RMSE=1.4932 RMSE=1.5465 RMSE=1.5042 RMSE=1.5048 Epoch: 20/150RMSE=1.5053 RMSE=1.5023 RMSE=1.5038 RMSE=1.5119 RMSE=1.5129 RMSE=1.5027 RMSE=1.5162 RMSE=1.5120 RMSE=1.5124 RMSE=1.5125 Epoch: 30/150RMSE=1.5099 RMSE=1.5093 RMSE=1.5136 RMSE=1.6603 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 1.660 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.0453 RMSE=4.1812 RMSE=3.0328 RMSE=2.5274 RMSE=2.2330 RMSE=2.0958 RMSE=2.2127 RMSE=1.8290 RMSE=2.2384 Epoch: 10/150RMSE=2.0133 RMSE=2.1726 RMSE=2.1980 RMSE=1.9426 RMSE=1.8570 RMSE=1.7756 RMSE=1.8644 RMSE=1.7929 RMSE=1.7876 RMSE=1.8487 Epoch: 20/150RMSE=1.7875 RMSE=1.7358 RMSE=1.7405 RMSE=1.7713 RMSE=1.7359 RMSE=1.7289 RMSE=1.7633 RMSE=1.7084 RMSE=1.7603 RMSE=1.8175 Epoch: 30/150RMSE=1.7217 RMSE=1.7279 RMSE=1.7330 RMSE=1.8362 RMSE=1.8530 RMSE=1.7533 RMSE=1.7510 RMSE=1.7234 RMSE=1.7275 RMSE=1.7465 Epoch: 40/150RMSE=1.7487 RMSE=1.7451 RMSE=1.7499 RMSE=1.7529 RMSE=1.7447 RMSE=1.7436 RMSE=1.7405 RMSE=1.7555 RMSE=1.6224 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 1.641 +/- 0.019\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.1383 RMSE=4.4788 RMSE=3.1766 RMSE=2.3652 RMSE=2.0136 RMSE=1.8328 RMSE=1.8535 RMSE=1.7275 RMSE=1.6990 Epoch: 10/150RMSE=1.7639 RMSE=1.5645 RMSE=1.7499 RMSE=1.7726 RMSE=1.5984 RMSE=1.5444 RMSE=1.5658 RMSE=1.7261 RMSE=1.5393 RMSE=1.4781 Epoch: 20/150RMSE=1.5605 RMSE=1.6920 RMSE=1.7129 RMSE=1.5900 RMSE=1.5183 RMSE=1.4897 RMSE=1.5334 RMSE=1.4175 RMSE=1.4701 RMSE=1.5100 Epoch: 30/150RMSE=1.4305 RMSE=1.4604 RMSE=1.3961 RMSE=1.4251 RMSE=1.4076 RMSE=1.4371 RMSE=1.4435 RMSE=1.4322 RMSE=1.4441 RMSE=1.4507 Epoch: 40/150RMSE=1.4686 RMSE=1.4491 RMSE=1.4380 RMSE=1.4530 RMSE=1.4469 RMSE=1.4548 RMSE=1.4373 RMSE=1.4353 RMSE=1.4494 RMSE=1.4472 Epoch: 50/150RMSE=1.4433 RMSE=1.4454 RMSE=1.4492 RMSE=1.6168 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 1.633 +/- 0.019\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.0400 RMSE=4.3072 RMSE=3.4063 RMSE=2.3987 RMSE=1.9612 RMSE=1.8857 RMSE=1.8639 RMSE=1.8976 RMSE=1.8109 Epoch: 10/150RMSE=1.6990 RMSE=1.6833 RMSE=1.8806 RMSE=1.5293 RMSE=1.5511 RMSE=1.5553 RMSE=1.5581 RMSE=1.4633 RMSE=1.4428 RMSE=1.5075 Epoch: 20/150RMSE=1.5034 RMSE=1.6104 RMSE=1.4266 RMSE=1.4925 RMSE=1.4776 RMSE=1.4540 RMSE=1.7615 RMSE=1.3962 RMSE=1.4135 RMSE=1.4269 Epoch: 30/150RMSE=1.4101 RMSE=1.4083 RMSE=1.3747 RMSE=1.3618 RMSE=1.3897 RMSE=1.3792 RMSE=1.3534 RMSE=1.3500 RMSE=1.3465 RMSE=1.3542 Epoch: 40/150RMSE=1.3636 RMSE=1.3723 RMSE=1.3818 RMSE=1.3779 RMSE=1.3613 RMSE=1.3668 RMSE=1.3742 RMSE=1.3887 RMSE=1.3877 RMSE=1.3783 Epoch: 50/150RMSE=1.3753 RMSE=1.3714 RMSE=1.3569 RMSE=1.3637 RMSE=1.3742 RMSE=1.3737 RMSE=1.3622 RMSE=1.3719 RMSE=1.3596 RMSE=1.4656 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 1.591 +/- 0.074\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.0597 RMSE=4.2617 RMSE=3.5078 RMSE=2.2953 RMSE=1.9707 RMSE=1.8426 RMSE=1.8116 RMSE=1.8085 RMSE=1.7667 Epoch: 10/150RMSE=1.8175 RMSE=1.7367 RMSE=1.5116 RMSE=1.6962 RMSE=1.4624 RMSE=1.5946 RMSE=1.4462 RMSE=1.4888 RMSE=1.3312 RMSE=1.6468 Epoch: 20/150RMSE=1.3508 RMSE=1.5598 RMSE=1.3008 RMSE=1.5154 RMSE=1.2609 RMSE=1.3226 RMSE=1.4508 RMSE=1.3227 RMSE=1.5331 RMSE=1.3306 Epoch: 30/150RMSE=1.2821 RMSE=1.3305 RMSE=1.3527 RMSE=1.3122 RMSE=1.2819 RMSE=1.3061 RMSE=1.2778 RMSE=1.2706 RMSE=1.2771 RMSE=1.2887 Epoch: 40/150RMSE=1.2987 RMSE=1.2822 RMSE=1.2885 RMSE=1.3049 RMSE=1.3012 RMSE=1.7374 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 1.621 +/- 0.089\n",
                        "\n",
                        "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=4.7947 RMSE=3.6369 RMSE=2.9021 RMSE=2.2793 RMSE=2.1538 RMSE=1.9721 RMSE=1.8187 RMSE=2.1532 RMSE=2.0115 Epoch: 10/150RMSE=1.8820 RMSE=1.6543 RMSE=1.7325 RMSE=1.7355 RMSE=1.6547 RMSE=1.5221 RMSE=1.4392 RMSE=1.6826 RMSE=1.3814 RMSE=1.6512 Epoch: 20/150RMSE=1.3532 RMSE=1.4774 RMSE=1.4422 RMSE=2.0945 RMSE=1.4444 RMSE=1.3346 RMSE=1.3309 RMSE=1.2694 RMSE=1.2769 RMSE=1.3010 Epoch: 30/150RMSE=1.2273 RMSE=1.3161 RMSE=1.1519 RMSE=1.1825 RMSE=1.2160 RMSE=1.2192 RMSE=1.2718 RMSE=1.2040 RMSE=1.2186 RMSE=1.2153 Epoch: 40/150RMSE=1.1910 RMSE=1.1950 RMSE=1.1889 RMSE=1.2131 RMSE=1.2032 RMSE=1.1894 RMSE=1.1804 RMSE=1.1749 RMSE=1.1638 RMSE=1.1692 Epoch: 50/150RMSE=1.1747 RMSE=1.1782 RMSE=1.1747 RMSE=1.3138 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 1.314 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.0148 RMSE=4.3667 RMSE=3.2850 RMSE=2.7047 RMSE=2.3366 RMSE=2.0281 RMSE=1.9008 RMSE=1.7217 RMSE=1.7298 Epoch: 10/150RMSE=1.6919 RMSE=1.4801 RMSE=2.1144 RMSE=1.5212 RMSE=1.6096 RMSE=1.3377 RMSE=1.2497 RMSE=1.2714 RMSE=1.4135 RMSE=1.1887 Epoch: 20/150RMSE=1.3883 RMSE=1.2412 RMSE=1.1306 RMSE=1.4026 RMSE=1.9238 RMSE=1.4273 RMSE=1.2585 RMSE=1.2472 RMSE=1.1866 RMSE=1.2157 Epoch: 30/150RMSE=1.1220 RMSE=1.1694 RMSE=1.1661 RMSE=1.2016 RMSE=1.1678 RMSE=1.1492 RMSE=1.1704 RMSE=1.1258 RMSE=1.1697 RMSE=1.1627 Epoch: 40/150RMSE=1.1658 RMSE=1.1529 RMSE=1.1528 RMSE=1.1492 RMSE=1.1509 RMSE=1.1687 RMSE=1.1629 RMSE=1.1541 RMSE=1.1493 RMSE=1.1478 Epoch: 50/150RMSE=1.1533 RMSE=1.9453 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 1.630 +/- 0.316\n",
                        "\n",
                        "Epoch: 1/150RMSE=4.9217 RMSE=4.2353 RMSE=3.4487 RMSE=2.7137 RMSE=2.3205 RMSE=2.0696 RMSE=1.7604 RMSE=1.8509 RMSE=1.8351 Epoch: 10/150RMSE=1.7475 RMSE=1.7454 RMSE=1.6315 RMSE=1.9141 RMSE=1.3484 RMSE=2.0289 RMSE=2.1743 RMSE=1.9982 RMSE=1.4962 RMSE=1.3156 Epoch: 20/150RMSE=1.3348 RMSE=1.3023 RMSE=1.2886 RMSE=1.2662 RMSE=1.4073 RMSE=1.3886 RMSE=1.2328 RMSE=1.4090 RMSE=1.3699 RMSE=1.4084 Epoch: 30/150RMSE=1.3718 RMSE=1.3315 RMSE=1.2530 RMSE=1.2975 RMSE=1.3034 RMSE=1.3253 RMSE=1.2989 RMSE=1.2765 RMSE=1.3001 RMSE=1.2916 Epoch: 40/150RMSE=1.2799 RMSE=1.2797 RMSE=1.3030 RMSE=1.2985 RMSE=1.3038 RMSE=1.3040 RMSE=1.3065 RMSE=1.5742 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 1.611 +/- 0.259\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.0165 RMSE=4.0074 RMSE=2.9920 RMSE=2.5195 RMSE=1.8899 RMSE=1.7283 RMSE=1.7261 RMSE=1.8005 RMSE=1.6412 Epoch: 10/150RMSE=1.4589 RMSE=1.4234 RMSE=1.4848 RMSE=1.6139 RMSE=1.3777 RMSE=1.4527 RMSE=1.3185 RMSE=1.3981 RMSE=1.2554 RMSE=1.3338 Epoch: 20/150RMSE=1.1906 RMSE=1.2718 RMSE=1.1099 RMSE=1.3638 RMSE=1.1721 RMSE=1.0584 RMSE=1.1649 RMSE=1.2197 RMSE=1.3773 RMSE=1.3216 Epoch: 30/150RMSE=1.2136 RMSE=1.2447 RMSE=1.1534 RMSE=1.0995 RMSE=1.1265 RMSE=1.1410 RMSE=1.0915 RMSE=1.0768 RMSE=1.0793 RMSE=1.1095 Epoch: 40/150RMSE=1.1188 RMSE=1.0914 RMSE=1.0926 RMSE=1.0986 RMSE=1.0974 RMSE=1.0967 RMSE=1.9503 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 1.696 +/- 0.268\n",
                        "\n",
                        "Epoch: 1/150RMSE=5.1187 RMSE=4.7263 RMSE=3.8392 RMSE=2.9978 RMSE=2.3391 RMSE=1.9533 RMSE=1.9265 RMSE=1.7749 RMSE=1.8984 Epoch: 10/150RMSE=1.5284 RMSE=1.5183 RMSE=1.6779 RMSE=1.5733 RMSE=1.7105 RMSE=1.4003 RMSE=1.5926 RMSE=1.4993 RMSE=1.4864 RMSE=1.3724 Epoch: 20/150RMSE=1.6143 RMSE=1.4112 RMSE=1.5334 RMSE=1.2975 RMSE=1.5857 RMSE=1.3731 RMSE=1.4122 RMSE=1.5351 RMSE=1.3918 RMSE=1.2380 Epoch: 30/150RMSE=1.3591 RMSE=1.3481 RMSE=1.3112 RMSE=1.3142 RMSE=1.3084 RMSE=1.2804 RMSE=1.2592 RMSE=1.3184 RMSE=1.3463 RMSE=1.3302 Epoch: 40/150RMSE=1.3106 RMSE=1.3201 RMSE=1.3003 RMSE=1.2995 RMSE=1.3061 RMSE=1.3193 RMSE=1.3163 RMSE=1.3093 RMSE=1.3064 RMSE=1.2975 RMSE=2.0939 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 1.775 +/- 0.288\n",
                        "\n"
                    ]
                }
            ],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python -W ignore  /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.1 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python -W ignore  /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.3 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python -W ignore  /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.5 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python -W ignore  /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.7 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python -W ignore  /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.9 --pooling='HGPSL'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Lipophilicity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=1.2876 RMSE=1.1484 RMSE=1.1440 RMSE=1.1290 RMSE=1.1224 RMSE=1.1190 RMSE=1.1058 RMSE=1.1097 RMSE=1.1513 Epoch: 10/150RMSE=1.1016 RMSE=1.0863 RMSE=1.1012 RMSE=1.0855 RMSE=1.0780 RMSE=1.0973 RMSE=1.0584 RMSE=1.0896 RMSE=1.0731 RMSE=1.0758 Epoch: 20/150RMSE=1.0860 RMSE=1.0534 RMSE=1.0609 RMSE=1.0833 RMSE=1.0769 RMSE=1.0836 RMSE=1.0602 RMSE=1.0572 RMSE=1.0550 RMSE=1.0669 Epoch: 30/150RMSE=1.0507 RMSE=1.0484 RMSE=1.0496 RMSE=1.0571 RMSE=1.0650 RMSE=1.0663 RMSE=1.0630 RMSE=1.0674 RMSE=1.0646 RMSE=1.0753 Epoch: 40/150RMSE=1.0727 RMSE=1.0721 RMSE=1.0792 RMSE=1.0712 RMSE=1.0725 RMSE=1.0706 RMSE=1.0607 RMSE=1.0692 RMSE=1.0638 RMSE=1.0654 Epoch: 50/150RMSE=1.0717 RMSE=1.0683 RMSE=0.9785 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 0.979 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.2799 RMSE=1.1682 RMSE=1.1349 RMSE=1.0932 RMSE=1.1255 RMSE=1.1181 RMSE=1.0823 RMSE=1.0951 RMSE=1.0968 Epoch: 10/150RMSE=1.1107 RMSE=1.0862 RMSE=1.0648 RMSE=1.0564 RMSE=1.0731 RMSE=1.0839 RMSE=1.0935 RMSE=1.0712 RMSE=1.0683 RMSE=1.0700 Epoch: 20/150RMSE=1.0748 RMSE=1.0621 RMSE=1.0594 RMSE=1.0642 RMSE=1.0674 RMSE=1.0707 RMSE=1.0659 RMSE=1.0651 RMSE=1.0710 RMSE=1.0692 Epoch: 30/150RMSE=1.0699 RMSE=1.0683 RMSE=1.0683 RMSE=1.0712 RMSE=0.9609 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 0.970 +/- 0.009\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.2513 RMSE=1.1553 RMSE=1.1410 RMSE=1.1262 RMSE=1.1395 RMSE=1.1300 RMSE=1.1253 RMSE=1.1245 RMSE=1.1092 Epoch: 10/150RMSE=1.0967 RMSE=1.0616 RMSE=1.0918 RMSE=1.1202 RMSE=1.0932 RMSE=1.1009 RMSE=1.0728 RMSE=1.0677 RMSE=1.0706 RMSE=1.0760 Epoch: 20/150RMSE=1.0814 RMSE=1.0815 RMSE=1.0826 RMSE=1.0769 RMSE=1.0854 RMSE=1.0811 RMSE=1.0862 RMSE=1.0837 RMSE=1.0850 RMSE=1.0863 Epoch: 30/150RMSE=1.0859 RMSE=1.0883 RMSE=0.9938 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 0.978 +/- 0.013\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.2696 RMSE=1.1930 RMSE=1.1412 RMSE=1.1466 RMSE=1.1015 RMSE=1.1204 RMSE=1.1058 RMSE=1.1232 RMSE=1.1223 Epoch: 10/150RMSE=1.0949 RMSE=1.0858 RMSE=1.1207 RMSE=1.1097 RMSE=1.1204 RMSE=1.1025 RMSE=1.0677 RMSE=1.0761 RMSE=1.0863 RMSE=1.0897 Epoch: 20/150RMSE=1.0888 RMSE=1.0786 RMSE=1.0926 RMSE=1.0796 RMSE=1.0830 RMSE=1.0951 RMSE=1.0944 RMSE=1.0927 RMSE=1.0943 RMSE=1.0947 Epoch: 30/150RMSE=1.0967 RMSE=1.0996 RMSE=1.0955 RMSE=1.1002 RMSE=1.0962 RMSE=1.0933 RMSE=1.0975 RMSE=1.0001 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 0.983 +/- 0.015\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.2646 RMSE=1.1273 RMSE=1.0965 RMSE=1.0903 RMSE=1.0978 RMSE=1.0810 RMSE=1.0886 RMSE=1.1017 RMSE=1.1211 Epoch: 10/150RMSE=1.0854 RMSE=1.0649 RMSE=1.0483 RMSE=1.0761 RMSE=1.0797 RMSE=1.0885 RMSE=1.0730 RMSE=1.0422 RMSE=1.0358 RMSE=1.0504 Epoch: 20/150RMSE=1.0348 RMSE=1.0459 RMSE=1.0350 RMSE=1.0429 RMSE=1.0381 RMSE=1.0198 RMSE=1.0235 RMSE=1.0406 RMSE=1.0334 RMSE=1.0346 Epoch: 30/150RMSE=1.0380 RMSE=1.0391 RMSE=1.0381 RMSE=1.0414 RMSE=1.0367 RMSE=1.0426 RMSE=1.0386 RMSE=1.0381 RMSE=1.0348 RMSE=1.0409 Epoch: 40/150RMSE=1.0455 RMSE=1.0430 RMSE=1.0416 RMSE=1.0407 RMSE=1.0448 RMSE=1.0461 RMSE=0.9797 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 0.983 +/- 0.014\n",
                        "\n",
                        "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=1.2351 RMSE=1.1218 RMSE=1.1300 RMSE=1.0569 RMSE=1.0282 RMSE=1.0171 RMSE=1.0117 RMSE=1.0246 RMSE=1.0856 Epoch: 10/150RMSE=1.0025 RMSE=1.0078 RMSE=1.0773 RMSE=1.0167 RMSE=0.9952 RMSE=1.1524 RMSE=1.0105 RMSE=0.9989 RMSE=0.9855 RMSE=1.0012 Epoch: 20/150RMSE=1.0563 RMSE=0.9872 RMSE=1.0256 RMSE=0.9639 RMSE=0.9867 RMSE=0.9853 RMSE=0.9783 RMSE=0.9770 RMSE=0.9843 RMSE=0.9721 Epoch: 30/150RMSE=0.9672 RMSE=0.9586 RMSE=0.9684 RMSE=0.9851 RMSE=0.9822 RMSE=0.9895 RMSE=0.9815 RMSE=0.9843 RMSE=0.9706 RMSE=0.9719 Epoch: 40/150RMSE=0.9599 RMSE=0.9669 RMSE=0.9722 RMSE=0.9730 RMSE=0.9727 RMSE=0.9739 RMSE=0.9714 RMSE=0.9724 RMSE=0.9726 RMSE=0.9747 Epoch: 50/150RMSE=0.9786 RMSE=0.9772 RMSE=0.9476 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 0.948 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.2394 RMSE=1.1245 RMSE=1.1287 RMSE=1.0832 RMSE=1.0441 RMSE=1.0609 RMSE=1.0817 RMSE=0.9935 RMSE=1.1515 Epoch: 10/150RMSE=0.9872 RMSE=0.9883 RMSE=1.0212 RMSE=0.9720 RMSE=0.9577 RMSE=1.0045 RMSE=0.9779 RMSE=0.9883 RMSE=1.0329 RMSE=0.9570 Epoch: 20/150RMSE=0.9846 RMSE=0.9632 RMSE=0.9619 RMSE=0.9551 RMSE=0.9860 RMSE=1.0174 RMSE=0.9685 RMSE=0.9719 RMSE=0.9719 RMSE=0.9642 Epoch: 30/150RMSE=0.9624 RMSE=0.9504 RMSE=0.9566 RMSE=0.9574 RMSE=0.9744 RMSE=0.9798 RMSE=0.9695 RMSE=0.9716 RMSE=0.9684 RMSE=0.9648 Epoch: 40/150RMSE=0.9583 RMSE=0.9565 RMSE=0.9565 RMSE=0.9589 RMSE=0.9553 RMSE=0.9590 RMSE=0.9524 RMSE=0.9573 RMSE=0.9532 RMSE=0.9558 Epoch: 50/150RMSE=0.9603 RMSE=0.9604 RMSE=0.9071 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 0.927 +/- 0.020\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.2623 RMSE=1.1455 RMSE=1.0835 RMSE=1.0485 RMSE=1.0556 RMSE=1.0533 RMSE=1.0623 RMSE=1.0993 RMSE=0.9563 Epoch: 10/150RMSE=0.9514 RMSE=0.9905 RMSE=0.9828 RMSE=0.9740 RMSE=0.9854 RMSE=0.9586 RMSE=0.9765 RMSE=0.9741 RMSE=0.9689 RMSE=0.9590 Epoch: 20/150RMSE=0.9782 RMSE=0.9844 RMSE=0.9840 RMSE=0.9682 RMSE=0.9602 RMSE=0.9653 RMSE=0.9646 RMSE=0.9691 RMSE=0.9647 RMSE=0.9664 Epoch: 30/150RMSE=0.9658 RMSE=0.9503 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 0.935 +/- 0.020\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.2078 RMSE=1.1179 RMSE=1.1219 RMSE=1.0535 RMSE=1.0557 RMSE=1.2894 RMSE=1.0842 RMSE=1.1183 RMSE=1.0216 Epoch: 10/150RMSE=0.9706 RMSE=0.9991 RMSE=1.0514 RMSE=1.0024 RMSE=1.0395 RMSE=1.0118 RMSE=1.0022 RMSE=0.9786 RMSE=0.9967 RMSE=0.9943 Epoch: 20/150RMSE=1.0008 RMSE=1.0048 RMSE=1.0133 RMSE=1.0174 RMSE=0.9968 RMSE=0.9939 RMSE=0.9926 RMSE=0.9888 RMSE=0.9917 RMSE=0.9877 Epoch: 30/150RMSE=0.9941 RMSE=0.9183 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 0.931 +/- 0.019\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.2462 RMSE=1.1461 RMSE=1.0901 RMSE=1.0740 RMSE=1.0739 RMSE=1.0403 RMSE=0.9927 RMSE=1.0166 RMSE=1.0127 Epoch: 10/150RMSE=1.0108 RMSE=1.0400 RMSE=0.9627 RMSE=1.0053 RMSE=0.9751 RMSE=0.9918 RMSE=0.9950 RMSE=0.9809 RMSE=0.9788 RMSE=0.9588 Epoch: 20/150RMSE=0.9711 RMSE=0.9657 RMSE=0.9674 RMSE=0.9807 RMSE=0.9921 RMSE=0.9737 RMSE=0.9834 RMSE=0.9687 RMSE=0.9869 RMSE=0.9949 Epoch: 30/150RMSE=0.9853 RMSE=0.9852 RMSE=0.9868 RMSE=0.9806 RMSE=0.9881 RMSE=0.9907 RMSE=0.9923 RMSE=0.9877 RMSE=0.9904 RMSE=0.9905 RMSE=0.9255 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 0.930 +/- 0.017\n",
                        "\n",
                        "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=1.2130 RMSE=1.1334 RMSE=1.1386 RMSE=1.0316 RMSE=0.9975 RMSE=1.0227 RMSE=0.9492 RMSE=1.0553 RMSE=0.9993 Epoch: 10/150RMSE=0.9921 RMSE=0.9842 RMSE=0.8974 RMSE=0.9293 RMSE=0.9298 RMSE=0.8966 RMSE=0.9289 RMSE=0.8999 RMSE=0.9425 RMSE=0.9169 Epoch: 20/150RMSE=0.9159 RMSE=0.9004 RMSE=0.9142 RMSE=0.9107 RMSE=0.9090 RMSE=0.9009 RMSE=0.9052 RMSE=0.9151 RMSE=0.8956 RMSE=0.8971 Epoch: 30/150RMSE=0.8956 RMSE=0.8873 RMSE=0.9024 RMSE=0.9042 RMSE=0.8977 RMSE=0.8973 RMSE=0.8950 RMSE=0.8958 RMSE=0.8997 RMSE=0.8994 Epoch: 40/150RMSE=0.9037 RMSE=0.9044 RMSE=0.9011 RMSE=0.9018 RMSE=0.9052 RMSE=0.9003 RMSE=0.9037 RMSE=0.9041 RMSE=0.9034 RMSE=0.9038 Epoch: 50/150RMSE=0.9039 RMSE=0.9056 RMSE=0.8293 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 0.829 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.1441 RMSE=1.0665 RMSE=1.0280 RMSE=1.0276 RMSE=1.0285 RMSE=0.9897 RMSE=0.9763 RMSE=1.0702 RMSE=1.0128 Epoch: 10/150RMSE=0.9985 RMSE=1.0019 RMSE=0.9308 RMSE=0.9430 RMSE=0.9544 RMSE=0.9631 RMSE=0.9451 RMSE=0.9659 RMSE=0.9839 RMSE=0.9684 Epoch: 20/150RMSE=0.9601 RMSE=0.9771 RMSE=0.9752 RMSE=0.9706 RMSE=0.9677 RMSE=0.9666 RMSE=0.9694 RMSE=0.9729 RMSE=0.9786 RMSE=0.9786 Epoch: 30/150RMSE=0.9744 RMSE=0.9797 RMSE=0.9770 RMSE=0.8543 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 0.842 +/- 0.012\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.1924 RMSE=1.0397 RMSE=1.0208 RMSE=1.0090 RMSE=0.9868 RMSE=0.9842 RMSE=1.0008 RMSE=0.9905 RMSE=1.0209 Epoch: 10/150RMSE=1.0198 RMSE=0.9818 RMSE=0.9775 RMSE=0.9735 RMSE=0.9923 RMSE=0.9764 RMSE=0.9651 RMSE=0.9702 RMSE=0.9597 RMSE=0.9564 Epoch: 20/150RMSE=0.9731 RMSE=0.9825 RMSE=0.9566 RMSE=0.9806 RMSE=0.9550 RMSE=0.9558 RMSE=0.9315 RMSE=0.9208 RMSE=0.9500 RMSE=0.9443 Epoch: 30/150RMSE=0.9530 RMSE=0.9621 RMSE=0.9517 RMSE=0.9478 RMSE=0.9336 RMSE=0.9466 RMSE=0.9339 RMSE=0.9352 RMSE=0.9364 RMSE=0.9394 Epoch: 40/150RMSE=0.9392 RMSE=0.9391 RMSE=0.9389 RMSE=0.9402 RMSE=0.9379 RMSE=0.9361 RMSE=0.9376 RMSE=0.9355 RMSE=0.8846 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 0.856 +/- 0.023\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.1367 RMSE=1.0907 RMSE=1.0431 RMSE=0.9762 RMSE=0.9736 RMSE=0.9882 RMSE=0.9982 RMSE=0.9667 RMSE=0.9964 Epoch: 10/150RMSE=1.0452 RMSE=1.0308 RMSE=0.9726 RMSE=0.9173 RMSE=0.9180 RMSE=0.9305 RMSE=0.9507 RMSE=0.9326 RMSE=0.9149 RMSE=0.9154 Epoch: 20/150RMSE=0.9260 RMSE=0.9207 RMSE=0.9270 RMSE=0.9415 RMSE=0.9370 RMSE=0.9270 RMSE=0.9251 RMSE=0.9255 RMSE=0.9211 RMSE=0.9235 Epoch: 30/150RMSE=0.9246 RMSE=0.9242 RMSE=0.9164 RMSE=0.9203 RMSE=0.9233 RMSE=0.9255 RMSE=0.9258 RMSE=0.9245 RMSE=0.9254 RMSE=0.8483 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 0.854 +/- 0.020\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.1599 RMSE=1.0690 RMSE=1.0634 RMSE=1.1149 RMSE=1.1216 RMSE=1.0367 RMSE=1.0537 RMSE=1.0292 RMSE=1.0879 Epoch: 10/150RMSE=0.9942 RMSE=1.0469 RMSE=1.0779 RMSE=1.0134 RMSE=0.9707 RMSE=1.1324 RMSE=1.0331 RMSE=1.0657 RMSE=1.0601 RMSE=0.9807 Epoch: 20/150RMSE=0.9673 RMSE=0.9616 RMSE=0.9867 RMSE=0.9493 RMSE=0.9904 RMSE=0.9497 RMSE=0.9735 RMSE=1.0030 RMSE=0.9316 RMSE=0.9486 Epoch: 30/150RMSE=0.9344 RMSE=0.9405 RMSE=0.9147 RMSE=0.9457 RMSE=0.9387 RMSE=0.9300 RMSE=0.9209 RMSE=0.9143 RMSE=0.9225 RMSE=0.9183 Epoch: 40/150RMSE=0.9353 RMSE=0.9351 RMSE=0.9457 RMSE=0.9426 RMSE=0.9375 RMSE=0.9394 RMSE=0.9376 RMSE=0.9296 RMSE=0.9397 RMSE=0.9383 Epoch: 50/150RMSE=0.9395 RMSE=0.9370 RMSE=0.9373 RMSE=0.9379 RMSE=0.9373 RMSE=0.9376 RMSE=0.9381 RMSE=0.9385 RMSE=0.8935 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 0.862 +/- 0.024\n",
                        "\n",
                        "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=1.2325 RMSE=1.0731 RMSE=1.0294 RMSE=0.9913 RMSE=1.0163 RMSE=0.9795 RMSE=0.9582 RMSE=0.9627 RMSE=0.9424 Epoch: 10/150RMSE=0.9319 RMSE=0.9591 RMSE=1.0060 RMSE=0.9403 RMSE=1.0631 RMSE=0.9002 RMSE=0.9091 RMSE=0.9043 RMSE=0.9182 RMSE=0.9023 Epoch: 20/150RMSE=0.9246 RMSE=0.8877 RMSE=0.9125 RMSE=0.9044 RMSE=0.9168 RMSE=0.8984 RMSE=0.9023 RMSE=0.8921 RMSE=0.8971 RMSE=0.9055 Epoch: 30/150RMSE=0.8903 RMSE=0.8919 RMSE=0.8906 RMSE=0.8882 RMSE=0.8895 RMSE=0.8888 RMSE=0.8886 RMSE=0.8915 RMSE=0.8923 RMSE=0.8922 Epoch: 40/150RMSE=0.8913 RMSE=0.8926 RMSE=0.8159 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 0.816 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.1962 RMSE=1.0930 RMSE=1.0987 RMSE=1.0938 RMSE=1.1526 RMSE=1.0177 RMSE=0.9938 RMSE=1.1782 RMSE=1.0007 Epoch: 10/150RMSE=1.0134 RMSE=0.9889 RMSE=1.1654 RMSE=1.2044 RMSE=1.0462 RMSE=1.0447 RMSE=0.9413 RMSE=0.9209 RMSE=0.9257 RMSE=0.9154 Epoch: 20/150RMSE=0.9395 RMSE=0.9092 RMSE=0.9473 RMSE=0.9601 RMSE=0.9558 RMSE=0.9098 RMSE=0.9174 RMSE=0.9165 RMSE=0.9173 RMSE=0.9305 Epoch: 30/150RMSE=0.9226 RMSE=0.9117 RMSE=0.9095 RMSE=0.9219 RMSE=0.9170 RMSE=0.9194 RMSE=0.9245 RMSE=0.9253 RMSE=0.9237 RMSE=0.9216 Epoch: 40/150RMSE=0.9217 RMSE=0.9246 RMSE=0.8334 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 0.825 +/- 0.009\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.1801 RMSE=1.0874 RMSE=1.0261 RMSE=1.0510 RMSE=0.9442 RMSE=1.0350 RMSE=0.9217 RMSE=0.9840 RMSE=0.9245 Epoch: 10/150RMSE=0.9557 RMSE=0.9471 RMSE=0.8807 RMSE=0.8975 RMSE=0.8937 RMSE=0.8825 RMSE=0.8596 RMSE=0.8546 RMSE=0.8821 RMSE=0.8791 Epoch: 20/150RMSE=0.8740 RMSE=0.8709 RMSE=0.8657 RMSE=0.8666 RMSE=0.8664 RMSE=0.8871 RMSE=0.8697 RMSE=0.8652 RMSE=0.8689 RMSE=0.8567 Epoch: 30/150RMSE=0.8535 RMSE=0.8500 RMSE=0.8550 RMSE=0.8542 RMSE=0.8581 RMSE=0.8568 RMSE=0.8548 RMSE=0.8516 RMSE=0.8567 RMSE=0.8517 Epoch: 40/150RMSE=0.8528 RMSE=0.8556 RMSE=0.8551 RMSE=0.8552 RMSE=0.8541 RMSE=0.8545 RMSE=0.8540 RMSE=0.8549 RMSE=0.8560 RMSE=0.8551 Epoch: 50/150RMSE=0.8554 RMSE=0.8543 RMSE=0.8000 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 0.816 +/- 0.014\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.1609 RMSE=1.1628 RMSE=1.0893 RMSE=1.0527 RMSE=1.0754 RMSE=0.9943 RMSE=0.9575 RMSE=1.0191 RMSE=0.9729 Epoch: 10/150RMSE=1.0265 RMSE=1.0874 RMSE=0.8985 RMSE=0.9319 RMSE=0.9027 RMSE=0.9125 RMSE=0.8994 RMSE=0.9028 RMSE=0.8807 RMSE=0.9035 Epoch: 20/150RMSE=0.8966 RMSE=0.8969 RMSE=0.9111 RMSE=0.8784 RMSE=0.8955 RMSE=0.8816 RMSE=0.8834 RMSE=0.8725 RMSE=0.8838 RMSE=0.8893 Epoch: 30/150RMSE=0.8805 RMSE=0.8747 RMSE=0.8798 RMSE=0.8761 RMSE=0.8681 RMSE=0.8666 RMSE=0.8749 RMSE=0.8756 RMSE=0.8664 RMSE=0.8725 Epoch: 40/150RMSE=0.8727 RMSE=0.8671 RMSE=0.8618 RMSE=0.8711 RMSE=0.8706 RMSE=0.8623 RMSE=0.8648 RMSE=0.8655 RMSE=0.8635 RMSE=0.8645 Epoch: 50/150RMSE=0.8645 RMSE=0.8678 RMSE=0.8680 RMSE=0.8649 RMSE=0.8657 RMSE=0.8681 RMSE=0.8671 RMSE=0.8673 RMSE=0.8676 RMSE=0.8672 Epoch: 60/150RMSE=0.8660 RMSE=0.8670 RMSE=0.8682 RMSE=0.8179 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 0.817 +/- 0.012\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.2016 RMSE=1.1576 RMSE=1.0627 RMSE=1.0415 RMSE=1.0071 RMSE=0.9612 RMSE=0.9799 RMSE=0.9851 RMSE=1.1037 Epoch: 10/150RMSE=0.9820 RMSE=0.9252 RMSE=0.8972 RMSE=0.8910 RMSE=0.9087 RMSE=0.9080 RMSE=0.9302 RMSE=0.9146 RMSE=0.8654 RMSE=0.8638 Epoch: 20/150RMSE=0.8743 RMSE=0.8596 RMSE=0.8618 RMSE=0.8738 RMSE=0.8684 RMSE=0.8770 RMSE=0.8670 RMSE=0.8684 RMSE=0.8657 RMSE=0.8700 Epoch: 30/150RMSE=0.8563 RMSE=0.8697 RMSE=0.8575 RMSE=0.8543 RMSE=0.8527 RMSE=0.8533 RMSE=0.8580 RMSE=0.8668 RMSE=0.8676 RMSE=0.8706 Epoch: 40/150RMSE=0.8656 RMSE=0.8708 RMSE=0.8655 RMSE=0.8636 RMSE=0.8660 RMSE=0.8643 RMSE=0.8642 RMSE=0.8632 RMSE=0.8685 RMSE=0.8680 Epoch: 50/150RMSE=0.8653 RMSE=0.8654 RMSE=0.8675 RMSE=0.8668 RMSE=0.8689 RMSE=0.7954 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 0.813 +/- 0.014\n",
                        "\n",
                        "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150RMSE=1.0787 RMSE=1.1000 RMSE=1.0768 RMSE=0.9865 RMSE=0.9486 RMSE=0.9938 RMSE=0.9182 RMSE=0.9827 RMSE=0.9704 Epoch: 10/150RMSE=0.9290 RMSE=1.0022 RMSE=0.8678 RMSE=0.8689 RMSE=0.8471 RMSE=0.8551 RMSE=0.8791 RMSE=0.9257 RMSE=0.8636 RMSE=0.8401 Epoch: 20/150RMSE=0.8535 RMSE=0.8281 RMSE=0.8467 RMSE=0.8457 RMSE=0.8355 RMSE=0.8214 RMSE=0.8569 RMSE=0.8482 RMSE=0.8279 RMSE=0.8600 Epoch: 30/150RMSE=0.8270 RMSE=0.8353 RMSE=0.8170 RMSE=0.8168 RMSE=0.8242 RMSE=0.8211 RMSE=0.8279 RMSE=0.8135 RMSE=0.8154 RMSE=0.8210 Epoch: 40/150RMSE=0.8139 RMSE=0.8142 RMSE=0.8181 RMSE=0.8213 RMSE=0.8236 RMSE=0.8197 RMSE=0.8208 RMSE=0.8196 RMSE=0.8153 RMSE=0.8158 Epoch: 50/150RMSE=0.8141 RMSE=0.8153 RMSE=0.8160 RMSE=0.8175 RMSE=0.8184 RMSE=0.8180 RMSE=0.8177 RMSE=0.8181 RMSE=0.7534 \n",
                        "********************1's fold 1's run over********************\n",
                        "RMSE: 0.753 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.1784 RMSE=1.0630 RMSE=1.1442 RMSE=1.0357 RMSE=0.9908 RMSE=0.9918 RMSE=1.0322 RMSE=0.9006 RMSE=0.9371 Epoch: 10/150RMSE=1.0079 RMSE=1.0542 RMSE=0.9621 RMSE=0.8878 RMSE=0.9113 RMSE=0.9129 RMSE=0.9413 RMSE=0.8954 RMSE=0.8563 RMSE=0.8365 Epoch: 20/150RMSE=0.8620 RMSE=0.8349 RMSE=0.8884 RMSE=0.8361 RMSE=0.8832 RMSE=0.8627 RMSE=0.8293 RMSE=0.8235 RMSE=0.8215 RMSE=0.8291 Epoch: 30/150RMSE=0.8325 RMSE=0.8436 RMSE=0.8365 RMSE=0.8339 RMSE=0.8377 RMSE=0.8402 RMSE=0.8388 RMSE=0.8367 RMSE=0.8354 RMSE=0.8352 Epoch: 40/150RMSE=0.8355 RMSE=0.8373 RMSE=0.8384 RMSE=0.8393 RMSE=0.8392 RMSE=0.8395 RMSE=0.8390 RMSE=0.8384 RMSE=0.8396 RMSE=0.7783 \n",
                        "********************1's fold 2's run over********************\n",
                        "RMSE: 0.766 +/- 0.012\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.1508 RMSE=1.0640 RMSE=1.0944 RMSE=1.0219 RMSE=1.0749 RMSE=0.9763 RMSE=0.9640 RMSE=1.0696 RMSE=0.9632 Epoch: 10/150RMSE=0.9292 RMSE=1.0045 RMSE=0.9444 RMSE=0.8898 RMSE=0.8981 RMSE=0.9740 RMSE=0.9240 RMSE=0.9532 RMSE=0.9486 RMSE=0.8904 Epoch: 20/150RMSE=0.8804 RMSE=0.8962 RMSE=0.8642 RMSE=0.9725 RMSE=0.9078 RMSE=0.9142 RMSE=0.9101 RMSE=0.8611 RMSE=0.8566 RMSE=0.8500 Epoch: 30/150RMSE=0.8594 RMSE=0.8694 RMSE=0.8517 RMSE=0.8570 RMSE=0.8561 RMSE=0.8503 RMSE=0.8554 RMSE=0.8555 RMSE=0.8564 RMSE=0.8592 Epoch: 40/150RMSE=0.8475 RMSE=0.8501 RMSE=0.8525 RMSE=0.8559 RMSE=0.8548 RMSE=0.8570 RMSE=0.8554 RMSE=0.8573 RMSE=0.8560 RMSE=0.8544 Epoch: 50/150RMSE=0.8555 RMSE=0.8549 RMSE=0.8585 RMSE=0.8582 RMSE=0.8572 RMSE=0.8553 RMSE=0.8560 RMSE=0.8562 RMSE=0.8559 RMSE=0.8560 Epoch: 60/150RMSE=0.8561 RMSE=0.7465 \n",
                        "********************1's fold 3's run over********************\n",
                        "RMSE: 0.759 +/- 0.014\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.0985 RMSE=1.0394 RMSE=0.9940 RMSE=1.0002 RMSE=0.9749 RMSE=0.9475 RMSE=0.9405 RMSE=1.0019 RMSE=0.9681 Epoch: 10/150RMSE=0.9240 RMSE=0.9271 RMSE=0.9449 RMSE=0.9340 RMSE=0.9144 RMSE=0.9137 RMSE=0.9672 RMSE=0.9341 RMSE=0.9620 RMSE=0.8683 Epoch: 20/150RMSE=0.8619 RMSE=0.8901 RMSE=0.8631 RMSE=0.9438 RMSE=0.8660 RMSE=0.8454 RMSE=0.8590 RMSE=0.8550 RMSE=0.8553 RMSE=0.8456 Epoch: 30/150RMSE=0.8101 RMSE=0.8133 RMSE=0.8159 RMSE=0.8227 RMSE=0.8182 RMSE=0.8102 RMSE=0.8008 RMSE=0.8035 RMSE=0.8054 RMSE=0.8001 Epoch: 40/150RMSE=0.8025 RMSE=0.7973 RMSE=0.7969 RMSE=0.7983 RMSE=0.7995 RMSE=0.7991 RMSE=0.8012 RMSE=0.7976 RMSE=0.7959 RMSE=0.8006 Epoch: 50/150RMSE=0.8021 RMSE=0.7988 RMSE=0.7983 RMSE=0.7988 RMSE=0.7966 RMSE=0.7995 RMSE=0.7990 RMSE=0.7988 RMSE=0.7981 RMSE=0.8003 Epoch: 60/150RMSE=0.8000 RMSE=0.8006 RMSE=0.7996 RMSE=0.8000 RMSE=0.7999 RMSE=0.7994 RMSE=0.7997 RMSE=0.7994 RMSE=0.8000 RMSE=0.7521 \n",
                        "********************1's fold 4's run over********************\n",
                        "RMSE: 0.758 +/- 0.012\n",
                        "\n",
                        "Epoch: 1/150RMSE=1.0868 RMSE=1.0034 RMSE=0.9727 RMSE=0.9118 RMSE=0.9194 RMSE=0.9084 RMSE=0.8801 RMSE=0.8549 RMSE=0.9025 Epoch: 10/150RMSE=1.0543 RMSE=0.9814 RMSE=0.8894 RMSE=0.8603 RMSE=0.8173 RMSE=0.8451 RMSE=0.8242 RMSE=0.8280 RMSE=0.8159 RMSE=0.8085 Epoch: 20/150RMSE=0.8361 RMSE=0.7999 RMSE=0.8360 RMSE=0.8312 RMSE=0.8091 RMSE=0.8442 RMSE=0.8082 RMSE=0.8144 RMSE=0.8130 RMSE=0.8127 Epoch: 30/150RMSE=0.8056 RMSE=0.8137 RMSE=0.8131 RMSE=0.8173 RMSE=0.7959 RMSE=0.7984 RMSE=0.8009 RMSE=0.7930 RMSE=0.7938 RMSE=0.7977 Epoch: 40/150RMSE=0.7971 RMSE=0.7951 RMSE=0.7939 RMSE=0.7956 RMSE=0.7952 RMSE=0.7956 RMSE=0.7967 RMSE=0.7953 RMSE=0.7959 RMSE=0.7951 Epoch: 50/150RMSE=0.7954 RMSE=0.7955 RMSE=0.7956 RMSE=0.7957 RMSE=0.7955 RMSE=0.7953 RMSE=0.7955 RMSE=0.7957 RMSE=0.7510 \n",
                        "********************1's fold 5's run over********************\n",
                        "RMSE: 0.756 +/- 0.011\n",
                        "\n"
                    ]
                }
            ],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.1 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.3 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.5 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.7 --pooling='HGPSL'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --hgpsl_ratio=0.9 --pooling='HGPSL'"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([0.5344, 0.0000, 0.0000, 0.4656, 0.0613, 0.0000, 0.0000, 0.0000, 0.5640,\n",
                        "        0.3748])\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\nfrom torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom torch_geometric.nn import GCNConv\nfrom torch.nn import Parameter\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Union, Optional, Callable\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax\nimport math\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, GCNConv, GATConv, ChebConv, GraphConv\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\nfrom torch_scatter import scatter_add\nfrom torch_sparse import spspmm, coalesce\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch_scatter import scatter_add, scatter_max\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = torch.nonzero(x > scores_min).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n            num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_weight, perm, num_nodes=None):\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        mask = perm.new_full((num_nodes, ), -1)\n        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n        mask[perm] = i\n        row, col = edge_index\n        row, col = mask[row], mask[col]\n        mask = (row >= 0) & (col >= 0)\n        row, col = row[mask], col[mask]\n        if edge_weight is not None:\n            edge_weight = edge_weight[mask]\n        return torch.stack([row, col], dim=0), edge_weight\ndef scatter_sort(x, batch, fill_value=-1e16):\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n    dense_x[index] = x\n    dense_x = dense_x.view(batch_size, max_num_nodes)\n    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n    sorted_x = sorted_x.view(-1)\n    filled_index = sorted_x != fill_value\n    sorted_x = sorted_x[filled_index]\n    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n    return sorted_x, cumsum_sorted_x\ndef _make_ix_like(batch):\n    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n    idx = torch.cat(idx, dim=0)\n    return idx\ndef _threshold_and_support(x, batch):\n    \"\"\"Sparsemax building block: compute the threshold\n    Args:\n        x: input tensor to apply the sparsemax\n        batch: group indicators\n    Returns:\n        the threshold value\n    \"\"\"\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    sorted_input, input_cumsum = scatter_sort(x, batch)\n    input_cumsum = input_cumsum - 1.0\n    rhos = _make_ix_like(batch).to(x.dtype)\n    support = rhos * sorted_input > input_cumsum\n    support_size = scatter_add(support.to(batch.dtype), batch)\n    idx = support_size + cum_num_nodes - 1\n    mask = idx < 0\n    idx[mask] = 0\n    tau = input_cumsum.gather(0, idx)\n    tau /= support_size.to(x.dtype)\n    return tau, support_size\nclass SparsemaxFunction(Function):\n    @staticmethod\n    def forward(ctx, x, batch):\n        \"\"\"sparsemax: normalizing sparse transform\n        Parameters:\n            ctx: context object\n            x (Tensor): shape (N, )\n            batch: group indicator\n        Returns:\n            output (Tensor): same shape as input\n        \"\"\"\n        max_val, _ = scatter_max(x, batch)\n        x -= max_val[batch]\n        tau, supp_size = _threshold_and_support(x, batch)\n        output = torch.clamp(x - tau[batch], min=0)\n        ctx.save_for_backward(supp_size, output, batch)\n        return output\n    @staticmethod\n    def backward(ctx, grad_output):\n        supp_size, output, batch = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n        return grad_input, None\nsparsemax = SparsemaxFunction.apply\nclass Sparsemax(nn.Module):\n    def __init__(self):\n        super(Sparsemax, self).__init__()\n    def forward(self, x, batch):\n        return sparsemax(x, batch)\nif __name__ == '__main__':\n    sparse_attention = Sparsemax()\n    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n    res = sparse_attention(input_x, input_batch)\n    print(res)\nclass TwoHopNeighborhood(object):\n    def __call__(self, data):\n        edge_index, edge_attr = data.edge_index, data.edge_attr\n        n = data.num_nodes\n        fill = 1e16\n        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n        edge_index = torch.cat([edge_index, index], dim=1)\n        if edge_attr is None:\n            data.edge_index, _ = coalesce(edge_index, None, n, n)\n        else:\n            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n            value = value.expand(-1, *list(edge_attr.size())[1:])\n            edge_attr = torch.cat([edge_attr, value], dim=0)\n            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min', fill_value=fill)\n            edge_attr[edge_attr >= fill] = 0\n            data.edge_attr = edge_attr\n        return data\n    def __repr__(self):\n        return '{}()'.format(self.__class__.__name__)\nclass GCN(MessagePassing):\n    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n        super(GCN, self).__init__(aggr='add', **kwargs)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n        nn.init.xavier_uniform_(self.weight.data)\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n            nn.init.zeros_(self.bias.data)\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n    def reset_parameters(self):\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight=None):\n        x = torch.matmul(x, self.weight)\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        if self.bias is not None:\n            aggr_out = aggr_out + self.bias\n        return aggr_out\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n        row, col = edge_index\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass HGPSLPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n        super(HGPSLPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.sample = sample\n        self.sparse = sparse\n        self.sl = sl\n        self.negative_slop = negative_slop\n        self.lamb = lamb\n        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n        nn.init.xavier_uniform_(self.att.data)\n        self.sparse_attention = Sparsemax()\n        self.neighbor_augment = TwoHopNeighborhood()\n        self.calc_information_score = NodeInformationScore()\n    def forward(self, x, edge_index, edge_attr, batch):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        original_x = x\n        perm = topk(score, self.ratio, batch)\n        x = x[perm]\n        batch = batch[perm]\n        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        if self.sl is False:\n            return x, induced_edge_index, induced_edge_attr, batch\n        if self.sample:\n            k_hop = 3\n            if edge_attr is None:\n                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n            for _ in range(k_hop - 1):\n                hop_data = self.neighbor_augment(hop_data)\n            hop_edge_index = hop_data.edge_index\n            hop_edge_attr = hop_data.edge_attr\n            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            adj[row, col] = weights\n            new_edge_index, weights = dense_to_sparse(adj)\n            row, col = new_edge_index\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        else:\n            if edge_attr is None:\n                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n                                               device=induced_edge_index.device)\n            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n            cum_num_nodes = num_nodes.cumsum(dim=0)\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n            new_edge_index, _ = dense_to_sparse(adj)\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop)\n            adj[row, col] = weights\n            induced_row, induced_col = induced_edge_index\n            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n            weights = adj[row, col]\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, num_nodes=x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        return x, new_edge_index, new_edge_attr, batch, perm"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n"
                    ]
                }
            ],
            "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nimport random\nimport pandas as pd\nimport torch_geometric.transforms as T\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data\nfrom torch_geometric.data.datapipes import functional_transform\nfrom torch_geometric.transforms import BaseTransform\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import Actor\nfrom torch_geometric.datasets import CitationFull\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimport shutil\ndataset_sparse = Planetoid(root=\"/data1/Pooling\", name='PubMed')\ndataset_sparse = dataset_sparse.shuffle()\nprint(dataset_sparse[0])"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "ename": "RuntimeError",
                    "evalue": "CUDA out of memory. Tried to allocate 46.35 GiB (GPU 0; 79.10 GiB total capacity; 48.61 GiB already allocated; 17.88 GiB free; 48.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[4], line 180\u001b[0m\n\u001b[1;32m    176\u001b[0m test_mask[test_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    178\u001b[0m val_mask \u001b[38;5;241m=\u001b[39m test_mask  \u001b[38;5;66;03m# In cross-validation, we use test_mask as val_mask\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m model, best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_node_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m val_accuracies\u001b[38;5;241m.\u001b[39mappend(best_val_acc)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
                        "Cell \u001b[0;32mIn[4], line 114\u001b[0m, in \u001b[0;36mtrain_node_classifier\u001b[0;34m(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs, patience, min_delta)\u001b[0m\n\u001b[1;32m    112\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    113\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 114\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out[train_mask], graph\u001b[38;5;241m.\u001b[39my[train_mask])\n\u001b[1;32m    116\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
                        "File \u001b[0;32m~/anaconda3/envs/CG-ODE/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "Cell \u001b[0;32mIn[4], line 80\u001b[0m, in \u001b[0;36mHierarchicalGCN_HGPSL.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     77\u001b[0m perms \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 80\u001b[0m     x, edge_index, _, batch, perm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpools\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_convs[i](x, edge_index)\n\u001b[1;32m     82\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n",
                        "File \u001b[0;32m~/anaconda3/envs/CG-ODE/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
                        "Cell \u001b[0;32mIn[2], line 467\u001b[0m, in \u001b[0;36mHGPSLPool.forward\u001b[0;34m(self, x, edge_index, edge_attr, batch)\u001b[0m\n\u001b[1;32m    464\u001b[0m new_edge_index, _ \u001b[38;5;241m=\u001b[39m dense_to_sparse(adj)\n\u001b[1;32m    465\u001b[0m row, col \u001b[38;5;241m=\u001b[39m new_edge_index\n\u001b[0;32m--> 467\u001b[0m weights \u001b[38;5;241m=\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    468\u001b[0m weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_slop)\n\u001b[1;32m    469\u001b[0m adj[row, col] \u001b[38;5;241m=\u001b[39m weights\n",
                        "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 46.35 GiB (GPU 0; 79.10 GiB total capacity; 48.61 GiB already allocated; 17.88 GiB free; 48.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
                    ]
                }
            ],
            "source": "import time\nimport tracemalloc\nimport torch\nfrom torch_geometric.datasets import Planetoid\nimport torch_geometric.transforms as T\nfrom torch_geometric.nn import GCNConv\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import SAGEConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport random\nfrom typing import Callable, Optional, Union\ndataset = dataset_sparse\ngraph = dataset[0]\nnum_classes = dataset.num_classes\nin_channels = dataset.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 1\npool_ratios = [0.5, 0.5]  \nclass HierarchicalGCN_HGPSL(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_HGPSL, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.act = act\n        self.sum_res = sum_res\n        channels = self.hidden_channels\n        self.down_convs = torch.nn.ModuleList()\n        self.pools = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(HGPSLPool(hidden_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        edge_attr = None\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = self.act(x)\n        xs = [x]\n        edge_indices = [edge_index]\n        perms = []\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, edge_attr, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = self.act(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n            perms.append(perm)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            perm = perms[j]\n            up = torch.zeros_like(res)\n            up[perm] = x\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = self.act(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x\ndef train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n    best_val_acc = 0\n    patience_counter = 0\n    model.to(device)\n    graph = graph.to(device)\n    for epoch in range(1, n_epochs + 1):\n        model.train()\n        optimizer.zero_grad()\n        out = model(graph.x, graph.edge_index)\n        loss = criterion(out[train_mask], graph.y[train_mask])\n        loss.backward()\n        optimizer.step()\n        val_acc = eval_node_classifier(model, graph, val_mask)\n        if val_acc > best_val_acc + min_delta:\n            best_val_acc = val_acc\n            patience_counter = 0  \n        else:\n            patience_counter += 1  \n        if patience_counter >= patience:\n            break\n    return model, best_val_acc\ndef eval_node_classifier(model, graph, mask):\n    model.eval()\n    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n    correct = (pred[mask] == graph.y[mask]).sum()\n    acc = int(correct) / int(mask.sum())\n    return acc\nkf = KFold(n_splits=5, shuffle=True)\nseeds = [42, 123, 456, 789, 101]\nresults = []\nval_accuracies_list = []\ntimes = []\nmemories = []\ngpu_memories = []\nfor seed in seeds:\n    graph = graph.to(device)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    val_accuracies = []\n    start_time = time.time()\n    tracemalloc.start()\n    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n        model = HierarchicalGCN_HGPSL(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n        criterion = nn.CrossEntropyLoss()\n        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n        train_mask[train_index] = True\n        test_mask[test_index] = True\n        val_mask = test_mask  \n        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n        val_accuracies.append(best_val_acc)\n        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n    mean_val_acc = np.mean(val_accuracies)\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    memory_usage = peak / 10**6  \n    if torch.cuda.is_available():\n        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  \n    else:\n        gpu_memory_usage = 0\n    elapsed_time = end_time - start_time\n    results.append({\n        'seed': seed,\n        'mean_val_acc': mean_val_acc,\n        'time': elapsed_time,\n        'memory': memory_usage,\n        'gpu_memory': gpu_memory_usage\n    })\n    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\nfor result in results:\n    print(result)\nmean_val_acc_values = [result['mean_val_acc'] for result in results]\ntotal_mean_val_acc = np.mean(mean_val_acc_values)\nstandard_deviation = np.std(mean_val_acc_values)\nprint(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\nprint(f\"Standard Deviation: {standard_deviation:.2f}\")"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "CG-ODE",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}