{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/boot/anaconda3/envs/XXX1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nimport sys\nimport torch\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom ogb.graphproppred import PygGraphPropPredDataset, Evaluator\nfrom torch_geometric.loader import DataLoader\nimport os\nimport random\nimport pandas as pd\nimport torch\nimport torch_geometric.transforms as T\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data\nfrom torch_geometric.data.datapipes import functional_transform\nfrom torch_geometric.transforms import BaseTransform\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import Actor\nfrom torch_geometric.datasets import GNNBenchmarkDataset\nfrom torch_geometric.datasets import TUDataset\nfrom sklearn.metrics import r2_score\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import MoleculeNet\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.utils import to_networkx\nfrom torch.nn import Linear\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport time\nimport psutil\nimport torch\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nmax_nodes = 150\ndataset_sparse = PygGraphPropPredDataset(root='/data/XXX/Pooling', name=\"ogbg-molpcba\")\nevaluator = Evaluator(\"ogbg-molpcba\")\neval_metric = dataset_sparse.eval_metric\ntrain_ratio = 0.8\nval_ratio = 0.1\ntest_ratio = 0.1\ndataset_sparse = dataset_sparse.shuffle()\nnum_total = len(dataset_sparse)\nnum_train = int(num_total * train_ratio)\nnum_val = int(num_total * val_ratio)\nnum_test = num_total - num_train - num_val\ntrain_dataset = dataset_sparse[:num_train]\nval_dataset = dataset_sparse[num_train:num_train + num_val]\ntest_dataset = dataset_sparse[num_train + num_val:]\ntrain_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\nvalid_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TopKPooling with HierarchicalGCN (2019)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0623, Val Acc: 0.1137, Test Acc: 0.1152\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0605, Val Acc: 0.1212, Test Acc: 0.1235\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0605, Val Acc: 0.1182, Test Acc: 0.1201\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0603, Val Acc: 0.1206, Test Acc: 0.1223\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0601, Val Acc: 0.1180, Test Acc: 0.1199\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0600, Val Acc: 0.1234, Test Acc: 0.1262\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0598, Val Acc: 0.1242, Test Acc: 0.1280\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0597, Val Acc: 0.1233, Test Acc: 0.1254\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0596, Val Acc: 0.1199, Test Acc: 0.1221\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0595, Val Acc: 0.1224, Test Acc: 0.1248\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0595, Val Acc: 0.1252, Test Acc: 0.1277\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0591, Val Acc: 0.1321, Test Acc: 0.1349\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0589, Val Acc: 0.1287, Test Acc: 0.1322\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0588, Val Acc: 0.1336, Test Acc: 0.1377\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0586, Val Acc: 0.1314, Test Acc: 0.1364\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0588, Val Acc: 0.1321, Test Acc: 0.1361\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0586, Val Acc: 0.1369, Test Acc: 0.1408\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0584, Val Acc: 0.1379, Test Acc: 0.1402\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0584, Val Acc: 0.1417, Test Acc: 0.1449\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0583, Val Acc: 0.1423, Test Acc: 0.1462\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0582, Val Acc: 0.1447, Test Acc: 0.1477\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0581, Val Acc: 0.1429, Test Acc: 0.1458\n",
                        "Seed: 42, Epoch: 023, Loss: 0.0581, Val Acc: 0.1440, Test Acc: 0.1473\n",
                        "Seed: 42, Epoch: 024, Loss: 0.0580, Val Acc: 0.1455, Test Acc: 0.1469\n",
                        "Seed: 42, Epoch: 025, Loss: 0.0579, Val Acc: 0.1441, Test Acc: 0.1482\n",
                        "Seed: 42, Epoch: 026, Loss: 0.0579, Val Acc: 0.1462, Test Acc: 0.1478\n",
                        "Seed: 42, Epoch: 027, Loss: 0.0579, Val Acc: 0.1464, Test Acc: 0.1490\n",
                        "Seed: 42, Epoch: 028, Loss: 0.0579, Val Acc: 0.1459, Test Acc: 0.1483\n",
                        "Seed: 42, Epoch: 029, Loss: 0.0578, Val Acc: 0.1458, Test Acc: 0.1488\n",
                        "Seed: 42, Epoch: 030, Loss: 0.0577, Val Acc: 0.1452, Test Acc: 0.1491\n",
                        "Seed: 42, Epoch: 031, Loss: 0.0578, Val Acc: 0.1436, Test Acc: 0.1455\n",
                        "Seed: 42, Epoch: 032, Loss: 0.0577, Val Acc: 0.1457, Test Acc: 0.1483\n",
                        "Seed: 42, Epoch: 033, Loss: 0.0577, Val Acc: 0.1458, Test Acc: 0.1492\n",
                        "Seed: 42, Epoch: 034, Loss: 0.0576, Val Acc: 0.1479, Test Acc: 0.1500\n",
                        "Seed: 42, Epoch: 035, Loss: 0.0577, Val Acc: 0.1441, Test Acc: 0.1475\n",
                        "Seed: 42, Epoch: 036, Loss: 0.0576, Val Acc: 0.1435, Test Acc: 0.1480\n",
                        "Seed: 42, Epoch: 037, Loss: 0.0577, Val Acc: 0.1487, Test Acc: 0.1506\n",
                        "Seed: 42, Epoch: 038, Loss: 0.0576, Val Acc: 0.1477, Test Acc: 0.1511\n",
                        "Seed: 42, Epoch: 039, Loss: 0.0575, Val Acc: 0.1493, Test Acc: 0.1524\n",
                        "Seed: 42, Epoch: 040, Loss: 0.0575, Val Acc: 0.1486, Test Acc: 0.1502\n",
                        "Seed: 42, Epoch: 041, Loss: 0.0575, Val Acc: 0.1472, Test Acc: 0.1495\n",
                        "Seed: 42, Epoch: 042, Loss: 0.0574, Val Acc: 0.1491, Test Acc: 0.1521\n",
                        "Seed: 42, Epoch: 043, Loss: 0.0575, Val Acc: 0.1486, Test Acc: 0.1521\n",
                        "Seed: 42, Epoch: 044, Loss: 0.0574, Val Acc: 0.1523, Test Acc: 0.1557\n",
                        "Seed: 42, Epoch: 045, Loss: 0.0573, Val Acc: 0.1475, Test Acc: 0.1494\n",
                        "Seed: 42, Epoch: 046, Loss: 0.0574, Val Acc: 0.1512, Test Acc: 0.1541\n",
                        "Seed: 42, Epoch: 047, Loss: 0.0573, Val Acc: 0.1480, Test Acc: 0.1498\n",
                        "Seed: 42, Epoch: 048, Loss: 0.0574, Val Acc: 0.1515, Test Acc: 0.1541\n",
                        "Seed: 42, Epoch: 049, Loss: 0.0574, Val Acc: 0.1508, Test Acc: 0.1536\n",
                        "Seed: 42, Epoch: 050, Loss: 0.0573, Val Acc: 0.1536, Test Acc: 0.1562\n",
                        "Seed: 42, Epoch: 051, Loss: 0.0572, Val Acc: 0.1513, Test Acc: 0.1536\n",
                        "Seed: 42, Epoch: 052, Loss: 0.0572, Val Acc: 0.1523, Test Acc: 0.1542\n",
                        "Seed: 42, Epoch: 053, Loss: 0.0572, Val Acc: 0.1486, Test Acc: 0.1518\n",
                        "Seed: 42, Epoch: 054, Loss: 0.0572, Val Acc: 0.1520, Test Acc: 0.1554\n",
                        "Seed: 42, Epoch: 055, Loss: 0.0572, Val Acc: 0.1512, Test Acc: 0.1526\n",
                        "Seed: 42, Epoch: 056, Loss: 0.0573, Val Acc: 0.1519, Test Acc: 0.1545\n",
                        "Seed: 42, Epoch: 057, Loss: 0.0573, Val Acc: 0.1553, Test Acc: 0.1564\n",
                        "Seed: 42, Epoch: 058, Loss: 0.0572, Val Acc: 0.1500, Test Acc: 0.1513\n",
                        "Seed: 42, Epoch: 059, Loss: 0.0573, Val Acc: 0.1517, Test Acc: 0.1529\n",
                        "Seed: 42, Epoch: 060, Loss: 0.0571, Val Acc: 0.1556, Test Acc: 0.1566\n",
                        "Seed: 42, Epoch: 061, Loss: 0.0572, Val Acc: 0.1574, Test Acc: 0.1607\n",
                        "Seed: 42, Epoch: 062, Loss: 0.0572, Val Acc: 0.1540, Test Acc: 0.1558\n",
                        "Seed: 42, Epoch: 063, Loss: 0.0571, Val Acc: 0.1525, Test Acc: 0.1558\n",
                        "Seed: 42, Epoch: 064, Loss: 0.0572, Val Acc: 0.1535, Test Acc: 0.1564\n",
                        "Seed: 42, Epoch: 065, Loss: 0.0571, Val Acc: 0.1547, Test Acc: 0.1583\n",
                        "Seed: 42, Epoch: 066, Loss: 0.0570, Val Acc: 0.1550, Test Acc: 0.1564\n",
                        "Seed: 42, Epoch: 067, Loss: 0.0571, Val Acc: 0.1587, Test Acc: 0.1596\n",
                        "Seed: 42, Epoch: 068, Loss: 0.0571, Val Acc: 0.1574, Test Acc: 0.1603\n",
                        "Seed: 42, Epoch: 069, Loss: 0.0571, Val Acc: 0.1538, Test Acc: 0.1567\n",
                        "Seed: 42, Epoch: 070, Loss: 0.0570, Val Acc: 0.1576, Test Acc: 0.1584\n",
                        "Seed: 42, Epoch: 071, Loss: 0.0571, Val Acc: 0.1473, Test Acc: 0.1491\n",
                        "Seed: 42, Epoch: 072, Loss: 0.0572, Val Acc: 0.1556, Test Acc: 0.1568\n",
                        "Seed: 42, Epoch: 073, Loss: 0.0571, Val Acc: 0.1570, Test Acc: 0.1577\n",
                        "Seed: 42, Epoch: 074, Loss: 0.0570, Val Acc: 0.1560, Test Acc: 0.1585\n",
                        "Seed: 42, Epoch: 075, Loss: 0.0571, Val Acc: 0.1540, Test Acc: 0.1544\n",
                        "Seed: 42, Epoch: 076, Loss: 0.0571, Val Acc: 0.1549, Test Acc: 0.1563\n",
                        "Seed: 42, Epoch: 077, Loss: 0.0570, Val Acc: 0.1562, Test Acc: 0.1569\n",
                        "Seed: 42, Epoch: 078, Loss: 0.0570, Val Acc: 0.1553, Test Acc: 0.1559\n",
                        "Seed: 42, Epoch: 079, Loss: 0.0570, Val Acc: 0.1589, Test Acc: 0.1596\n",
                        "Seed: 42, Epoch: 080, Loss: 0.0570, Val Acc: 0.1569, Test Acc: 0.1588\n",
                        "Seed: 42, Epoch: 081, Loss: 0.0570, Val Acc: 0.1580, Test Acc: 0.1594\n",
                        "Seed: 42, Epoch: 082, Loss: 0.0569, Val Acc: 0.1542, Test Acc: 0.1546\n",
                        "Seed: 42, Epoch: 083, Loss: 0.0570, Val Acc: 0.1600, Test Acc: 0.1610\n",
                        "Seed: 42, Epoch: 084, Loss: 0.0569, Val Acc: 0.1519, Test Acc: 0.1537\n",
                        "Seed: 42, Epoch: 085, Loss: 0.0570, Val Acc: 0.1570, Test Acc: 0.1577\n",
                        "Seed: 42, Epoch: 086, Loss: 0.0570, Val Acc: 0.1562, Test Acc: 0.1586\n",
                        "Seed: 42, Epoch: 087, Loss: 0.0569, Val Acc: 0.1571, Test Acc: 0.1604\n",
                        "Seed: 42, Epoch: 088, Loss: 0.0569, Val Acc: 0.1571, Test Acc: 0.1591\n",
                        "Seed: 42, Epoch: 089, Loss: 0.0568, Val Acc: 0.1523, Test Acc: 0.1546\n",
                        "Seed: 42, Epoch: 090, Loss: 0.0570, Val Acc: 0.1570, Test Acc: 0.1597\n",
                        "Seed: 42, Epoch: 091, Loss: 0.0569, Val Acc: 0.1592, Test Acc: 0.1593\n",
                        "Seed: 42, Epoch: 092, Loss: 0.0569, Val Acc: 0.1541, Test Acc: 0.1578\n",
                        "Seed: 42, Epoch: 093, Loss: 0.0569, Val Acc: 0.1562, Test Acc: 0.1593\n",
                        "Seed: 42, Epoch: 094, Loss: 0.0569, Val Acc: 0.1562, Test Acc: 0.1596\n",
                        "Seed: 42, Epoch: 095, Loss: 0.0569, Val Acc: 0.1602, Test Acc: 0.1604\n",
                        "Seed: 42, Epoch: 096, Loss: 0.0570, Val Acc: 0.1526, Test Acc: 0.1542\n",
                        "Seed: 42, Epoch: 097, Loss: 0.0569, Val Acc: 0.1529, Test Acc: 0.1557\n",
                        "Seed: 42, Epoch: 098, Loss: 0.0570, Val Acc: 0.1539, Test Acc: 0.1557\n",
                        "Seed: 42, Epoch: 099, Loss: 0.0569, Val Acc: 0.1545, Test Acc: 0.1573\n",
                        "Seed: 42, Epoch: 100, Loss: 0.0569, Val Acc: 0.1554, Test Acc: 0.1574\n",
                        "Seed: 42, Epoch: 101, Loss: 0.0570, Val Acc: 0.1556, Test Acc: 0.1576\n",
                        "Seed: 42, Epoch: 102, Loss: 0.0569, Val Acc: 0.1567, Test Acc: 0.1579\n",
                        "Seed: 42, Epoch: 103, Loss: 0.0569, Val Acc: 0.1578, Test Acc: 0.1599\n",
                        "Seed: 42, Epoch: 104, Loss: 0.0569, Val Acc: 0.1556, Test Acc: 0.1571\n",
                        "Seed: 42, Epoch: 105, Loss: 0.0575, Val Acc: 0.1486, Test Acc: 0.1517\n",
                        "Seed: 42, Epoch: 106, Loss: 0.0573, Val Acc: 0.1484, Test Acc: 0.1504\n",
                        "Seed: 42, Epoch: 107, Loss: 0.0571, Val Acc: 0.1546, Test Acc: 0.1576\n",
                        "Seed: 42, Epoch: 108, Loss: 0.0570, Val Acc: 0.1551, Test Acc: 0.1571\n",
                        "Seed: 42, Epoch: 109, Loss: 0.0569, Val Acc: 0.1537, Test Acc: 0.1557\n",
                        "Seed: 42, Epoch: 110, Loss: 0.0570, Val Acc: 0.1558, Test Acc: 0.1576\n",
                        "Seed: 42, Epoch: 111, Loss: 0.0570, Val Acc: 0.1507, Test Acc: 0.1542\n",
                        "Seed: 42, Epoch: 112, Loss: 0.0570, Val Acc: 0.1523, Test Acc: 0.1552\n",
                        "Seed: 42, Epoch: 113, Loss: 0.0569, Val Acc: 0.1499, Test Acc: 0.1536\n",
                        "Seed: 42, Epoch: 114, Loss: 0.0570, Val Acc: 0.1519, Test Acc: 0.1553\n",
                        "Seed: 42, Epoch: 115, Loss: 0.0572, Val Acc: 0.1504, Test Acc: 0.1523\n",
                        "Seed: 42, Epoch: 116, Loss: 0.0573, Val Acc: 0.1524, Test Acc: 0.1547\n",
                        "Seed: 42, Epoch: 117, Loss: 0.0571, Val Acc: 0.1553, Test Acc: 0.1589\n",
                        "Seed: 42, Epoch: 118, Loss: 0.0570, Val Acc: 0.1547, Test Acc: 0.1554\n",
                        "Seed: 42, Epoch: 119, Loss: 0.0571, Val Acc: 0.1575, Test Acc: 0.1596\n",
                        "Seed: 42, Epoch: 120, Loss: 0.0571, Val Acc: 0.1574, Test Acc: 0.1596\n",
                        "Seed: 42, Epoch: 121, Loss: 0.0570, Val Acc: 0.1579, Test Acc: 0.1603\n",
                        "Seed: 42, Epoch: 122, Loss: 0.0569, Val Acc: 0.1560, Test Acc: 0.1587\n",
                        "Seed: 42, Epoch: 123, Loss: 0.0570, Val Acc: 0.1569, Test Acc: 0.1600\n",
                        "Seed: 42, Epoch: 124, Loss: 0.0570, Val Acc: 0.1579, Test Acc: 0.1597\n",
                        "Seed: 42, Epoch: 125, Loss: 0.0570, Val Acc: 0.1575, Test Acc: 0.1603\n",
                        "Seed: 42, Epoch: 126, Loss: 0.0570, Val Acc: 0.1561, Test Acc: 0.1580\n",
                        "Seed: 42, Epoch: 127, Loss: 0.0570, Val Acc: 0.1557, Test Acc: 0.1575\n",
                        "Seed: 42, Epoch: 128, Loss: 0.0570, Val Acc: 0.1566, Test Acc: 0.1574\n",
                        "Seed: 42, Epoch: 129, Loss: 0.0570, Val Acc: 0.1573, Test Acc: 0.1583\n",
                        "Seed: 42, Epoch: 130, Loss: 0.0569, Val Acc: 0.1412, Test Acc: 0.1437\n",
                        "Seed: 42, Epoch: 131, Loss: 0.0576, Val Acc: 0.1564, Test Acc: 0.1582\n",
                        "Seed: 42, Epoch: 132, Loss: 0.0572, Val Acc: 0.1530, Test Acc: 0.1553\n",
                        "Seed: 42, Epoch: 133, Loss: 0.0570, Val Acc: 0.1539, Test Acc: 0.1570\n",
                        "Seed: 42, Epoch: 134, Loss: 0.0571, Val Acc: 0.1526, Test Acc: 0.1552\n",
                        "Seed: 42, Epoch: 135, Loss: 0.0571, Val Acc: 0.1559, Test Acc: 0.1567\n",
                        "Seed: 42, Epoch: 136, Loss: 0.0571, Val Acc: 0.1554, Test Acc: 0.1586\n",
                        "Seed: 42, Epoch: 137, Loss: 0.0572, Val Acc: 0.1618, Test Acc: 0.1632\n",
                        "Seed: 42, Epoch: 138, Loss: 0.0570, Val Acc: 0.1561, Test Acc: 0.1576\n",
                        "Seed: 42, Epoch: 139, Loss: 0.0571, Val Acc: 0.1528, Test Acc: 0.1563\n",
                        "Seed: 42, Epoch: 140, Loss: 0.0571, Val Acc: 0.1522, Test Acc: 0.1549\n",
                        "Seed: 42, Epoch: 141, Loss: 0.0571, Val Acc: 0.1594, Test Acc: 0.1613\n",
                        "Seed: 42, Epoch: 142, Loss: 0.0572, Val Acc: 0.1547, Test Acc: 0.1571\n",
                        "Seed: 42, Epoch: 143, Loss: 0.0570, Val Acc: 0.1510, Test Acc: 0.1538\n",
                        "Seed: 42, Epoch: 144, Loss: 0.0571, Val Acc: 0.1544, Test Acc: 0.1571\n",
                        "Seed: 42, Epoch: 145, Loss: 0.0571, Val Acc: 0.1519, Test Acc: 0.1540\n",
                        "Seed: 42, Epoch: 146, Loss: 0.0571, Val Acc: 0.1540, Test Acc: 0.1572\n",
                        "Seed: 42, Epoch: 147, Loss: 0.0571, Val Acc: 0.1533, Test Acc: 0.1570\n",
                        "Seed: 42, Epoch: 148, Loss: 0.0570, Val Acc: 0.1541, Test Acc: 0.1573\n",
                        "Seed: 42, Epoch: 149, Loss: 0.0570, Val Acc: 0.1537, Test Acc: 0.1563\n",
                        "Seed: 42, Epoch: 150, Loss: 0.0571, Val Acc: 0.1495, Test Acc: 0.1530\n",
                        "Seed: 42, Epoch: 151, Loss: 0.0571, Val Acc: 0.1542, Test Acc: 0.1579\n",
                        "Seed: 42, Epoch: 152, Loss: 0.0570, Val Acc: 0.1573, Test Acc: 0.1608\n",
                        "Seed: 42, Epoch: 153, Loss: 0.0570, Val Acc: 0.1545, Test Acc: 0.1569\n",
                        "Seed: 42, Epoch: 154, Loss: 0.0571, Val Acc: 0.1492, Test Acc: 0.1513\n",
                        "Seed: 42, Epoch: 155, Loss: 0.0571, Val Acc: 0.1490, Test Acc: 0.1509\n",
                        "Seed: 42, Epoch: 156, Loss: 0.0571, Val Acc: 0.1506, Test Acc: 0.1547\n",
                        "Seed: 42, Epoch: 157, Loss: 0.0571, Val Acc: 0.1533, Test Acc: 0.1556\n",
                        "Seed: 42, Epoch: 158, Loss: 0.0572, Val Acc: 0.1546, Test Acc: 0.1577\n",
                        "Seed: 42, Epoch: 159, Loss: 0.0572, Val Acc: 0.1526, Test Acc: 0.1566\n",
                        "Seed: 42, Epoch: 160, Loss: 0.0572, Val Acc: 0.1513, Test Acc: 0.1542\n",
                        "Seed: 42, Epoch: 161, Loss: 0.0572, Val Acc: 0.1503, Test Acc: 0.1533\n",
                        "Seed: 42, Epoch: 162, Loss: 0.0572, Val Acc: 0.1540, Test Acc: 0.1581\n",
                        "Seed: 42, Epoch: 163, Loss: 0.0573, Val Acc: 0.1495, Test Acc: 0.1533\n",
                        "Seed: 42, Epoch: 164, Loss: 0.0572, Val Acc: 0.1516, Test Acc: 0.1544\n",
                        "Seed: 42, Epoch: 165, Loss: 0.0572, Val Acc: 0.1513, Test Acc: 0.1553\n",
                        "Seed: 42, Epoch: 166, Loss: 0.0571, Val Acc: 0.1498, Test Acc: 0.1533\n",
                        "Seed: 42, Epoch: 167, Loss: 0.0572, Val Acc: 0.1513, Test Acc: 0.1545\n",
                        "Seed: 42, Epoch: 168, Loss: 0.0571, Val Acc: 0.1523, Test Acc: 0.1557\n",
                        "Seed: 42, Epoch: 169, Loss: 0.0571, Val Acc: 0.1526, Test Acc: 0.1552\n",
                        "Seed: 42, Epoch: 170, Loss: 0.0571, Val Acc: 0.1518, Test Acc: 0.1544\n",
                        "Seed: 42, Epoch: 171, Loss: 0.0572, Val Acc: 0.1549, Test Acc: 0.1589\n",
                        "Seed: 42, Epoch: 172, Loss: 0.0570, Val Acc: 0.1473, Test Acc: 0.1493\n",
                        "Seed: 42, Epoch: 173, Loss: 0.0573, Val Acc: 0.1548, Test Acc: 0.1586\n",
                        "Seed: 42, Epoch: 174, Loss: 0.0570, Val Acc: 0.1528, Test Acc: 0.1560\n",
                        "Seed: 42, Epoch: 175, Loss: 0.0570, Val Acc: 0.1549, Test Acc: 0.1584\n",
                        "Seed: 42, Epoch: 176, Loss: 0.0570, Val Acc: 0.1566, Test Acc: 0.1600\n",
                        "Seed: 42, Epoch: 177, Loss: 0.0571, Val Acc: 0.1521, Test Acc: 0.1565\n",
                        "Seed: 42, Epoch: 178, Loss: 0.0571, Val Acc: 0.1545, Test Acc: 0.1575\n",
                        "Seed: 42, Epoch: 179, Loss: 0.0570, Val Acc: 0.1565, Test Acc: 0.1597\n",
                        "Seed: 42, Epoch: 180, Loss: 0.0570, Val Acc: 0.1525, Test Acc: 0.1548\n",
                        "Seed: 42, Epoch: 181, Loss: 0.0570, Val Acc: 0.1561, Test Acc: 0.1596\n",
                        "Seed: 42, Epoch: 182, Loss: 0.0570, Val Acc: 0.1514, Test Acc: 0.1540\n",
                        "Seed: 42, Epoch: 183, Loss: 0.0571, Val Acc: 0.1536, Test Acc: 0.1576\n",
                        "Seed: 42, Epoch: 184, Loss: 0.0571, Val Acc: 0.1538, Test Acc: 0.1569\n",
                        "Seed: 42, Epoch: 185, Loss: 0.0570, Val Acc: 0.1533, Test Acc: 0.1554\n",
                        "Seed: 42, Epoch: 186, Loss: 0.0570, Val Acc: 0.1531, Test Acc: 0.1567\n",
                        "Seed: 42, Epoch: 187, Loss: 0.0569, Val Acc: 0.1531, Test Acc: 0.1566\n",
                        "Early stopping at epoch 187 for seed 42\n",
                        "Seed: 43, Epoch: 001, Loss: 0.0630, Val Acc: 0.1225, Test Acc: 0.1183\n",
                        "Seed: 43, Epoch: 002, Loss: 0.0604, Val Acc: 0.1276, Test Acc: 0.1248\n",
                        "Seed: 43, Epoch: 003, Loss: 0.0601, Val Acc: 0.1259, Test Acc: 0.1228\n",
                        "Seed: 43, Epoch: 004, Loss: 0.0602, Val Acc: 0.1224, Test Acc: 0.1204\n",
                        "Seed: 43, Epoch: 005, Loss: 0.0599, Val Acc: 0.1212, Test Acc: 0.1201\n",
                        "Seed: 43, Epoch: 006, Loss: 0.0598, Val Acc: 0.1179, Test Acc: 0.1168\n",
                        "Seed: 43, Epoch: 007, Loss: 0.0599, Val Acc: 0.1168, Test Acc: 0.1155\n",
                        "Seed: 43, Epoch: 008, Loss: 0.0597, Val Acc: 0.1185, Test Acc: 0.1167\n",
                        "Seed: 43, Epoch: 009, Loss: 0.0597, Val Acc: 0.1244, Test Acc: 0.1216\n",
                        "Seed: 43, Epoch: 010, Loss: 0.0596, Val Acc: 0.1171, Test Acc: 0.1162\n",
                        "Seed: 43, Epoch: 011, Loss: 0.0595, Val Acc: 0.1200, Test Acc: 0.1183\n",
                        "Seed: 43, Epoch: 012, Loss: 0.0595, Val Acc: 0.1234, Test Acc: 0.1207\n",
                        "Seed: 43, Epoch: 013, Loss: 0.0594, Val Acc: 0.1230, Test Acc: 0.1204\n",
                        "Seed: 43, Epoch: 014, Loss: 0.0595, Val Acc: 0.1249, Test Acc: 0.1229\n",
                        "Seed: 43, Epoch: 015, Loss: 0.0593, Val Acc: 0.1228, Test Acc: 0.1208\n",
                        "Seed: 43, Epoch: 016, Loss: 0.0593, Val Acc: 0.1247, Test Acc: 0.1224\n",
                        "Seed: 43, Epoch: 017, Loss: 0.0592, Val Acc: 0.1256, Test Acc: 0.1228\n",
                        "Seed: 43, Epoch: 018, Loss: 0.0590, Val Acc: 0.1264, Test Acc: 0.1252\n",
                        "Seed: 43, Epoch: 019, Loss: 0.0590, Val Acc: 0.1260, Test Acc: 0.1249\n",
                        "Seed: 43, Epoch: 020, Loss: 0.0590, Val Acc: 0.1303, Test Acc: 0.1287\n",
                        "Seed: 43, Epoch: 021, Loss: 0.0590, Val Acc: 0.1280, Test Acc: 0.1268\n",
                        "Seed: 43, Epoch: 022, Loss: 0.0590, Val Acc: 0.1307, Test Acc: 0.1292\n",
                        "Seed: 43, Epoch: 023, Loss: 0.0589, Val Acc: 0.1305, Test Acc: 0.1277\n",
                        "Seed: 43, Epoch: 024, Loss: 0.0589, Val Acc: 0.1305, Test Acc: 0.1279\n",
                        "Seed: 43, Epoch: 025, Loss: 0.0589, Val Acc: 0.1314, Test Acc: 0.1297\n",
                        "Seed: 43, Epoch: 026, Loss: 0.0588, Val Acc: 0.1323, Test Acc: 0.1311\n",
                        "Seed: 43, Epoch: 027, Loss: 0.0587, Val Acc: 0.1308, Test Acc: 0.1290\n",
                        "Seed: 43, Epoch: 028, Loss: 0.0588, Val Acc: 0.1346, Test Acc: 0.1327\n",
                        "Seed: 43, Epoch: 029, Loss: 0.0586, Val Acc: 0.1376, Test Acc: 0.1358\n",
                        "Seed: 43, Epoch: 030, Loss: 0.0584, Val Acc: 0.1366, Test Acc: 0.1344\n",
                        "Seed: 43, Epoch: 031, Loss: 0.0584, Val Acc: 0.1343, Test Acc: 0.1334\n",
                        "Seed: 43, Epoch: 032, Loss: 0.0584, Val Acc: 0.1374, Test Acc: 0.1352\n",
                        "Seed: 43, Epoch: 033, Loss: 0.0582, Val Acc: 0.1382, Test Acc: 0.1351\n",
                        "Seed: 43, Epoch: 034, Loss: 0.0582, Val Acc: 0.1388, Test Acc: 0.1371\n",
                        "Seed: 43, Epoch: 035, Loss: 0.0583, Val Acc: 0.1355, Test Acc: 0.1346\n",
                        "Seed: 43, Epoch: 036, Loss: 0.0581, Val Acc: 0.1395, Test Acc: 0.1361\n",
                        "Seed: 43, Epoch: 037, Loss: 0.0581, Val Acc: 0.1398, Test Acc: 0.1367\n",
                        "Seed: 43, Epoch: 038, Loss: 0.0580, Val Acc: 0.1414, Test Acc: 0.1385\n",
                        "Seed: 43, Epoch: 039, Loss: 0.0580, Val Acc: 0.1409, Test Acc: 0.1384\n",
                        "Seed: 43, Epoch: 040, Loss: 0.0580, Val Acc: 0.1411, Test Acc: 0.1369\n",
                        "Seed: 43, Epoch: 041, Loss: 0.0580, Val Acc: 0.1372, Test Acc: 0.1345\n",
                        "Seed: 43, Epoch: 042, Loss: 0.0580, Val Acc: 0.1424, Test Acc: 0.1404\n",
                        "Seed: 43, Epoch: 043, Loss: 0.0579, Val Acc: 0.1457, Test Acc: 0.1425\n",
                        "Seed: 43, Epoch: 044, Loss: 0.0578, Val Acc: 0.1429, Test Acc: 0.1403\n",
                        "Seed: 43, Epoch: 045, Loss: 0.0579, Val Acc: 0.1470, Test Acc: 0.1450\n",
                        "Seed: 43, Epoch: 046, Loss: 0.0578, Val Acc: 0.1446, Test Acc: 0.1435\n",
                        "Seed: 43, Epoch: 047, Loss: 0.0578, Val Acc: 0.1434, Test Acc: 0.1412\n",
                        "Seed: 43, Epoch: 048, Loss: 0.0578, Val Acc: 0.1492, Test Acc: 0.1466\n",
                        "Seed: 43, Epoch: 049, Loss: 0.0577, Val Acc: 0.1411, Test Acc: 0.1387\n",
                        "Seed: 43, Epoch: 050, Loss: 0.0578, Val Acc: 0.1434, Test Acc: 0.1401\n",
                        "Seed: 43, Epoch: 051, Loss: 0.0578, Val Acc: 0.1467, Test Acc: 0.1461\n",
                        "Seed: 43, Epoch: 052, Loss: 0.0576, Val Acc: 0.1469, Test Acc: 0.1450\n",
                        "Seed: 43, Epoch: 053, Loss: 0.0576, Val Acc: 0.1497, Test Acc: 0.1488\n",
                        "Seed: 43, Epoch: 054, Loss: 0.0576, Val Acc: 0.1459, Test Acc: 0.1444\n",
                        "Seed: 43, Epoch: 055, Loss: 0.0576, Val Acc: 0.1500, Test Acc: 0.1487\n",
                        "Seed: 43, Epoch: 056, Loss: 0.0576, Val Acc: 0.1492, Test Acc: 0.1478\n",
                        "Seed: 43, Epoch: 057, Loss: 0.0575, Val Acc: 0.1491, Test Acc: 0.1473\n",
                        "Seed: 43, Epoch: 058, Loss: 0.0575, Val Acc: 0.1482, Test Acc: 0.1463\n",
                        "Seed: 43, Epoch: 059, Loss: 0.0576, Val Acc: 0.1494, Test Acc: 0.1480\n",
                        "Seed: 43, Epoch: 060, Loss: 0.0575, Val Acc: 0.1506, Test Acc: 0.1483\n",
                        "Seed: 43, Epoch: 061, Loss: 0.0576, Val Acc: 0.1507, Test Acc: 0.1484\n",
                        "Seed: 43, Epoch: 062, Loss: 0.0574, Val Acc: 0.1510, Test Acc: 0.1508\n",
                        "Seed: 43, Epoch: 063, Loss: 0.0574, Val Acc: 0.1535, Test Acc: 0.1515\n",
                        "Seed: 43, Epoch: 064, Loss: 0.0573, Val Acc: 0.1556, Test Acc: 0.1531\n",
                        "Seed: 43, Epoch: 065, Loss: 0.0573, Val Acc: 0.1485, Test Acc: 0.1465\n",
                        "Seed: 43, Epoch: 066, Loss: 0.0576, Val Acc: 0.1509, Test Acc: 0.1485\n",
                        "Seed: 43, Epoch: 067, Loss: 0.0574, Val Acc: 0.1540, Test Acc: 0.1518\n",
                        "Seed: 43, Epoch: 068, Loss: 0.0573, Val Acc: 0.1475, Test Acc: 0.1473\n",
                        "Seed: 43, Epoch: 069, Loss: 0.0575, Val Acc: 0.1530, Test Acc: 0.1513\n",
                        "Seed: 43, Epoch: 070, Loss: 0.0573, Val Acc: 0.1547, Test Acc: 0.1516\n",
                        "Seed: 43, Epoch: 071, Loss: 0.0575, Val Acc: 0.1477, Test Acc: 0.1463\n",
                        "Seed: 43, Epoch: 072, Loss: 0.0575, Val Acc: 0.1523, Test Acc: 0.1498\n",
                        "Seed: 43, Epoch: 073, Loss: 0.0573, Val Acc: 0.1494, Test Acc: 0.1475\n",
                        "Seed: 43, Epoch: 074, Loss: 0.0573, Val Acc: 0.1532, Test Acc: 0.1513\n",
                        "Seed: 43, Epoch: 075, Loss: 0.0578, Val Acc: 0.1367, Test Acc: 0.1360\n",
                        "Seed: 43, Epoch: 076, Loss: 0.0582, Val Acc: 0.1406, Test Acc: 0.1390\n",
                        "Seed: 43, Epoch: 077, Loss: 0.0581, Val Acc: 0.1475, Test Acc: 0.1458\n",
                        "Seed: 43, Epoch: 078, Loss: 0.0580, Val Acc: 0.1396, Test Acc: 0.1377\n",
                        "Seed: 43, Epoch: 079, Loss: 0.0582, Val Acc: 0.1391, Test Acc: 0.1369\n",
                        "Seed: 43, Epoch: 080, Loss: 0.0583, Val Acc: 0.1377, Test Acc: 0.1350\n",
                        "Seed: 43, Epoch: 081, Loss: 0.0584, Val Acc: 0.1348, Test Acc: 0.1338\n",
                        "Seed: 43, Epoch: 082, Loss: 0.0586, Val Acc: 0.1312, Test Acc: 0.1308\n",
                        "Seed: 43, Epoch: 083, Loss: 0.0589, Val Acc: 0.1242, Test Acc: 0.1214\n",
                        "Seed: 43, Epoch: 084, Loss: 0.0587, Val Acc: 0.1376, Test Acc: 0.1358\n",
                        "Seed: 43, Epoch: 085, Loss: 0.0585, Val Acc: 0.1377, Test Acc: 0.1349\n",
                        "Seed: 43, Epoch: 086, Loss: 0.0585, Val Acc: 0.1333, Test Acc: 0.1313\n",
                        "Seed: 43, Epoch: 087, Loss: 0.0585, Val Acc: 0.1375, Test Acc: 0.1360\n",
                        "Seed: 43, Epoch: 088, Loss: 0.0583, Val Acc: 0.1368, Test Acc: 0.1346\n",
                        "Seed: 43, Epoch: 089, Loss: 0.0583, Val Acc: 0.1415, Test Acc: 0.1396\n",
                        "Seed: 43, Epoch: 090, Loss: 0.0581, Val Acc: 0.1396, Test Acc: 0.1374\n",
                        "Seed: 43, Epoch: 091, Loss: 0.0582, Val Acc: 0.1407, Test Acc: 0.1383\n",
                        "Seed: 43, Epoch: 092, Loss: 0.0581, Val Acc: 0.1287, Test Acc: 0.1274\n",
                        "Seed: 43, Epoch: 093, Loss: 0.0583, Val Acc: 0.1376, Test Acc: 0.1355\n",
                        "Seed: 43, Epoch: 094, Loss: 0.0582, Val Acc: 0.1423, Test Acc: 0.1407\n",
                        "Seed: 43, Epoch: 095, Loss: 0.0583, Val Acc: 0.1421, Test Acc: 0.1380\n",
                        "Seed: 43, Epoch: 096, Loss: 0.0583, Val Acc: 0.1320, Test Acc: 0.1307\n",
                        "Seed: 43, Epoch: 097, Loss: 0.0584, Val Acc: 0.1390, Test Acc: 0.1361\n",
                        "Seed: 43, Epoch: 098, Loss: 0.0582, Val Acc: 0.1391, Test Acc: 0.1353\n",
                        "Seed: 43, Epoch: 099, Loss: 0.0583, Val Acc: 0.1393, Test Acc: 0.1370\n",
                        "Seed: 43, Epoch: 100, Loss: 0.0582, Val Acc: 0.1345, Test Acc: 0.1330\n",
                        "Seed: 43, Epoch: 101, Loss: 0.0582, Val Acc: 0.1403, Test Acc: 0.1370\n",
                        "Seed: 43, Epoch: 102, Loss: 0.0583, Val Acc: 0.1403, Test Acc: 0.1386\n",
                        "Seed: 43, Epoch: 103, Loss: 0.0583, Val Acc: 0.1401, Test Acc: 0.1381\n",
                        "Seed: 43, Epoch: 104, Loss: 0.0583, Val Acc: 0.1411, Test Acc: 0.1380\n",
                        "Seed: 43, Epoch: 105, Loss: 0.0582, Val Acc: 0.1394, Test Acc: 0.1379\n",
                        "Seed: 43, Epoch: 106, Loss: 0.0582, Val Acc: 0.1416, Test Acc: 0.1386\n",
                        "Seed: 43, Epoch: 107, Loss: 0.0581, Val Acc: 0.1435, Test Acc: 0.1419\n",
                        "Seed: 43, Epoch: 108, Loss: 0.0580, Val Acc: 0.1398, Test Acc: 0.1400\n",
                        "Seed: 43, Epoch: 109, Loss: 0.0580, Val Acc: 0.1417, Test Acc: 0.1388\n",
                        "Seed: 43, Epoch: 110, Loss: 0.0580, Val Acc: 0.1398, Test Acc: 0.1388\n",
                        "Seed: 43, Epoch: 111, Loss: 0.0580, Val Acc: 0.1451, Test Acc: 0.1441\n",
                        "Seed: 43, Epoch: 112, Loss: 0.0579, Val Acc: 0.1425, Test Acc: 0.1414\n",
                        "Seed: 43, Epoch: 113, Loss: 0.0580, Val Acc: 0.1415, Test Acc: 0.1400\n",
                        "Seed: 43, Epoch: 114, Loss: 0.0580, Val Acc: 0.1424, Test Acc: 0.1406\n",
                        "Early stopping at epoch 114 for seed 43\n",
                        "Seed: 44, Epoch: 001, Loss: 0.0623, Val Acc: 0.1183, Test Acc: 0.1144\n",
                        "Seed: 44, Epoch: 002, Loss: 0.0603, Val Acc: 0.1122, Test Acc: 0.1094\n",
                        "Seed: 44, Epoch: 003, Loss: 0.0601, Val Acc: 0.1160, Test Acc: 0.1134\n",
                        "Seed: 44, Epoch: 004, Loss: 0.0599, Val Acc: 0.1154, Test Acc: 0.1126\n",
                        "Seed: 44, Epoch: 005, Loss: 0.0597, Val Acc: 0.1165, Test Acc: 0.1143\n",
                        "Seed: 44, Epoch: 006, Loss: 0.0597, Val Acc: 0.1176, Test Acc: 0.1153\n",
                        "Seed: 44, Epoch: 007, Loss: 0.0596, Val Acc: 0.1162, Test Acc: 0.1133\n",
                        "Seed: 44, Epoch: 008, Loss: 0.0595, Val Acc: 0.1214, Test Acc: 0.1184\n",
                        "Seed: 44, Epoch: 009, Loss: 0.0595, Val Acc: 0.1207, Test Acc: 0.1181\n",
                        "Seed: 44, Epoch: 010, Loss: 0.0594, Val Acc: 0.1234, Test Acc: 0.1206\n",
                        "Seed: 44, Epoch: 011, Loss: 0.0593, Val Acc: 0.1174, Test Acc: 0.1170\n",
                        "Seed: 44, Epoch: 012, Loss: 0.0593, Val Acc: 0.1212, Test Acc: 0.1200\n",
                        "Seed: 44, Epoch: 013, Loss: 0.0592, Val Acc: 0.1237, Test Acc: 0.1223\n",
                        "Seed: 44, Epoch: 014, Loss: 0.0591, Val Acc: 0.1222, Test Acc: 0.1207\n",
                        "Seed: 44, Epoch: 015, Loss: 0.0590, Val Acc: 0.1264, Test Acc: 0.1243\n",
                        "Seed: 44, Epoch: 016, Loss: 0.0590, Val Acc: 0.1309, Test Acc: 0.1290\n",
                        "Seed: 44, Epoch: 017, Loss: 0.0589, Val Acc: 0.1321, Test Acc: 0.1293\n",
                        "Seed: 44, Epoch: 018, Loss: 0.0588, Val Acc: 0.1339, Test Acc: 0.1322\n",
                        "Seed: 44, Epoch: 019, Loss: 0.0588, Val Acc: 0.1297, Test Acc: 0.1287\n",
                        "Seed: 44, Epoch: 020, Loss: 0.0587, Val Acc: 0.1326, Test Acc: 0.1313\n",
                        "Seed: 44, Epoch: 021, Loss: 0.0586, Val Acc: 0.1324, Test Acc: 0.1303\n",
                        "Seed: 44, Epoch: 022, Loss: 0.0586, Val Acc: 0.1332, Test Acc: 0.1326\n",
                        "Seed: 44, Epoch: 023, Loss: 0.0586, Val Acc: 0.1372, Test Acc: 0.1355\n",
                        "Seed: 44, Epoch: 024, Loss: 0.0585, Val Acc: 0.1347, Test Acc: 0.1337\n",
                        "Seed: 44, Epoch: 025, Loss: 0.0585, Val Acc: 0.1374, Test Acc: 0.1363\n",
                        "Seed: 44, Epoch: 026, Loss: 0.0584, Val Acc: 0.1349, Test Acc: 0.1341\n",
                        "Seed: 44, Epoch: 027, Loss: 0.0583, Val Acc: 0.1402, Test Acc: 0.1398\n",
                        "Seed: 44, Epoch: 028, Loss: 0.0583, Val Acc: 0.1419, Test Acc: 0.1412\n",
                        "Seed: 44, Epoch: 029, Loss: 0.0583, Val Acc: 0.1406, Test Acc: 0.1396\n",
                        "Seed: 44, Epoch: 030, Loss: 0.0582, Val Acc: 0.1399, Test Acc: 0.1390\n",
                        "Seed: 44, Epoch: 031, Loss: 0.0581, Val Acc: 0.1403, Test Acc: 0.1387\n",
                        "Seed: 44, Epoch: 032, Loss: 0.0581, Val Acc: 0.1420, Test Acc: 0.1407\n",
                        "Seed: 44, Epoch: 033, Loss: 0.0581, Val Acc: 0.1436, Test Acc: 0.1421\n",
                        "Seed: 44, Epoch: 034, Loss: 0.0580, Val Acc: 0.1444, Test Acc: 0.1425\n",
                        "Seed: 44, Epoch: 035, Loss: 0.0580, Val Acc: 0.1429, Test Acc: 0.1405\n",
                        "Seed: 44, Epoch: 036, Loss: 0.0581, Val Acc: 0.1437, Test Acc: 0.1409\n",
                        "Seed: 44, Epoch: 037, Loss: 0.0580, Val Acc: 0.1416, Test Acc: 0.1391\n",
                        "Seed: 44, Epoch: 038, Loss: 0.0579, Val Acc: 0.1438, Test Acc: 0.1426\n",
                        "Seed: 44, Epoch: 039, Loss: 0.0579, Val Acc: 0.1430, Test Acc: 0.1401\n",
                        "Seed: 44, Epoch: 040, Loss: 0.0579, Val Acc: 0.1517, Test Acc: 0.1487\n",
                        "Seed: 44, Epoch: 041, Loss: 0.0578, Val Acc: 0.1458, Test Acc: 0.1434\n",
                        "Seed: 44, Epoch: 042, Loss: 0.0578, Val Acc: 0.1443, Test Acc: 0.1425\n",
                        "Seed: 44, Epoch: 043, Loss: 0.0578, Val Acc: 0.1475, Test Acc: 0.1450\n",
                        "Seed: 44, Epoch: 044, Loss: 0.0577, Val Acc: 0.1478, Test Acc: 0.1451\n",
                        "Seed: 44, Epoch: 045, Loss: 0.0577, Val Acc: 0.1497, Test Acc: 0.1476\n",
                        "Seed: 44, Epoch: 046, Loss: 0.0576, Val Acc: 0.1416, Test Acc: 0.1380\n",
                        "Seed: 44, Epoch: 047, Loss: 0.0577, Val Acc: 0.1453, Test Acc: 0.1429\n",
                        "Seed: 44, Epoch: 048, Loss: 0.0577, Val Acc: 0.1509, Test Acc: 0.1481\n",
                        "Seed: 44, Epoch: 049, Loss: 0.0576, Val Acc: 0.1439, Test Acc: 0.1409\n",
                        "Seed: 44, Epoch: 050, Loss: 0.0576, Val Acc: 0.1494, Test Acc: 0.1468\n",
                        "Seed: 44, Epoch: 051, Loss: 0.0576, Val Acc: 0.1465, Test Acc: 0.1438\n",
                        "Seed: 44, Epoch: 052, Loss: 0.0577, Val Acc: 0.1460, Test Acc: 0.1433\n",
                        "Seed: 44, Epoch: 053, Loss: 0.0575, Val Acc: 0.1486, Test Acc: 0.1458\n",
                        "Seed: 44, Epoch: 054, Loss: 0.0576, Val Acc: 0.1531, Test Acc: 0.1500\n",
                        "Seed: 44, Epoch: 055, Loss: 0.0575, Val Acc: 0.1521, Test Acc: 0.1474\n",
                        "Seed: 44, Epoch: 056, Loss: 0.0575, Val Acc: 0.1510, Test Acc: 0.1475\n",
                        "Seed: 44, Epoch: 057, Loss: 0.0577, Val Acc: 0.1522, Test Acc: 0.1499\n",
                        "Seed: 44, Epoch: 058, Loss: 0.0575, Val Acc: 0.1490, Test Acc: 0.1461\n",
                        "Seed: 44, Epoch: 059, Loss: 0.0575, Val Acc: 0.1518, Test Acc: 0.1471\n",
                        "Seed: 44, Epoch: 060, Loss: 0.0576, Val Acc: 0.1485, Test Acc: 0.1451\n",
                        "Seed: 44, Epoch: 061, Loss: 0.0576, Val Acc: 0.1482, Test Acc: 0.1443\n",
                        "Seed: 44, Epoch: 062, Loss: 0.0577, Val Acc: 0.1490, Test Acc: 0.1455\n",
                        "Seed: 44, Epoch: 063, Loss: 0.0574, Val Acc: 0.1540, Test Acc: 0.1502\n",
                        "Seed: 44, Epoch: 064, Loss: 0.0574, Val Acc: 0.1514, Test Acc: 0.1478\n",
                        "Seed: 44, Epoch: 065, Loss: 0.0573, Val Acc: 0.1537, Test Acc: 0.1490\n",
                        "Seed: 44, Epoch: 066, Loss: 0.0573, Val Acc: 0.1512, Test Acc: 0.1469\n",
                        "Seed: 44, Epoch: 067, Loss: 0.0574, Val Acc: 0.1502, Test Acc: 0.1470\n",
                        "Seed: 44, Epoch: 068, Loss: 0.0573, Val Acc: 0.1563, Test Acc: 0.1527\n",
                        "Seed: 44, Epoch: 069, Loss: 0.0573, Val Acc: 0.1532, Test Acc: 0.1496\n",
                        "Seed: 44, Epoch: 070, Loss: 0.0573, Val Acc: 0.1515, Test Acc: 0.1488\n",
                        "Seed: 44, Epoch: 071, Loss: 0.0572, Val Acc: 0.1533, Test Acc: 0.1498\n",
                        "Seed: 44, Epoch: 072, Loss: 0.0573, Val Acc: 0.1535, Test Acc: 0.1506\n",
                        "Seed: 44, Epoch: 073, Loss: 0.0572, Val Acc: 0.1503, Test Acc: 0.1487\n",
                        "Seed: 44, Epoch: 074, Loss: 0.0572, Val Acc: 0.1536, Test Acc: 0.1496\n",
                        "Seed: 44, Epoch: 075, Loss: 0.0573, Val Acc: 0.1522, Test Acc: 0.1508\n",
                        "Seed: 44, Epoch: 076, Loss: 0.0572, Val Acc: 0.1543, Test Acc: 0.1516\n",
                        "Seed: 44, Epoch: 077, Loss: 0.0572, Val Acc: 0.1511, Test Acc: 0.1483\n",
                        "Seed: 44, Epoch: 078, Loss: 0.0572, Val Acc: 0.1522, Test Acc: 0.1487\n",
                        "Seed: 44, Epoch: 079, Loss: 0.0572, Val Acc: 0.1502, Test Acc: 0.1475\n",
                        "Seed: 44, Epoch: 080, Loss: 0.0572, Val Acc: 0.1520, Test Acc: 0.1500\n",
                        "Seed: 44, Epoch: 081, Loss: 0.0572, Val Acc: 0.1554, Test Acc: 0.1523\n",
                        "Seed: 44, Epoch: 082, Loss: 0.0572, Val Acc: 0.1543, Test Acc: 0.1505\n",
                        "Seed: 44, Epoch: 083, Loss: 0.0571, Val Acc: 0.1524, Test Acc: 0.1502\n",
                        "Seed: 44, Epoch: 084, Loss: 0.0572, Val Acc: 0.1542, Test Acc: 0.1505\n",
                        "Seed: 44, Epoch: 085, Loss: 0.0571, Val Acc: 0.1519, Test Acc: 0.1490\n",
                        "Seed: 44, Epoch: 086, Loss: 0.0571, Val Acc: 0.1537, Test Acc: 0.1503\n",
                        "Seed: 44, Epoch: 087, Loss: 0.0572, Val Acc: 0.1501, Test Acc: 0.1492\n",
                        "Seed: 44, Epoch: 088, Loss: 0.0572, Val Acc: 0.1501, Test Acc: 0.1466\n",
                        "Seed: 44, Epoch: 089, Loss: 0.0573, Val Acc: 0.1527, Test Acc: 0.1504\n",
                        "Seed: 44, Epoch: 090, Loss: 0.0571, Val Acc: 0.1547, Test Acc: 0.1526\n",
                        "Seed: 44, Epoch: 091, Loss: 0.0572, Val Acc: 0.1547, Test Acc: 0.1530\n",
                        "Seed: 44, Epoch: 092, Loss: 0.0573, Val Acc: 0.1511, Test Acc: 0.1481\n",
                        "Seed: 44, Epoch: 093, Loss: 0.0571, Val Acc: 0.1518, Test Acc: 0.1494\n",
                        "Seed: 44, Epoch: 094, Loss: 0.0572, Val Acc: 0.1512, Test Acc: 0.1478\n",
                        "Seed: 44, Epoch: 095, Loss: 0.0572, Val Acc: 0.1544, Test Acc: 0.1517\n",
                        "Seed: 44, Epoch: 096, Loss: 0.0572, Val Acc: 0.1519, Test Acc: 0.1478\n",
                        "Seed: 44, Epoch: 097, Loss: 0.0572, Val Acc: 0.1524, Test Acc: 0.1511\n",
                        "Seed: 44, Epoch: 098, Loss: 0.0571, Val Acc: 0.1541, Test Acc: 0.1513\n",
                        "Seed: 44, Epoch: 099, Loss: 0.0571, Val Acc: 0.1489, Test Acc: 0.1477\n",
                        "Seed: 44, Epoch: 100, Loss: 0.0572, Val Acc: 0.1526, Test Acc: 0.1491\n",
                        "Seed: 44, Epoch: 101, Loss: 0.0572, Val Acc: 0.1541, Test Acc: 0.1521\n",
                        "Seed: 44, Epoch: 102, Loss: 0.0572, Val Acc: 0.1513, Test Acc: 0.1504\n",
                        "Seed: 44, Epoch: 103, Loss: 0.0571, Val Acc: 0.1550, Test Acc: 0.1517\n",
                        "Seed: 44, Epoch: 104, Loss: 0.0571, Val Acc: 0.1560, Test Acc: 0.1530\n",
                        "Seed: 44, Epoch: 105, Loss: 0.0570, Val Acc: 0.1545, Test Acc: 0.1509\n",
                        "Seed: 44, Epoch: 106, Loss: 0.0572, Val Acc: 0.1573, Test Acc: 0.1536\n",
                        "Seed: 44, Epoch: 107, Loss: 0.0572, Val Acc: 0.1549, Test Acc: 0.1539\n",
                        "Seed: 44, Epoch: 108, Loss: 0.0572, Val Acc: 0.1559, Test Acc: 0.1525\n",
                        "Seed: 44, Epoch: 109, Loss: 0.0570, Val Acc: 0.1544, Test Acc: 0.1520\n",
                        "Seed: 44, Epoch: 110, Loss: 0.0570, Val Acc: 0.1562, Test Acc: 0.1530\n",
                        "Seed: 44, Epoch: 111, Loss: 0.0570, Val Acc: 0.1574, Test Acc: 0.1548\n",
                        "Seed: 44, Epoch: 112, Loss: 0.0571, Val Acc: 0.1556, Test Acc: 0.1523\n",
                        "Seed: 44, Epoch: 113, Loss: 0.0571, Val Acc: 0.1567, Test Acc: 0.1541\n",
                        "Seed: 44, Epoch: 114, Loss: 0.0570, Val Acc: 0.1533, Test Acc: 0.1498\n",
                        "Seed: 44, Epoch: 115, Loss: 0.0571, Val Acc: 0.1519, Test Acc: 0.1487\n",
                        "Seed: 44, Epoch: 116, Loss: 0.0570, Val Acc: 0.1584, Test Acc: 0.1550\n",
                        "Seed: 44, Epoch: 117, Loss: 0.0570, Val Acc: 0.1531, Test Acc: 0.1510\n",
                        "Seed: 44, Epoch: 118, Loss: 0.0570, Val Acc: 0.1548, Test Acc: 0.1516\n",
                        "Seed: 44, Epoch: 119, Loss: 0.0570, Val Acc: 0.1473, Test Acc: 0.1444\n",
                        "Seed: 44, Epoch: 120, Loss: 0.0571, Val Acc: 0.1482, Test Acc: 0.1443\n",
                        "Seed: 44, Epoch: 121, Loss: 0.0577, Val Acc: 0.1552, Test Acc: 0.1508\n",
                        "Seed: 44, Epoch: 122, Loss: 0.0570, Val Acc: 0.1561, Test Acc: 0.1532\n",
                        "Seed: 44, Epoch: 123, Loss: 0.0570, Val Acc: 0.1534, Test Acc: 0.1505\n",
                        "Seed: 44, Epoch: 124, Loss: 0.0570, Val Acc: 0.1501, Test Acc: 0.1488\n",
                        "Seed: 44, Epoch: 125, Loss: 0.0571, Val Acc: 0.1519, Test Acc: 0.1494\n",
                        "Seed: 44, Epoch: 126, Loss: 0.0570, Val Acc: 0.1561, Test Acc: 0.1529\n",
                        "Seed: 44, Epoch: 127, Loss: 0.0569, Val Acc: 0.1576, Test Acc: 0.1556\n",
                        "Seed: 44, Epoch: 128, Loss: 0.0569, Val Acc: 0.1538, Test Acc: 0.1516\n",
                        "Seed: 44, Epoch: 129, Loss: 0.0571, Val Acc: 0.1516, Test Acc: 0.1478\n",
                        "Seed: 44, Epoch: 130, Loss: 0.0575, Val Acc: 0.1520, Test Acc: 0.1485\n",
                        "Seed: 44, Epoch: 131, Loss: 0.0572, Val Acc: 0.1462, Test Acc: 0.1445\n",
                        "Seed: 44, Epoch: 132, Loss: 0.0573, Val Acc: 0.1544, Test Acc: 0.1535\n",
                        "Seed: 44, Epoch: 133, Loss: 0.0572, Val Acc: 0.1506, Test Acc: 0.1480\n",
                        "Seed: 44, Epoch: 134, Loss: 0.0570, Val Acc: 0.1529, Test Acc: 0.1507\n",
                        "Seed: 44, Epoch: 135, Loss: 0.0571, Val Acc: 0.1529, Test Acc: 0.1519\n",
                        "Seed: 44, Epoch: 136, Loss: 0.0569, Val Acc: 0.1555, Test Acc: 0.1527\n",
                        "Seed: 44, Epoch: 137, Loss: 0.0569, Val Acc: 0.1527, Test Acc: 0.1493\n",
                        "Seed: 44, Epoch: 138, Loss: 0.0570, Val Acc: 0.1520, Test Acc: 0.1492\n",
                        "Seed: 44, Epoch: 139, Loss: 0.0570, Val Acc: 0.1504, Test Acc: 0.1472\n",
                        "Seed: 44, Epoch: 140, Loss: 0.0572, Val Acc: 0.1539, Test Acc: 0.1507\n",
                        "Seed: 44, Epoch: 141, Loss: 0.0569, Val Acc: 0.1531, Test Acc: 0.1501\n",
                        "Seed: 44, Epoch: 142, Loss: 0.0569, Val Acc: 0.1574, Test Acc: 0.1533\n",
                        "Seed: 44, Epoch: 143, Loss: 0.0569, Val Acc: 0.1522, Test Acc: 0.1498\n",
                        "Seed: 44, Epoch: 144, Loss: 0.0569, Val Acc: 0.1552, Test Acc: 0.1523\n",
                        "Seed: 44, Epoch: 145, Loss: 0.0569, Val Acc: 0.1550, Test Acc: 0.1532\n",
                        "Seed: 44, Epoch: 146, Loss: 0.0569, Val Acc: 0.1531, Test Acc: 0.1498\n",
                        "Seed: 44, Epoch: 147, Loss: 0.0569, Val Acc: 0.1565, Test Acc: 0.1522\n",
                        "Seed: 44, Epoch: 148, Loss: 0.0570, Val Acc: 0.1493, Test Acc: 0.1464\n",
                        "Seed: 44, Epoch: 149, Loss: 0.0569, Val Acc: 0.1529, Test Acc: 0.1504\n",
                        "Seed: 44, Epoch: 150, Loss: 0.0568, Val Acc: 0.1550, Test Acc: 0.1524\n",
                        "Seed: 44, Epoch: 151, Loss: 0.0569, Val Acc: 0.1548, Test Acc: 0.1515\n",
                        "Seed: 44, Epoch: 152, Loss: 0.0569, Val Acc: 0.1546, Test Acc: 0.1508\n",
                        "Seed: 44, Epoch: 153, Loss: 0.0571, Val Acc: 0.1540, Test Acc: 0.1524\n",
                        "Seed: 44, Epoch: 154, Loss: 0.0569, Val Acc: 0.1559, Test Acc: 0.1535\n",
                        "Seed: 44, Epoch: 155, Loss: 0.0568, Val Acc: 0.1580, Test Acc: 0.1547\n",
                        "Seed: 44, Epoch: 156, Loss: 0.0568, Val Acc: 0.1590, Test Acc: 0.1555\n",
                        "Seed: 44, Epoch: 157, Loss: 0.0567, Val Acc: 0.1532, Test Acc: 0.1509\n",
                        "Seed: 44, Epoch: 158, Loss: 0.0568, Val Acc: 0.1588, Test Acc: 0.1567\n",
                        "Seed: 44, Epoch: 159, Loss: 0.0567, Val Acc: 0.1587, Test Acc: 0.1545\n",
                        "Seed: 44, Epoch: 160, Loss: 0.0567, Val Acc: 0.1516, Test Acc: 0.1478\n",
                        "Seed: 44, Epoch: 161, Loss: 0.0569, Val Acc: 0.1590, Test Acc: 0.1565\n",
                        "Seed: 44, Epoch: 162, Loss: 0.0571, Val Acc: 0.1475, Test Acc: 0.1426\n",
                        "Seed: 44, Epoch: 163, Loss: 0.0574, Val Acc: 0.1513, Test Acc: 0.1475\n",
                        "Seed: 44, Epoch: 164, Loss: 0.0567, Val Acc: 0.1560, Test Acc: 0.1529\n",
                        "Seed: 44, Epoch: 165, Loss: 0.0567, Val Acc: 0.1600, Test Acc: 0.1567\n",
                        "Seed: 44, Epoch: 166, Loss: 0.0567, Val Acc: 0.1580, Test Acc: 0.1547\n",
                        "Seed: 44, Epoch: 167, Loss: 0.0567, Val Acc: 0.1553, Test Acc: 0.1516\n",
                        "Seed: 44, Epoch: 168, Loss: 0.0568, Val Acc: 0.1588, Test Acc: 0.1544\n",
                        "Seed: 44, Epoch: 169, Loss: 0.0567, Val Acc: 0.1575, Test Acc: 0.1537\n",
                        "Seed: 44, Epoch: 170, Loss: 0.0567, Val Acc: 0.1609, Test Acc: 0.1561\n",
                        "Seed: 44, Epoch: 171, Loss: 0.0566, Val Acc: 0.1570, Test Acc: 0.1527\n",
                        "Seed: 44, Epoch: 172, Loss: 0.0566, Val Acc: 0.1559, Test Acc: 0.1519\n",
                        "Seed: 44, Epoch: 173, Loss: 0.0567, Val Acc: 0.1599, Test Acc: 0.1548\n",
                        "Seed: 44, Epoch: 174, Loss: 0.0566, Val Acc: 0.1571, Test Acc: 0.1535\n",
                        "Seed: 44, Epoch: 175, Loss: 0.0566, Val Acc: 0.1615, Test Acc: 0.1579\n",
                        "Seed: 44, Epoch: 176, Loss: 0.0567, Val Acc: 0.1585, Test Acc: 0.1543\n",
                        "Seed: 44, Epoch: 177, Loss: 0.0567, Val Acc: 0.1556, Test Acc: 0.1532\n",
                        "Seed: 44, Epoch: 178, Loss: 0.0566, Val Acc: 0.1592, Test Acc: 0.1554\n",
                        "Seed: 44, Epoch: 179, Loss: 0.0565, Val Acc: 0.1609, Test Acc: 0.1576\n",
                        "Seed: 44, Epoch: 180, Loss: 0.0566, Val Acc: 0.1590, Test Acc: 0.1543\n",
                        "Seed: 44, Epoch: 181, Loss: 0.0566, Val Acc: 0.1561, Test Acc: 0.1523\n",
                        "Seed: 44, Epoch: 182, Loss: 0.0566, Val Acc: 0.1604, Test Acc: 0.1562\n",
                        "Seed: 44, Epoch: 183, Loss: 0.0566, Val Acc: 0.1604, Test Acc: 0.1556\n",
                        "Seed: 44, Epoch: 184, Loss: 0.0566, Val Acc: 0.1569, Test Acc: 0.1530\n",
                        "Seed: 44, Epoch: 185, Loss: 0.0565, Val Acc: 0.1580, Test Acc: 0.1541\n",
                        "Seed: 44, Epoch: 186, Loss: 0.0567, Val Acc: 0.1595, Test Acc: 0.1538\n",
                        "Seed: 44, Epoch: 187, Loss: 0.0566, Val Acc: 0.1618, Test Acc: 0.1571\n",
                        "Seed: 44, Epoch: 188, Loss: 0.0565, Val Acc: 0.1547, Test Acc: 0.1510\n",
                        "Seed: 44, Epoch: 189, Loss: 0.0567, Val Acc: 0.1619, Test Acc: 0.1574\n",
                        "Seed: 44, Epoch: 190, Loss: 0.0566, Val Acc: 0.1619, Test Acc: 0.1589\n",
                        "Seed: 44, Epoch: 191, Loss: 0.0568, Val Acc: 0.1576, Test Acc: 0.1537\n",
                        "Seed: 44, Epoch: 192, Loss: 0.0566, Val Acc: 0.1620, Test Acc: 0.1560\n",
                        "Seed: 44, Epoch: 193, Loss: 0.0565, Val Acc: 0.1614, Test Acc: 0.1579\n",
                        "Seed: 44, Epoch: 194, Loss: 0.0566, Val Acc: 0.1613, Test Acc: 0.1559\n",
                        "Seed: 44, Epoch: 195, Loss: 0.0564, Val Acc: 0.1598, Test Acc: 0.1566\n",
                        "Seed: 44, Epoch: 196, Loss: 0.0565, Val Acc: 0.1596, Test Acc: 0.1536\n",
                        "Seed: 44, Epoch: 197, Loss: 0.0565, Val Acc: 0.1616, Test Acc: 0.1582\n",
                        "Seed: 44, Epoch: 198, Loss: 0.0565, Val Acc: 0.1615, Test Acc: 0.1581\n",
                        "Seed: 44, Epoch: 199, Loss: 0.0568, Val Acc: 0.1553, Test Acc: 0.1509\n",
                        "Seed: 44, Epoch: 200, Loss: 0.0569, Val Acc: 0.1553, Test Acc: 0.1494\n",
                        "Average Time: 5846.92 seconds\n",
                        "Var Time: 1891782.85 seconds\n",
                        "Average Memory: 4724.67 MB\n",
                        "Average Best Val AP: 0.1598\n",
                        "Std Best Test AP: 0.0042\n",
                        "Average Test AP: 0.1579\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nclass HierarchicalGCN_TOPK(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_TOPK, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, improved = True)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = TopKPooling(hidden_channels, ratio=0.3)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, improved = True)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = TopKPooling(hidden_channels, ratio=0.3)\n        self.conv3 = GCNConv(hidden_channels, out_channels, improved = True)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        init_x = data.x\n        x = x.float()\n        init_x = init_x.float()\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, edge_attr, batch, _, _ = self.pool1(x, edge_index, edge_attr, batch=batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, edge_attr, batch, _, _ = self.pool2(x, edge_index, edge_attr, batch=batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=4096, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False)\n    model = HierarchicalGCN_TOPK(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SAGPooling with HierarchicalGCN (2019)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0647, Val AP: 0.1107, Test AP: 0.1148\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0610, Val AP: 0.1158, Test AP: 0.1187\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0604, Val AP: 0.1163, Test AP: 0.1196\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0603, Val AP: 0.1176, Test AP: 0.1204\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0603, Val AP: 0.1188, Test AP: 0.1197\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0601, Val AP: 0.1184, Test AP: 0.1203\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0599, Val AP: 0.1195, Test AP: 0.1213\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0596, Val AP: 0.1210, Test AP: 0.1232\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0596, Val AP: 0.1219, Test AP: 0.1241\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0597, Val AP: 0.1224, Test AP: 0.1249\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0596, Val AP: 0.1220, Test AP: 0.1245\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0595, Val AP: 0.1244, Test AP: 0.1270\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0594, Val AP: 0.1218, Test AP: 0.1245\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0593, Val AP: 0.1244, Test AP: 0.1277\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0592, Val AP: 0.1214, Test AP: 0.1243\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0594, Val AP: 0.1266, Test AP: 0.1297\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0591, Val AP: 0.1246, Test AP: 0.1277\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0592, Val AP: 0.1235, Test AP: 0.1254\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0591, Val AP: 0.1280, Test AP: 0.1302\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0589, Val AP: 0.1273, Test AP: 0.1298\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0589, Val AP: 0.1277, Test AP: 0.1296\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0589, Val AP: 0.1297, Test AP: 0.1315\n",
                        "Seed: 42, Epoch: 023, Loss: 0.0588, Val AP: 0.1308, Test AP: 0.1339\n",
                        "Seed: 42, Epoch: 024, Loss: 0.0588, Val AP: 0.1329, Test AP: 0.1354\n",
                        "Seed: 42, Epoch: 025, Loss: 0.0589, Val AP: 0.1276, Test AP: 0.1304\n",
                        "Seed: 42, Epoch: 026, Loss: 0.0588, Val AP: 0.1325, Test AP: 0.1344\n",
                        "Seed: 42, Epoch: 027, Loss: 0.0588, Val AP: 0.1248, Test AP: 0.1265\n",
                        "Seed: 42, Epoch: 028, Loss: 0.0588, Val AP: 0.1282, Test AP: 0.1307\n",
                        "Seed: 42, Epoch: 029, Loss: 0.0588, Val AP: 0.1275, Test AP: 0.1295\n",
                        "Seed: 42, Epoch: 030, Loss: 0.0587, Val AP: 0.1312, Test AP: 0.1330\n",
                        "Seed: 42, Epoch: 031, Loss: 0.0585, Val AP: 0.1375, Test AP: 0.1398\n",
                        "Seed: 42, Epoch: 032, Loss: 0.0585, Val AP: 0.1421, Test AP: 0.1441\n",
                        "Seed: 42, Epoch: 033, Loss: 0.0583, Val AP: 0.1388, Test AP: 0.1415\n",
                        "Seed: 42, Epoch: 034, Loss: 0.0583, Val AP: 0.1399, Test AP: 0.1419\n",
                        "Seed: 42, Epoch: 035, Loss: 0.0581, Val AP: 0.1434, Test AP: 0.1458\n",
                        "Seed: 42, Epoch: 036, Loss: 0.0579, Val AP: 0.1437, Test AP: 0.1461\n",
                        "Seed: 42, Epoch: 037, Loss: 0.0579, Val AP: 0.1460, Test AP: 0.1487\n",
                        "Seed: 42, Epoch: 038, Loss: 0.0577, Val AP: 0.1478, Test AP: 0.1496\n",
                        "Seed: 42, Epoch: 039, Loss: 0.0576, Val AP: 0.1470, Test AP: 0.1492\n",
                        "Seed: 42, Epoch: 040, Loss: 0.0576, Val AP: 0.1499, Test AP: 0.1515\n",
                        "Seed: 42, Epoch: 041, Loss: 0.0575, Val AP: 0.1503, Test AP: 0.1519\n",
                        "Seed: 42, Epoch: 042, Loss: 0.0575, Val AP: 0.1528, Test AP: 0.1555\n",
                        "Seed: 42, Epoch: 043, Loss: 0.0574, Val AP: 0.1523, Test AP: 0.1543\n",
                        "Seed: 42, Epoch: 044, Loss: 0.0573, Val AP: 0.1516, Test AP: 0.1527\n",
                        "Seed: 42, Epoch: 045, Loss: 0.0572, Val AP: 0.1563, Test AP: 0.1589\n",
                        "Seed: 42, Epoch: 046, Loss: 0.0572, Val AP: 0.1533, Test AP: 0.1540\n",
                        "Seed: 42, Epoch: 047, Loss: 0.0572, Val AP: 0.1575, Test AP: 0.1585\n",
                        "Seed: 42, Epoch: 048, Loss: 0.0571, Val AP: 0.1558, Test AP: 0.1580\n",
                        "Seed: 42, Epoch: 049, Loss: 0.0570, Val AP: 0.1556, Test AP: 0.1578\n",
                        "Seed: 42, Epoch: 050, Loss: 0.0569, Val AP: 0.1606, Test AP: 0.1622\n",
                        "Seed: 42, Epoch: 051, Loss: 0.0568, Val AP: 0.1613, Test AP: 0.1628\n",
                        "Seed: 42, Epoch: 052, Loss: 0.0568, Val AP: 0.1633, Test AP: 0.1646\n",
                        "Seed: 42, Epoch: 053, Loss: 0.0568, Val AP: 0.1574, Test AP: 0.1591\n",
                        "Seed: 42, Epoch: 054, Loss: 0.0568, Val AP: 0.1589, Test AP: 0.1595\n",
                        "Seed: 42, Epoch: 055, Loss: 0.0567, Val AP: 0.1693, Test AP: 0.1704\n",
                        "Seed: 42, Epoch: 056, Loss: 0.0566, Val AP: 0.1627, Test AP: 0.1631\n",
                        "Seed: 42, Epoch: 057, Loss: 0.0566, Val AP: 0.1638, Test AP: 0.1650\n",
                        "Seed: 42, Epoch: 058, Loss: 0.0566, Val AP: 0.1663, Test AP: 0.1669\n",
                        "Seed: 42, Epoch: 059, Loss: 0.0564, Val AP: 0.1619, Test AP: 0.1625\n",
                        "Seed: 42, Epoch: 060, Loss: 0.0564, Val AP: 0.1678, Test AP: 0.1684\n",
                        "Seed: 42, Epoch: 061, Loss: 0.0564, Val AP: 0.1701, Test AP: 0.1715\n",
                        "Seed: 42, Epoch: 062, Loss: 0.0564, Val AP: 0.1689, Test AP: 0.1695\n",
                        "Seed: 42, Epoch: 063, Loss: 0.0564, Val AP: 0.1674, Test AP: 0.1689\n",
                        "Seed: 42, Epoch: 064, Loss: 0.0564, Val AP: 0.1612, Test AP: 0.1616\n",
                        "Seed: 42, Epoch: 065, Loss: 0.0563, Val AP: 0.1684, Test AP: 0.1688\n",
                        "Seed: 42, Epoch: 066, Loss: 0.0562, Val AP: 0.1680, Test AP: 0.1681\n",
                        "Seed: 42, Epoch: 067, Loss: 0.0562, Val AP: 0.1723, Test AP: 0.1732\n",
                        "Seed: 42, Epoch: 068, Loss: 0.0560, Val AP: 0.1766, Test AP: 0.1766\n",
                        "Seed: 42, Epoch: 069, Loss: 0.0560, Val AP: 0.1730, Test AP: 0.1730\n",
                        "Seed: 42, Epoch: 070, Loss: 0.0561, Val AP: 0.1759, Test AP: 0.1765\n",
                        "Seed: 42, Epoch: 071, Loss: 0.0560, Val AP: 0.1666, Test AP: 0.1671\n",
                        "Seed: 42, Epoch: 072, Loss: 0.0560, Val AP: 0.1718, Test AP: 0.1724\n",
                        "Seed: 42, Epoch: 073, Loss: 0.0559, Val AP: 0.1706, Test AP: 0.1687\n",
                        "Seed: 42, Epoch: 074, Loss: 0.0560, Val AP: 0.1613, Test AP: 0.1613\n",
                        "Seed: 42, Epoch: 075, Loss: 0.0559, Val AP: 0.1770, Test AP: 0.1755\n",
                        "Seed: 42, Epoch: 076, Loss: 0.0558, Val AP: 0.1627, Test AP: 0.1629\n",
                        "Seed: 42, Epoch: 077, Loss: 0.0557, Val AP: 0.1745, Test AP: 0.1734\n",
                        "Seed: 42, Epoch: 078, Loss: 0.0558, Val AP: 0.1740, Test AP: 0.1738\n",
                        "Seed: 42, Epoch: 079, Loss: 0.0558, Val AP: 0.1790, Test AP: 0.1783\n",
                        "Seed: 42, Epoch: 080, Loss: 0.0557, Val AP: 0.1745, Test AP: 0.1732\n",
                        "Seed: 42, Epoch: 081, Loss: 0.0557, Val AP: 0.1774, Test AP: 0.1758\n",
                        "Seed: 42, Epoch: 082, Loss: 0.0556, Val AP: 0.1820, Test AP: 0.1808\n",
                        "Seed: 42, Epoch: 083, Loss: 0.0555, Val AP: 0.1794, Test AP: 0.1766\n",
                        "Seed: 42, Epoch: 084, Loss: 0.0556, Val AP: 0.1762, Test AP: 0.1758\n",
                        "Seed: 42, Epoch: 085, Loss: 0.0556, Val AP: 0.1701, Test AP: 0.1686\n",
                        "Seed: 42, Epoch: 086, Loss: 0.0555, Val AP: 0.1807, Test AP: 0.1798\n",
                        "Seed: 42, Epoch: 087, Loss: 0.0554, Val AP: 0.1767, Test AP: 0.1743\n",
                        "Seed: 42, Epoch: 088, Loss: 0.0555, Val AP: 0.1812, Test AP: 0.1793\n",
                        "Seed: 42, Epoch: 089, Loss: 0.0554, Val AP: 0.1772, Test AP: 0.1761\n",
                        "Seed: 42, Epoch: 090, Loss: 0.0554, Val AP: 0.1837, Test AP: 0.1813\n",
                        "Seed: 42, Epoch: 091, Loss: 0.0553, Val AP: 0.1889, Test AP: 0.1854\n",
                        "Seed: 42, Epoch: 092, Loss: 0.0553, Val AP: 0.1826, Test AP: 0.1804\n",
                        "Seed: 42, Epoch: 093, Loss: 0.0552, Val AP: 0.1840, Test AP: 0.1824\n",
                        "Seed: 42, Epoch: 094, Loss: 0.0552, Val AP: 0.1862, Test AP: 0.1838\n",
                        "Seed: 42, Epoch: 095, Loss: 0.0552, Val AP: 0.1854, Test AP: 0.1815\n",
                        "Seed: 42, Epoch: 096, Loss: 0.0554, Val AP: 0.1890, Test AP: 0.1861\n",
                        "Seed: 42, Epoch: 097, Loss: 0.0552, Val AP: 0.1921, Test AP: 0.1889\n",
                        "Seed: 42, Epoch: 098, Loss: 0.0551, Val AP: 0.1900, Test AP: 0.1875\n",
                        "Seed: 42, Epoch: 099, Loss: 0.0550, Val AP: 0.1884, Test AP: 0.1860\n",
                        "Seed: 42, Epoch: 100, Loss: 0.0551, Val AP: 0.1900, Test AP: 0.1865\n",
                        "Seed: 42, Epoch: 101, Loss: 0.0550, Val AP: 0.1898, Test AP: 0.1875\n",
                        "Seed: 42, Epoch: 102, Loss: 0.0550, Val AP: 0.1902, Test AP: 0.1870\n",
                        "Seed: 42, Epoch: 103, Loss: 0.0550, Val AP: 0.1861, Test AP: 0.1833\n",
                        "Seed: 42, Epoch: 104, Loss: 0.0549, Val AP: 0.1885, Test AP: 0.1872\n",
                        "Seed: 42, Epoch: 105, Loss: 0.0549, Val AP: 0.1885, Test AP: 0.1878\n",
                        "Seed: 42, Epoch: 106, Loss: 0.0549, Val AP: 0.1913, Test AP: 0.1892\n",
                        "Seed: 42, Epoch: 107, Loss: 0.0549, Val AP: 0.1892, Test AP: 0.1881\n",
                        "Seed: 42, Epoch: 108, Loss: 0.0549, Val AP: 0.1946, Test AP: 0.1933\n",
                        "Seed: 42, Epoch: 109, Loss: 0.0550, Val AP: 0.1879, Test AP: 0.1861\n",
                        "Seed: 42, Epoch: 110, Loss: 0.0548, Val AP: 0.1891, Test AP: 0.1897\n",
                        "Seed: 42, Epoch: 111, Loss: 0.0548, Val AP: 0.1948, Test AP: 0.1922\n",
                        "Seed: 42, Epoch: 112, Loss: 0.0548, Val AP: 0.1917, Test AP: 0.1900\n",
                        "Seed: 42, Epoch: 113, Loss: 0.0548, Val AP: 0.1924, Test AP: 0.1905\n",
                        "Seed: 42, Epoch: 114, Loss: 0.0548, Val AP: 0.1897, Test AP: 0.1886\n",
                        "Seed: 42, Epoch: 115, Loss: 0.0547, Val AP: 0.1937, Test AP: 0.1926\n",
                        "Seed: 42, Epoch: 116, Loss: 0.0547, Val AP: 0.1947, Test AP: 0.1938\n",
                        "Seed: 42, Epoch: 117, Loss: 0.0547, Val AP: 0.1913, Test AP: 0.1905\n",
                        "Seed: 42, Epoch: 118, Loss: 0.0547, Val AP: 0.1893, Test AP: 0.1881\n",
                        "Seed: 42, Epoch: 119, Loss: 0.0548, Val AP: 0.1893, Test AP: 0.1885\n",
                        "Seed: 42, Epoch: 120, Loss: 0.0546, Val AP: 0.1969, Test AP: 0.1950\n",
                        "Seed: 42, Epoch: 121, Loss: 0.0547, Val AP: 0.1935, Test AP: 0.1934\n",
                        "Seed: 42, Epoch: 122, Loss: 0.0546, Val AP: 0.1850, Test AP: 0.1843\n",
                        "Seed: 42, Epoch: 123, Loss: 0.0546, Val AP: 0.1919, Test AP: 0.1908\n",
                        "Seed: 42, Epoch: 124, Loss: 0.0546, Val AP: 0.1983, Test AP: 0.1977\n",
                        "Seed: 42, Epoch: 125, Loss: 0.0546, Val AP: 0.1919, Test AP: 0.1915\n",
                        "Seed: 42, Epoch: 126, Loss: 0.0546, Val AP: 0.1937, Test AP: 0.1935\n",
                        "Seed: 42, Epoch: 127, Loss: 0.0546, Val AP: 0.1914, Test AP: 0.1897\n",
                        "Seed: 42, Epoch: 128, Loss: 0.0546, Val AP: 0.1928, Test AP: 0.1911\n",
                        "Seed: 42, Epoch: 129, Loss: 0.0546, Val AP: 0.1948, Test AP: 0.1938\n",
                        "Seed: 42, Epoch: 130, Loss: 0.0545, Val AP: 0.1948, Test AP: 0.1930\n",
                        "Seed: 42, Epoch: 131, Loss: 0.0544, Val AP: 0.1958, Test AP: 0.1962\n",
                        "Seed: 42, Epoch: 132, Loss: 0.0545, Val AP: 0.1934, Test AP: 0.1929\n",
                        "Seed: 42, Epoch: 133, Loss: 0.0545, Val AP: 0.1969, Test AP: 0.1961\n",
                        "Seed: 42, Epoch: 134, Loss: 0.0544, Val AP: 0.1925, Test AP: 0.1916\n",
                        "Seed: 42, Epoch: 135, Loss: 0.0544, Val AP: 0.1939, Test AP: 0.1942\n",
                        "Seed: 42, Epoch: 136, Loss: 0.0543, Val AP: 0.1914, Test AP: 0.1907\n",
                        "Seed: 42, Epoch: 137, Loss: 0.0544, Val AP: 0.1971, Test AP: 0.1962\n",
                        "Seed: 42, Epoch: 138, Loss: 0.0543, Val AP: 0.1899, Test AP: 0.1890\n",
                        "Seed: 42, Epoch: 139, Loss: 0.0543, Val AP: 0.1951, Test AP: 0.1936\n",
                        "Seed: 42, Epoch: 140, Loss: 0.0544, Val AP: 0.1948, Test AP: 0.1925\n",
                        "Seed: 42, Epoch: 141, Loss: 0.0543, Val AP: 0.1967, Test AP: 0.1967\n",
                        "Seed: 42, Epoch: 142, Loss: 0.0543, Val AP: 0.1941, Test AP: 0.1944\n",
                        "Seed: 42, Epoch: 143, Loss: 0.0544, Val AP: 0.1986, Test AP: 0.1989\n",
                        "Seed: 42, Epoch: 144, Loss: 0.0543, Val AP: 0.1942, Test AP: 0.1930\n",
                        "Seed: 42, Epoch: 145, Loss: 0.0544, Val AP: 0.1953, Test AP: 0.1959\n",
                        "Seed: 42, Epoch: 146, Loss: 0.0543, Val AP: 0.2007, Test AP: 0.2001\n",
                        "Seed: 42, Epoch: 147, Loss: 0.0543, Val AP: 0.1938, Test AP: 0.1938\n",
                        "Seed: 42, Epoch: 148, Loss: 0.0542, Val AP: 0.1999, Test AP: 0.1997\n",
                        "Seed: 42, Epoch: 149, Loss: 0.0542, Val AP: 0.1937, Test AP: 0.1922\n",
                        "Seed: 42, Epoch: 150, Loss: 0.0542, Val AP: 0.1969, Test AP: 0.1958\n",
                        "Seed: 42, Epoch: 151, Loss: 0.0542, Val AP: 0.1989, Test AP: 0.1987\n",
                        "Seed: 42, Epoch: 152, Loss: 0.0542, Val AP: 0.1926, Test AP: 0.1922\n",
                        "Seed: 42, Epoch: 153, Loss: 0.0541, Val AP: 0.1962, Test AP: 0.1979\n",
                        "Seed: 42, Epoch: 154, Loss: 0.0542, Val AP: 0.1982, Test AP: 0.1989\n",
                        "Seed: 42, Epoch: 155, Loss: 0.0541, Val AP: 0.1991, Test AP: 0.1996\n",
                        "Seed: 42, Epoch: 156, Loss: 0.0541, Val AP: 0.1987, Test AP: 0.1984\n",
                        "Seed: 42, Epoch: 157, Loss: 0.0542, Val AP: 0.2013, Test AP: 0.1995\n",
                        "Seed: 42, Epoch: 158, Loss: 0.0542, Val AP: 0.1979, Test AP: 0.1979\n",
                        "Seed: 42, Epoch: 159, Loss: 0.0541, Val AP: 0.2018, Test AP: 0.2009\n",
                        "Seed: 42, Epoch: 160, Loss: 0.0541, Val AP: 0.1971, Test AP: 0.1977\n",
                        "Seed: 42, Epoch: 161, Loss: 0.0540, Val AP: 0.2015, Test AP: 0.2013\n",
                        "Seed: 42, Epoch: 162, Loss: 0.0541, Val AP: 0.1961, Test AP: 0.1942\n",
                        "Seed: 42, Epoch: 163, Loss: 0.0540, Val AP: 0.2017, Test AP: 0.2019\n",
                        "Seed: 42, Epoch: 164, Loss: 0.0541, Val AP: 0.1983, Test AP: 0.1970\n",
                        "Seed: 42, Epoch: 165, Loss: 0.0541, Val AP: 0.2054, Test AP: 0.2059\n",
                        "Seed: 42, Epoch: 166, Loss: 0.0541, Val AP: 0.2020, Test AP: 0.2020\n",
                        "Seed: 42, Epoch: 167, Loss: 0.0540, Val AP: 0.1955, Test AP: 0.1960\n",
                        "Seed: 42, Epoch: 168, Loss: 0.0540, Val AP: 0.2010, Test AP: 0.2002\n",
                        "Seed: 42, Epoch: 169, Loss: 0.0540, Val AP: 0.1968, Test AP: 0.1962\n",
                        "Seed: 42, Epoch: 170, Loss: 0.0539, Val AP: 0.1940, Test AP: 0.1948\n",
                        "Seed: 42, Epoch: 171, Loss: 0.0540, Val AP: 0.2027, Test AP: 0.2019\n",
                        "Seed: 42, Epoch: 172, Loss: 0.0539, Val AP: 0.2045, Test AP: 0.2050\n",
                        "Seed: 42, Epoch: 173, Loss: 0.0539, Val AP: 0.1981, Test AP: 0.1991\n",
                        "Seed: 42, Epoch: 174, Loss: 0.0539, Val AP: 0.1971, Test AP: 0.1955\n",
                        "Seed: 42, Epoch: 175, Loss: 0.0539, Val AP: 0.2034, Test AP: 0.2021\n",
                        "Seed: 42, Epoch: 176, Loss: 0.0540, Val AP: 0.1992, Test AP: 0.1992\n",
                        "Seed: 42, Epoch: 177, Loss: 0.0539, Val AP: 0.1969, Test AP: 0.1955\n",
                        "Seed: 42, Epoch: 178, Loss: 0.0538, Val AP: 0.2051, Test AP: 0.2042\n",
                        "Seed: 42, Epoch: 179, Loss: 0.0539, Val AP: 0.2006, Test AP: 0.1991\n",
                        "Seed: 42, Epoch: 180, Loss: 0.0539, Val AP: 0.1999, Test AP: 0.1991\n",
                        "Seed: 42, Epoch: 181, Loss: 0.0539, Val AP: 0.2000, Test AP: 0.1977\n",
                        "Seed: 42, Epoch: 182, Loss: 0.0539, Val AP: 0.2048, Test AP: 0.2037\n",
                        "Seed: 42, Epoch: 183, Loss: 0.0538, Val AP: 0.2022, Test AP: 0.2004\n",
                        "Seed: 42, Epoch: 184, Loss: 0.0538, Val AP: 0.2030, Test AP: 0.2013\n",
                        "Seed: 42, Epoch: 185, Loss: 0.0537, Val AP: 0.1985, Test AP: 0.1982\n",
                        "Seed: 42, Epoch: 186, Loss: 0.0538, Val AP: 0.2012, Test AP: 0.1999\n",
                        "Seed: 42, Epoch: 187, Loss: 0.0538, Val AP: 0.2042, Test AP: 0.2009\n",
                        "Seed: 42, Epoch: 188, Loss: 0.0537, Val AP: 0.2014, Test AP: 0.1993\n",
                        "Seed: 42, Epoch: 189, Loss: 0.0537, Val AP: 0.2002, Test AP: 0.1973\n",
                        "Seed: 42, Epoch: 190, Loss: 0.0537, Val AP: 0.2017, Test AP: 0.1996\n",
                        "Seed: 42, Epoch: 191, Loss: 0.0537, Val AP: 0.2037, Test AP: 0.2022\n",
                        "Seed: 42, Epoch: 192, Loss: 0.0537, Val AP: 0.2054, Test AP: 0.2034\n",
                        "Seed: 42, Epoch: 193, Loss: 0.0537, Val AP: 0.2026, Test AP: 0.2014\n",
                        "Seed: 42, Epoch: 194, Loss: 0.0537, Val AP: 0.2030, Test AP: 0.2009\n",
                        "Seed: 42, Epoch: 195, Loss: 0.0538, Val AP: 0.2054, Test AP: 0.2037\n",
                        "Seed: 42, Epoch: 196, Loss: 0.0537, Val AP: 0.2040, Test AP: 0.2022\n",
                        "Seed: 42, Epoch: 197, Loss: 0.0537, Val AP: 0.2052, Test AP: 0.2016\n",
                        "Seed: 42, Epoch: 198, Loss: 0.0537, Val AP: 0.2057, Test AP: 0.2032\n",
                        "Seed: 42, Epoch: 199, Loss: 0.0537, Val AP: 0.2101, Test AP: 0.2079\n",
                        "Seed: 42, Epoch: 200, Loss: 0.0536, Val AP: 0.2009, Test AP: 0.1985\n",
                        "Seed: 43, Epoch: 001, Loss: 0.0631, Val AP: 0.1165, Test AP: 0.1159\n",
                        "Seed: 43, Epoch: 002, Loss: 0.0607, Val AP: 0.1186, Test AP: 0.1195\n",
                        "Seed: 43, Epoch: 003, Loss: 0.0602, Val AP: 0.1218, Test AP: 0.1237\n",
                        "Seed: 43, Epoch: 004, Loss: 0.0603, Val AP: 0.1216, Test AP: 0.1232\n",
                        "Seed: 43, Epoch: 005, Loss: 0.0602, Val AP: 0.1177, Test AP: 0.1193\n",
                        "Seed: 43, Epoch: 006, Loss: 0.0602, Val AP: 0.1260, Test AP: 0.1270\n",
                        "Seed: 43, Epoch: 007, Loss: 0.0599, Val AP: 0.1273, Test AP: 0.1290\n",
                        "Seed: 43, Epoch: 008, Loss: 0.0597, Val AP: 0.1176, Test AP: 0.1203\n",
                        "Seed: 43, Epoch: 009, Loss: 0.0599, Val AP: 0.1264, Test AP: 0.1270\n",
                        "Seed: 43, Epoch: 010, Loss: 0.0596, Val AP: 0.1239, Test AP: 0.1261\n",
                        "Seed: 43, Epoch: 011, Loss: 0.0594, Val AP: 0.1234, Test AP: 0.1257\n",
                        "Seed: 43, Epoch: 012, Loss: 0.0595, Val AP: 0.1195, Test AP: 0.1218\n",
                        "Seed: 43, Epoch: 013, Loss: 0.0594, Val AP: 0.1279, Test AP: 0.1305\n",
                        "Seed: 43, Epoch: 014, Loss: 0.0592, Val AP: 0.1253, Test AP: 0.1275\n",
                        "Seed: 43, Epoch: 015, Loss: 0.0592, Val AP: 0.1284, Test AP: 0.1298\n",
                        "Seed: 43, Epoch: 016, Loss: 0.0591, Val AP: 0.1262, Test AP: 0.1280\n",
                        "Seed: 43, Epoch: 017, Loss: 0.0592, Val AP: 0.1263, Test AP: 0.1288\n",
                        "Seed: 43, Epoch: 018, Loss: 0.0593, Val AP: 0.1245, Test AP: 0.1260\n",
                        "Seed: 43, Epoch: 019, Loss: 0.0593, Val AP: 0.1241, Test AP: 0.1251\n",
                        "Seed: 43, Epoch: 020, Loss: 0.0593, Val AP: 0.1264, Test AP: 0.1276\n",
                        "Seed: 43, Epoch: 021, Loss: 0.0592, Val AP: 0.1230, Test AP: 0.1239\n",
                        "Seed: 43, Epoch: 022, Loss: 0.0592, Val AP: 0.1258, Test AP: 0.1260\n",
                        "Seed: 43, Epoch: 023, Loss: 0.0592, Val AP: 0.1247, Test AP: 0.1257\n",
                        "Seed: 43, Epoch: 024, Loss: 0.0592, Val AP: 0.1266, Test AP: 0.1273\n",
                        "Seed: 43, Epoch: 025, Loss: 0.0591, Val AP: 0.1258, Test AP: 0.1266\n",
                        "Seed: 43, Epoch: 026, Loss: 0.0590, Val AP: 0.1270, Test AP: 0.1273\n",
                        "Seed: 43, Epoch: 027, Loss: 0.0590, Val AP: 0.1248, Test AP: 0.1242\n",
                        "Seed: 43, Epoch: 028, Loss: 0.0590, Val AP: 0.1278, Test AP: 0.1292\n",
                        "Seed: 43, Epoch: 029, Loss: 0.0589, Val AP: 0.1298, Test AP: 0.1304\n",
                        "Seed: 43, Epoch: 030, Loss: 0.0588, Val AP: 0.1271, Test AP: 0.1277\n",
                        "Seed: 43, Epoch: 031, Loss: 0.0588, Val AP: 0.1308, Test AP: 0.1312\n",
                        "Seed: 43, Epoch: 032, Loss: 0.0587, Val AP: 0.1275, Test AP: 0.1280\n",
                        "Seed: 43, Epoch: 033, Loss: 0.0587, Val AP: 0.1272, Test AP: 0.1286\n",
                        "Seed: 43, Epoch: 034, Loss: 0.0587, Val AP: 0.1319, Test AP: 0.1323\n",
                        "Seed: 43, Epoch: 035, Loss: 0.0586, Val AP: 0.1309, Test AP: 0.1312\n",
                        "Seed: 43, Epoch: 036, Loss: 0.0585, Val AP: 0.1303, Test AP: 0.1318\n",
                        "Seed: 43, Epoch: 037, Loss: 0.0585, Val AP: 0.1349, Test AP: 0.1360\n",
                        "Seed: 43, Epoch: 038, Loss: 0.0584, Val AP: 0.1328, Test AP: 0.1353\n",
                        "Seed: 43, Epoch: 039, Loss: 0.0584, Val AP: 0.1341, Test AP: 0.1358\n",
                        "Seed: 43, Epoch: 040, Loss: 0.0585, Val AP: 0.1348, Test AP: 0.1356\n",
                        "Seed: 43, Epoch: 041, Loss: 0.0583, Val AP: 0.1370, Test AP: 0.1390\n",
                        "Seed: 43, Epoch: 042, Loss: 0.0582, Val AP: 0.1354, Test AP: 0.1367\n",
                        "Seed: 43, Epoch: 043, Loss: 0.0582, Val AP: 0.1395, Test AP: 0.1406\n",
                        "Seed: 43, Epoch: 044, Loss: 0.0581, Val AP: 0.1386, Test AP: 0.1394\n",
                        "Seed: 43, Epoch: 045, Loss: 0.0581, Val AP: 0.1378, Test AP: 0.1386\n",
                        "Seed: 43, Epoch: 046, Loss: 0.0581, Val AP: 0.1418, Test AP: 0.1429\n",
                        "Seed: 43, Epoch: 047, Loss: 0.0580, Val AP: 0.1450, Test AP: 0.1461\n",
                        "Seed: 43, Epoch: 048, Loss: 0.0579, Val AP: 0.1392, Test AP: 0.1412\n",
                        "Seed: 43, Epoch: 049, Loss: 0.0579, Val AP: 0.1406, Test AP: 0.1427\n",
                        "Seed: 43, Epoch: 050, Loss: 0.0578, Val AP: 0.1420, Test AP: 0.1432\n",
                        "Seed: 43, Epoch: 051, Loss: 0.0578, Val AP: 0.1421, Test AP: 0.1437\n",
                        "Seed: 43, Epoch: 052, Loss: 0.0578, Val AP: 0.1452, Test AP: 0.1466\n",
                        "Seed: 43, Epoch: 053, Loss: 0.0577, Val AP: 0.1407, Test AP: 0.1426\n",
                        "Seed: 43, Epoch: 054, Loss: 0.0578, Val AP: 0.1433, Test AP: 0.1457\n",
                        "Seed: 43, Epoch: 055, Loss: 0.0576, Val AP: 0.1482, Test AP: 0.1495\n",
                        "Seed: 43, Epoch: 056, Loss: 0.0575, Val AP: 0.1458, Test AP: 0.1477\n",
                        "Seed: 43, Epoch: 057, Loss: 0.0576, Val AP: 0.1445, Test AP: 0.1469\n",
                        "Seed: 43, Epoch: 058, Loss: 0.0575, Val AP: 0.1451, Test AP: 0.1464\n",
                        "Seed: 43, Epoch: 059, Loss: 0.0575, Val AP: 0.1438, Test AP: 0.1464\n",
                        "Seed: 43, Epoch: 060, Loss: 0.0574, Val AP: 0.1480, Test AP: 0.1488\n",
                        "Seed: 43, Epoch: 061, Loss: 0.0574, Val AP: 0.1506, Test AP: 0.1529\n",
                        "Seed: 43, Epoch: 062, Loss: 0.0574, Val AP: 0.1469, Test AP: 0.1499\n",
                        "Seed: 43, Epoch: 063, Loss: 0.0573, Val AP: 0.1485, Test AP: 0.1502\n",
                        "Seed: 43, Epoch: 064, Loss: 0.0573, Val AP: 0.1508, Test AP: 0.1522\n",
                        "Seed: 43, Epoch: 065, Loss: 0.0573, Val AP: 0.1452, Test AP: 0.1480\n",
                        "Seed: 43, Epoch: 066, Loss: 0.0573, Val AP: 0.1458, Test AP: 0.1481\n",
                        "Seed: 43, Epoch: 067, Loss: 0.0572, Val AP: 0.1484, Test AP: 0.1508\n",
                        "Seed: 43, Epoch: 068, Loss: 0.0571, Val AP: 0.1509, Test AP: 0.1528\n",
                        "Seed: 43, Epoch: 069, Loss: 0.0571, Val AP: 0.1544, Test AP: 0.1544\n",
                        "Seed: 43, Epoch: 070, Loss: 0.0571, Val AP: 0.1545, Test AP: 0.1552\n",
                        "Seed: 43, Epoch: 071, Loss: 0.0571, Val AP: 0.1512, Test AP: 0.1526\n",
                        "Seed: 43, Epoch: 072, Loss: 0.0570, Val AP: 0.1523, Test AP: 0.1538\n",
                        "Seed: 43, Epoch: 073, Loss: 0.0571, Val AP: 0.1523, Test AP: 0.1547\n",
                        "Seed: 43, Epoch: 074, Loss: 0.0569, Val AP: 0.1513, Test AP: 0.1523\n",
                        "Seed: 43, Epoch: 075, Loss: 0.0569, Val AP: 0.1571, Test AP: 0.1579\n",
                        "Seed: 43, Epoch: 076, Loss: 0.0569, Val AP: 0.1551, Test AP: 0.1572\n",
                        "Seed: 43, Epoch: 077, Loss: 0.0568, Val AP: 0.1538, Test AP: 0.1553\n",
                        "Seed: 43, Epoch: 078, Loss: 0.0568, Val AP: 0.1570, Test AP: 0.1572\n",
                        "Seed: 43, Epoch: 079, Loss: 0.0570, Val AP: 0.1572, Test AP: 0.1586\n",
                        "Seed: 43, Epoch: 080, Loss: 0.0568, Val AP: 0.1531, Test AP: 0.1552\n",
                        "Seed: 43, Epoch: 081, Loss: 0.0568, Val AP: 0.1521, Test AP: 0.1542\n",
                        "Seed: 43, Epoch: 082, Loss: 0.0569, Val AP: 0.1566, Test AP: 0.1576\n",
                        "Seed: 43, Epoch: 083, Loss: 0.0569, Val AP: 0.1593, Test AP: 0.1590\n",
                        "Seed: 43, Epoch: 084, Loss: 0.0568, Val AP: 0.1521, Test AP: 0.1548\n",
                        "Seed: 43, Epoch: 085, Loss: 0.0567, Val AP: 0.1558, Test AP: 0.1574\n",
                        "Seed: 43, Epoch: 086, Loss: 0.0567, Val AP: 0.1572, Test AP: 0.1599\n",
                        "Seed: 43, Epoch: 087, Loss: 0.0567, Val AP: 0.1508, Test AP: 0.1532\n",
                        "Seed: 43, Epoch: 088, Loss: 0.0567, Val AP: 0.1583, Test AP: 0.1594\n",
                        "Seed: 43, Epoch: 089, Loss: 0.0567, Val AP: 0.1566, Test AP: 0.1585\n",
                        "Seed: 43, Epoch: 090, Loss: 0.0567, Val AP: 0.1543, Test AP: 0.1568\n",
                        "Seed: 43, Epoch: 091, Loss: 0.0567, Val AP: 0.1603, Test AP: 0.1611\n",
                        "Seed: 43, Epoch: 092, Loss: 0.0567, Val AP: 0.1556, Test AP: 0.1569\n",
                        "Seed: 43, Epoch: 093, Loss: 0.0569, Val AP: 0.1559, Test AP: 0.1585\n",
                        "Seed: 43, Epoch: 094, Loss: 0.0567, Val AP: 0.1594, Test AP: 0.1608\n",
                        "Seed: 43, Epoch: 095, Loss: 0.0567, Val AP: 0.1578, Test AP: 0.1603\n",
                        "Seed: 43, Epoch: 096, Loss: 0.0567, Val AP: 0.1593, Test AP: 0.1614\n",
                        "Seed: 43, Epoch: 097, Loss: 0.0567, Val AP: 0.1534, Test AP: 0.1577\n",
                        "Seed: 43, Epoch: 098, Loss: 0.0567, Val AP: 0.1596, Test AP: 0.1630\n",
                        "Seed: 43, Epoch: 099, Loss: 0.0566, Val AP: 0.1562, Test AP: 0.1597\n",
                        "Seed: 43, Epoch: 100, Loss: 0.0566, Val AP: 0.1569, Test AP: 0.1599\n",
                        "Seed: 43, Epoch: 101, Loss: 0.0566, Val AP: 0.1546, Test AP: 0.1573\n",
                        "Seed: 43, Epoch: 102, Loss: 0.0565, Val AP: 0.1621, Test AP: 0.1654\n",
                        "Seed: 43, Epoch: 103, Loss: 0.0565, Val AP: 0.1528, Test AP: 0.1566\n",
                        "Seed: 43, Epoch: 104, Loss: 0.0565, Val AP: 0.1556, Test AP: 0.1595\n",
                        "Seed: 43, Epoch: 105, Loss: 0.0565, Val AP: 0.1540, Test AP: 0.1568\n",
                        "Seed: 43, Epoch: 106, Loss: 0.0564, Val AP: 0.1615, Test AP: 0.1646\n",
                        "Seed: 43, Epoch: 107, Loss: 0.0565, Val AP: 0.1627, Test AP: 0.1661\n",
                        "Seed: 43, Epoch: 108, Loss: 0.0564, Val AP: 0.1605, Test AP: 0.1620\n",
                        "Seed: 43, Epoch: 109, Loss: 0.0564, Val AP: 0.1603, Test AP: 0.1638\n",
                        "Seed: 43, Epoch: 110, Loss: 0.0565, Val AP: 0.1582, Test AP: 0.1617\n",
                        "Seed: 43, Epoch: 111, Loss: 0.0564, Val AP: 0.1568, Test AP: 0.1598\n",
                        "Seed: 43, Epoch: 112, Loss: 0.0564, Val AP: 0.1625, Test AP: 0.1640\n",
                        "Seed: 43, Epoch: 113, Loss: 0.0564, Val AP: 0.1598, Test AP: 0.1638\n",
                        "Seed: 43, Epoch: 114, Loss: 0.0564, Val AP: 0.1648, Test AP: 0.1678\n",
                        "Seed: 43, Epoch: 115, Loss: 0.0563, Val AP: 0.1653, Test AP: 0.1679\n",
                        "Seed: 43, Epoch: 116, Loss: 0.0564, Val AP: 0.1644, Test AP: 0.1689\n",
                        "Seed: 43, Epoch: 117, Loss: 0.0563, Val AP: 0.1636, Test AP: 0.1673\n",
                        "Seed: 43, Epoch: 118, Loss: 0.0562, Val AP: 0.1622, Test AP: 0.1660\n",
                        "Seed: 43, Epoch: 119, Loss: 0.0562, Val AP: 0.1621, Test AP: 0.1660\n",
                        "Seed: 43, Epoch: 120, Loss: 0.0561, Val AP: 0.1676, Test AP: 0.1711\n",
                        "Seed: 43, Epoch: 121, Loss: 0.0561, Val AP: 0.1631, Test AP: 0.1667\n",
                        "Seed: 43, Epoch: 122, Loss: 0.0561, Val AP: 0.1641, Test AP: 0.1673\n",
                        "Seed: 43, Epoch: 123, Loss: 0.0561, Val AP: 0.1655, Test AP: 0.1691\n",
                        "Seed: 43, Epoch: 124, Loss: 0.0561, Val AP: 0.1662, Test AP: 0.1704\n",
                        "Seed: 43, Epoch: 125, Loss: 0.0561, Val AP: 0.1649, Test AP: 0.1689\n",
                        "Seed: 43, Epoch: 126, Loss: 0.0561, Val AP: 0.1646, Test AP: 0.1689\n",
                        "Seed: 43, Epoch: 127, Loss: 0.0560, Val AP: 0.1636, Test AP: 0.1668\n",
                        "Seed: 43, Epoch: 128, Loss: 0.0560, Val AP: 0.1664, Test AP: 0.1698\n",
                        "Seed: 43, Epoch: 129, Loss: 0.0560, Val AP: 0.1671, Test AP: 0.1699\n",
                        "Seed: 43, Epoch: 130, Loss: 0.0559, Val AP: 0.1704, Test AP: 0.1737\n",
                        "Seed: 43, Epoch: 131, Loss: 0.0559, Val AP: 0.1699, Test AP: 0.1740\n",
                        "Seed: 43, Epoch: 132, Loss: 0.0561, Val AP: 0.1657, Test AP: 0.1712\n",
                        "Seed: 43, Epoch: 133, Loss: 0.0559, Val AP: 0.1692, Test AP: 0.1731\n",
                        "Seed: 43, Epoch: 134, Loss: 0.0559, Val AP: 0.1667, Test AP: 0.1700\n",
                        "Seed: 43, Epoch: 135, Loss: 0.0559, Val AP: 0.1673, Test AP: 0.1720\n",
                        "Seed: 43, Epoch: 136, Loss: 0.0560, Val AP: 0.1639, Test AP: 0.1690\n",
                        "Seed: 43, Epoch: 137, Loss: 0.0560, Val AP: 0.1706, Test AP: 0.1733\n",
                        "Seed: 43, Epoch: 138, Loss: 0.0560, Val AP: 0.1733, Test AP: 0.1762\n",
                        "Seed: 43, Epoch: 139, Loss: 0.0559, Val AP: 0.1719, Test AP: 0.1766\n",
                        "Seed: 43, Epoch: 140, Loss: 0.0558, Val AP: 0.1711, Test AP: 0.1755\n",
                        "Seed: 43, Epoch: 141, Loss: 0.0558, Val AP: 0.1758, Test AP: 0.1801\n",
                        "Seed: 43, Epoch: 142, Loss: 0.0558, Val AP: 0.1707, Test AP: 0.1723\n",
                        "Seed: 43, Epoch: 143, Loss: 0.0558, Val AP: 0.1705, Test AP: 0.1752\n",
                        "Seed: 43, Epoch: 144, Loss: 0.0558, Val AP: 0.1686, Test AP: 0.1735\n",
                        "Seed: 43, Epoch: 145, Loss: 0.0558, Val AP: 0.1684, Test AP: 0.1735\n",
                        "Seed: 43, Epoch: 146, Loss: 0.0559, Val AP: 0.1732, Test AP: 0.1776\n",
                        "Seed: 43, Epoch: 147, Loss: 0.0558, Val AP: 0.1751, Test AP: 0.1783\n",
                        "Seed: 43, Epoch: 148, Loss: 0.0557, Val AP: 0.1728, Test AP: 0.1773\n",
                        "Seed: 43, Epoch: 149, Loss: 0.0558, Val AP: 0.1712, Test AP: 0.1752\n",
                        "Seed: 43, Epoch: 150, Loss: 0.0558, Val AP: 0.1768, Test AP: 0.1792\n",
                        "Seed: 43, Epoch: 151, Loss: 0.0557, Val AP: 0.1716, Test AP: 0.1763\n",
                        "Seed: 43, Epoch: 152, Loss: 0.0559, Val AP: 0.1719, Test AP: 0.1760\n",
                        "Seed: 43, Epoch: 153, Loss: 0.0557, Val AP: 0.1677, Test AP: 0.1728\n",
                        "Seed: 43, Epoch: 154, Loss: 0.0557, Val AP: 0.1734, Test AP: 0.1759\n",
                        "Seed: 43, Epoch: 155, Loss: 0.0557, Val AP: 0.1741, Test AP: 0.1776\n",
                        "Seed: 43, Epoch: 156, Loss: 0.0557, Val AP: 0.1711, Test AP: 0.1752\n",
                        "Seed: 43, Epoch: 157, Loss: 0.0556, Val AP: 0.1748, Test AP: 0.1768\n",
                        "Seed: 43, Epoch: 158, Loss: 0.0557, Val AP: 0.1685, Test AP: 0.1718\n",
                        "Seed: 43, Epoch: 159, Loss: 0.0556, Val AP: 0.1725, Test AP: 0.1770\n",
                        "Seed: 43, Epoch: 160, Loss: 0.0557, Val AP: 0.1725, Test AP: 0.1731\n",
                        "Seed: 43, Epoch: 161, Loss: 0.0557, Val AP: 0.1771, Test AP: 0.1804\n",
                        "Seed: 43, Epoch: 162, Loss: 0.0556, Val AP: 0.1751, Test AP: 0.1788\n",
                        "Seed: 43, Epoch: 163, Loss: 0.0557, Val AP: 0.1772, Test AP: 0.1799\n",
                        "Seed: 43, Epoch: 164, Loss: 0.0557, Val AP: 0.1687, Test AP: 0.1750\n",
                        "Seed: 43, Epoch: 165, Loss: 0.0556, Val AP: 0.1743, Test AP: 0.1776\n",
                        "Seed: 43, Epoch: 166, Loss: 0.0556, Val AP: 0.1738, Test AP: 0.1788\n",
                        "Seed: 43, Epoch: 167, Loss: 0.0556, Val AP: 0.1753, Test AP: 0.1784\n",
                        "Seed: 43, Epoch: 168, Loss: 0.0557, Val AP: 0.1735, Test AP: 0.1773\n",
                        "Seed: 43, Epoch: 169, Loss: 0.0556, Val AP: 0.1704, Test AP: 0.1749\n",
                        "Seed: 43, Epoch: 170, Loss: 0.0556, Val AP: 0.1741, Test AP: 0.1787\n",
                        "Seed: 43, Epoch: 171, Loss: 0.0556, Val AP: 0.1700, Test AP: 0.1723\n",
                        "Seed: 43, Epoch: 172, Loss: 0.0556, Val AP: 0.1759, Test AP: 0.1800\n",
                        "Seed: 43, Epoch: 173, Loss: 0.0555, Val AP: 0.1763, Test AP: 0.1801\n",
                        "Seed: 43, Epoch: 174, Loss: 0.0556, Val AP: 0.1704, Test AP: 0.1748\n",
                        "Seed: 43, Epoch: 175, Loss: 0.0555, Val AP: 0.1756, Test AP: 0.1800\n",
                        "Seed: 43, Epoch: 176, Loss: 0.0556, Val AP: 0.1742, Test AP: 0.1775\n",
                        "Seed: 43, Epoch: 177, Loss: 0.0555, Val AP: 0.1776, Test AP: 0.1809\n",
                        "Seed: 43, Epoch: 178, Loss: 0.0555, Val AP: 0.1766, Test AP: 0.1804\n",
                        "Seed: 43, Epoch: 179, Loss: 0.0555, Val AP: 0.1803, Test AP: 0.1836\n",
                        "Seed: 43, Epoch: 180, Loss: 0.0555, Val AP: 0.1782, Test AP: 0.1807\n",
                        "Seed: 43, Epoch: 181, Loss: 0.0555, Val AP: 0.1800, Test AP: 0.1824\n",
                        "Seed: 43, Epoch: 182, Loss: 0.0556, Val AP: 0.1753, Test AP: 0.1773\n",
                        "Seed: 43, Epoch: 183, Loss: 0.0556, Val AP: 0.1717, Test AP: 0.1736\n",
                        "Seed: 43, Epoch: 184, Loss: 0.0555, Val AP: 0.1782, Test AP: 0.1792\n",
                        "Seed: 43, Epoch: 185, Loss: 0.0555, Val AP: 0.1758, Test AP: 0.1799\n",
                        "Seed: 43, Epoch: 186, Loss: 0.0555, Val AP: 0.1768, Test AP: 0.1803\n",
                        "Seed: 43, Epoch: 187, Loss: 0.0555, Val AP: 0.1816, Test AP: 0.1850\n",
                        "Seed: 43, Epoch: 188, Loss: 0.0555, Val AP: 0.1767, Test AP: 0.1784\n",
                        "Seed: 43, Epoch: 189, Loss: 0.0555, Val AP: 0.1733, Test AP: 0.1766\n",
                        "Seed: 43, Epoch: 190, Loss: 0.0555, Val AP: 0.1775, Test AP: 0.1814\n",
                        "Seed: 43, Epoch: 191, Loss: 0.0555, Val AP: 0.1778, Test AP: 0.1808\n",
                        "Seed: 43, Epoch: 192, Loss: 0.0555, Val AP: 0.1748, Test AP: 0.1795\n",
                        "Seed: 43, Epoch: 193, Loss: 0.0555, Val AP: 0.1754, Test AP: 0.1794\n",
                        "Seed: 43, Epoch: 194, Loss: 0.0554, Val AP: 0.1768, Test AP: 0.1801\n",
                        "Seed: 43, Epoch: 195, Loss: 0.0554, Val AP: 0.1776, Test AP: 0.1809\n",
                        "Seed: 43, Epoch: 196, Loss: 0.0554, Val AP: 0.1736, Test AP: 0.1765\n",
                        "Seed: 43, Epoch: 197, Loss: 0.0553, Val AP: 0.1834, Test AP: 0.1856\n",
                        "Seed: 43, Epoch: 198, Loss: 0.0554, Val AP: 0.1791, Test AP: 0.1823\n",
                        "Seed: 43, Epoch: 199, Loss: 0.0554, Val AP: 0.1773, Test AP: 0.1798\n",
                        "Seed: 43, Epoch: 200, Loss: 0.0554, Val AP: 0.1817, Test AP: 0.1848\n",
                        "Seed: 44, Epoch: 001, Loss: 0.0645, Val AP: 0.1143, Test AP: 0.1155\n",
                        "Seed: 44, Epoch: 002, Loss: 0.0607, Val AP: 0.1181, Test AP: 0.1197\n",
                        "Seed: 44, Epoch: 003, Loss: 0.0603, Val AP: 0.1167, Test AP: 0.1184\n",
                        "Seed: 44, Epoch: 004, Loss: 0.0603, Val AP: 0.1173, Test AP: 0.1184\n",
                        "Seed: 44, Epoch: 005, Loss: 0.0601, Val AP: 0.1179, Test AP: 0.1190\n",
                        "Seed: 44, Epoch: 006, Loss: 0.0598, Val AP: 0.1211, Test AP: 0.1222\n",
                        "Seed: 44, Epoch: 007, Loss: 0.0597, Val AP: 0.1214, Test AP: 0.1225\n",
                        "Seed: 44, Epoch: 008, Loss: 0.0595, Val AP: 0.1263, Test AP: 0.1276\n",
                        "Seed: 44, Epoch: 009, Loss: 0.0594, Val AP: 0.1259, Test AP: 0.1264\n",
                        "Seed: 44, Epoch: 010, Loss: 0.0593, Val AP: 0.1295, Test AP: 0.1308\n",
                        "Seed: 44, Epoch: 011, Loss: 0.0592, Val AP: 0.1267, Test AP: 0.1277\n",
                        "Seed: 44, Epoch: 012, Loss: 0.0591, Val AP: 0.1275, Test AP: 0.1278\n",
                        "Seed: 44, Epoch: 013, Loss: 0.0591, Val AP: 0.1322, Test AP: 0.1333\n",
                        "Seed: 44, Epoch: 014, Loss: 0.0589, Val AP: 0.1283, Test AP: 0.1292\n",
                        "Seed: 44, Epoch: 015, Loss: 0.0588, Val AP: 0.1361, Test AP: 0.1372\n",
                        "Seed: 44, Epoch: 016, Loss: 0.0587, Val AP: 0.1411, Test AP: 0.1423\n",
                        "Seed: 44, Epoch: 017, Loss: 0.0585, Val AP: 0.1433, Test AP: 0.1443\n",
                        "Seed: 44, Epoch: 018, Loss: 0.0583, Val AP: 0.1442, Test AP: 0.1443\n",
                        "Seed: 44, Epoch: 019, Loss: 0.0582, Val AP: 0.1417, Test AP: 0.1426\n",
                        "Seed: 44, Epoch: 020, Loss: 0.0580, Val AP: 0.1568, Test AP: 0.1571\n",
                        "Seed: 44, Epoch: 021, Loss: 0.0579, Val AP: 0.1545, Test AP: 0.1544\n",
                        "Seed: 44, Epoch: 022, Loss: 0.0578, Val AP: 0.1542, Test AP: 0.1563\n",
                        "Seed: 44, Epoch: 023, Loss: 0.0577, Val AP: 0.1638, Test AP: 0.1642\n",
                        "Seed: 44, Epoch: 024, Loss: 0.0575, Val AP: 0.1598, Test AP: 0.1585\n",
                        "Seed: 44, Epoch: 025, Loss: 0.0575, Val AP: 0.1552, Test AP: 0.1576\n",
                        "Seed: 44, Epoch: 026, Loss: 0.0575, Val AP: 0.1660, Test AP: 0.1678\n",
                        "Seed: 44, Epoch: 027, Loss: 0.0573, Val AP: 0.1551, Test AP: 0.1580\n",
                        "Seed: 44, Epoch: 028, Loss: 0.0572, Val AP: 0.1618, Test AP: 0.1628\n",
                        "Seed: 44, Epoch: 029, Loss: 0.0570, Val AP: 0.1571, Test AP: 0.1576\n",
                        "Seed: 44, Epoch: 030, Loss: 0.0570, Val AP: 0.1616, Test AP: 0.1654\n",
                        "Seed: 44, Epoch: 031, Loss: 0.0570, Val AP: 0.1735, Test AP: 0.1738\n",
                        "Seed: 44, Epoch: 032, Loss: 0.0568, Val AP: 0.1608, Test AP: 0.1653\n",
                        "Seed: 44, Epoch: 033, Loss: 0.0568, Val AP: 0.1737, Test AP: 0.1757\n",
                        "Seed: 44, Epoch: 034, Loss: 0.0567, Val AP: 0.1687, Test AP: 0.1725\n",
                        "Seed: 44, Epoch: 035, Loss: 0.0564, Val AP: 0.1713, Test AP: 0.1745\n",
                        "Seed: 44, Epoch: 036, Loss: 0.0565, Val AP: 0.1784, Test AP: 0.1805\n",
                        "Seed: 44, Epoch: 037, Loss: 0.0563, Val AP: 0.1898, Test AP: 0.1875\n",
                        "Seed: 44, Epoch: 038, Loss: 0.0562, Val AP: 0.1911, Test AP: 0.1925\n",
                        "Seed: 44, Epoch: 039, Loss: 0.0562, Val AP: 0.1858, Test AP: 0.1880\n",
                        "Seed: 44, Epoch: 040, Loss: 0.0561, Val AP: 0.1794, Test AP: 0.1822\n",
                        "Seed: 44, Epoch: 041, Loss: 0.0561, Val AP: 0.1726, Test AP: 0.1772\n",
                        "Seed: 44, Epoch: 042, Loss: 0.0559, Val AP: 0.1869, Test AP: 0.1890\n",
                        "Seed: 44, Epoch: 043, Loss: 0.0559, Val AP: 0.1848, Test AP: 0.1875\n",
                        "Seed: 44, Epoch: 044, Loss: 0.0557, Val AP: 0.1921, Test AP: 0.1930\n",
                        "Seed: 44, Epoch: 045, Loss: 0.0557, Val AP: 0.2012, Test AP: 0.1994\n",
                        "Seed: 44, Epoch: 046, Loss: 0.0556, Val AP: 0.1966, Test AP: 0.1974\n",
                        "Seed: 44, Epoch: 047, Loss: 0.0556, Val AP: 0.1954, Test AP: 0.1979\n",
                        "Seed: 44, Epoch: 048, Loss: 0.0554, Val AP: 0.1936, Test AP: 0.1950\n",
                        "Seed: 44, Epoch: 049, Loss: 0.0553, Val AP: 0.2019, Test AP: 0.2025\n",
                        "Seed: 44, Epoch: 050, Loss: 0.0553, Val AP: 0.1970, Test AP: 0.2002\n",
                        "Seed: 44, Epoch: 051, Loss: 0.0552, Val AP: 0.1957, Test AP: 0.1972\n",
                        "Seed: 44, Epoch: 052, Loss: 0.0552, Val AP: 0.1934, Test AP: 0.1948\n",
                        "Seed: 44, Epoch: 053, Loss: 0.0551, Val AP: 0.1989, Test AP: 0.1986\n",
                        "Seed: 44, Epoch: 054, Loss: 0.0551, Val AP: 0.2026, Test AP: 0.2031\n",
                        "Seed: 44, Epoch: 055, Loss: 0.0550, Val AP: 0.2023, Test AP: 0.2033\n",
                        "Seed: 44, Epoch: 056, Loss: 0.0550, Val AP: 0.2010, Test AP: 0.2004\n",
                        "Seed: 44, Epoch: 057, Loss: 0.0550, Val AP: 0.2060, Test AP: 0.2052\n",
                        "Seed: 44, Epoch: 058, Loss: 0.0549, Val AP: 0.2119, Test AP: 0.2098\n",
                        "Seed: 44, Epoch: 059, Loss: 0.0548, Val AP: 0.1996, Test AP: 0.2021\n",
                        "Seed: 44, Epoch: 060, Loss: 0.0547, Val AP: 0.2106, Test AP: 0.2103\n",
                        "Seed: 44, Epoch: 061, Loss: 0.0548, Val AP: 0.2100, Test AP: 0.2100\n",
                        "Seed: 44, Epoch: 062, Loss: 0.0546, Val AP: 0.2054, Test AP: 0.2065\n",
                        "Seed: 44, Epoch: 063, Loss: 0.0545, Val AP: 0.2009, Test AP: 0.1971\n",
                        "Seed: 44, Epoch: 064, Loss: 0.0545, Val AP: 0.2158, Test AP: 0.2167\n",
                        "Seed: 44, Epoch: 065, Loss: 0.0544, Val AP: 0.2044, Test AP: 0.2055\n",
                        "Seed: 44, Epoch: 066, Loss: 0.0545, Val AP: 0.2037, Test AP: 0.2020\n",
                        "Seed: 44, Epoch: 067, Loss: 0.0545, Val AP: 0.2093, Test AP: 0.2070\n",
                        "Seed: 44, Epoch: 068, Loss: 0.0545, Val AP: 0.2116, Test AP: 0.2116\n",
                        "Seed: 44, Epoch: 069, Loss: 0.0544, Val AP: 0.2201, Test AP: 0.2173\n",
                        "Seed: 44, Epoch: 070, Loss: 0.0543, Val AP: 0.2104, Test AP: 0.2131\n",
                        "Seed: 44, Epoch: 071, Loss: 0.0542, Val AP: 0.2153, Test AP: 0.2128\n",
                        "Seed: 44, Epoch: 072, Loss: 0.0542, Val AP: 0.2114, Test AP: 0.2124\n",
                        "Seed: 44, Epoch: 073, Loss: 0.0542, Val AP: 0.2072, Test AP: 0.2079\n",
                        "Seed: 44, Epoch: 074, Loss: 0.0541, Val AP: 0.2163, Test AP: 0.2145\n",
                        "Seed: 44, Epoch: 075, Loss: 0.0541, Val AP: 0.2178, Test AP: 0.2184\n",
                        "Seed: 44, Epoch: 076, Loss: 0.0541, Val AP: 0.2110, Test AP: 0.2134\n",
                        "Seed: 44, Epoch: 077, Loss: 0.0541, Val AP: 0.2173, Test AP: 0.2181\n",
                        "Seed: 44, Epoch: 078, Loss: 0.0540, Val AP: 0.2216, Test AP: 0.2200\n",
                        "Seed: 44, Epoch: 079, Loss: 0.0539, Val AP: 0.2084, Test AP: 0.2107\n",
                        "Seed: 44, Epoch: 080, Loss: 0.0539, Val AP: 0.2192, Test AP: 0.2183\n",
                        "Seed: 44, Epoch: 081, Loss: 0.0540, Val AP: 0.2117, Test AP: 0.2145\n",
                        "Seed: 44, Epoch: 082, Loss: 0.0538, Val AP: 0.2160, Test AP: 0.2145\n",
                        "Seed: 44, Epoch: 083, Loss: 0.0538, Val AP: 0.2197, Test AP: 0.2198\n",
                        "Seed: 44, Epoch: 084, Loss: 0.0538, Val AP: 0.2150, Test AP: 0.2145\n",
                        "Seed: 44, Epoch: 085, Loss: 0.0538, Val AP: 0.2171, Test AP: 0.2176\n",
                        "Seed: 44, Epoch: 086, Loss: 0.0537, Val AP: 0.2100, Test AP: 0.2118\n",
                        "Seed: 44, Epoch: 087, Loss: 0.0538, Val AP: 0.2162, Test AP: 0.2190\n",
                        "Seed: 44, Epoch: 088, Loss: 0.0537, Val AP: 0.2198, Test AP: 0.2192\n",
                        "Seed: 44, Epoch: 089, Loss: 0.0537, Val AP: 0.2225, Test AP: 0.2228\n",
                        "Seed: 44, Epoch: 090, Loss: 0.0536, Val AP: 0.2218, Test AP: 0.2225\n",
                        "Seed: 44, Epoch: 091, Loss: 0.0537, Val AP: 0.2223, Test AP: 0.2217\n",
                        "Seed: 44, Epoch: 092, Loss: 0.0537, Val AP: 0.2192, Test AP: 0.2208\n",
                        "Seed: 44, Epoch: 093, Loss: 0.0537, Val AP: 0.2249, Test AP: 0.2238\n",
                        "Seed: 44, Epoch: 094, Loss: 0.0535, Val AP: 0.2064, Test AP: 0.2089\n",
                        "Seed: 44, Epoch: 095, Loss: 0.0536, Val AP: 0.2255, Test AP: 0.2262\n",
                        "Seed: 44, Epoch: 096, Loss: 0.0535, Val AP: 0.2191, Test AP: 0.2204\n",
                        "Seed: 44, Epoch: 097, Loss: 0.0535, Val AP: 0.2217, Test AP: 0.2205\n",
                        "Seed: 44, Epoch: 098, Loss: 0.0534, Val AP: 0.2285, Test AP: 0.2289\n",
                        "Seed: 44, Epoch: 099, Loss: 0.0534, Val AP: 0.2228, Test AP: 0.2233\n",
                        "Seed: 44, Epoch: 100, Loss: 0.0534, Val AP: 0.2199, Test AP: 0.2212\n",
                        "Seed: 44, Epoch: 101, Loss: 0.0534, Val AP: 0.2259, Test AP: 0.2267\n",
                        "Seed: 44, Epoch: 102, Loss: 0.0533, Val AP: 0.2175, Test AP: 0.2193\n",
                        "Seed: 44, Epoch: 103, Loss: 0.0535, Val AP: 0.2181, Test AP: 0.2183\n",
                        "Seed: 44, Epoch: 104, Loss: 0.0537, Val AP: 0.2146, Test AP: 0.2162\n",
                        "Seed: 44, Epoch: 105, Loss: 0.0539, Val AP: 0.2157, Test AP: 0.2179\n",
                        "Seed: 44, Epoch: 106, Loss: 0.0539, Val AP: 0.2117, Test AP: 0.2148\n",
                        "Seed: 44, Epoch: 107, Loss: 0.0538, Val AP: 0.2212, Test AP: 0.2213\n",
                        "Seed: 44, Epoch: 108, Loss: 0.0539, Val AP: 0.2106, Test AP: 0.2108\n",
                        "Seed: 44, Epoch: 109, Loss: 0.0539, Val AP: 0.2106, Test AP: 0.2121\n",
                        "Seed: 44, Epoch: 110, Loss: 0.0538, Val AP: 0.2021, Test AP: 0.2025\n",
                        "Seed: 44, Epoch: 111, Loss: 0.0538, Val AP: 0.2129, Test AP: 0.2150\n",
                        "Seed: 44, Epoch: 112, Loss: 0.0538, Val AP: 0.2165, Test AP: 0.2158\n",
                        "Seed: 44, Epoch: 113, Loss: 0.0538, Val AP: 0.2165, Test AP: 0.2175\n",
                        "Seed: 44, Epoch: 114, Loss: 0.0538, Val AP: 0.2199, Test AP: 0.2215\n",
                        "Seed: 44, Epoch: 115, Loss: 0.0535, Val AP: 0.2215, Test AP: 0.2233\n",
                        "Seed: 44, Epoch: 116, Loss: 0.0535, Val AP: 0.2133, Test AP: 0.2152\n",
                        "Seed: 44, Epoch: 117, Loss: 0.0537, Val AP: 0.2214, Test AP: 0.2218\n",
                        "Seed: 44, Epoch: 118, Loss: 0.0537, Val AP: 0.2259, Test AP: 0.2273\n",
                        "Seed: 44, Epoch: 119, Loss: 0.0536, Val AP: 0.2079, Test AP: 0.2099\n",
                        "Seed: 44, Epoch: 120, Loss: 0.0535, Val AP: 0.2129, Test AP: 0.2157\n",
                        "Seed: 44, Epoch: 121, Loss: 0.0537, Val AP: 0.2144, Test AP: 0.2160\n",
                        "Seed: 44, Epoch: 122, Loss: 0.0535, Val AP: 0.2162, Test AP: 0.2198\n",
                        "Seed: 44, Epoch: 123, Loss: 0.0535, Val AP: 0.2174, Test AP: 0.2201\n",
                        "Seed: 44, Epoch: 124, Loss: 0.0536, Val AP: 0.2217, Test AP: 0.2227\n",
                        "Seed: 44, Epoch: 125, Loss: 0.0536, Val AP: 0.2205, Test AP: 0.2219\n",
                        "Seed: 44, Epoch: 126, Loss: 0.0534, Val AP: 0.2124, Test AP: 0.2169\n",
                        "Seed: 44, Epoch: 127, Loss: 0.0536, Val AP: 0.2135, Test AP: 0.2168\n",
                        "Seed: 44, Epoch: 128, Loss: 0.0535, Val AP: 0.2249, Test AP: 0.2279\n",
                        "Seed: 44, Epoch: 129, Loss: 0.0534, Val AP: 0.2196, Test AP: 0.2247\n",
                        "Seed: 44, Epoch: 130, Loss: 0.0535, Val AP: 0.2192, Test AP: 0.2235\n",
                        "Seed: 44, Epoch: 131, Loss: 0.0535, Val AP: 0.2221, Test AP: 0.2237\n",
                        "Seed: 44, Epoch: 132, Loss: 0.0534, Val AP: 0.2183, Test AP: 0.2197\n",
                        "Seed: 44, Epoch: 133, Loss: 0.0535, Val AP: 0.2191, Test AP: 0.2205\n",
                        "Seed: 44, Epoch: 134, Loss: 0.0534, Val AP: 0.2171, Test AP: 0.2179\n",
                        "Seed: 44, Epoch: 135, Loss: 0.0534, Val AP: 0.2179, Test AP: 0.2210\n",
                        "Seed: 44, Epoch: 136, Loss: 0.0534, Val AP: 0.2201, Test AP: 0.2223\n",
                        "Seed: 44, Epoch: 137, Loss: 0.0533, Val AP: 0.2157, Test AP: 0.2143\n",
                        "Seed: 44, Epoch: 138, Loss: 0.0533, Val AP: 0.2150, Test AP: 0.2177\n",
                        "Seed: 44, Epoch: 139, Loss: 0.0533, Val AP: 0.2202, Test AP: 0.2220\n",
                        "Seed: 44, Epoch: 140, Loss: 0.0532, Val AP: 0.2151, Test AP: 0.2199\n",
                        "Seed: 44, Epoch: 141, Loss: 0.0533, Val AP: 0.2231, Test AP: 0.2256\n",
                        "Seed: 44, Epoch: 142, Loss: 0.0532, Val AP: 0.2216, Test AP: 0.2245\n",
                        "Seed: 44, Epoch: 143, Loss: 0.0532, Val AP: 0.2177, Test AP: 0.2228\n",
                        "Seed: 44, Epoch: 144, Loss: 0.0531, Val AP: 0.2139, Test AP: 0.2139\n",
                        "Seed: 44, Epoch: 145, Loss: 0.0532, Val AP: 0.2292, Test AP: 0.2318\n",
                        "Seed: 44, Epoch: 146, Loss: 0.0533, Val AP: 0.2144, Test AP: 0.2146\n",
                        "Seed: 44, Epoch: 147, Loss: 0.0531, Val AP: 0.2247, Test AP: 0.2274\n",
                        "Seed: 44, Epoch: 148, Loss: 0.0531, Val AP: 0.2261, Test AP: 0.2291\n",
                        "Seed: 44, Epoch: 149, Loss: 0.0532, Val AP: 0.2231, Test AP: 0.2258\n",
                        "Seed: 44, Epoch: 150, Loss: 0.0530, Val AP: 0.2249, Test AP: 0.2255\n",
                        "Seed: 44, Epoch: 151, Loss: 0.0532, Val AP: 0.2291, Test AP: 0.2300\n",
                        "Seed: 44, Epoch: 152, Loss: 0.0531, Val AP: 0.2173, Test AP: 0.2197\n",
                        "Seed: 44, Epoch: 153, Loss: 0.0531, Val AP: 0.2302, Test AP: 0.2302\n",
                        "Seed: 44, Epoch: 154, Loss: 0.0529, Val AP: 0.2200, Test AP: 0.2204\n",
                        "Seed: 44, Epoch: 155, Loss: 0.0530, Val AP: 0.2236, Test AP: 0.2262\n",
                        "Seed: 44, Epoch: 156, Loss: 0.0530, Val AP: 0.2238, Test AP: 0.2245\n",
                        "Seed: 44, Epoch: 157, Loss: 0.0530, Val AP: 0.2251, Test AP: 0.2275\n",
                        "Seed: 44, Epoch: 158, Loss: 0.0529, Val AP: 0.2257, Test AP: 0.2262\n",
                        "Seed: 44, Epoch: 159, Loss: 0.0531, Val AP: 0.2215, Test AP: 0.2222\n",
                        "Seed: 44, Epoch: 160, Loss: 0.0529, Val AP: 0.2048, Test AP: 0.2119\n",
                        "Seed: 44, Epoch: 161, Loss: 0.0530, Val AP: 0.2250, Test AP: 0.2293\n",
                        "Seed: 44, Epoch: 162, Loss: 0.0529, Val AP: 0.2225, Test AP: 0.2258\n",
                        "Seed: 44, Epoch: 163, Loss: 0.0529, Val AP: 0.2291, Test AP: 0.2274\n",
                        "Seed: 44, Epoch: 164, Loss: 0.0530, Val AP: 0.2190, Test AP: 0.2251\n",
                        "Seed: 44, Epoch: 165, Loss: 0.0530, Val AP: 0.2268, Test AP: 0.2291\n",
                        "Seed: 44, Epoch: 166, Loss: 0.0529, Val AP: 0.2281, Test AP: 0.2294\n",
                        "Seed: 44, Epoch: 167, Loss: 0.0530, Val AP: 0.2250, Test AP: 0.2254\n",
                        "Seed: 44, Epoch: 168, Loss: 0.0529, Val AP: 0.2321, Test AP: 0.2337\n",
                        "Seed: 44, Epoch: 169, Loss: 0.0528, Val AP: 0.2287, Test AP: 0.2271\n",
                        "Seed: 44, Epoch: 170, Loss: 0.0528, Val AP: 0.2238, Test AP: 0.2259\n",
                        "Seed: 44, Epoch: 171, Loss: 0.0528, Val AP: 0.2258, Test AP: 0.2270\n",
                        "Seed: 44, Epoch: 172, Loss: 0.0528, Val AP: 0.2278, Test AP: 0.2275\n",
                        "Seed: 44, Epoch: 173, Loss: 0.0529, Val AP: 0.2307, Test AP: 0.2309\n",
                        "Seed: 44, Epoch: 174, Loss: 0.0528, Val AP: 0.2223, Test AP: 0.2233\n",
                        "Seed: 44, Epoch: 175, Loss: 0.0528, Val AP: 0.2188, Test AP: 0.2208\n",
                        "Seed: 44, Epoch: 176, Loss: 0.0527, Val AP: 0.2233, Test AP: 0.2252\n",
                        "Seed: 44, Epoch: 177, Loss: 0.0527, Val AP: 0.2302, Test AP: 0.2334\n",
                        "Seed: 44, Epoch: 178, Loss: 0.0527, Val AP: 0.2275, Test AP: 0.2280\n",
                        "Seed: 44, Epoch: 179, Loss: 0.0527, Val AP: 0.2324, Test AP: 0.2340\n",
                        "Seed: 44, Epoch: 180, Loss: 0.0527, Val AP: 0.2291, Test AP: 0.2278\n",
                        "Seed: 44, Epoch: 181, Loss: 0.0527, Val AP: 0.2340, Test AP: 0.2354\n",
                        "Seed: 44, Epoch: 182, Loss: 0.0526, Val AP: 0.2168, Test AP: 0.2195\n",
                        "Seed: 44, Epoch: 183, Loss: 0.0527, Val AP: 0.2270, Test AP: 0.2294\n",
                        "Seed: 44, Epoch: 184, Loss: 0.0527, Val AP: 0.2278, Test AP: 0.2301\n",
                        "Seed: 44, Epoch: 185, Loss: 0.0528, Val AP: 0.2243, Test AP: 0.2242\n",
                        "Seed: 44, Epoch: 186, Loss: 0.0526, Val AP: 0.2354, Test AP: 0.2348\n",
                        "Seed: 44, Epoch: 187, Loss: 0.0526, Val AP: 0.2332, Test AP: 0.2337\n",
                        "Seed: 44, Epoch: 188, Loss: 0.0526, Val AP: 0.2312, Test AP: 0.2348\n",
                        "Seed: 44, Epoch: 189, Loss: 0.0526, Val AP: 0.2296, Test AP: 0.2303\n",
                        "Seed: 44, Epoch: 190, Loss: 0.0526, Val AP: 0.2337, Test AP: 0.2343\n",
                        "Seed: 44, Epoch: 191, Loss: 0.0525, Val AP: 0.2303, Test AP: 0.2300\n",
                        "Seed: 44, Epoch: 192, Loss: 0.0526, Val AP: 0.2290, Test AP: 0.2302\n",
                        "Seed: 44, Epoch: 193, Loss: 0.0525, Val AP: 0.2326, Test AP: 0.2339\n",
                        "Seed: 44, Epoch: 194, Loss: 0.0525, Val AP: 0.2241, Test AP: 0.2265\n",
                        "Seed: 44, Epoch: 195, Loss: 0.0524, Val AP: 0.2338, Test AP: 0.2335\n",
                        "Seed: 44, Epoch: 196, Loss: 0.0525, Val AP: 0.2382, Test AP: 0.2390\n",
                        "Seed: 44, Epoch: 197, Loss: 0.0526, Val AP: 0.2285, Test AP: 0.2329\n",
                        "Seed: 44, Epoch: 198, Loss: 0.0526, Val AP: 0.2340, Test AP: 0.2350\n",
                        "Seed: 44, Epoch: 199, Loss: 0.0524, Val AP: 0.2329, Test AP: 0.2347\n",
                        "Seed: 44, Epoch: 200, Loss: 0.0524, Val AP: 0.2366, Test AP: 0.2393\n",
                        "Average Time: 7253.79 seconds\n",
                        "Var Time: 214569.55 seconds\n",
                        "Average Memory: 8126.00 MB\n",
                        "Average Best Val AP: 0.2106\n",
                        "Std Best Test AP: 0.0219\n",
                        "Average Test AP: 0.2108\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nclass HierarchicalGCN_SAG(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_SAG, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, improved = True)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = SAGPooling(hidden_channels, ratio=0.7)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, improved = True)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = SAGPooling(hidden_channels, ratio=0.7)\n        self.conv3 = GCNConv(hidden_channels, out_channels, improved = True)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        init_x = data.x\n        x = x.float()\n        init_x = init_x.float()\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, edge_attr, batch, _, _ = self.pool1(x, edge_index, edge_attr, batch=batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, edge_attr, batch, _, _ = self.pool2(x, edge_index, edge_attr, batch=batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_SAG(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=4096, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False)\n    model = HierarchicalGCN_SAG(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ASAPooling with HierarchicalGCN (2019)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nclass HierarchicalGCN_ASA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_ASA, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, improved = True)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = ASAPooling(hidden_channels, ratio=0.9)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, improved = True)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = ASAPooling(hidden_channels, ratio=0.9)\n        self.conv3 = GCNConv(hidden_channels, out_channels, improved = True)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        init_x = data.x\n        x = x.float()\n        init_x = init_x.float()\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _ = self.pool1(x, edge_index, batch=batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _ = self.pool2(x, edge_index, batch=batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_ASA(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=4096, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False)\n    model = HierarchicalGCN_ASA(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### PANPooling with HierarchicalGCN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nclass PANPooling(torch.nn.Module):\n    r\"\"\" General Graph pooling layer based on PAN, which can work with all layers.\n    \"\"\"\n    def __init__(self, in_channels, ratio=0.5, pan_pool_weight=None, min_score=None, multiplier=1,\n                 nonlinearity=torch.tanh, filter_size=3, panpool_filter_weight=None):\n        super(PANPooling, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.nonlinearity = nonlinearity\n        self.filter_size = filter_size\n        if panpool_filter_weight is None:\n            self.panpool_filter_weight = torch.nn.Parameter(0.5 * torch.ones(filter_size), requires_grad=True)\n        self.transform = Parameter(torch.ones(in_channels), requires_grad=True)\n        if pan_pool_weight is None:\n            self.pan_pool_weight = torch.nn.Parameter(0.5 * torch.ones(2), requires_grad=True)\n        else:\n            self.pan_pool_weight = pan_pool_weight\n    def forward(self, x, edge_index, M=None, batch=None, num_nodes=None):\n        \"\"\"\"\"\"\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        edge_index, edge_weight = self.panentropy_sparse(edge_index, num_nodes)\n        num_nodes = x.size(0)\n        degree = torch.zeros(num_nodes, device=edge_index.device)\n        degree = scatter_add(edge_weight, edge_index[0], out=degree)\n        xtransform = torch.matmul(x, self.transform)\n        x_transform_norm = xtransform \n        degree_norm = degree \n        score = self.pan_pool_weight[0] * x_transform_norm + self.pan_pool_weight[1] * degree_norm\n        if self.min_score is None:\n            score = self.nonlinearity(score)\n        else:\n            score = softmax(score, batch)\n        perm = self.topk(score, self.ratio, batch, self.min_score)\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n        batch = batch[perm]\n        edge_index, edge_weight = self.filter_adj(edge_index, edge_weight, perm, num_nodes=score.size(0))\n        return x, edge_index, edge_weight, batch, perm, score[perm]\n    def topk(self, x, ratio, batch, min_score=None, tol=1e-7):\n        if min_score is not None:\n            scores_max = scatter_max(x, batch)[0][batch] - tol\n            scores_min = scores_max.clamp(max=min_score)\n            perm = torch.nonzero(x > scores_min).view(-1)\n        else:\n            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n            batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n            cum_num_nodes = torch.cat(\n                [num_nodes.new_zeros(1),\n                 num_nodes.cumsum(dim=0)[:-1]], dim=0)\n            index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n            index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n            dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n            dense_x[index] = x\n            dense_x = dense_x.view(batch_size, max_num_nodes)\n            _, perm = dense_x.sort(dim=-1, descending=True)\n            perm = perm + cum_num_nodes.view(-1, 1)\n            perm = perm.view(-1)\n            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n            mask = [\n                torch.arange(k[i], dtype=torch.long, device=x.device) +\n                i * max_num_nodes for i in range(batch_size)\n            ]\n            mask = torch.cat(mask, dim=0)\n            perm = perm[mask]\n        return perm\n    def filter_adj(self, edge_index, edge_weight, perm, num_nodes=None):\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        mask = perm.new_full((num_nodes, ), -1)\n        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n        mask[perm] = i\n        row, col = edge_index\n        row, col = mask[row], mask[col]\n        mask = (row >= 0) & (col >= 0)\n        row, col = row[mask], col[mask]\n        if edge_weight is not None:\n            edge_weight = edge_weight[mask]\n        return torch.stack([row, col], dim=0), edge_weight\n    def panentropy_sparse(self, edge_index, num_nodes):\n        edge_value = torch.ones(edge_index.size(1), device=edge_index.device)\n        edge_index, edge_value = coalesce(edge_index, edge_value, num_nodes, num_nodes)\n        pan_index, pan_value = eye(num_nodes, device=edge_index.device)\n        indextmp = pan_index.clone().to(edge_index.device)\n        valuetmp = pan_value.clone().to(edge_index.device)\n        pan_value = self.panpool_filter_weight[0] * pan_value\n        for i in range(self.filter_size - 1):\n            indextmp, valuetmp = spspmm(indextmp, valuetmp, edge_index, edge_value, num_nodes, num_nodes, num_nodes)\n            valuetmp = valuetmp * self.panpool_filter_weight[i+1]\n            indextmp, valuetmp = coalesce(indextmp, valuetmp, num_nodes, num_nodes)\n            pan_index = torch.cat((pan_index, indextmp), 1)\n            pan_value = torch.cat((pan_value, valuetmp))\n        return coalesce(pan_index, pan_value, num_nodes, num_nodes, op='add')"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0641, Val AP: 0.1125, Test AP: 0.1075\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0608, Val AP: 0.1230, Test AP: 0.1193\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0604, Val AP: 0.1211, Test AP: 0.1178\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0603, Val AP: 0.1168, Test AP: 0.1154\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0602, Val AP: 0.1153, Test AP: 0.1143\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0599, Val AP: 0.1193, Test AP: 0.1181\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0596, Val AP: 0.1222, Test AP: 0.1203\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0596, Val AP: 0.1247, Test AP: 0.1218\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0594, Val AP: 0.1189, Test AP: 0.1175\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0592, Val AP: 0.1281, Test AP: 0.1254\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0590, Val AP: 0.1350, Test AP: 0.1316\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0587, Val AP: 0.1381, Test AP: 0.1366\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0585, Val AP: 0.1388, Test AP: 0.1358\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0583, Val AP: 0.1465, Test AP: 0.1440\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0582, Val AP: 0.1505, Test AP: 0.1483\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0580, Val AP: 0.1527, Test AP: 0.1514\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0579, Val AP: 0.1527, Test AP: 0.1514\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0578, Val AP: 0.1610, Test AP: 0.1592\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0576, Val AP: 0.1551, Test AP: 0.1542\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0575, Val AP: 0.1614, Test AP: 0.1606\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0574, Val AP: 0.1704, Test AP: 0.1703\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0572, Val AP: 0.1603, Test AP: 0.1593\n",
                        "Seed: 42, Epoch: 023, Loss: 0.0572, Val AP: 0.1656, Test AP: 0.1650\n",
                        "Seed: 42, Epoch: 024, Loss: 0.0571, Val AP: 0.1642, Test AP: 0.1630\n",
                        "Seed: 42, Epoch: 025, Loss: 0.0570, Val AP: 0.1740, Test AP: 0.1729\n",
                        "Seed: 42, Epoch: 026, Loss: 0.0569, Val AP: 0.1721, Test AP: 0.1721\n",
                        "Seed: 42, Epoch: 027, Loss: 0.0569, Val AP: 0.1694, Test AP: 0.1695\n",
                        "Seed: 42, Epoch: 028, Loss: 0.0568, Val AP: 0.1728, Test AP: 0.1736\n",
                        "Seed: 42, Epoch: 029, Loss: 0.0568, Val AP: 0.1672, Test AP: 0.1668\n",
                        "Seed: 42, Epoch: 030, Loss: 0.0566, Val AP: 0.1655, Test AP: 0.1675\n",
                        "Seed: 42, Epoch: 031, Loss: 0.0567, Val AP: 0.1711, Test AP: 0.1734\n",
                        "Seed: 42, Epoch: 032, Loss: 0.0568, Val AP: 0.1715, Test AP: 0.1736\n",
                        "Seed: 42, Epoch: 033, Loss: 0.0566, Val AP: 0.1637, Test AP: 0.1653\n",
                        "Seed: 42, Epoch: 034, Loss: 0.0567, Val AP: 0.1654, Test AP: 0.1689\n",
                        "Seed: 42, Epoch: 035, Loss: 0.0566, Val AP: 0.1721, Test AP: 0.1754\n",
                        "Seed: 42, Epoch: 036, Loss: 0.0564, Val AP: 0.1721, Test AP: 0.1744\n",
                        "Seed: 42, Epoch: 037, Loss: 0.0565, Val AP: 0.1706, Test AP: 0.1738\n",
                        "Seed: 42, Epoch: 038, Loss: 0.0563, Val AP: 0.1520, Test AP: 0.1514\n",
                        "Seed: 42, Epoch: 039, Loss: 0.0563, Val AP: 0.1755, Test AP: 0.1795\n",
                        "Seed: 42, Epoch: 040, Loss: 0.0565, Val AP: 0.1774, Test AP: 0.1806\n",
                        "Seed: 42, Epoch: 041, Loss: 0.0564, Val AP: 0.1680, Test AP: 0.1724\n",
                        "Seed: 42, Epoch: 042, Loss: 0.0562, Val AP: 0.1715, Test AP: 0.1728\n",
                        "Seed: 42, Epoch: 043, Loss: 0.0561, Val AP: 0.1729, Test AP: 0.1765\n",
                        "Seed: 42, Epoch: 044, Loss: 0.0561, Val AP: 0.1707, Test AP: 0.1715\n",
                        "Seed: 42, Epoch: 045, Loss: 0.0563, Val AP: 0.1841, Test AP: 0.1860\n",
                        "Seed: 42, Epoch: 046, Loss: 0.0562, Val AP: 0.1611, Test AP: 0.1614\n",
                        "Seed: 42, Epoch: 047, Loss: 0.0561, Val AP: 0.1606, Test AP: 0.1608\n",
                        "Seed: 42, Epoch: 048, Loss: 0.0563, Val AP: 0.1757, Test AP: 0.1794\n",
                        "Seed: 42, Epoch: 049, Loss: 0.0564, Val AP: 0.1565, Test AP: 0.1559\n",
                        "Seed: 42, Epoch: 050, Loss: 0.0560, Val AP: 0.1678, Test AP: 0.1672\n",
                        "Seed: 42, Epoch: 051, Loss: 0.0562, Val AP: 0.1771, Test AP: 0.1783\n",
                        "Seed: 42, Epoch: 052, Loss: 0.0560, Val AP: 0.1810, Test AP: 0.1825\n",
                        "Seed: 42, Epoch: 053, Loss: 0.0561, Val AP: 0.1646, Test AP: 0.1637\n",
                        "Seed: 42, Epoch: 054, Loss: 0.0561, Val AP: 0.1779, Test AP: 0.1806\n",
                        "Seed: 42, Epoch: 055, Loss: 0.0560, Val AP: 0.1711, Test AP: 0.1725\n",
                        "Seed: 42, Epoch: 056, Loss: 0.0560, Val AP: 0.1807, Test AP: 0.1848\n",
                        "Seed: 42, Epoch: 057, Loss: 0.0558, Val AP: 0.1763, Test AP: 0.1774\n",
                        "Seed: 42, Epoch: 058, Loss: 0.0560, Val AP: 0.1870, Test AP: 0.1901\n",
                        "Seed: 42, Epoch: 059, Loss: 0.0559, Val AP: 0.1686, Test AP: 0.1674\n",
                        "Seed: 42, Epoch: 060, Loss: 0.0559, Val AP: 0.1858, Test AP: 0.1872\n",
                        "Seed: 42, Epoch: 061, Loss: 0.0557, Val AP: 0.1858, Test AP: 0.1885\n",
                        "Seed: 42, Epoch: 062, Loss: 0.0558, Val AP: 0.1795, Test AP: 0.1810\n",
                        "Seed: 42, Epoch: 063, Loss: 0.0557, Val AP: 0.1859, Test AP: 0.1866\n",
                        "Seed: 42, Epoch: 064, Loss: 0.0558, Val AP: 0.1829, Test AP: 0.1861\n",
                        "Seed: 42, Epoch: 065, Loss: 0.0557, Val AP: 0.1912, Test AP: 0.1911\n",
                        "Seed: 42, Epoch: 066, Loss: 0.0557, Val AP: 0.1702, Test AP: 0.1692\n",
                        "Seed: 42, Epoch: 067, Loss: 0.0558, Val AP: 0.1709, Test AP: 0.1711\n",
                        "Seed: 42, Epoch: 068, Loss: 0.0558, Val AP: 0.1752, Test AP: 0.1746\n",
                        "Seed: 42, Epoch: 069, Loss: 0.0559, Val AP: 0.1778, Test AP: 0.1761\n",
                        "Seed: 42, Epoch: 070, Loss: 0.0559, Val AP: 0.1705, Test AP: 0.1690\n",
                        "Seed: 42, Epoch: 071, Loss: 0.0558, Val AP: 0.1777, Test AP: 0.1800\n",
                        "Seed: 42, Epoch: 072, Loss: 0.0558, Val AP: 0.1825, Test AP: 0.1846\n",
                        "Seed: 42, Epoch: 073, Loss: 0.0555, Val AP: 0.1759, Test AP: 0.1759\n",
                        "Seed: 42, Epoch: 074, Loss: 0.0559, Val AP: 0.1752, Test AP: 0.1737\n",
                        "Seed: 42, Epoch: 075, Loss: 0.0558, Val AP: 0.1805, Test AP: 0.1788\n",
                        "Seed: 42, Epoch: 076, Loss: 0.0555, Val AP: 0.1744, Test AP: 0.1727\n",
                        "Seed: 42, Epoch: 077, Loss: 0.0555, Val AP: 0.1732, Test AP: 0.1727\n",
                        "Seed: 42, Epoch: 078, Loss: 0.0556, Val AP: 0.1679, Test AP: 0.1675\n",
                        "Seed: 42, Epoch: 079, Loss: 0.0554, Val AP: 0.1807, Test AP: 0.1817\n",
                        "Seed: 42, Epoch: 080, Loss: 0.0555, Val AP: 0.1795, Test AP: 0.1787\n",
                        "Seed: 42, Epoch: 081, Loss: 0.0555, Val AP: 0.1950, Test AP: 0.1944\n",
                        "Seed: 42, Epoch: 082, Loss: 0.0553, Val AP: 0.1690, Test AP: 0.1684\n",
                        "Seed: 42, Epoch: 083, Loss: 0.0554, Val AP: 0.1909, Test AP: 0.1916\n",
                        "Seed: 42, Epoch: 084, Loss: 0.0551, Val AP: 0.1895, Test AP: 0.1900\n",
                        "Seed: 42, Epoch: 085, Loss: 0.0554, Val AP: 0.1776, Test AP: 0.1773\n",
                        "Seed: 42, Epoch: 086, Loss: 0.0554, Val AP: 0.1979, Test AP: 0.1978\n",
                        "Seed: 42, Epoch: 087, Loss: 0.0554, Val AP: 0.1835, Test AP: 0.1824\n",
                        "Seed: 42, Epoch: 088, Loss: 0.0555, Val AP: 0.1941, Test AP: 0.1945\n",
                        "Seed: 42, Epoch: 089, Loss: 0.0554, Val AP: 0.1926, Test AP: 0.1928\n",
                        "Seed: 42, Epoch: 090, Loss: 0.0553, Val AP: 0.1879, Test AP: 0.1889\n",
                        "Seed: 42, Epoch: 091, Loss: 0.0555, Val AP: 0.1819, Test AP: 0.1829\n",
                        "Seed: 42, Epoch: 092, Loss: 0.0550, Val AP: 0.1941, Test AP: 0.1943\n",
                        "Seed: 42, Epoch: 093, Loss: 0.0550, Val AP: 0.1962, Test AP: 0.1961\n",
                        "Seed: 42, Epoch: 094, Loss: 0.0549, Val AP: 0.1958, Test AP: 0.1947\n",
                        "Seed: 42, Epoch: 095, Loss: 0.0549, Val AP: 0.1915, Test AP: 0.1909\n",
                        "Seed: 42, Epoch: 096, Loss: 0.0550, Val AP: 0.1970, Test AP: 0.1971\n",
                        "Seed: 42, Epoch: 097, Loss: 0.0550, Val AP: 0.1940, Test AP: 0.1943\n",
                        "Seed: 42, Epoch: 098, Loss: 0.0549, Val AP: 0.1942, Test AP: 0.1943\n",
                        "Seed: 42, Epoch: 099, Loss: 0.0549, Val AP: 0.1939, Test AP: 0.1950\n",
                        "Seed: 42, Epoch: 100, Loss: 0.0550, Val AP: 0.1847, Test AP: 0.1853\n",
                        "Seed: 42, Epoch: 101, Loss: 0.0548, Val AP: 0.1932, Test AP: 0.1923\n",
                        "Seed: 42, Epoch: 102, Loss: 0.0549, Val AP: 0.1937, Test AP: 0.1926\n",
                        "Seed: 42, Epoch: 103, Loss: 0.0549, Val AP: 0.1916, Test AP: 0.1914\n",
                        "Seed: 42, Epoch: 104, Loss: 0.0548, Val AP: 0.1890, Test AP: 0.1903\n",
                        "Seed: 42, Epoch: 105, Loss: 0.0548, Val AP: 0.1966, Test AP: 0.1966\n",
                        "Seed: 42, Epoch: 106, Loss: 0.0549, Val AP: 0.1930, Test AP: 0.1922\n",
                        "Seed: 42, Epoch: 107, Loss: 0.0548, Val AP: 0.2007, Test AP: 0.1994\n",
                        "Seed: 42, Epoch: 108, Loss: 0.0548, Val AP: 0.1916, Test AP: 0.1913\n",
                        "Seed: 42, Epoch: 109, Loss: 0.0549, Val AP: 0.1979, Test AP: 0.1974\n",
                        "Seed: 42, Epoch: 110, Loss: 0.0548, Val AP: 0.1924, Test AP: 0.1931\n",
                        "Seed: 42, Epoch: 111, Loss: 0.0549, Val AP: 0.2021, Test AP: 0.2011\n",
                        "Seed: 42, Epoch: 112, Loss: 0.0550, Val AP: 0.1928, Test AP: 0.1934\n",
                        "Seed: 42, Epoch: 113, Loss: 0.0547, Val AP: 0.2004, Test AP: 0.2001\n",
                        "Seed: 42, Epoch: 114, Loss: 0.0548, Val AP: 0.1954, Test AP: 0.1951\n",
                        "Seed: 42, Epoch: 115, Loss: 0.0548, Val AP: 0.1786, Test AP: 0.1775\n",
                        "Seed: 42, Epoch: 116, Loss: 0.0551, Val AP: 0.1813, Test AP: 0.1799\n",
                        "Seed: 42, Epoch: 117, Loss: 0.0547, Val AP: 0.1918, Test AP: 0.1915\n",
                        "Seed: 42, Epoch: 118, Loss: 0.0549, Val AP: 0.1968, Test AP: 0.1979\n",
                        "Seed: 42, Epoch: 119, Loss: 0.0548, Val AP: 0.1946, Test AP: 0.1950\n",
                        "Seed: 42, Epoch: 120, Loss: 0.0547, Val AP: 0.1987, Test AP: 0.1975\n",
                        "Seed: 42, Epoch: 121, Loss: 0.0547, Val AP: 0.2017, Test AP: 0.1998\n",
                        "Seed: 42, Epoch: 122, Loss: 0.0548, Val AP: 0.1989, Test AP: 0.1970\n",
                        "Seed: 42, Epoch: 123, Loss: 0.0548, Val AP: 0.1979, Test AP: 0.1986\n",
                        "Seed: 42, Epoch: 124, Loss: 0.0547, Val AP: 0.1988, Test AP: 0.2002\n",
                        "Seed: 42, Epoch: 125, Loss: 0.0547, Val AP: 0.1997, Test AP: 0.1995\n",
                        "Seed: 42, Epoch: 126, Loss: 0.0545, Val AP: 0.1903, Test AP: 0.1906\n",
                        "Seed: 42, Epoch: 127, Loss: 0.0547, Val AP: 0.1957, Test AP: 0.1951\n",
                        "Seed: 42, Epoch: 128, Loss: 0.0547, Val AP: 0.1862, Test AP: 0.1859\n",
                        "Seed: 42, Epoch: 129, Loss: 0.0547, Val AP: 0.1955, Test AP: 0.1937\n",
                        "Seed: 42, Epoch: 130, Loss: 0.0548, Val AP: 0.2055, Test AP: 0.2043\n",
                        "Seed: 42, Epoch: 131, Loss: 0.0548, Val AP: 0.1988, Test AP: 0.1992\n",
                        "Seed: 42, Epoch: 132, Loss: 0.0546, Val AP: 0.1922, Test AP: 0.1928\n",
                        "Seed: 42, Epoch: 133, Loss: 0.0545, Val AP: 0.1957, Test AP: 0.1953\n",
                        "Seed: 42, Epoch: 134, Loss: 0.0546, Val AP: 0.1981, Test AP: 0.1975\n",
                        "Seed: 42, Epoch: 135, Loss: 0.0546, Val AP: 0.1928, Test AP: 0.1914\n",
                        "Seed: 42, Epoch: 136, Loss: 0.0545, Val AP: 0.2016, Test AP: 0.2002\n",
                        "Seed: 42, Epoch: 137, Loss: 0.0545, Val AP: 0.2013, Test AP: 0.2021\n",
                        "Seed: 42, Epoch: 138, Loss: 0.0546, Val AP: 0.1981, Test AP: 0.1974\n",
                        "Seed: 42, Epoch: 139, Loss: 0.0544, Val AP: 0.1954, Test AP: 0.1937\n",
                        "Seed: 42, Epoch: 140, Loss: 0.0546, Val AP: 0.1938, Test AP: 0.1948\n",
                        "Seed: 42, Epoch: 141, Loss: 0.0545, Val AP: 0.2005, Test AP: 0.2018\n",
                        "Seed: 42, Epoch: 142, Loss: 0.0546, Val AP: 0.2011, Test AP: 0.1987\n",
                        "Seed: 42, Epoch: 143, Loss: 0.0546, Val AP: 0.1953, Test AP: 0.1956\n",
                        "Seed: 42, Epoch: 144, Loss: 0.0545, Val AP: 0.1966, Test AP: 0.1961\n",
                        "Seed: 42, Epoch: 145, Loss: 0.0543, Val AP: 0.2043, Test AP: 0.2038\n",
                        "Seed: 42, Epoch: 146, Loss: 0.0545, Val AP: 0.1983, Test AP: 0.1992\n",
                        "Seed: 42, Epoch: 147, Loss: 0.0545, Val AP: 0.1813, Test AP: 0.1807\n",
                        "Seed: 42, Epoch: 148, Loss: 0.0547, Val AP: 0.1858, Test AP: 0.1841\n",
                        "Seed: 42, Epoch: 149, Loss: 0.0545, Val AP: 0.1985, Test AP: 0.1985\n",
                        "Seed: 42, Epoch: 150, Loss: 0.0544, Val AP: 0.1967, Test AP: 0.1950\n",
                        "Seed: 43, Epoch: 001, Loss: 0.0636, Val AP: 0.1039, Test AP: 0.0976\n",
                        "Seed: 43, Epoch: 002, Loss: 0.0613, Val AP: 0.1209, Test AP: 0.1172\n",
                        "Seed: 43, Epoch: 003, Loss: 0.0601, Val AP: 0.1169, Test AP: 0.1142\n",
                        "Seed: 43, Epoch: 004, Loss: 0.0597, Val AP: 0.1233, Test AP: 0.1201\n",
                        "Seed: 43, Epoch: 005, Loss: 0.0598, Val AP: 0.1221, Test AP: 0.1185\n",
                        "Seed: 43, Epoch: 006, Loss: 0.0595, Val AP: 0.1300, Test AP: 0.1246\n",
                        "Seed: 43, Epoch: 007, Loss: 0.0593, Val AP: 0.1246, Test AP: 0.1211\n",
                        "Seed: 43, Epoch: 008, Loss: 0.0591, Val AP: 0.1297, Test AP: 0.1247\n",
                        "Seed: 43, Epoch: 009, Loss: 0.0590, Val AP: 0.1331, Test AP: 0.1288\n",
                        "Seed: 43, Epoch: 010, Loss: 0.0586, Val AP: 0.1369, Test AP: 0.1314\n",
                        "Seed: 43, Epoch: 011, Loss: 0.0584, Val AP: 0.1459, Test AP: 0.1390\n",
                        "Seed: 43, Epoch: 012, Loss: 0.0582, Val AP: 0.1459, Test AP: 0.1389\n",
                        "Seed: 43, Epoch: 013, Loss: 0.0580, Val AP: 0.1477, Test AP: 0.1436\n",
                        "Seed: 43, Epoch: 014, Loss: 0.0578, Val AP: 0.1543, Test AP: 0.1492\n",
                        "Seed: 43, Epoch: 015, Loss: 0.0576, Val AP: 0.1578, Test AP: 0.1517\n",
                        "Seed: 43, Epoch: 016, Loss: 0.0575, Val AP: 0.1580, Test AP: 0.1522\n",
                        "Seed: 43, Epoch: 017, Loss: 0.0575, Val AP: 0.1600, Test AP: 0.1542\n",
                        "Seed: 43, Epoch: 018, Loss: 0.0573, Val AP: 0.1673, Test AP: 0.1641\n",
                        "Seed: 43, Epoch: 019, Loss: 0.0571, Val AP: 0.1678, Test AP: 0.1646\n",
                        "Seed: 43, Epoch: 020, Loss: 0.0570, Val AP: 0.1720, Test AP: 0.1691\n",
                        "Seed: 43, Epoch: 021, Loss: 0.0569, Val AP: 0.1696, Test AP: 0.1680\n",
                        "Seed: 43, Epoch: 022, Loss: 0.0568, Val AP: 0.1637, Test AP: 0.1629\n",
                        "Seed: 43, Epoch: 023, Loss: 0.0566, Val AP: 0.1747, Test AP: 0.1730\n",
                        "Seed: 43, Epoch: 024, Loss: 0.0565, Val AP: 0.1782, Test AP: 0.1760\n",
                        "Seed: 43, Epoch: 025, Loss: 0.0564, Val AP: 0.1796, Test AP: 0.1791\n",
                        "Seed: 43, Epoch: 026, Loss: 0.0563, Val AP: 0.1830, Test AP: 0.1807\n",
                        "Seed: 43, Epoch: 027, Loss: 0.0563, Val AP: 0.1845, Test AP: 0.1820\n",
                        "Seed: 43, Epoch: 028, Loss: 0.0561, Val AP: 0.1836, Test AP: 0.1827\n",
                        "Seed: 43, Epoch: 029, Loss: 0.0561, Val AP: 0.1857, Test AP: 0.1847\n",
                        "Seed: 43, Epoch: 030, Loss: 0.0560, Val AP: 0.1808, Test AP: 0.1807\n",
                        "Seed: 43, Epoch: 031, Loss: 0.0560, Val AP: 0.1833, Test AP: 0.1829\n",
                        "Seed: 43, Epoch: 032, Loss: 0.0559, Val AP: 0.1854, Test AP: 0.1864\n",
                        "Seed: 43, Epoch: 033, Loss: 0.0558, Val AP: 0.1839, Test AP: 0.1856\n",
                        "Seed: 43, Epoch: 034, Loss: 0.0558, Val AP: 0.1919, Test AP: 0.1909\n",
                        "Seed: 43, Epoch: 035, Loss: 0.0557, Val AP: 0.1919, Test AP: 0.1916\n",
                        "Seed: 43, Epoch: 036, Loss: 0.0556, Val AP: 0.1853, Test AP: 0.1878\n",
                        "Seed: 43, Epoch: 037, Loss: 0.0556, Val AP: 0.1980, Test AP: 0.1976\n",
                        "Seed: 43, Epoch: 038, Loss: 0.0556, Val AP: 0.1876, Test AP: 0.1870\n",
                        "Seed: 43, Epoch: 039, Loss: 0.0555, Val AP: 0.1856, Test AP: 0.1889\n",
                        "Seed: 43, Epoch: 040, Loss: 0.0555, Val AP: 0.1923, Test AP: 0.1899\n",
                        "Seed: 43, Epoch: 041, Loss: 0.0555, Val AP: 0.1929, Test AP: 0.1957\n",
                        "Seed: 43, Epoch: 042, Loss: 0.0553, Val AP: 0.1925, Test AP: 0.1932\n",
                        "Seed: 43, Epoch: 043, Loss: 0.0554, Val AP: 0.1817, Test AP: 0.1844\n",
                        "Seed: 43, Epoch: 044, Loss: 0.0553, Val AP: 0.1902, Test AP: 0.1926\n",
                        "Seed: 43, Epoch: 045, Loss: 0.0553, Val AP: 0.1878, Test AP: 0.1897\n",
                        "Seed: 43, Epoch: 046, Loss: 0.0552, Val AP: 0.1970, Test AP: 0.1998\n",
                        "Seed: 43, Epoch: 047, Loss: 0.0552, Val AP: 0.1934, Test AP: 0.1946\n",
                        "Seed: 43, Epoch: 048, Loss: 0.0552, Val AP: 0.1949, Test AP: 0.1963\n",
                        "Seed: 43, Epoch: 049, Loss: 0.0551, Val AP: 0.1890, Test AP: 0.1913\n",
                        "Seed: 43, Epoch: 050, Loss: 0.0551, Val AP: 0.1902, Test AP: 0.1938\n",
                        "Seed: 43, Epoch: 051, Loss: 0.0550, Val AP: 0.1904, Test AP: 0.1929\n",
                        "Seed: 43, Epoch: 052, Loss: 0.0549, Val AP: 0.1997, Test AP: 0.1996\n",
                        "Seed: 43, Epoch: 053, Loss: 0.0549, Val AP: 0.1967, Test AP: 0.1990\n",
                        "Seed: 43, Epoch: 054, Loss: 0.0550, Val AP: 0.1929, Test AP: 0.1965\n",
                        "Seed: 43, Epoch: 055, Loss: 0.0550, Val AP: 0.1942, Test AP: 0.1971\n",
                        "Seed: 43, Epoch: 056, Loss: 0.0548, Val AP: 0.2002, Test AP: 0.2008\n",
                        "Seed: 43, Epoch: 057, Loss: 0.0549, Val AP: 0.2013, Test AP: 0.2015\n",
                        "Seed: 43, Epoch: 058, Loss: 0.0548, Val AP: 0.2029, Test AP: 0.2063\n",
                        "Seed: 43, Epoch: 059, Loss: 0.0548, Val AP: 0.1859, Test AP: 0.1894\n",
                        "Seed: 43, Epoch: 060, Loss: 0.0548, Val AP: 0.1987, Test AP: 0.2005\n",
                        "Seed: 43, Epoch: 061, Loss: 0.0548, Val AP: 0.1954, Test AP: 0.1966\n",
                        "Seed: 43, Epoch: 062, Loss: 0.0547, Val AP: 0.1979, Test AP: 0.1985\n",
                        "Seed: 43, Epoch: 063, Loss: 0.0548, Val AP: 0.1978, Test AP: 0.1995\n",
                        "Seed: 43, Epoch: 064, Loss: 0.0546, Val AP: 0.2065, Test AP: 0.2074\n",
                        "Seed: 43, Epoch: 065, Loss: 0.0547, Val AP: 0.2000, Test AP: 0.2028\n",
                        "Seed: 43, Epoch: 066, Loss: 0.0547, Val AP: 0.1990, Test AP: 0.2012\n",
                        "Seed: 43, Epoch: 067, Loss: 0.0547, Val AP: 0.1993, Test AP: 0.2011\n",
                        "Seed: 43, Epoch: 068, Loss: 0.0546, Val AP: 0.1952, Test AP: 0.1973\n",
                        "Seed: 43, Epoch: 069, Loss: 0.0546, Val AP: 0.1968, Test AP: 0.1998\n",
                        "Seed: 43, Epoch: 070, Loss: 0.0545, Val AP: 0.1974, Test AP: 0.1992\n",
                        "Seed: 43, Epoch: 071, Loss: 0.0545, Val AP: 0.2022, Test AP: 0.2044\n",
                        "Seed: 43, Epoch: 072, Loss: 0.0545, Val AP: 0.1985, Test AP: 0.2021\n",
                        "Seed: 43, Epoch: 073, Loss: 0.0545, Val AP: 0.1967, Test AP: 0.2005\n",
                        "Seed: 43, Epoch: 074, Loss: 0.0545, Val AP: 0.2011, Test AP: 0.2040\n",
                        "Seed: 43, Epoch: 075, Loss: 0.0545, Val AP: 0.1926, Test AP: 0.1926\n",
                        "Seed: 43, Epoch: 076, Loss: 0.0546, Val AP: 0.2013, Test AP: 0.2044\n",
                        "Seed: 43, Epoch: 077, Loss: 0.0545, Val AP: 0.2031, Test AP: 0.2062\n",
                        "Seed: 43, Epoch: 078, Loss: 0.0545, Val AP: 0.2021, Test AP: 0.2072\n",
                        "Seed: 43, Epoch: 079, Loss: 0.0545, Val AP: 0.2041, Test AP: 0.2069\n",
                        "Seed: 43, Epoch: 080, Loss: 0.0544, Val AP: 0.2028, Test AP: 0.2037\n",
                        "Seed: 43, Epoch: 081, Loss: 0.0544, Val AP: 0.2045, Test AP: 0.2085\n",
                        "Seed: 43, Epoch: 082, Loss: 0.0544, Val AP: 0.2030, Test AP: 0.2042\n",
                        "Seed: 43, Epoch: 083, Loss: 0.0543, Val AP: 0.2000, Test AP: 0.2045\n",
                        "Seed: 43, Epoch: 084, Loss: 0.0543, Val AP: 0.2044, Test AP: 0.2048\n",
                        "Seed: 43, Epoch: 085, Loss: 0.0543, Val AP: 0.2079, Test AP: 0.2108\n",
                        "Seed: 43, Epoch: 086, Loss: 0.0542, Val AP: 0.2035, Test AP: 0.2064\n",
                        "Seed: 43, Epoch: 087, Loss: 0.0542, Val AP: 0.2100, Test AP: 0.2140\n",
                        "Seed: 43, Epoch: 088, Loss: 0.0543, Val AP: 0.1984, Test AP: 0.2019\n",
                        "Seed: 43, Epoch: 089, Loss: 0.0542, Val AP: 0.2115, Test AP: 0.2140\n",
                        "Seed: 43, Epoch: 090, Loss: 0.0541, Val AP: 0.2109, Test AP: 0.2139\n",
                        "Seed: 43, Epoch: 091, Loss: 0.0541, Val AP: 0.2107, Test AP: 0.2142\n",
                        "Seed: 43, Epoch: 092, Loss: 0.0541, Val AP: 0.1958, Test AP: 0.2017\n",
                        "Seed: 43, Epoch: 093, Loss: 0.0541, Val AP: 0.2075, Test AP: 0.2122\n",
                        "Seed: 43, Epoch: 094, Loss: 0.0541, Val AP: 0.2099, Test AP: 0.2114\n",
                        "Seed: 43, Epoch: 095, Loss: 0.0540, Val AP: 0.2062, Test AP: 0.2096\n",
                        "Seed: 43, Epoch: 096, Loss: 0.0541, Val AP: 0.2105, Test AP: 0.2139\n",
                        "Seed: 43, Epoch: 097, Loss: 0.0540, Val AP: 0.2102, Test AP: 0.2136\n",
                        "Seed: 43, Epoch: 098, Loss: 0.0541, Val AP: 0.2084, Test AP: 0.2124\n",
                        "Seed: 43, Epoch: 099, Loss: 0.0540, Val AP: 0.1996, Test AP: 0.2043\n",
                        "Seed: 43, Epoch: 100, Loss: 0.0540, Val AP: 0.2040, Test AP: 0.2079\n",
                        "Seed: 43, Epoch: 101, Loss: 0.0540, Val AP: 0.2157, Test AP: 0.2165\n",
                        "Seed: 43, Epoch: 102, Loss: 0.0540, Val AP: 0.2133, Test AP: 0.2166\n",
                        "Seed: 43, Epoch: 103, Loss: 0.0540, Val AP: 0.2116, Test AP: 0.2151\n",
                        "Seed: 43, Epoch: 104, Loss: 0.0539, Val AP: 0.2116, Test AP: 0.2147\n",
                        "Seed: 43, Epoch: 105, Loss: 0.0539, Val AP: 0.2094, Test AP: 0.2128\n",
                        "Seed: 43, Epoch: 106, Loss: 0.0539, Val AP: 0.2122, Test AP: 0.2139\n",
                        "Seed: 43, Epoch: 107, Loss: 0.0538, Val AP: 0.2104, Test AP: 0.2126\n",
                        "Seed: 43, Epoch: 108, Loss: 0.0539, Val AP: 0.2159, Test AP: 0.2179\n",
                        "Seed: 43, Epoch: 109, Loss: 0.0538, Val AP: 0.2064, Test AP: 0.2093\n",
                        "Seed: 43, Epoch: 110, Loss: 0.0538, Val AP: 0.2114, Test AP: 0.2148\n",
                        "Seed: 43, Epoch: 111, Loss: 0.0537, Val AP: 0.2085, Test AP: 0.2131\n",
                        "Seed: 43, Epoch: 112, Loss: 0.0538, Val AP: 0.2143, Test AP: 0.2160\n",
                        "Seed: 43, Epoch: 113, Loss: 0.0537, Val AP: 0.2195, Test AP: 0.2217\n",
                        "Seed: 43, Epoch: 114, Loss: 0.0537, Val AP: 0.2185, Test AP: 0.2224\n",
                        "Seed: 43, Epoch: 115, Loss: 0.0538, Val AP: 0.2066, Test AP: 0.2095\n",
                        "Seed: 43, Epoch: 116, Loss: 0.0537, Val AP: 0.2134, Test AP: 0.2164\n",
                        "Seed: 43, Epoch: 117, Loss: 0.0537, Val AP: 0.2123, Test AP: 0.2151\n",
                        "Seed: 43, Epoch: 118, Loss: 0.0537, Val AP: 0.2140, Test AP: 0.2175\n",
                        "Seed: 43, Epoch: 119, Loss: 0.0537, Val AP: 0.2126, Test AP: 0.2157\n",
                        "Seed: 43, Epoch: 120, Loss: 0.0537, Val AP: 0.2155, Test AP: 0.2151\n",
                        "Seed: 43, Epoch: 121, Loss: 0.0537, Val AP: 0.2159, Test AP: 0.2186\n",
                        "Seed: 43, Epoch: 122, Loss: 0.0536, Val AP: 0.2088, Test AP: 0.2135\n",
                        "Seed: 43, Epoch: 123, Loss: 0.0537, Val AP: 0.2098, Test AP: 0.2123\n",
                        "Seed: 43, Epoch: 124, Loss: 0.0536, Val AP: 0.2167, Test AP: 0.2203\n",
                        "Seed: 43, Epoch: 125, Loss: 0.0536, Val AP: 0.2156, Test AP: 0.2172\n",
                        "Seed: 43, Epoch: 126, Loss: 0.0536, Val AP: 0.2168, Test AP: 0.2188\n",
                        "Seed: 43, Epoch: 127, Loss: 0.0536, Val AP: 0.2129, Test AP: 0.2154\n",
                        "Seed: 43, Epoch: 128, Loss: 0.0536, Val AP: 0.2176, Test AP: 0.2210\n",
                        "Seed: 43, Epoch: 129, Loss: 0.0536, Val AP: 0.2227, Test AP: 0.2257\n",
                        "Seed: 43, Epoch: 130, Loss: 0.0536, Val AP: 0.2086, Test AP: 0.2123\n",
                        "Seed: 43, Epoch: 131, Loss: 0.0535, Val AP: 0.2180, Test AP: 0.2200\n",
                        "Seed: 43, Epoch: 132, Loss: 0.0535, Val AP: 0.2097, Test AP: 0.2151\n",
                        "Seed: 43, Epoch: 133, Loss: 0.0535, Val AP: 0.2199, Test AP: 0.2219\n",
                        "Seed: 43, Epoch: 134, Loss: 0.0534, Val AP: 0.2147, Test AP: 0.2177\n",
                        "Seed: 43, Epoch: 135, Loss: 0.0535, Val AP: 0.2170, Test AP: 0.2202\n",
                        "Seed: 43, Epoch: 136, Loss: 0.0534, Val AP: 0.2190, Test AP: 0.2208\n",
                        "Seed: 43, Epoch: 137, Loss: 0.0534, Val AP: 0.2186, Test AP: 0.2218\n",
                        "Seed: 43, Epoch: 138, Loss: 0.0534, Val AP: 0.2167, Test AP: 0.2194\n",
                        "Seed: 43, Epoch: 139, Loss: 0.0534, Val AP: 0.2238, Test AP: 0.2250\n",
                        "Seed: 43, Epoch: 140, Loss: 0.0534, Val AP: 0.2170, Test AP: 0.2204\n",
                        "Seed: 43, Epoch: 141, Loss: 0.0534, Val AP: 0.2182, Test AP: 0.2204\n",
                        "Seed: 43, Epoch: 142, Loss: 0.0535, Val AP: 0.2216, Test AP: 0.2237\n",
                        "Seed: 43, Epoch: 143, Loss: 0.0534, Val AP: 0.2190, Test AP: 0.2229\n",
                        "Seed: 43, Epoch: 144, Loss: 0.0533, Val AP: 0.2242, Test AP: 0.2271\n",
                        "Seed: 43, Epoch: 145, Loss: 0.0534, Val AP: 0.2100, Test AP: 0.2145\n",
                        "Seed: 43, Epoch: 146, Loss: 0.0534, Val AP: 0.2174, Test AP: 0.2206\n",
                        "Seed: 43, Epoch: 147, Loss: 0.0534, Val AP: 0.2198, Test AP: 0.2228\n",
                        "Seed: 43, Epoch: 148, Loss: 0.0535, Val AP: 0.2181, Test AP: 0.2198\n",
                        "Seed: 43, Epoch: 149, Loss: 0.0533, Val AP: 0.2198, Test AP: 0.2210\n",
                        "Seed: 43, Epoch: 150, Loss: 0.0535, Val AP: 0.2202, Test AP: 0.2251\n",
                        "Seed: 44, Epoch: 001, Loss: 0.0651, Val AP: 0.1144, Test AP: 0.1191\n",
                        "Seed: 44, Epoch: 002, Loss: 0.0610, Val AP: 0.1183, Test AP: 0.1201\n",
                        "Seed: 44, Epoch: 003, Loss: 0.0603, Val AP: 0.1176, Test AP: 0.1198\n",
                        "Seed: 44, Epoch: 004, Loss: 0.0602, Val AP: 0.1196, Test AP: 0.1195\n",
                        "Seed: 44, Epoch: 005, Loss: 0.0599, Val AP: 0.1220, Test AP: 0.1222\n",
                        "Seed: 44, Epoch: 006, Loss: 0.0599, Val AP: 0.1224, Test AP: 0.1219\n",
                        "Seed: 44, Epoch: 007, Loss: 0.0596, Val AP: 0.1205, Test AP: 0.1204\n",
                        "Seed: 44, Epoch: 008, Loss: 0.0595, Val AP: 0.1244, Test AP: 0.1244\n",
                        "Seed: 44, Epoch: 009, Loss: 0.0594, Val AP: 0.1239, Test AP: 0.1244\n",
                        "Seed: 44, Epoch: 010, Loss: 0.0592, Val AP: 0.1253, Test AP: 0.1254\n",
                        "Seed: 44, Epoch: 011, Loss: 0.0590, Val AP: 0.1292, Test AP: 0.1301\n",
                        "Seed: 44, Epoch: 012, Loss: 0.0590, Val AP: 0.1309, Test AP: 0.1334\n",
                        "Seed: 44, Epoch: 013, Loss: 0.0587, Val AP: 0.1328, Test AP: 0.1343\n",
                        "Seed: 44, Epoch: 014, Loss: 0.0586, Val AP: 0.1370, Test AP: 0.1397\n",
                        "Seed: 44, Epoch: 015, Loss: 0.0583, Val AP: 0.1392, Test AP: 0.1430\n",
                        "Seed: 44, Epoch: 016, Loss: 0.0582, Val AP: 0.1439, Test AP: 0.1486\n",
                        "Seed: 44, Epoch: 017, Loss: 0.0580, Val AP: 0.1477, Test AP: 0.1518\n",
                        "Seed: 44, Epoch: 018, Loss: 0.0578, Val AP: 0.1443, Test AP: 0.1485\n",
                        "Seed: 44, Epoch: 019, Loss: 0.0579, Val AP: 0.1496, Test AP: 0.1530\n",
                        "Seed: 44, Epoch: 020, Loss: 0.0576, Val AP: 0.1528, Test AP: 0.1560\n",
                        "Seed: 44, Epoch: 021, Loss: 0.0575, Val AP: 0.1608, Test AP: 0.1629\n",
                        "Seed: 44, Epoch: 022, Loss: 0.0575, Val AP: 0.1595, Test AP: 0.1626\n",
                        "Seed: 44, Epoch: 023, Loss: 0.0573, Val AP: 0.1680, Test AP: 0.1718\n",
                        "Seed: 44, Epoch: 024, Loss: 0.0572, Val AP: 0.1711, Test AP: 0.1744\n",
                        "Seed: 44, Epoch: 025, Loss: 0.0571, Val AP: 0.1687, Test AP: 0.1704\n",
                        "Seed: 44, Epoch: 026, Loss: 0.0569, Val AP: 0.1711, Test AP: 0.1714\n",
                        "Seed: 44, Epoch: 027, Loss: 0.0569, Val AP: 0.1671, Test AP: 0.1672\n",
                        "Seed: 44, Epoch: 028, Loss: 0.0569, Val AP: 0.1700, Test AP: 0.1712\n",
                        "Seed: 44, Epoch: 029, Loss: 0.0566, Val AP: 0.1757, Test AP: 0.1762\n",
                        "Seed: 44, Epoch: 030, Loss: 0.0566, Val AP: 0.1769, Test AP: 0.1766\n",
                        "Seed: 44, Epoch: 031, Loss: 0.0565, Val AP: 0.1802, Test AP: 0.1800\n",
                        "Seed: 44, Epoch: 032, Loss: 0.0564, Val AP: 0.1717, Test AP: 0.1686\n",
                        "Seed: 44, Epoch: 033, Loss: 0.0563, Val AP: 0.1739, Test AP: 0.1719\n",
                        "Seed: 44, Epoch: 034, Loss: 0.0563, Val AP: 0.1834, Test AP: 0.1842\n",
                        "Seed: 44, Epoch: 035, Loss: 0.0562, Val AP: 0.1831, Test AP: 0.1842\n",
                        "Seed: 44, Epoch: 036, Loss: 0.0562, Val AP: 0.1858, Test AP: 0.1857\n",
                        "Seed: 44, Epoch: 037, Loss: 0.0561, Val AP: 0.1842, Test AP: 0.1828\n",
                        "Seed: 44, Epoch: 038, Loss: 0.0560, Val AP: 0.1834, Test AP: 0.1833\n",
                        "Seed: 44, Epoch: 039, Loss: 0.0560, Val AP: 0.1740, Test AP: 0.1720\n",
                        "Seed: 44, Epoch: 040, Loss: 0.0559, Val AP: 0.1790, Test AP: 0.1791\n",
                        "Seed: 44, Epoch: 041, Loss: 0.0558, Val AP: 0.1885, Test AP: 0.1887\n",
                        "Seed: 44, Epoch: 042, Loss: 0.0559, Val AP: 0.1893, Test AP: 0.1886\n",
                        "Seed: 44, Epoch: 043, Loss: 0.0557, Val AP: 0.1836, Test AP: 0.1799\n",
                        "Seed: 44, Epoch: 044, Loss: 0.0558, Val AP: 0.1868, Test AP: 0.1854\n",
                        "Seed: 44, Epoch: 045, Loss: 0.0557, Val AP: 0.1917, Test AP: 0.1902\n",
                        "Seed: 44, Epoch: 046, Loss: 0.0556, Val AP: 0.1912, Test AP: 0.1908\n",
                        "Seed: 44, Epoch: 047, Loss: 0.0557, Val AP: 0.1936, Test AP: 0.1929\n",
                        "Seed: 44, Epoch: 048, Loss: 0.0555, Val AP: 0.1801, Test AP: 0.1787\n",
                        "Seed: 44, Epoch: 049, Loss: 0.0555, Val AP: 0.1921, Test AP: 0.1897\n",
                        "Seed: 44, Epoch: 050, Loss: 0.0554, Val AP: 0.1802, Test AP: 0.1771\n",
                        "Seed: 44, Epoch: 051, Loss: 0.0555, Val AP: 0.1917, Test AP: 0.1883\n",
                        "Seed: 44, Epoch: 052, Loss: 0.0554, Val AP: 0.1999, Test AP: 0.2009\n",
                        "Seed: 44, Epoch: 053, Loss: 0.0553, Val AP: 0.1906, Test AP: 0.1896\n",
                        "Seed: 44, Epoch: 054, Loss: 0.0554, Val AP: 0.1993, Test AP: 0.2005\n",
                        "Seed: 44, Epoch: 055, Loss: 0.0554, Val AP: 0.1970, Test AP: 0.1955\n",
                        "Seed: 44, Epoch: 056, Loss: 0.0553, Val AP: 0.2000, Test AP: 0.1999\n",
                        "Seed: 44, Epoch: 057, Loss: 0.0552, Val AP: 0.1982, Test AP: 0.1983\n",
                        "Seed: 44, Epoch: 058, Loss: 0.0553, Val AP: 0.1935, Test AP: 0.1937\n",
                        "Seed: 44, Epoch: 059, Loss: 0.0552, Val AP: 0.1999, Test AP: 0.1978\n",
                        "Seed: 44, Epoch: 060, Loss: 0.0551, Val AP: 0.1912, Test AP: 0.1891\n",
                        "Seed: 44, Epoch: 061, Loss: 0.0552, Val AP: 0.1987, Test AP: 0.1976\n",
                        "Seed: 44, Epoch: 062, Loss: 0.0551, Val AP: 0.2016, Test AP: 0.2043\n",
                        "Seed: 44, Epoch: 063, Loss: 0.0550, Val AP: 0.1984, Test AP: 0.1965\n",
                        "Seed: 44, Epoch: 064, Loss: 0.0551, Val AP: 0.2005, Test AP: 0.1996\n",
                        "Seed: 44, Epoch: 065, Loss: 0.0550, Val AP: 0.2021, Test AP: 0.2009\n",
                        "Seed: 44, Epoch: 066, Loss: 0.0550, Val AP: 0.2011, Test AP: 0.1994\n",
                        "Seed: 44, Epoch: 067, Loss: 0.0550, Val AP: 0.2014, Test AP: 0.1999\n",
                        "Seed: 44, Epoch: 068, Loss: 0.0549, Val AP: 0.2032, Test AP: 0.2023\n",
                        "Seed: 44, Epoch: 069, Loss: 0.0549, Val AP: 0.2050, Test AP: 0.2038\n",
                        "Seed: 44, Epoch: 070, Loss: 0.0549, Val AP: 0.1988, Test AP: 0.1970\n",
                        "Seed: 44, Epoch: 071, Loss: 0.0550, Val AP: 0.2055, Test AP: 0.2052\n",
                        "Seed: 44, Epoch: 072, Loss: 0.0549, Val AP: 0.1932, Test AP: 0.1922\n",
                        "Seed: 44, Epoch: 073, Loss: 0.0549, Val AP: 0.2002, Test AP: 0.1989\n",
                        "Seed: 44, Epoch: 074, Loss: 0.0548, Val AP: 0.2049, Test AP: 0.2022\n",
                        "Seed: 44, Epoch: 075, Loss: 0.0548, Val AP: 0.2114, Test AP: 0.2106\n",
                        "Seed: 44, Epoch: 076, Loss: 0.0550, Val AP: 0.2044, Test AP: 0.2035\n",
                        "Seed: 44, Epoch: 077, Loss: 0.0548, Val AP: 0.2073, Test AP: 0.2059\n",
                        "Seed: 44, Epoch: 078, Loss: 0.0547, Val AP: 0.2078, Test AP: 0.2062\n",
                        "Seed: 44, Epoch: 079, Loss: 0.0547, Val AP: 0.2091, Test AP: 0.2075\n",
                        "Seed: 44, Epoch: 080, Loss: 0.0548, Val AP: 0.1955, Test AP: 0.1924\n",
                        "Seed: 44, Epoch: 081, Loss: 0.0549, Val AP: 0.1969, Test AP: 0.1957\n",
                        "Seed: 44, Epoch: 082, Loss: 0.0549, Val AP: 0.2084, Test AP: 0.2076\n",
                        "Seed: 44, Epoch: 083, Loss: 0.0547, Val AP: 0.2032, Test AP: 0.2036\n",
                        "Seed: 44, Epoch: 084, Loss: 0.0547, Val AP: 0.2092, Test AP: 0.2081\n",
                        "Seed: 44, Epoch: 085, Loss: 0.0545, Val AP: 0.2040, Test AP: 0.2025\n",
                        "Seed: 44, Epoch: 086, Loss: 0.0549, Val AP: 0.2126, Test AP: 0.2118\n",
                        "Seed: 44, Epoch: 087, Loss: 0.0548, Val AP: 0.2061, Test AP: 0.2078\n",
                        "Seed: 44, Epoch: 088, Loss: 0.0547, Val AP: 0.2115, Test AP: 0.2119\n",
                        "Seed: 44, Epoch: 089, Loss: 0.0548, Val AP: 0.2176, Test AP: 0.2200\n",
                        "Seed: 44, Epoch: 090, Loss: 0.0545, Val AP: 0.2059, Test AP: 0.2044\n",
                        "Seed: 44, Epoch: 091, Loss: 0.0547, Val AP: 0.2129, Test AP: 0.2118\n",
                        "Seed: 44, Epoch: 092, Loss: 0.0547, Val AP: 0.2124, Test AP: 0.2108\n",
                        "Seed: 44, Epoch: 093, Loss: 0.0547, Val AP: 0.2085, Test AP: 0.2056\n",
                        "Seed: 44, Epoch: 094, Loss: 0.0545, Val AP: 0.2061, Test AP: 0.2051\n",
                        "Seed: 44, Epoch: 095, Loss: 0.0547, Val AP: 0.2129, Test AP: 0.2134\n",
                        "Seed: 44, Epoch: 096, Loss: 0.0544, Val AP: 0.2131, Test AP: 0.2124\n",
                        "Seed: 44, Epoch: 097, Loss: 0.0545, Val AP: 0.2181, Test AP: 0.2175\n",
                        "Seed: 44, Epoch: 098, Loss: 0.0543, Val AP: 0.2125, Test AP: 0.2121\n",
                        "Seed: 44, Epoch: 099, Loss: 0.0544, Val AP: 0.2090, Test AP: 0.2068\n",
                        "Seed: 44, Epoch: 100, Loss: 0.0545, Val AP: 0.2085, Test AP: 0.2073\n",
                        "Seed: 44, Epoch: 101, Loss: 0.0543, Val AP: 0.2158, Test AP: 0.2161\n",
                        "Seed: 44, Epoch: 102, Loss: 0.0543, Val AP: 0.2142, Test AP: 0.2125\n",
                        "Seed: 44, Epoch: 103, Loss: 0.0543, Val AP: 0.2150, Test AP: 0.2131\n",
                        "Seed: 44, Epoch: 104, Loss: 0.0542, Val AP: 0.2137, Test AP: 0.2126\n",
                        "Seed: 44, Epoch: 105, Loss: 0.0543, Val AP: 0.2167, Test AP: 0.2181\n",
                        "Seed: 44, Epoch: 106, Loss: 0.0542, Val AP: 0.2165, Test AP: 0.2167\n",
                        "Seed: 44, Epoch: 107, Loss: 0.0542, Val AP: 0.2080, Test AP: 0.2075\n",
                        "Seed: 44, Epoch: 108, Loss: 0.0544, Val AP: 0.2166, Test AP: 0.2158\n",
                        "Seed: 44, Epoch: 109, Loss: 0.0541, Val AP: 0.2025, Test AP: 0.1991\n",
                        "Seed: 44, Epoch: 110, Loss: 0.0542, Val AP: 0.2182, Test AP: 0.2160\n",
                        "Seed: 44, Epoch: 111, Loss: 0.0541, Val AP: 0.2186, Test AP: 0.2188\n",
                        "Seed: 44, Epoch: 112, Loss: 0.0543, Val AP: 0.2166, Test AP: 0.2178\n",
                        "Seed: 44, Epoch: 113, Loss: 0.0541, Val AP: 0.2011, Test AP: 0.2010\n",
                        "Seed: 44, Epoch: 114, Loss: 0.0542, Val AP: 0.2051, Test AP: 0.2015\n",
                        "Seed: 44, Epoch: 115, Loss: 0.0540, Val AP: 0.2130, Test AP: 0.2130\n",
                        "Seed: 44, Epoch: 116, Loss: 0.0541, Val AP: 0.2149, Test AP: 0.2159\n",
                        "Seed: 44, Epoch: 117, Loss: 0.0540, Val AP: 0.2109, Test AP: 0.2089\n",
                        "Seed: 44, Epoch: 118, Loss: 0.0544, Val AP: 0.2126, Test AP: 0.2109\n",
                        "Seed: 44, Epoch: 119, Loss: 0.0542, Val AP: 0.2101, Test AP: 0.2097\n",
                        "Seed: 44, Epoch: 120, Loss: 0.0541, Val AP: 0.2125, Test AP: 0.2104\n",
                        "Seed: 44, Epoch: 121, Loss: 0.0541, Val AP: 0.2151, Test AP: 0.2156\n",
                        "Seed: 44, Epoch: 122, Loss: 0.0541, Val AP: 0.2199, Test AP: 0.2191\n",
                        "Seed: 44, Epoch: 123, Loss: 0.0541, Val AP: 0.2217, Test AP: 0.2229\n",
                        "Seed: 44, Epoch: 124, Loss: 0.0540, Val AP: 0.2146, Test AP: 0.2121\n",
                        "Seed: 44, Epoch: 125, Loss: 0.0545, Val AP: 0.2099, Test AP: 0.2054\n",
                        "Seed: 44, Epoch: 126, Loss: 0.0544, Val AP: 0.2140, Test AP: 0.2131\n",
                        "Seed: 44, Epoch: 127, Loss: 0.0541, Val AP: 0.2120, Test AP: 0.2118\n",
                        "Seed: 44, Epoch: 128, Loss: 0.0543, Val AP: 0.2086, Test AP: 0.2072\n",
                        "Seed: 44, Epoch: 129, Loss: 0.0544, Val AP: 0.2192, Test AP: 0.2202\n",
                        "Seed: 44, Epoch: 130, Loss: 0.0542, Val AP: 0.1943, Test AP: 0.1931\n",
                        "Seed: 44, Epoch: 131, Loss: 0.0540, Val AP: 0.2118, Test AP: 0.2079\n",
                        "Seed: 44, Epoch: 132, Loss: 0.0542, Val AP: 0.2128, Test AP: 0.2108\n",
                        "Seed: 44, Epoch: 133, Loss: 0.0541, Val AP: 0.2160, Test AP: 0.2142\n",
                        "Seed: 44, Epoch: 134, Loss: 0.0540, Val AP: 0.2203, Test AP: 0.2214\n",
                        "Seed: 44, Epoch: 135, Loss: 0.0540, Val AP: 0.1902, Test AP: 0.1879\n",
                        "Seed: 44, Epoch: 136, Loss: 0.0543, Val AP: 0.2168, Test AP: 0.2170\n",
                        "Seed: 44, Epoch: 137, Loss: 0.0539, Val AP: 0.2150, Test AP: 0.2109\n",
                        "Seed: 44, Epoch: 138, Loss: 0.0541, Val AP: 0.2197, Test AP: 0.2199\n",
                        "Seed: 44, Epoch: 139, Loss: 0.0539, Val AP: 0.2194, Test AP: 0.2214\n",
                        "Seed: 44, Epoch: 140, Loss: 0.0540, Val AP: 0.2052, Test AP: 0.2010\n",
                        "Seed: 44, Epoch: 141, Loss: 0.0540, Val AP: 0.2171, Test AP: 0.2151\n",
                        "Seed: 44, Epoch: 142, Loss: 0.0540, Val AP: 0.2135, Test AP: 0.2097\n",
                        "Seed: 44, Epoch: 143, Loss: 0.0541, Val AP: 0.2176, Test AP: 0.2160\n",
                        "Seed: 44, Epoch: 144, Loss: 0.0537, Val AP: 0.2159, Test AP: 0.2139\n",
                        "Seed: 44, Epoch: 145, Loss: 0.0539, Val AP: 0.2211, Test AP: 0.2193\n",
                        "Seed: 44, Epoch: 146, Loss: 0.0539, Val AP: 0.1952, Test AP: 0.1934\n",
                        "Seed: 44, Epoch: 147, Loss: 0.0538, Val AP: 0.2189, Test AP: 0.2199\n",
                        "Seed: 44, Epoch: 148, Loss: 0.0540, Val AP: 0.2205, Test AP: 0.2177\n",
                        "Seed: 44, Epoch: 149, Loss: 0.0537, Val AP: 0.2166, Test AP: 0.2140\n",
                        "Seed: 44, Epoch: 150, Loss: 0.0539, Val AP: 0.2196, Test AP: 0.2190\n",
                        "Average Time: 9914.87 seconds\n",
                        "Var Time: 3053429.18 seconds\n",
                        "Average Memory: 6960.00 MB\n",
                        "Average Best Val AP: 0.2172\n",
                        "Std Best Test AP: 0.0099\n",
                        "Average Test AP: 0.2181\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nclass HierarchicalGCN_PAN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_PAN, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, improved = True)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = PANPooling(hidden_channels, ratio=0.7, pan_pool_weight=None, min_score=None, multiplier=1,\n                 nonlinearity=torch.tanh, filter_size=1, panpool_filter_weight=None)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, improved = True)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = PANPooling(hidden_channels, ratio=0.7, pan_pool_weight=None, min_score=None, multiplier=1,\n                 nonlinearity=torch.tanh, filter_size=1, panpool_filter_weight=None)\n        self.conv3 = GCNConv(hidden_channels, out_channels, improved = True)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        init_x = data.x\n        x = x.float()\n        init_x = init_x.float()\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, perm, score_perm = self.pool1(x, edge_index, batch=batch, M=None)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, perm, score_perm = self.pool2(x, edge_index, batch=batch, M=None)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_PAN(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=4096, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=4096, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=4096, shuffle=False)\n    model = HierarchicalGCN_PAN(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### COPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\ndef cumsum(x: Tensor, dim: int = 0) -> Tensor:\n    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n    Args:\n        x (torch.Tensor): The input tensor.\n        dim (int, optional): The dimension to do the operation over.\n            (default: :obj:`0`)\n    Example:\n        >>> x = torch.tensor([2, 4, 1])\n        >>> cumsum(x)\n        tensor([0, 2, 6, 7])\n    \"\"\"\n    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n    out = x.new_empty(size)\n    out.narrow(dim, 0, 1).zero_()\n    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n    return out\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\ndef topk(\n    x: Tensor,\n    ratio: Optional[Union[float, int]],\n    batch: Tensor,\n    min_score: Optional[float] = None,\n    tol: float = 1e-7,\n) -> Tensor:\n    if min_score is not None:\n        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero().view(-1)\n        return perm\n    if ratio is not None:\n        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n        if ratio >= 1:\n            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n        else:\n            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n        x, x_perm = torch.sort(x.view(-1), descending=True)\n        batch = batch[x_perm]\n        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n        ptr = cumsum(num_nodes)\n        batched_arange = arange - ptr[batch]\n        mask = batched_arange < k[batch]\n        return x_perm[batch_perm[mask]]\n    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n                     \"must be specified\")\nclass GPR_prop(MessagePassing):\n    '''\n    propagation class for GPR_GNN\n    '''\n    def __init__(self, K, alpha, Init, Gamma=None, bias=True, **kwargs):\n        super(GPR_prop, self).__init__(aggr='add', **kwargs)\n        self.K = K\n        self.Init = Init\n        self.alpha = alpha\n        assert Init in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']\n        if Init == 'SGC':\n            TEMP = 0.0*np.ones(K+1)\n            TEMP[alpha] = 1.0\n        elif Init == 'PPR':\n            TEMP = alpha*(1-alpha)**np.arange(K+1)\n            TEMP[-1] = (1-alpha)**K\n        elif Init == 'NPPR':\n            TEMP = (alpha)**np.arange(K+1)\n            TEMP = TEMP/np.sum(np.abs(TEMP))\n        elif Init == 'Random':\n            bound = np.sqrt(3/(K+1))\n            TEMP = np.random.uniform(-bound, bound, K+1)\n            TEMP = TEMP/np.sum(np.abs(TEMP))\n        elif Init == 'WS':\n            TEMP = Gamma\n        self.temp = Parameter(torch.tensor(TEMP))\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.temp)\n        for k in range(self.K+1):\n            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n        self.temp.data[-1] = (1-self.alpha)**self.K\n    def forward(self, x, edge_index, edge_weight=None):\n        edge_index, norm = gcn_norm(\n            edge_index, edge_weight, num_nodes=x.size(0), dtype=x.dtype)\n        hidden = x*(self.temp[0])\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=norm)\n            gamma = self.temp[k+1]\n            hidden = hidden + gamma*x\n        return hidden\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def __repr__(self):\n        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n                                           self.temp)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) \n        edge_index = edge_index.type(torch.long)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass graph_attention(torch.nn.Module):\n    src_nodes_dim = 0  \n    trg_nodes_dim = 1  \n    nodes_dim = 0      \n    head_dim = 1       \n    def __init__(self, num_in_features, num_out_features, num_of_heads, dropout_prob=0.6, log_attention_weights=False):\n        super().__init__()\n        self.num_of_heads = num_of_heads\n        self.num_out_features = num_out_features\n        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.init_params()\n    def init_params(self):\n        \"\"\"\n        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n        Feel free to experiment - there may be better initializations depending on your problem.\n        \"\"\"\n        nn.init.xavier_uniform_(self.linear_proj.weight)\n        nn.init.xavier_uniform_(self.scoring_fn_target)\n        nn.init.xavier_uniform_(self.scoring_fn_source)\n    def forward(self, x, edge_index):\n        in_nodes_features = x  \n        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n        scores_per_edge = scores_source_lifted + scores_target_lifted\n        return torch.sigmoid(scores_per_edge)\n    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n        \"\"\"\n        Lifts i.e. duplicates certain vectors depending on the edge index.\n        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n        \"\"\"\n        src_nodes_index = edge_index[self.src_nodes_dim]\n        trg_nodes_index = edge_index[self.trg_nodes_dim]\n        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n        return scores_source, scores_target, nodes_features_matrix_proj_lifted\nclass CoPooling(torch.nn.Module):\n    def __init__(self, ratio=0.5, K=0.05, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=None):\n        super(CoPooling, self).__init__()\n        self.ratio = ratio\n        self.calc_information_score = NodeInformationScore()\n        self.edge_ratio = edge_ratio\n        self.prop1 = GPR_prop(K, alpha, Init, Gamma)\n        score_dim = 32\n        self.G_att = graph_attention(num_in_features=nhid, num_out_features=score_dim, num_of_heads=1)\n        self.weight = Parameter(torch.Tensor(2*nhid, nhid))\n        nn.init.xavier_uniform_(self.weight.data)\n        self.bias = Parameter(torch.Tensor(nhid))\n        nn.init.zeros_(self.bias.data)\n        self.reset_parameters()\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight.data)\n        nn.init.zeros_(self.bias.data)\n        self.prop1.reset_parameters()\n        self.G_att.init_params()\n    def forward(self, x, edge_index, edge_attr, batch=None, nodes_index=None, node_attr=None):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        ori_batch = batch.clone()\n        device = x.device\n        num_nodes = x.shape[0]\n        x_cut = self.prop1(x, edge_index) \n        attention = self.G_att(x_cut, edge_index) \n        attention = attention.sum(dim=1) \n        edge_index, attention = add_self_loops(edge_index, attention, 1.0, num_nodes) \n        edge_index_t, attention_t = transpose(edge_index, attention, num_nodes, num_nodes)\n        edge_tmp = torch.cat((edge_index, edge_index_t), 1)\n        att_tmp = torch.cat((attention, attention_t),0)\n        edge_index, attention = coalesce(edge_tmp, att_tmp, num_nodes, num_nodes, 'mean')\n        attention_np = attention.cpu().data.numpy()\n        cut_val = np.percentile(attention_np, int(100*(1-self.edge_ratio))) \n        attention = attention * (attention >= cut_val) \n        kep_idx = attention > 0.0\n        cut_edge_index, cut_edge_attr = edge_index[:, kep_idx], attention[kep_idx]\n        x_information_score = self.calc_information_score(x, cut_edge_index, cut_edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        perm = topk(score, self.ratio, batch)\n        x_topk = x[perm]\n        batch = batch[perm]\n        if nodes_index is not None:\n            nodes_index = nodes_index[perm]\n        if node_attr is not None:\n            node_attr = node_attr[perm]\n        if cut_edge_index is not None or cut_edge_index.nelement() != 0:\n            induced_edge_index, induced_edge_attr = filter_adj(cut_edge_index, cut_edge_attr, perm, num_nodes=num_nodes)\n        else:\n            print('All edges are cut!')\n            induced_edge_index, induced_edge_attr = cut_edge_index, cut_edge_attr\n        attention_dense = (to_dense_adj(cut_edge_index, edge_attr=cut_edge_attr, max_num_nodes=num_nodes)).squeeze()\n        x = F.relu(torch.matmul(torch.cat((x_topk, torch.matmul(attention_dense[perm],x)), 1), self.weight) + self.bias)\n        return x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0621, Val AP: 0.1176, Test AP: 0.1225\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0603, Val AP: 0.1144, Test AP: 0.1190\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0603, Val AP: 0.1173, Test AP: 0.1225\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0603, Val AP: 0.1171, Test AP: 0.1220\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0603, Val AP: 0.1181, Test AP: 0.1231\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0603, Val AP: 0.1183, Test AP: 0.1233\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0603, Val AP: 0.1180, Test AP: 0.1231\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0603, Val AP: 0.1177, Test AP: 0.1232\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0603, Val AP: 0.1182, Test AP: 0.1233\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0602, Val AP: 0.1207, Test AP: 0.1250\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0602, Val AP: 0.1211, Test AP: 0.1267\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0602, Val AP: 0.1209, Test AP: 0.1264\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0601, Val AP: 0.1213, Test AP: 0.1271\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0601, Val AP: 0.1192, Test AP: 0.1246\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0601, Val AP: 0.1203, Test AP: 0.1256\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0601, Val AP: 0.1196, Test AP: 0.1249\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0601, Val AP: 0.1207, Test AP: 0.1259\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0601, Val AP: 0.1215, Test AP: 0.1267\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0601, Val AP: 0.1217, Test AP: 0.1271\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0600, Val AP: 0.1209, Test AP: 0.1267\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0600, Val AP: 0.1227, Test AP: 0.1278\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0600, Val AP: 0.1226, Test AP: 0.1288\n",
                        "Seed: 42, Epoch: 023, Loss: 0.0601, Val AP: 0.1211, Test AP: 0.1273\n",
                        "Seed: 42, Epoch: 024, Loss: 0.0600, Val AP: 0.1220, Test AP: 0.1285\n",
                        "Seed: 42, Epoch: 025, Loss: 0.0600, Val AP: 0.1224, Test AP: 0.1287\n",
                        "Seed: 42, Epoch: 026, Loss: 0.0600, Val AP: 0.1218, Test AP: 0.1283\n",
                        "Seed: 42, Epoch: 027, Loss: 0.0600, Val AP: 0.1239, Test AP: 0.1296\n",
                        "Seed: 42, Epoch: 028, Loss: 0.0600, Val AP: 0.1230, Test AP: 0.1296\n",
                        "Seed: 42, Epoch: 029, Loss: 0.0600, Val AP: 0.1204, Test AP: 0.1262\n",
                        "Seed: 42, Epoch: 030, Loss: 0.0600, Val AP: 0.1216, Test AP: 0.1277\n",
                        "Seed: 42, Epoch: 031, Loss: 0.0600, Val AP: 0.1234, Test AP: 0.1299\n",
                        "Seed: 42, Epoch: 032, Loss: 0.0600, Val AP: 0.1228, Test AP: 0.1295\n",
                        "Seed: 42, Epoch: 033, Loss: 0.0600, Val AP: 0.1234, Test AP: 0.1301\n",
                        "Seed: 42, Epoch: 034, Loss: 0.0599, Val AP: 0.1226, Test AP: 0.1294\n",
                        "Seed: 42, Epoch: 035, Loss: 0.0599, Val AP: 0.1254, Test AP: 0.1312\n",
                        "Seed: 42, Epoch: 036, Loss: 0.0599, Val AP: 0.1234, Test AP: 0.1300\n",
                        "Seed: 42, Epoch: 037, Loss: 0.0598, Val AP: 0.1252, Test AP: 0.1310\n",
                        "Seed: 42, Epoch: 038, Loss: 0.0599, Val AP: 0.1237, Test AP: 0.1305\n",
                        "Seed: 42, Epoch: 039, Loss: 0.0599, Val AP: 0.1230, Test AP: 0.1300\n",
                        "Seed: 42, Epoch: 040, Loss: 0.0599, Val AP: 0.1247, Test AP: 0.1318\n",
                        "Seed: 42, Epoch: 041, Loss: 0.0599, Val AP: 0.1232, Test AP: 0.1295\n",
                        "Seed: 42, Epoch: 042, Loss: 0.0599, Val AP: 0.1244, Test AP: 0.1314\n",
                        "Seed: 42, Epoch: 043, Loss: 0.0600, Val AP: 0.1229, Test AP: 0.1304\n",
                        "Seed: 42, Epoch: 044, Loss: 0.0598, Val AP: 0.1272, Test AP: 0.1361\n",
                        "Seed: 42, Epoch: 045, Loss: 0.0596, Val AP: 0.1264, Test AP: 0.1353\n",
                        "Seed: 42, Epoch: 046, Loss: 0.0592, Val AP: 0.1349, Test AP: 0.1449\n",
                        "Seed: 42, Epoch: 047, Loss: 0.0589, Val AP: 0.1237, Test AP: 0.1316\n",
                        "Seed: 42, Epoch: 048, Loss: 0.0588, Val AP: 0.1382, Test AP: 0.1487\n",
                        "Seed: 42, Epoch: 049, Loss: 0.0585, Val AP: 0.1325, Test AP: 0.1414\n",
                        "Seed: 42, Epoch: 050, Loss: 0.0582, Val AP: 0.1379, Test AP: 0.1492\n",
                        "Seed: 42, Epoch: 051, Loss: 0.0580, Val AP: 0.1448, Test AP: 0.1556\n",
                        "Seed: 42, Epoch: 052, Loss: 0.0579, Val AP: 0.1521, Test AP: 0.1622\n",
                        "Seed: 42, Epoch: 053, Loss: 0.0577, Val AP: 0.1450, Test AP: 0.1551\n",
                        "Seed: 42, Epoch: 054, Loss: 0.0575, Val AP: 0.1562, Test AP: 0.1667\n",
                        "Seed: 42, Epoch: 055, Loss: 0.0575, Val AP: 0.1588, Test AP: 0.1687\n",
                        "Seed: 42, Epoch: 056, Loss: 0.0573, Val AP: 0.1570, Test AP: 0.1662\n",
                        "Seed: 42, Epoch: 057, Loss: 0.0573, Val AP: 0.1571, Test AP: 0.1674\n",
                        "Seed: 42, Epoch: 058, Loss: 0.0571, Val AP: 0.1579, Test AP: 0.1671\n",
                        "Seed: 42, Epoch: 059, Loss: 0.0571, Val AP: 0.1662, Test AP: 0.1755\n",
                        "Seed: 42, Epoch: 060, Loss: 0.0570, Val AP: 0.1619, Test AP: 0.1702\n",
                        "Seed: 42, Epoch: 061, Loss: 0.0569, Val AP: 0.1581, Test AP: 0.1667\n",
                        "Seed: 42, Epoch: 062, Loss: 0.0569, Val AP: 0.1617, Test AP: 0.1726\n",
                        "Seed: 42, Epoch: 063, Loss: 0.0568, Val AP: 0.1594, Test AP: 0.1681\n",
                        "Seed: 42, Epoch: 064, Loss: 0.0569, Val AP: 0.1685, Test AP: 0.1789\n",
                        "Seed: 42, Epoch: 065, Loss: 0.0567, Val AP: 0.1731, Test AP: 0.1844\n",
                        "Seed: 42, Epoch: 066, Loss: 0.0566, Val AP: 0.1731, Test AP: 0.1842\n",
                        "Seed: 42, Epoch: 067, Loss: 0.0566, Val AP: 0.1637, Test AP: 0.1723\n",
                        "Seed: 42, Epoch: 068, Loss: 0.0564, Val AP: 0.1763, Test AP: 0.1863\n",
                        "Seed: 42, Epoch: 069, Loss: 0.0564, Val AP: 0.1742, Test AP: 0.1859\n",
                        "Seed: 42, Epoch: 070, Loss: 0.0564, Val AP: 0.1822, Test AP: 0.1926\n",
                        "Seed: 42, Epoch: 071, Loss: 0.0563, Val AP: 0.1716, Test AP: 0.1789\n",
                        "Seed: 42, Epoch: 072, Loss: 0.0562, Val AP: 0.1860, Test AP: 0.1964\n",
                        "Seed: 42, Epoch: 073, Loss: 0.0561, Val AP: 0.1855, Test AP: 0.1937\n",
                        "Seed: 42, Epoch: 074, Loss: 0.0561, Val AP: 0.1766, Test AP: 0.1865\n",
                        "Seed: 42, Epoch: 075, Loss: 0.0561, Val AP: 0.1728, Test AP: 0.1812\n",
                        "Seed: 42, Epoch: 076, Loss: 0.0562, Val AP: 0.1808, Test AP: 0.1898\n",
                        "Seed: 42, Epoch: 077, Loss: 0.0560, Val AP: 0.1801, Test AP: 0.1862\n",
                        "Seed: 42, Epoch: 078, Loss: 0.0559, Val AP: 0.1810, Test AP: 0.1903\n",
                        "Seed: 42, Epoch: 079, Loss: 0.0564, Val AP: 0.1885, Test AP: 0.1983\n",
                        "Seed: 42, Epoch: 080, Loss: 0.0559, Val AP: 0.1852, Test AP: 0.1958\n",
                        "Seed: 42, Epoch: 081, Loss: 0.0558, Val AP: 0.1902, Test AP: 0.1997\n",
                        "Seed: 42, Epoch: 082, Loss: 0.0557, Val AP: 0.1825, Test AP: 0.1911\n",
                        "Seed: 42, Epoch: 083, Loss: 0.0556, Val AP: 0.1735, Test AP: 0.1825\n",
                        "Seed: 42, Epoch: 084, Loss: 0.0556, Val AP: 0.1890, Test AP: 0.1974\n",
                        "Seed: 42, Epoch: 085, Loss: 0.0555, Val AP: 0.1937, Test AP: 0.2032\n",
                        "Seed: 42, Epoch: 086, Loss: 0.0557, Val AP: 0.1866, Test AP: 0.1974\n",
                        "Seed: 42, Epoch: 087, Loss: 0.0554, Val AP: 0.1947, Test AP: 0.2051\n",
                        "Seed: 42, Epoch: 088, Loss: 0.0554, Val AP: 0.1957, Test AP: 0.2047\n",
                        "Seed: 42, Epoch: 089, Loss: 0.0554, Val AP: 0.1940, Test AP: 0.2029\n",
                        "Seed: 42, Epoch: 090, Loss: 0.0554, Val AP: 0.1941, Test AP: 0.2026\n",
                        "Seed: 42, Epoch: 091, Loss: 0.0553, Val AP: 0.1823, Test AP: 0.1912\n",
                        "Seed: 42, Epoch: 092, Loss: 0.0553, Val AP: 0.1921, Test AP: 0.2011\n",
                        "Seed: 42, Epoch: 093, Loss: 0.0552, Val AP: 0.1976, Test AP: 0.2083\n",
                        "Seed: 42, Epoch: 094, Loss: 0.0552, Val AP: 0.1862, Test AP: 0.1958\n",
                        "Seed: 42, Epoch: 095, Loss: 0.0554, Val AP: 0.1921, Test AP: 0.2032\n",
                        "Seed: 42, Epoch: 096, Loss: 0.0552, Val AP: 0.1931, Test AP: 0.2023\n",
                        "Seed: 42, Epoch: 097, Loss: 0.0551, Val AP: 0.1955, Test AP: 0.2038\n",
                        "Seed: 42, Epoch: 098, Loss: 0.0550, Val AP: 0.1969, Test AP: 0.2056\n",
                        "Seed: 42, Epoch: 099, Loss: 0.0551, Val AP: 0.2020, Test AP: 0.2100\n",
                        "Seed: 42, Epoch: 100, Loss: 0.0551, Val AP: 0.1873, Test AP: 0.1975\n",
                        "Seed: 42, Epoch: 101, Loss: 0.0550, Val AP: 0.1944, Test AP: 0.2016\n",
                        "Seed: 42, Epoch: 102, Loss: 0.0550, Val AP: 0.1915, Test AP: 0.2007\n",
                        "Seed: 42, Epoch: 103, Loss: 0.0550, Val AP: 0.1858, Test AP: 0.1941\n",
                        "Seed: 42, Epoch: 104, Loss: 0.0550, Val AP: 0.1959, Test AP: 0.2048\n",
                        "Seed: 42, Epoch: 105, Loss: 0.0549, Val AP: 0.2038, Test AP: 0.2121\n",
                        "Seed: 42, Epoch: 106, Loss: 0.0549, Val AP: 0.2027, Test AP: 0.2123\n",
                        "Seed: 42, Epoch: 107, Loss: 0.0550, Val AP: 0.1963, Test AP: 0.2066\n",
                        "Seed: 42, Epoch: 108, Loss: 0.0551, Val AP: 0.1966, Test AP: 0.2063\n",
                        "Seed: 42, Epoch: 109, Loss: 0.0549, Val AP: 0.1957, Test AP: 0.2054\n",
                        "Seed: 42, Epoch: 110, Loss: 0.0548, Val AP: 0.2018, Test AP: 0.2110\n",
                        "Seed: 42, Epoch: 111, Loss: 0.0548, Val AP: 0.2071, Test AP: 0.2177\n",
                        "Seed: 42, Epoch: 112, Loss: 0.0548, Val AP: 0.2016, Test AP: 0.2127\n",
                        "Seed: 42, Epoch: 113, Loss: 0.0548, Val AP: 0.2026, Test AP: 0.2138\n",
                        "Seed: 42, Epoch: 114, Loss: 0.0548, Val AP: 0.1997, Test AP: 0.2100\n",
                        "Seed: 42, Epoch: 115, Loss: 0.0548, Val AP: 0.2102, Test AP: 0.2193\n",
                        "Seed: 42, Epoch: 116, Loss: 0.0547, Val AP: 0.1973, Test AP: 0.2064\n",
                        "Seed: 42, Epoch: 117, Loss: 0.0547, Val AP: 0.2049, Test AP: 0.2146\n",
                        "Seed: 42, Epoch: 118, Loss: 0.0547, Val AP: 0.1807, Test AP: 0.1898\n",
                        "Seed: 42, Epoch: 119, Loss: 0.0548, Val AP: 0.2027, Test AP: 0.2128\n",
                        "Seed: 42, Epoch: 120, Loss: 0.0547, Val AP: 0.2039, Test AP: 0.2126\n",
                        "Seed: 42, Epoch: 121, Loss: 0.0547, Val AP: 0.2048, Test AP: 0.2139\n",
                        "Seed: 42, Epoch: 122, Loss: 0.0547, Val AP: 0.2084, Test AP: 0.2180\n",
                        "Seed: 42, Epoch: 123, Loss: 0.0546, Val AP: 0.1959, Test AP: 0.2031\n",
                        "Seed: 42, Epoch: 124, Loss: 0.0546, Val AP: 0.2069, Test AP: 0.2155\n",
                        "Seed: 42, Epoch: 125, Loss: 0.0546, Val AP: 0.2060, Test AP: 0.2156\n",
                        "Seed: 42, Epoch: 126, Loss: 0.0545, Val AP: 0.2069, Test AP: 0.2158\n",
                        "Seed: 42, Epoch: 127, Loss: 0.0545, Val AP: 0.1840, Test AP: 0.1936\n",
                        "Seed: 42, Epoch: 128, Loss: 0.0545, Val AP: 0.2070, Test AP: 0.2163\n",
                        "Seed: 42, Epoch: 129, Loss: 0.0545, Val AP: 0.2056, Test AP: 0.2126\n",
                        "Seed: 42, Epoch: 130, Loss: 0.0545, Val AP: 0.1996, Test AP: 0.2091\n",
                        "Seed: 42, Epoch: 131, Loss: 0.0546, Val AP: 0.2067, Test AP: 0.2160\n",
                        "Seed: 42, Epoch: 132, Loss: 0.0545, Val AP: 0.1920, Test AP: 0.2022\n",
                        "Seed: 42, Epoch: 133, Loss: 0.0545, Val AP: 0.2073, Test AP: 0.2162\n",
                        "Seed: 42, Epoch: 134, Loss: 0.0545, Val AP: 0.2033, Test AP: 0.2118\n",
                        "Seed: 42, Epoch: 135, Loss: 0.0545, Val AP: 0.2014, Test AP: 0.2091\n",
                        "Seed: 42, Epoch: 136, Loss: 0.0544, Val AP: 0.2011, Test AP: 0.2101\n",
                        "Seed: 42, Epoch: 137, Loss: 0.0544, Val AP: 0.1974, Test AP: 0.2070\n",
                        "Seed: 42, Epoch: 138, Loss: 0.0544, Val AP: 0.1964, Test AP: 0.2047\n",
                        "Seed: 42, Epoch: 139, Loss: 0.0544, Val AP: 0.2006, Test AP: 0.2094\n",
                        "Seed: 42, Epoch: 140, Loss: 0.0544, Val AP: 0.2048, Test AP: 0.2133\n",
                        "Seed: 42, Epoch: 141, Loss: 0.0544, Val AP: 0.2042, Test AP: 0.2141\n",
                        "Seed: 42, Epoch: 142, Loss: 0.0544, Val AP: 0.2096, Test AP: 0.2190\n",
                        "Seed: 42, Epoch: 143, Loss: 0.0544, Val AP: 0.2065, Test AP: 0.2158\n",
                        "Seed: 42, Epoch: 144, Loss: 0.0543, Val AP: 0.1983, Test AP: 0.2070\n",
                        "Seed: 42, Epoch: 145, Loss: 0.0545, Val AP: 0.1989, Test AP: 0.2082\n",
                        "Seed: 42, Epoch: 146, Loss: 0.0544, Val AP: 0.1999, Test AP: 0.2084\n",
                        "Seed: 42, Epoch: 147, Loss: 0.0543, Val AP: 0.2051, Test AP: 0.2154\n",
                        "Seed: 42, Epoch: 148, Loss: 0.0548, Val AP: 0.1964, Test AP: 0.2041\n",
                        "Seed: 42, Epoch: 149, Loss: 0.0548, Val AP: 0.2042, Test AP: 0.2122\n",
                        "Seed: 42, Epoch: 150, Loss: 0.0543, Val AP: 0.2107, Test AP: 0.2206\n",
                        "Seed: 43, Epoch: 001, Loss: 0.0618, Val AP: 0.1182, Test AP: 0.1192\n",
                        "Seed: 43, Epoch: 002, Loss: 0.0601, Val AP: 0.1222, Test AP: 0.1222\n",
                        "Seed: 43, Epoch: 003, Loss: 0.0598, Val AP: 0.1227, Test AP: 0.1223\n",
                        "Seed: 43, Epoch: 004, Loss: 0.0595, Val AP: 0.1299, Test AP: 0.1292\n",
                        "Seed: 43, Epoch: 005, Loss: 0.0593, Val AP: 0.1315, Test AP: 0.1317\n",
                        "Seed: 43, Epoch: 006, Loss: 0.0591, Val AP: 0.1295, Test AP: 0.1292\n",
                        "Seed: 43, Epoch: 007, Loss: 0.0589, Val AP: 0.1364, Test AP: 0.1373\n",
                        "Seed: 43, Epoch: 008, Loss: 0.0586, Val AP: 0.1305, Test AP: 0.1313\n",
                        "Seed: 43, Epoch: 009, Loss: 0.0582, Val AP: 0.1452, Test AP: 0.1478\n",
                        "Seed: 43, Epoch: 010, Loss: 0.0578, Val AP: 0.1654, Test AP: 0.1658\n",
                        "Seed: 43, Epoch: 011, Loss: 0.0574, Val AP: 0.1720, Test AP: 0.1741\n",
                        "Seed: 43, Epoch: 012, Loss: 0.0571, Val AP: 0.1723, Test AP: 0.1733\n",
                        "Seed: 43, Epoch: 013, Loss: 0.0567, Val AP: 0.1775, Test AP: 0.1803\n",
                        "Seed: 43, Epoch: 014, Loss: 0.0564, Val AP: 0.1876, Test AP: 0.1895\n",
                        "Seed: 43, Epoch: 015, Loss: 0.0561, Val AP: 0.1834, Test AP: 0.1844\n",
                        "Seed: 43, Epoch: 016, Loss: 0.0557, Val AP: 0.1892, Test AP: 0.1921\n",
                        "Seed: 43, Epoch: 017, Loss: 0.0555, Val AP: 0.2000, Test AP: 0.2044\n",
                        "Seed: 43, Epoch: 018, Loss: 0.0552, Val AP: 0.1979, Test AP: 0.2002\n",
                        "Seed: 43, Epoch: 019, Loss: 0.0551, Val AP: 0.2096, Test AP: 0.2119\n",
                        "Seed: 43, Epoch: 020, Loss: 0.0549, Val AP: 0.2055, Test AP: 0.2060\n",
                        "Seed: 43, Epoch: 021, Loss: 0.0547, Val AP: 0.1996, Test AP: 0.2019\n",
                        "Seed: 43, Epoch: 022, Loss: 0.0545, Val AP: 0.2137, Test AP: 0.2161\n",
                        "Seed: 43, Epoch: 023, Loss: 0.0544, Val AP: 0.2129, Test AP: 0.2142\n",
                        "Seed: 43, Epoch: 024, Loss: 0.0543, Val AP: 0.2061, Test AP: 0.2068\n",
                        "Seed: 43, Epoch: 025, Loss: 0.0542, Val AP: 0.2160, Test AP: 0.2177\n",
                        "Seed: 43, Epoch: 026, Loss: 0.0541, Val AP: 0.2135, Test AP: 0.2139\n",
                        "Seed: 43, Epoch: 027, Loss: 0.0540, Val AP: 0.2139, Test AP: 0.2145\n",
                        "Seed: 43, Epoch: 028, Loss: 0.0539, Val AP: 0.2252, Test AP: 0.2255\n",
                        "Seed: 43, Epoch: 029, Loss: 0.0537, Val AP: 0.2124, Test AP: 0.2127\n",
                        "Seed: 43, Epoch: 030, Loss: 0.0537, Val AP: 0.2274, Test AP: 0.2285\n",
                        "Seed: 43, Epoch: 031, Loss: 0.0536, Val AP: 0.2190, Test AP: 0.2198\n",
                        "Seed: 43, Epoch: 032, Loss: 0.0534, Val AP: 0.2289, Test AP: 0.2303\n",
                        "Seed: 43, Epoch: 033, Loss: 0.0533, Val AP: 0.2288, Test AP: 0.2299\n",
                        "Seed: 43, Epoch: 034, Loss: 0.0533, Val AP: 0.2333, Test AP: 0.2344\n",
                        "Seed: 43, Epoch: 035, Loss: 0.0533, Val AP: 0.2055, Test AP: 0.2046\n",
                        "Seed: 43, Epoch: 036, Loss: 0.0533, Val AP: 0.2203, Test AP: 0.2189\n",
                        "Seed: 43, Epoch: 037, Loss: 0.0531, Val AP: 0.2374, Test AP: 0.2378\n",
                        "Seed: 43, Epoch: 038, Loss: 0.0530, Val AP: 0.2377, Test AP: 0.2386\n",
                        "Seed: 43, Epoch: 039, Loss: 0.0529, Val AP: 0.2382, Test AP: 0.2391\n",
                        "Seed: 43, Epoch: 040, Loss: 0.0528, Val AP: 0.2338, Test AP: 0.2326\n",
                        "Seed: 43, Epoch: 041, Loss: 0.0529, Val AP: 0.2288, Test AP: 0.2288\n",
                        "Seed: 43, Epoch: 042, Loss: 0.0527, Val AP: 0.2365, Test AP: 0.2362\n",
                        "Seed: 43, Epoch: 043, Loss: 0.0528, Val AP: 0.2133, Test AP: 0.2164\n",
                        "Seed: 43, Epoch: 044, Loss: 0.0528, Val AP: 0.2440, Test AP: 0.2449\n",
                        "Seed: 43, Epoch: 045, Loss: 0.0527, Val AP: 0.2414, Test AP: 0.2403\n",
                        "Seed: 43, Epoch: 046, Loss: 0.0526, Val AP: 0.2314, Test AP: 0.2299\n",
                        "Seed: 43, Epoch: 047, Loss: 0.0525, Val AP: 0.2445, Test AP: 0.2431\n",
                        "Seed: 43, Epoch: 048, Loss: 0.0526, Val AP: 0.2314, Test AP: 0.2332\n",
                        "Seed: 43, Epoch: 049, Loss: 0.0525, Val AP: 0.2428, Test AP: 0.2438\n",
                        "Seed: 43, Epoch: 050, Loss: 0.0525, Val AP: 0.2410, Test AP: 0.2420\n",
                        "Seed: 43, Epoch: 051, Loss: 0.0524, Val AP: 0.2480, Test AP: 0.2472\n",
                        "Seed: 43, Epoch: 052, Loss: 0.0524, Val AP: 0.2433, Test AP: 0.2445\n",
                        "Seed: 43, Epoch: 053, Loss: 0.0524, Val AP: 0.2473, Test AP: 0.2485\n",
                        "Seed: 43, Epoch: 054, Loss: 0.0525, Val AP: 0.2112, Test AP: 0.2133\n",
                        "Seed: 43, Epoch: 055, Loss: 0.0525, Val AP: 0.2419, Test AP: 0.2423\n",
                        "Seed: 43, Epoch: 056, Loss: 0.0523, Val AP: 0.2394, Test AP: 0.2390\n",
                        "Seed: 43, Epoch: 057, Loss: 0.0523, Val AP: 0.2396, Test AP: 0.2420\n",
                        "Seed: 43, Epoch: 058, Loss: 0.0525, Val AP: 0.2476, Test AP: 0.2506\n",
                        "Seed: 43, Epoch: 059, Loss: 0.0526, Val AP: 0.2242, Test AP: 0.2275\n",
                        "Seed: 43, Epoch: 060, Loss: 0.0523, Val AP: 0.2524, Test AP: 0.2538\n",
                        "Seed: 43, Epoch: 061, Loss: 0.0525, Val AP: 0.2118, Test AP: 0.2128\n",
                        "Seed: 43, Epoch: 062, Loss: 0.0522, Val AP: 0.2342, Test AP: 0.2378\n",
                        "Seed: 43, Epoch: 063, Loss: 0.0524, Val AP: 0.2259, Test AP: 0.2264\n",
                        "Seed: 43, Epoch: 064, Loss: 0.0522, Val AP: 0.2482, Test AP: 0.2501\n",
                        "Seed: 43, Epoch: 065, Loss: 0.0521, Val AP: 0.2463, Test AP: 0.2477\n",
                        "Seed: 43, Epoch: 066, Loss: 0.0521, Val AP: 0.2495, Test AP: 0.2506\n",
                        "Seed: 43, Epoch: 067, Loss: 0.0520, Val AP: 0.2325, Test AP: 0.2334\n",
                        "Seed: 43, Epoch: 068, Loss: 0.0521, Val AP: 0.2490, Test AP: 0.2492\n",
                        "Seed: 43, Epoch: 069, Loss: 0.0521, Val AP: 0.2471, Test AP: 0.2489\n",
                        "Seed: 43, Epoch: 070, Loss: 0.0520, Val AP: 0.2446, Test AP: 0.2465\n",
                        "Seed: 43, Epoch: 071, Loss: 0.0522, Val AP: 0.2322, Test AP: 0.2338\n",
                        "Seed: 43, Epoch: 072, Loss: 0.0523, Val AP: 0.2482, Test AP: 0.2497\n",
                        "Seed: 43, Epoch: 073, Loss: 0.0521, Val AP: 0.2369, Test AP: 0.2386\n",
                        "Seed: 43, Epoch: 074, Loss: 0.0520, Val AP: 0.2332, Test AP: 0.2361\n",
                        "Seed: 43, Epoch: 075, Loss: 0.0520, Val AP: 0.2384, Test AP: 0.2401\n",
                        "Seed: 43, Epoch: 076, Loss: 0.0520, Val AP: 0.2474, Test AP: 0.2485\n",
                        "Seed: 43, Epoch: 077, Loss: 0.0522, Val AP: 0.2496, Test AP: 0.2546\n",
                        "Seed: 43, Epoch: 078, Loss: 0.0520, Val AP: 0.2522, Test AP: 0.2541\n",
                        "Seed: 43, Epoch: 079, Loss: 0.0520, Val AP: 0.2402, Test AP: 0.2413\n",
                        "Seed: 43, Epoch: 080, Loss: 0.0519, Val AP: 0.2520, Test AP: 0.2551\n",
                        "Seed: 43, Epoch: 081, Loss: 0.0518, Val AP: 0.2484, Test AP: 0.2501\n",
                        "Seed: 43, Epoch: 082, Loss: 0.0518, Val AP: 0.2486, Test AP: 0.2514\n",
                        "Seed: 43, Epoch: 083, Loss: 0.0518, Val AP: 0.2569, Test AP: 0.2588\n",
                        "Seed: 43, Epoch: 084, Loss: 0.0519, Val AP: 0.2528, Test AP: 0.2533\n",
                        "Seed: 43, Epoch: 085, Loss: 0.0518, Val AP: 0.2498, Test AP: 0.2505\n",
                        "Seed: 43, Epoch: 086, Loss: 0.0517, Val AP: 0.2584, Test AP: 0.2603\n",
                        "Seed: 43, Epoch: 087, Loss: 0.0517, Val AP: 0.2476, Test AP: 0.2501\n",
                        "Seed: 43, Epoch: 088, Loss: 0.0516, Val AP: 0.2456, Test AP: 0.2441\n",
                        "Seed: 43, Epoch: 089, Loss: 0.0516, Val AP: 0.2432, Test AP: 0.2450\n",
                        "Seed: 43, Epoch: 090, Loss: 0.0517, Val AP: 0.2513, Test AP: 0.2527\n",
                        "Seed: 43, Epoch: 091, Loss: 0.0517, Val AP: 0.2375, Test AP: 0.2383\n",
                        "Seed: 43, Epoch: 092, Loss: 0.0515, Val AP: 0.2578, Test AP: 0.2573\n",
                        "Seed: 43, Epoch: 093, Loss: 0.0515, Val AP: 0.2278, Test AP: 0.2282\n",
                        "Seed: 43, Epoch: 094, Loss: 0.0516, Val AP: 0.2410, Test AP: 0.2430\n",
                        "Seed: 43, Epoch: 095, Loss: 0.0514, Val AP: 0.2318, Test AP: 0.2322\n",
                        "Seed: 43, Epoch: 096, Loss: 0.0516, Val AP: 0.2482, Test AP: 0.2497\n",
                        "Seed: 43, Epoch: 097, Loss: 0.0517, Val AP: 0.2511, Test AP: 0.2503\n",
                        "Seed: 43, Epoch: 098, Loss: 0.0515, Val AP: 0.2559, Test AP: 0.2567\n",
                        "Seed: 43, Epoch: 099, Loss: 0.0514, Val AP: 0.2435, Test AP: 0.2431\n",
                        "Seed: 43, Epoch: 100, Loss: 0.0513, Val AP: 0.2353, Test AP: 0.2364\n",
                        "Seed: 43, Epoch: 101, Loss: 0.0516, Val AP: 0.2558, Test AP: 0.2586\n",
                        "Seed: 43, Epoch: 102, Loss: 0.0513, Val AP: 0.2559, Test AP: 0.2561\n",
                        "Seed: 43, Epoch: 103, Loss: 0.0514, Val AP: 0.2572, Test AP: 0.2558\n",
                        "Seed: 43, Epoch: 104, Loss: 0.0514, Val AP: 0.1490, Test AP: 0.1473\n",
                        "Seed: 43, Epoch: 105, Loss: 0.0518, Val AP: 0.2532, Test AP: 0.2522\n",
                        "Seed: 43, Epoch: 106, Loss: 0.0512, Val AP: 0.2641, Test AP: 0.2640\n",
                        "Seed: 43, Epoch: 107, Loss: 0.0513, Val AP: 0.2636, Test AP: 0.2637\n",
                        "Seed: 43, Epoch: 108, Loss: 0.0514, Val AP: 0.2585, Test AP: 0.2600\n",
                        "Seed: 43, Epoch: 109, Loss: 0.0512, Val AP: 0.2671, Test AP: 0.2686\n",
                        "Seed: 43, Epoch: 110, Loss: 0.0514, Val AP: 0.2470, Test AP: 0.2479\n",
                        "Seed: 43, Epoch: 111, Loss: 0.0513, Val AP: 0.2642, Test AP: 0.2652\n",
                        "Seed: 43, Epoch: 112, Loss: 0.0513, Val AP: 0.2619, Test AP: 0.2628\n",
                        "Seed: 43, Epoch: 113, Loss: 0.0512, Val AP: 0.2644, Test AP: 0.2660\n",
                        "Seed: 43, Epoch: 114, Loss: 0.0512, Val AP: 0.2442, Test AP: 0.2431\n",
                        "Seed: 43, Epoch: 115, Loss: 0.0514, Val AP: 0.2517, Test AP: 0.2512\n",
                        "Seed: 43, Epoch: 116, Loss: 0.0513, Val AP: 0.2532, Test AP: 0.2523\n",
                        "Seed: 43, Epoch: 117, Loss: 0.0512, Val AP: 0.2502, Test AP: 0.2491\n",
                        "Seed: 43, Epoch: 118, Loss: 0.0513, Val AP: 0.2560, Test AP: 0.2557\n",
                        "Seed: 43, Epoch: 119, Loss: 0.0514, Val AP: 0.2524, Test AP: 0.2530\n",
                        "Seed: 43, Epoch: 120, Loss: 0.0512, Val AP: 0.2577, Test AP: 0.2575\n",
                        "Seed: 43, Epoch: 121, Loss: 0.0512, Val AP: 0.2530, Test AP: 0.2517\n",
                        "Seed: 43, Epoch: 122, Loss: 0.0511, Val AP: 0.2536, Test AP: 0.2536\n",
                        "Seed: 43, Epoch: 123, Loss: 0.0512, Val AP: 0.2506, Test AP: 0.2515\n",
                        "Seed: 43, Epoch: 124, Loss: 0.0513, Val AP: 0.2623, Test AP: 0.2603\n",
                        "Seed: 43, Epoch: 125, Loss: 0.0514, Val AP: 0.2654, Test AP: 0.2647\n",
                        "Seed: 43, Epoch: 126, Loss: 0.0510, Val AP: 0.2676, Test AP: 0.2672\n",
                        "Seed: 43, Epoch: 127, Loss: 0.0510, Val AP: 0.2570, Test AP: 0.2564\n",
                        "Seed: 43, Epoch: 128, Loss: 0.0510, Val AP: 0.2573, Test AP: 0.2569\n",
                        "Seed: 43, Epoch: 129, Loss: 0.0520, Val AP: 0.2634, Test AP: 0.2633\n",
                        "Seed: 43, Epoch: 130, Loss: 0.0510, Val AP: 0.2600, Test AP: 0.2592\n",
                        "Seed: 43, Epoch: 131, Loss: 0.0511, Val AP: 0.2565, Test AP: 0.2556\n",
                        "Seed: 43, Epoch: 132, Loss: 0.0510, Val AP: 0.2602, Test AP: 0.2587\n",
                        "Seed: 43, Epoch: 133, Loss: 0.0510, Val AP: 0.2568, Test AP: 0.2556\n",
                        "Seed: 43, Epoch: 134, Loss: 0.0511, Val AP: 0.2573, Test AP: 0.2594\n",
                        "Seed: 43, Epoch: 135, Loss: 0.0509, Val AP: 0.2576, Test AP: 0.2581\n",
                        "Seed: 43, Epoch: 136, Loss: 0.0511, Val AP: 0.2704, Test AP: 0.2696\n",
                        "Seed: 43, Epoch: 137, Loss: 0.0510, Val AP: 0.2570, Test AP: 0.2567\n",
                        "Seed: 43, Epoch: 138, Loss: 0.0510, Val AP: 0.2650, Test AP: 0.2645\n",
                        "Seed: 43, Epoch: 139, Loss: 0.0509, Val AP: 0.2405, Test AP: 0.2414\n",
                        "Seed: 43, Epoch: 140, Loss: 0.0511, Val AP: 0.2653, Test AP: 0.2654\n",
                        "Seed: 43, Epoch: 141, Loss: 0.0509, Val AP: 0.2587, Test AP: 0.2604\n",
                        "Seed: 43, Epoch: 142, Loss: 0.0511, Val AP: 0.2693, Test AP: 0.2689\n",
                        "Seed: 43, Epoch: 143, Loss: 0.0509, Val AP: 0.2630, Test AP: 0.2634\n",
                        "Seed: 43, Epoch: 144, Loss: 0.0510, Val AP: 0.1448, Test AP: 0.1434\n",
                        "Seed: 43, Epoch: 145, Loss: 0.0520, Val AP: 0.2431, Test AP: 0.2431\n",
                        "Seed: 43, Epoch: 146, Loss: 0.0509, Val AP: 0.2632, Test AP: 0.2639\n",
                        "Seed: 43, Epoch: 147, Loss: 0.0508, Val AP: 0.2753, Test AP: 0.2769\n",
                        "Seed: 43, Epoch: 148, Loss: 0.0508, Val AP: 0.2605, Test AP: 0.2609\n",
                        "Seed: 43, Epoch: 149, Loss: 0.0509, Val AP: 0.2673, Test AP: 0.2661\n",
                        "Seed: 43, Epoch: 150, Loss: 0.0508, Val AP: 0.2606, Test AP: 0.2602\n",
                        "Seed: 44, Epoch: 001, Loss: 0.0619, Val AP: 0.1224, Test AP: 0.1165\n",
                        "Seed: 44, Epoch: 002, Loss: 0.0602, Val AP: 0.1229, Test AP: 0.1171\n",
                        "Seed: 44, Epoch: 003, Loss: 0.0601, Val AP: 0.1245, Test AP: 0.1188\n",
                        "Seed: 44, Epoch: 004, Loss: 0.0597, Val AP: 0.1359, Test AP: 0.1295\n",
                        "Seed: 44, Epoch: 005, Loss: 0.0593, Val AP: 0.1387, Test AP: 0.1306\n",
                        "Seed: 44, Epoch: 006, Loss: 0.0592, Val AP: 0.1414, Test AP: 0.1350\n",
                        "Seed: 44, Epoch: 007, Loss: 0.0587, Val AP: 0.1487, Test AP: 0.1412\n",
                        "Seed: 44, Epoch: 008, Loss: 0.0583, Val AP: 0.1560, Test AP: 0.1480\n",
                        "Seed: 44, Epoch: 009, Loss: 0.0579, Val AP: 0.1575, Test AP: 0.1489\n",
                        "Seed: 44, Epoch: 010, Loss: 0.0577, Val AP: 0.1695, Test AP: 0.1629\n",
                        "Seed: 44, Epoch: 011, Loss: 0.0572, Val AP: 0.1698, Test AP: 0.1643\n",
                        "Seed: 44, Epoch: 012, Loss: 0.0569, Val AP: 0.1809, Test AP: 0.1745\n",
                        "Seed: 44, Epoch: 013, Loss: 0.0565, Val AP: 0.1914, Test AP: 0.1862\n",
                        "Seed: 44, Epoch: 014, Loss: 0.0561, Val AP: 0.1952, Test AP: 0.1918\n",
                        "Seed: 44, Epoch: 015, Loss: 0.0559, Val AP: 0.1818, Test AP: 0.1781\n",
                        "Seed: 44, Epoch: 016, Loss: 0.0556, Val AP: 0.1930, Test AP: 0.1870\n",
                        "Seed: 44, Epoch: 017, Loss: 0.0553, Val AP: 0.2038, Test AP: 0.1999\n",
                        "Seed: 44, Epoch: 018, Loss: 0.0551, Val AP: 0.2058, Test AP: 0.2022\n",
                        "Seed: 44, Epoch: 019, Loss: 0.0549, Val AP: 0.2063, Test AP: 0.1996\n",
                        "Seed: 44, Epoch: 020, Loss: 0.0548, Val AP: 0.1967, Test AP: 0.1916\n",
                        "Seed: 44, Epoch: 021, Loss: 0.0547, Val AP: 0.2084, Test AP: 0.2008\n",
                        "Seed: 44, Epoch: 022, Loss: 0.0545, Val AP: 0.2098, Test AP: 0.2037\n",
                        "Seed: 44, Epoch: 023, Loss: 0.0543, Val AP: 0.2031, Test AP: 0.1964\n",
                        "Seed: 44, Epoch: 024, Loss: 0.0543, Val AP: 0.2182, Test AP: 0.2125\n",
                        "Seed: 44, Epoch: 025, Loss: 0.0541, Val AP: 0.2251, Test AP: 0.2174\n",
                        "Seed: 44, Epoch: 026, Loss: 0.0540, Val AP: 0.2178, Test AP: 0.2116\n",
                        "Seed: 44, Epoch: 027, Loss: 0.0538, Val AP: 0.2222, Test AP: 0.2155\n",
                        "Seed: 44, Epoch: 028, Loss: 0.0538, Val AP: 0.2286, Test AP: 0.2232\n",
                        "Seed: 44, Epoch: 029, Loss: 0.0537, Val AP: 0.2253, Test AP: 0.2187\n",
                        "Seed: 44, Epoch: 030, Loss: 0.0535, Val AP: 0.2374, Test AP: 0.2292\n",
                        "Seed: 44, Epoch: 031, Loss: 0.0534, Val AP: 0.2385, Test AP: 0.2322\n",
                        "Seed: 44, Epoch: 032, Loss: 0.0533, Val AP: 0.2347, Test AP: 0.2262\n",
                        "Seed: 44, Epoch: 033, Loss: 0.0532, Val AP: 0.2326, Test AP: 0.2267\n",
                        "Seed: 44, Epoch: 034, Loss: 0.0531, Val AP: 0.2386, Test AP: 0.2351\n",
                        "Seed: 44, Epoch: 035, Loss: 0.0530, Val AP: 0.2297, Test AP: 0.2250\n",
                        "Seed: 44, Epoch: 036, Loss: 0.0529, Val AP: 0.2405, Test AP: 0.2340\n",
                        "Seed: 44, Epoch: 037, Loss: 0.0529, Val AP: 0.2520, Test AP: 0.2448\n",
                        "Seed: 44, Epoch: 038, Loss: 0.0528, Val AP: 0.2263, Test AP: 0.2218\n",
                        "Seed: 44, Epoch: 039, Loss: 0.0527, Val AP: 0.2327, Test AP: 0.2282\n",
                        "Seed: 44, Epoch: 040, Loss: 0.0527, Val AP: 0.2462, Test AP: 0.2411\n",
                        "Seed: 44, Epoch: 041, Loss: 0.0526, Val AP: 0.2544, Test AP: 0.2477\n",
                        "Seed: 44, Epoch: 042, Loss: 0.0525, Val AP: 0.2338, Test AP: 0.2272\n",
                        "Seed: 44, Epoch: 043, Loss: 0.0525, Val AP: 0.2303, Test AP: 0.2254\n",
                        "Seed: 44, Epoch: 044, Loss: 0.0524, Val AP: 0.2493, Test AP: 0.2430\n",
                        "Seed: 44, Epoch: 045, Loss: 0.0524, Val AP: 0.2501, Test AP: 0.2419\n",
                        "Seed: 44, Epoch: 046, Loss: 0.0523, Val AP: 0.2460, Test AP: 0.2393\n",
                        "Seed: 44, Epoch: 047, Loss: 0.0523, Val AP: 0.2313, Test AP: 0.2248\n",
                        "Seed: 44, Epoch: 048, Loss: 0.0522, Val AP: 0.2474, Test AP: 0.2411\n",
                        "Seed: 44, Epoch: 049, Loss: 0.0522, Val AP: 0.2466, Test AP: 0.2403\n",
                        "Seed: 44, Epoch: 050, Loss: 0.0521, Val AP: 0.2416, Test AP: 0.2364\n",
                        "Seed: 44, Epoch: 051, Loss: 0.0523, Val AP: 0.2649, Test AP: 0.2570\n",
                        "Seed: 44, Epoch: 052, Loss: 0.0521, Val AP: 0.2390, Test AP: 0.2332\n",
                        "Seed: 44, Epoch: 053, Loss: 0.0521, Val AP: 0.2550, Test AP: 0.2484\n",
                        "Seed: 44, Epoch: 054, Loss: 0.0520, Val AP: 0.2538, Test AP: 0.2486\n",
                        "Seed: 44, Epoch: 055, Loss: 0.0520, Val AP: 0.2593, Test AP: 0.2534\n",
                        "Seed: 44, Epoch: 056, Loss: 0.0519, Val AP: 0.2488, Test AP: 0.2450\n",
                        "Seed: 44, Epoch: 057, Loss: 0.0519, Val AP: 0.2607, Test AP: 0.2538\n",
                        "Seed: 44, Epoch: 058, Loss: 0.0519, Val AP: 0.2528, Test AP: 0.2465\n",
                        "Seed: 44, Epoch: 059, Loss: 0.0518, Val AP: 0.2437, Test AP: 0.2382\n",
                        "Seed: 44, Epoch: 060, Loss: 0.0519, Val AP: 0.2398, Test AP: 0.2333\n",
                        "Seed: 44, Epoch: 061, Loss: 0.0519, Val AP: 0.2484, Test AP: 0.2417\n",
                        "Seed: 44, Epoch: 062, Loss: 0.0518, Val AP: 0.2473, Test AP: 0.2408\n",
                        "Seed: 44, Epoch: 063, Loss: 0.0518, Val AP: 0.2529, Test AP: 0.2465\n",
                        "Seed: 44, Epoch: 064, Loss: 0.0517, Val AP: 0.2477, Test AP: 0.2395\n",
                        "Seed: 44, Epoch: 065, Loss: 0.0517, Val AP: 0.2547, Test AP: 0.2465\n",
                        "Seed: 44, Epoch: 066, Loss: 0.0516, Val AP: 0.2583, Test AP: 0.2522\n",
                        "Seed: 44, Epoch: 067, Loss: 0.0517, Val AP: 0.2643, Test AP: 0.2582\n",
                        "Seed: 44, Epoch: 068, Loss: 0.0516, Val AP: 0.2611, Test AP: 0.2531\n",
                        "Seed: 44, Epoch: 069, Loss: 0.0516, Val AP: 0.2688, Test AP: 0.2610\n",
                        "Seed: 44, Epoch: 070, Loss: 0.0515, Val AP: 0.2560, Test AP: 0.2477\n",
                        "Seed: 44, Epoch: 071, Loss: 0.0515, Val AP: 0.2574, Test AP: 0.2508\n",
                        "Seed: 44, Epoch: 072, Loss: 0.0517, Val AP: 0.2423, Test AP: 0.2377\n",
                        "Seed: 44, Epoch: 073, Loss: 0.0520, Val AP: 0.2498, Test AP: 0.2396\n",
                        "Seed: 44, Epoch: 074, Loss: 0.0515, Val AP: 0.2615, Test AP: 0.2542\n",
                        "Seed: 44, Epoch: 075, Loss: 0.0514, Val AP: 0.2605, Test AP: 0.2531\n",
                        "Seed: 44, Epoch: 076, Loss: 0.0515, Val AP: 0.2631, Test AP: 0.2542\n",
                        "Seed: 44, Epoch: 077, Loss: 0.0514, Val AP: 0.2616, Test AP: 0.2538\n",
                        "Seed: 44, Epoch: 078, Loss: 0.0514, Val AP: 0.2516, Test AP: 0.2456\n",
                        "Seed: 44, Epoch: 079, Loss: 0.0514, Val AP: 0.2576, Test AP: 0.2515\n",
                        "Seed: 44, Epoch: 080, Loss: 0.0514, Val AP: 0.2488, Test AP: 0.2425\n",
                        "Seed: 44, Epoch: 081, Loss: 0.0513, Val AP: 0.2441, Test AP: 0.2391\n",
                        "Seed: 44, Epoch: 082, Loss: 0.0513, Val AP: 0.2523, Test AP: 0.2448\n",
                        "Seed: 44, Epoch: 083, Loss: 0.0513, Val AP: 0.2495, Test AP: 0.2433\n",
                        "Seed: 44, Epoch: 084, Loss: 0.0514, Val AP: 0.2638, Test AP: 0.2555\n",
                        "Seed: 44, Epoch: 085, Loss: 0.0514, Val AP: 0.2529, Test AP: 0.2448\n",
                        "Seed: 44, Epoch: 086, Loss: 0.0514, Val AP: 0.2627, Test AP: 0.2554\n",
                        "Seed: 44, Epoch: 087, Loss: 0.0514, Val AP: 0.2601, Test AP: 0.2503\n",
                        "Seed: 44, Epoch: 088, Loss: 0.0513, Val AP: 0.2568, Test AP: 0.2496\n",
                        "Seed: 44, Epoch: 089, Loss: 0.0513, Val AP: 0.2615, Test AP: 0.2551\n",
                        "Seed: 44, Epoch: 090, Loss: 0.0513, Val AP: 0.2554, Test AP: 0.2481\n",
                        "Seed: 44, Epoch: 091, Loss: 0.0512, Val AP: 0.2627, Test AP: 0.2546\n",
                        "Seed: 44, Epoch: 092, Loss: 0.0511, Val AP: 0.2511, Test AP: 0.2460\n",
                        "Seed: 44, Epoch: 093, Loss: 0.0511, Val AP: 0.2730, Test AP: 0.2626\n",
                        "Seed: 44, Epoch: 094, Loss: 0.0512, Val AP: 0.2456, Test AP: 0.2413\n",
                        "Seed: 44, Epoch: 095, Loss: 0.0512, Val AP: 0.2628, Test AP: 0.2568\n",
                        "Seed: 44, Epoch: 096, Loss: 0.0511, Val AP: 0.2691, Test AP: 0.2609\n",
                        "Seed: 44, Epoch: 097, Loss: 0.0512, Val AP: 0.2511, Test AP: 0.2421\n",
                        "Seed: 44, Epoch: 098, Loss: 0.0511, Val AP: 0.2654, Test AP: 0.2589\n",
                        "Seed: 44, Epoch: 099, Loss: 0.0511, Val AP: 0.2494, Test AP: 0.2451\n",
                        "Seed: 44, Epoch: 100, Loss: 0.0511, Val AP: 0.2514, Test AP: 0.2449\n",
                        "Seed: 44, Epoch: 101, Loss: 0.0511, Val AP: 0.2631, Test AP: 0.2558\n",
                        "Seed: 44, Epoch: 102, Loss: 0.0512, Val AP: 0.2651, Test AP: 0.2584\n",
                        "Seed: 44, Epoch: 103, Loss: 0.0510, Val AP: 0.2630, Test AP: 0.2549\n",
                        "Seed: 44, Epoch: 104, Loss: 0.0511, Val AP: 0.2756, Test AP: 0.2659\n",
                        "Seed: 44, Epoch: 105, Loss: 0.0511, Val AP: 0.2739, Test AP: 0.2648\n",
                        "Seed: 44, Epoch: 106, Loss: 0.0511, Val AP: 0.2572, Test AP: 0.2505\n",
                        "Seed: 44, Epoch: 107, Loss: 0.0511, Val AP: 0.2585, Test AP: 0.2515\n",
                        "Seed: 44, Epoch: 108, Loss: 0.0511, Val AP: 0.2639, Test AP: 0.2553\n",
                        "Seed: 44, Epoch: 109, Loss: 0.0511, Val AP: 0.2600, Test AP: 0.2546\n",
                        "Seed: 44, Epoch: 110, Loss: 0.0511, Val AP: 0.2705, Test AP: 0.2627\n",
                        "Seed: 44, Epoch: 111, Loss: 0.0511, Val AP: 0.2667, Test AP: 0.2601\n",
                        "Seed: 44, Epoch: 112, Loss: 0.0509, Val AP: 0.2642, Test AP: 0.2583\n",
                        "Seed: 44, Epoch: 113, Loss: 0.0509, Val AP: 0.2661, Test AP: 0.2569\n",
                        "Seed: 44, Epoch: 114, Loss: 0.0509, Val AP: 0.2660, Test AP: 0.2574\n",
                        "Seed: 44, Epoch: 115, Loss: 0.0509, Val AP: 0.2630, Test AP: 0.2537\n",
                        "Seed: 44, Epoch: 116, Loss: 0.0509, Val AP: 0.2705, Test AP: 0.2608\n",
                        "Seed: 44, Epoch: 117, Loss: 0.0510, Val AP: 0.2492, Test AP: 0.2415\n",
                        "Seed: 44, Epoch: 118, Loss: 0.0510, Val AP: 0.2574, Test AP: 0.2508\n",
                        "Seed: 44, Epoch: 119, Loss: 0.0509, Val AP: 0.2690, Test AP: 0.2606\n",
                        "Seed: 44, Epoch: 120, Loss: 0.0509, Val AP: 0.2571, Test AP: 0.2489\n",
                        "Seed: 44, Epoch: 121, Loss: 0.0509, Val AP: 0.2662, Test AP: 0.2586\n",
                        "Seed: 44, Epoch: 122, Loss: 0.0509, Val AP: 0.2638, Test AP: 0.2573\n",
                        "Seed: 44, Epoch: 123, Loss: 0.0509, Val AP: 0.2578, Test AP: 0.2508\n",
                        "Seed: 44, Epoch: 124, Loss: 0.0509, Val AP: 0.2648, Test AP: 0.2583\n",
                        "Seed: 44, Epoch: 125, Loss: 0.0509, Val AP: 0.2768, Test AP: 0.2674\n",
                        "Seed: 44, Epoch: 126, Loss: 0.0508, Val AP: 0.2617, Test AP: 0.2543\n",
                        "Seed: 44, Epoch: 127, Loss: 0.0509, Val AP: 0.2668, Test AP: 0.2598\n",
                        "Seed: 44, Epoch: 128, Loss: 0.0507, Val AP: 0.2650, Test AP: 0.2566\n",
                        "Seed: 44, Epoch: 129, Loss: 0.0507, Val AP: 0.2725, Test AP: 0.2648\n",
                        "Seed: 44, Epoch: 130, Loss: 0.0509, Val AP: 0.2517, Test AP: 0.2452\n",
                        "Seed: 44, Epoch: 131, Loss: 0.0508, Val AP: 0.2668, Test AP: 0.2613\n",
                        "Seed: 44, Epoch: 132, Loss: 0.0508, Val AP: 0.2729, Test AP: 0.2656\n",
                        "Seed: 44, Epoch: 133, Loss: 0.0507, Val AP: 0.2690, Test AP: 0.2607\n",
                        "Seed: 44, Epoch: 134, Loss: 0.0507, Val AP: 0.2548, Test AP: 0.2485\n",
                        "Seed: 44, Epoch: 135, Loss: 0.0507, Val AP: 0.2695, Test AP: 0.2651\n",
                        "Seed: 44, Epoch: 136, Loss: 0.0506, Val AP: 0.2599, Test AP: 0.2530\n",
                        "Seed: 44, Epoch: 137, Loss: 0.0506, Val AP: 0.2703, Test AP: 0.2622\n",
                        "Seed: 44, Epoch: 138, Loss: 0.0508, Val AP: 0.2545, Test AP: 0.2501\n",
                        "Seed: 44, Epoch: 139, Loss: 0.0507, Val AP: 0.2690, Test AP: 0.2627\n",
                        "Seed: 44, Epoch: 140, Loss: 0.0506, Val AP: 0.2676, Test AP: 0.2628\n",
                        "Seed: 44, Epoch: 141, Loss: 0.0507, Val AP: 0.2629, Test AP: 0.2570\n",
                        "Seed: 44, Epoch: 142, Loss: 0.0507, Val AP: 0.2634, Test AP: 0.2557\n",
                        "Seed: 44, Epoch: 143, Loss: 0.0507, Val AP: 0.2674, Test AP: 0.2608\n",
                        "Seed: 44, Epoch: 144, Loss: 0.0506, Val AP: 0.2710, Test AP: 0.2637\n",
                        "Seed: 44, Epoch: 145, Loss: 0.0505, Val AP: 0.2559, Test AP: 0.2521\n",
                        "Seed: 44, Epoch: 146, Loss: 0.0507, Val AP: 0.2540, Test AP: 0.2471\n",
                        "Seed: 44, Epoch: 147, Loss: 0.0506, Val AP: 0.2598, Test AP: 0.2540\n",
                        "Seed: 44, Epoch: 148, Loss: 0.0506, Val AP: 0.2633, Test AP: 0.2577\n",
                        "Seed: 44, Epoch: 149, Loss: 0.0506, Val AP: 0.2630, Test AP: 0.2535\n",
                        "Seed: 44, Epoch: 150, Loss: 0.0508, Val AP: 0.2626, Test AP: 0.2551\n",
                        "Average Time: 14804.56 seconds\n",
                        "Var Time: 80446.68 seconds\n",
                        "Average Memory: 48816.00 MB\n",
                        "Average Best Val AP: 0.2543\n",
                        "Std Best Test AP: 0.0246\n",
                        "Average Test AP: 0.2550\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nclass HierarchicalGCN_CO(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_CO, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, improved = True)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = CoPooling(ratio=0.9, K=1, edge_ratio=0.6, nhid=512, alpha=0.1, Init='Random', Gamma=1.0)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, improved = True)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = CoPooling(ratio=0.9, K=1, edge_ratio=0.6, nhid=512, alpha=0.1, Init='Random', Gamma=1.0)\n        self.conv3 = GCNConv(hidden_channels, out_channels, improved = True)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        init_x = data.x\n        x = x.float()\n        init_x = init_x.float()\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, perm, _, batch, _, _, _ = self.pool1(x, edge_index, edge_attr=edge_attr, batch=batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, perm, _, batch, _, _, _ = self.pool2(x, edge_index, edge_attr=edge_attr, batch=batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n    model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CGIPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\n@dataclass(init=False)\nclass SelectOutput:\n    r\"\"\"The output of the :class:`Select` method, which holds an assignment\n    from selected nodes to their respective cluster(s).\n    Args:\n        node_index (torch.Tensor): The indices of the selected nodes.\n        num_nodes (int): The number of nodes.\n        cluster_index (torch.Tensor): The indices of the clusters each node in\n            :obj:`node_index` is assigned to.\n        num_clusters (int): The number of clusters.\n        weight (torch.Tensor, optional): A weight vector, denoting the strength\n            of the assignment of a node to its cluster. (default: :obj:`None`)\n    \"\"\"\n    node_index: Tensor\n    num_nodes: int\n    cluster_index: Tensor\n    num_clusters: int\n    weight: Optional[Tensor] = None\n    def __init__(\n        self,\n        node_index: Tensor,\n        num_nodes: int,\n        cluster_index: Tensor,\n        num_clusters: int,\n        weight: Optional[Tensor] = None,\n    ):\n        if node_index.dim() != 1:\n            raise ValueError(f\"Expected 'node_index' to be one-dimensional \"\n                             f\"(got {node_index.dim()} dimensions)\")\n        if cluster_index.dim() != 1:\n            raise ValueError(f\"Expected 'cluster_index' to be one-dimensional \"\n                             f\"(got {cluster_index.dim()} dimensions)\")\n        if node_index.numel() != cluster_index.numel():\n            raise ValueError(f\"Expected 'node_index' and 'cluster_index' to \"\n                             f\"hold the same number of values (got \"\n                             f\"{node_index.numel()} and \"\n                             f\"{cluster_index.numel()} values)\")\n        if weight is not None and weight.dim() != 1:\n            raise ValueError(f\"Expected 'weight' vector to be one-dimensional \"\n                             f\"(got {weight.dim()} dimensions)\")\n        if weight is not None and weight.numel() != node_index.numel():\n            raise ValueError(f\"Expected 'weight' to hold {node_index.numel()} \"\n                             f\"values (got {weight.numel()} values)\")\n        self.node_index = node_index\n        self.num_nodes = num_nodes\n        self.cluster_index = cluster_index\n        self.num_clusters = num_clusters\n        self.weight = weight\nclass Select(torch.nn.Module):\n    r\"\"\"An abstract base class for implementing custom node selections as\n    described in the `\"Understanding Pooling in Graph Neural Networks\"\n    <https://arxiv.org/abs/1905.05178>`_ paper, which maps the nodes of an\n    input graph to supernodes in the coarsened graph.\n    Specifically, :class:`Select` returns a :class:`SelectOutput` output, which\n    holds a (sparse) mapping :math:`\\mathbf{C} \\in {[0, 1]}^{N \\times C}` that\n    assigns selected nodes to one or more of :math:`C` super nodes.\n    \"\"\"\n    def reset_parameters(self):\n        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n        pass\n    def forward(self, *args, **kwargs) -> SelectOutput:\n        raise NotImplementedError\n    def __repr__(self) -> str:\n        return f'{self.__class__.__name__}()'\ndef cumsum(x: Tensor, dim: int = 0) -> Tensor:\n    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n    Args:\n        x (torch.Tensor): The input tensor.\n        dim (int, optional): The dimension to do the operation over.\n            (default: :obj:`0`)\n    Example:\n        >>> x = torch.tensor([2, 4, 1])\n        >>> cumsum(x)\n        tensor([0, 2, 6, 7])\n    \"\"\"\n    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n    out = x.new_empty(size)\n    out.narrow(dim, 0, 1).zero_()\n    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n    return out\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\ndef topk(\n    x: Tensor,\n    ratio: Optional[Union[float, int]],\n    batch: Tensor,\n    min_score: Optional[float] = None,\n    tol: float = 1e-7,\n) -> Tensor:\n    if min_score is not None:\n        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero().view(-1)\n        return perm\n    if ratio is not None:\n        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n        if ratio >= 1:\n            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n        else:\n            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n        x, x_perm = torch.sort(x.view(-1), descending=True)\n        batch = batch[x_perm]\n        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n        ptr = cumsum(num_nodes)\n        batched_arange = arange - ptr[batch]\n        mask = batched_arange < k[batch]\n        return x_perm[batch_perm[mask]]\n    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n                     \"must be specified\")\nclass Discriminator(torch.nn.Module):\n    def __init__(self, in_channels):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(in_channels * 2, in_channels)\n        self.fc2 = nn.Linear(in_channels, 1)\n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = F.sigmoid(self.fc2(x))\n        return x\nclass CGIPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, non_lin=torch.tanh):\n        super(CGIPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.non_lin = non_lin\n        self.hidden_dim = in_channels\n        self.transform = GraphConv(in_channels, self.hidden_dim)\n        self.pp_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n        self.np_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n        self.positive_pooling = GraphConv(self.hidden_dim, 1)\n        self.negative_pooling = GraphConv(self.hidden_dim, 1)\n        self.discriminator = Discriminator(self.hidden_dim)\n        self.loss_fn = torch.nn.BCELoss()\n    def forward(self, x, edge_index, edge_attr=None, batch=None):\n        device = x.device  \n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_transform = F.leaky_relu(self.transform(x, edge_index), 0.2)\n        x_tp = F.leaky_relu(self.pp_conv(x, edge_index), 0.2)\n        x_tn = F.leaky_relu(self.np_conv(x, edge_index), 0.2)\n        s_pp = self.positive_pooling(x_tp, edge_index).squeeze()\n        s_np = self.negative_pooling(x_tn, edge_index).squeeze()\n        perm_positive = topk(s_pp, 1, batch)\n        perm_negative = topk(s_np, 1, batch)\n        x_pp = x_transform[perm_positive] * self.non_lin(s_pp[perm_positive]).view(-1, 1)\n        x_np = x_transform[perm_negative] * self.non_lin(s_np[perm_negative]).view(-1, 1)\n        x_pp_readout = gap(x_pp, batch[perm_positive])\n        x_np_readout = gap(x_np, batch[perm_negative])\n        x_readout = gap(x_transform, batch)\n        positive_pair = torch.cat([x_pp_readout, x_readout], dim=1)\n        negative_pair = torch.cat([x_np_readout, x_readout], dim=1)\n        real = torch.ones(positive_pair.shape[0], device=device)  \n        fake = torch.zeros(negative_pair.shape[0], device=device)  \n        score = (s_pp - s_np)\n        perm = topk(score, self.ratio, batch)\n        x = x_transform[perm] * self.non_lin(score[perm]).view(-1, 1)\n        batch = batch[perm]\n        filter_edge_index, filter_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        return x, filter_edge_index, filter_edge_attr, batch, perm"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0649, Val AP: 0.1149, Test AP: 0.1142\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0608, Val AP: 0.1208, Test AP: 0.1222\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0603, Val AP: 0.1208, Test AP: 0.1218\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0602, Val AP: 0.1204, Test AP: 0.1213\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0602, Val AP: 0.1208, Test AP: 0.1217\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0602, Val AP: 0.1205, Test AP: 0.1213\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0602, Val AP: 0.1207, Test AP: 0.1213\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0602, Val AP: 0.1206, Test AP: 0.1214\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0602, Val AP: 0.1208, Test AP: 0.1215\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0602, Val AP: 0.1208, Test AP: 0.1215\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0602, Val AP: 0.1210, Test AP: 0.1218\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0603, Val AP: 0.1210, Test AP: 0.1217\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0602, Val AP: 0.1206, Test AP: 0.1215\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0602, Val AP: 0.1212, Test AP: 0.1218\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0602, Val AP: 0.1214, Test AP: 0.1222\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0602, Val AP: 0.1197, Test AP: 0.1211\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0602, Val AP: 0.1217, Test AP: 0.1225\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0602, Val AP: 0.1213, Test AP: 0.1220\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0602, Val AP: 0.1214, Test AP: 0.1222\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0602, Val AP: 0.1214, Test AP: 0.1225\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0602, Val AP: 0.1211, Test AP: 0.1220\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0602, Val AP: 0.1215, Test AP: 0.1223\n",
                        "Seed: 42, Epoch: 023, Loss: 0.0602, Val AP: 0.1209, Test AP: 0.1221\n",
                        "Seed: 42, Epoch: 024, Loss: 0.0602, Val AP: 0.1210, Test AP: 0.1221\n",
                        "Seed: 42, Epoch: 025, Loss: 0.0602, Val AP: 0.1216, Test AP: 0.1228\n",
                        "Seed: 42, Epoch: 026, Loss: 0.0601, Val AP: 0.1219, Test AP: 0.1229\n",
                        "Seed: 42, Epoch: 027, Loss: 0.0601, Val AP: 0.1213, Test AP: 0.1222\n",
                        "Seed: 42, Epoch: 028, Loss: 0.0602, Val AP: 0.1222, Test AP: 0.1230\n",
                        "Seed: 42, Epoch: 029, Loss: 0.0601, Val AP: 0.1220, Test AP: 0.1229\n",
                        "Seed: 42, Epoch: 030, Loss: 0.0601, Val AP: 0.1223, Test AP: 0.1234\n",
                        "Seed: 42, Epoch: 031, Loss: 0.0601, Val AP: 0.1221, Test AP: 0.1234\n",
                        "Seed: 42, Epoch: 032, Loss: 0.0601, Val AP: 0.1226, Test AP: 0.1236\n",
                        "Seed: 42, Epoch: 033, Loss: 0.0601, Val AP: 0.1217, Test AP: 0.1236\n",
                        "Seed: 42, Epoch: 034, Loss: 0.0601, Val AP: 0.1218, Test AP: 0.1242\n",
                        "Seed: 42, Epoch: 035, Loss: 0.0601, Val AP: 0.1206, Test AP: 0.1206\n",
                        "Seed: 42, Epoch: 036, Loss: 0.0601, Val AP: 0.1221, Test AP: 0.1233\n",
                        "Seed: 42, Epoch: 037, Loss: 0.0601, Val AP: 0.1226, Test AP: 0.1247\n",
                        "Seed: 42, Epoch: 038, Loss: 0.0601, Val AP: 0.1228, Test AP: 0.1246\n",
                        "Seed: 42, Epoch: 039, Loss: 0.0601, Val AP: 0.1199, Test AP: 0.1202\n",
                        "Seed: 42, Epoch: 040, Loss: 0.0600, Val AP: 0.1225, Test AP: 0.1240\n",
                        "Seed: 42, Epoch: 041, Loss: 0.0601, Val AP: 0.1220, Test AP: 0.1245\n",
                        "Seed: 42, Epoch: 042, Loss: 0.0601, Val AP: 0.1215, Test AP: 0.1245\n",
                        "Seed: 42, Epoch: 043, Loss: 0.0602, Val AP: 0.1227, Test AP: 0.1244\n",
                        "Seed: 42, Epoch: 044, Loss: 0.0602, Val AP: 0.1223, Test AP: 0.1242\n",
                        "Seed: 42, Epoch: 045, Loss: 0.0601, Val AP: 0.1229, Test AP: 0.1246\n",
                        "Seed: 42, Epoch: 046, Loss: 0.0601, Val AP: 0.1225, Test AP: 0.1227\n",
                        "Seed: 42, Epoch: 047, Loss: 0.0600, Val AP: 0.1232, Test AP: 0.1241\n",
                        "Seed: 42, Epoch: 048, Loss: 0.0602, Val AP: 0.1215, Test AP: 0.1237\n",
                        "Seed: 42, Epoch: 049, Loss: 0.0604, Val AP: 0.1218, Test AP: 0.1232\n",
                        "Seed: 42, Epoch: 050, Loss: 0.0601, Val AP: 0.1227, Test AP: 0.1238\n",
                        "Seed: 42, Epoch: 051, Loss: 0.0600, Val AP: 0.1231, Test AP: 0.1242\n",
                        "Seed: 42, Epoch: 052, Loss: 0.0598, Val AP: 0.1247, Test AP: 0.1273\n",
                        "Seed: 42, Epoch: 053, Loss: 0.0596, Val AP: 0.1267, Test AP: 0.1291\n",
                        "Seed: 42, Epoch: 054, Loss: 0.0596, Val AP: 0.1254, Test AP: 0.1273\n",
                        "Seed: 42, Epoch: 055, Loss: 0.0595, Val AP: 0.1309, Test AP: 0.1343\n",
                        "Seed: 42, Epoch: 056, Loss: 0.0594, Val AP: 0.1320, Test AP: 0.1346\n",
                        "Seed: 42, Epoch: 057, Loss: 0.0593, Val AP: 0.1329, Test AP: 0.1362\n",
                        "Seed: 42, Epoch: 058, Loss: 0.0593, Val AP: 0.1261, Test AP: 0.1286\n",
                        "Seed: 42, Epoch: 059, Loss: 0.0592, Val AP: 0.1345, Test AP: 0.1373\n",
                        "Seed: 42, Epoch: 060, Loss: 0.0592, Val AP: 0.1328, Test AP: 0.1340\n",
                        "Seed: 42, Epoch: 061, Loss: 0.0592, Val AP: 0.1316, Test AP: 0.1340\n",
                        "Seed: 42, Epoch: 062, Loss: 0.0591, Val AP: 0.1346, Test AP: 0.1373\n",
                        "Seed: 42, Epoch: 063, Loss: 0.0591, Val AP: 0.1367, Test AP: 0.1393\n",
                        "Seed: 42, Epoch: 064, Loss: 0.0590, Val AP: 0.1342, Test AP: 0.1351\n",
                        "Seed: 42, Epoch: 065, Loss: 0.0589, Val AP: 0.1346, Test AP: 0.1375\n",
                        "Seed: 42, Epoch: 066, Loss: 0.0590, Val AP: 0.1347, Test AP: 0.1373\n",
                        "Seed: 42, Epoch: 067, Loss: 0.0589, Val AP: 0.1331, Test AP: 0.1361\n",
                        "Seed: 42, Epoch: 068, Loss: 0.0589, Val AP: 0.1333, Test AP: 0.1346\n",
                        "Seed: 42, Epoch: 069, Loss: 0.0589, Val AP: 0.1356, Test AP: 0.1384\n",
                        "Seed: 42, Epoch: 070, Loss: 0.0588, Val AP: 0.1398, Test AP: 0.1432\n",
                        "Seed: 42, Epoch: 071, Loss: 0.0588, Val AP: 0.1362, Test AP: 0.1385\n",
                        "Seed: 42, Epoch: 072, Loss: 0.0587, Val AP: 0.1381, Test AP: 0.1403\n",
                        "Seed: 42, Epoch: 073, Loss: 0.0600, Val AP: 0.1211, Test AP: 0.1216\n",
                        "Seed: 42, Epoch: 074, Loss: 0.0602, Val AP: 0.1228, Test AP: 0.1249\n",
                        "Seed: 42, Epoch: 075, Loss: 0.0601, Val AP: 0.1227, Test AP: 0.1240\n",
                        "Seed: 42, Epoch: 076, Loss: 0.0601, Val AP: 0.1225, Test AP: 0.1240\n",
                        "Seed: 42, Epoch: 077, Loss: 0.0601, Val AP: 0.1225, Test AP: 0.1249\n",
                        "Seed: 42, Epoch: 078, Loss: 0.0601, Val AP: 0.1228, Test AP: 0.1241\n",
                        "Seed: 42, Epoch: 079, Loss: 0.0600, Val AP: 0.1232, Test AP: 0.1243\n",
                        "Seed: 42, Epoch: 080, Loss: 0.0601, Val AP: 0.1226, Test AP: 0.1245\n",
                        "Seed: 42, Epoch: 081, Loss: 0.0600, Val AP: 0.1231, Test AP: 0.1251\n",
                        "Seed: 42, Epoch: 082, Loss: 0.0600, Val AP: 0.1226, Test AP: 0.1238\n",
                        "Seed: 42, Epoch: 083, Loss: 0.0600, Val AP: 0.1226, Test AP: 0.1247\n",
                        "Seed: 42, Epoch: 084, Loss: 0.0601, Val AP: 0.1223, Test AP: 0.1243\n",
                        "Seed: 42, Epoch: 085, Loss: 0.0601, Val AP: 0.1227, Test AP: 0.1249\n",
                        "Seed: 42, Epoch: 086, Loss: 0.0600, Val AP: 0.1235, Test AP: 0.1250\n",
                        "Seed: 42, Epoch: 087, Loss: 0.0600, Val AP: 0.1235, Test AP: 0.1252\n",
                        "Seed: 42, Epoch: 088, Loss: 0.0600, Val AP: 0.1238, Test AP: 0.1249\n",
                        "Seed: 42, Epoch: 089, Loss: 0.0600, Val AP: 0.1231, Test AP: 0.1258\n",
                        "Seed: 42, Epoch: 090, Loss: 0.0600, Val AP: 0.1237, Test AP: 0.1248\n",
                        "Seed: 42, Epoch: 091, Loss: 0.0600, Val AP: 0.1229, Test AP: 0.1229\n",
                        "Seed: 42, Epoch: 092, Loss: 0.0599, Val AP: 0.1239, Test AP: 0.1247\n",
                        "Seed: 42, Epoch: 093, Loss: 0.0599, Val AP: 0.1231, Test AP: 0.1246\n",
                        "Seed: 42, Epoch: 094, Loss: 0.0599, Val AP: 0.1237, Test AP: 0.1255\n",
                        "Seed: 42, Epoch: 095, Loss: 0.0599, Val AP: 0.1244, Test AP: 0.1259\n",
                        "Seed: 42, Epoch: 096, Loss: 0.0599, Val AP: 0.1245, Test AP: 0.1256\n",
                        "Seed: 42, Epoch: 097, Loss: 0.0599, Val AP: 0.1240, Test AP: 0.1255\n",
                        "Seed: 42, Epoch: 098, Loss: 0.0599, Val AP: 0.1248, Test AP: 0.1262\n",
                        "Seed: 42, Epoch: 099, Loss: 0.0599, Val AP: 0.1242, Test AP: 0.1259\n",
                        "Seed: 42, Epoch: 100, Loss: 0.0599, Val AP: 0.1244, Test AP: 0.1248\n",
                        "Seed: 42, Epoch: 101, Loss: 0.0598, Val AP: 0.1211, Test AP: 0.1226\n",
                        "Seed: 42, Epoch: 102, Loss: 0.0597, Val AP: 0.1222, Test AP: 0.1240\n",
                        "Seed: 42, Epoch: 103, Loss: 0.0596, Val AP: 0.1261, Test AP: 0.1276\n",
                        "Seed: 42, Epoch: 104, Loss: 0.0596, Val AP: 0.1222, Test AP: 0.1261\n",
                        "Seed: 42, Epoch: 105, Loss: 0.0594, Val AP: 0.1292, Test AP: 0.1305\n",
                        "Seed: 42, Epoch: 106, Loss: 0.0594, Val AP: 0.1289, Test AP: 0.1297\n",
                        "Seed: 42, Epoch: 107, Loss: 0.0594, Val AP: 0.1306, Test AP: 0.1320\n",
                        "Seed: 42, Epoch: 108, Loss: 0.0593, Val AP: 0.1270, Test AP: 0.1289\n",
                        "Seed: 42, Epoch: 109, Loss: 0.0593, Val AP: 0.1296, Test AP: 0.1304\n",
                        "Seed: 42, Epoch: 110, Loss: 0.0592, Val AP: 0.1266, Test AP: 0.1276\n",
                        "Seed: 42, Epoch: 111, Loss: 0.0592, Val AP: 0.1276, Test AP: 0.1280\n",
                        "Seed: 42, Epoch: 112, Loss: 0.0592, Val AP: 0.1287, Test AP: 0.1283\n",
                        "Seed: 42, Epoch: 113, Loss: 0.0592, Val AP: 0.1265, Test AP: 0.1273\n",
                        "Seed: 42, Epoch: 114, Loss: 0.0592, Val AP: 0.1320, Test AP: 0.1312\n",
                        "Seed: 42, Epoch: 115, Loss: 0.0592, Val AP: 0.1255, Test AP: 0.1257\n",
                        "Seed: 42, Epoch: 116, Loss: 0.0592, Val AP: 0.1301, Test AP: 0.1301\n",
                        "Seed: 42, Epoch: 117, Loss: 0.0591, Val AP: 0.1265, Test AP: 0.1263\n",
                        "Seed: 42, Epoch: 118, Loss: 0.0592, Val AP: 0.1323, Test AP: 0.1319\n",
                        "Seed: 42, Epoch: 119, Loss: 0.0591, Val AP: 0.1311, Test AP: 0.1304\n",
                        "Seed: 42, Epoch: 120, Loss: 0.0591, Val AP: 0.1307, Test AP: 0.1304\n",
                        "Early stopping at epoch 120 for seed 42\n",
                        "Seed: 43, Epoch: 001, Loss: 0.0639, Val AP: 0.1229, Test AP: 0.1210\n",
                        "Seed: 43, Epoch: 002, Loss: 0.0604, Val AP: 0.1243, Test AP: 0.1241\n",
                        "Seed: 43, Epoch: 003, Loss: 0.0603, Val AP: 0.1220, Test AP: 0.1206\n",
                        "Seed: 43, Epoch: 004, Loss: 0.0603, Val AP: 0.1197, Test AP: 0.1189\n",
                        "Seed: 43, Epoch: 005, Loss: 0.0602, Val AP: 0.1221, Test AP: 0.1245\n",
                        "Seed: 43, Epoch: 006, Loss: 0.0600, Val AP: 0.1275, Test AP: 0.1271\n",
                        "Seed: 43, Epoch: 007, Loss: 0.0600, Val AP: 0.1272, Test AP: 0.1268\n",
                        "Seed: 43, Epoch: 008, Loss: 0.0598, Val AP: 0.1180, Test AP: 0.1204\n",
                        "Seed: 43, Epoch: 009, Loss: 0.0596, Val AP: 0.1281, Test AP: 0.1290\n",
                        "Seed: 43, Epoch: 010, Loss: 0.0592, Val AP: 0.1290, Test AP: 0.1294\n",
                        "Seed: 43, Epoch: 011, Loss: 0.0590, Val AP: 0.1259, Test AP: 0.1280\n",
                        "Seed: 43, Epoch: 012, Loss: 0.0588, Val AP: 0.1440, Test AP: 0.1421\n",
                        "Seed: 43, Epoch: 013, Loss: 0.0585, Val AP: 0.1463, Test AP: 0.1457\n",
                        "Seed: 43, Epoch: 014, Loss: 0.0583, Val AP: 0.1497, Test AP: 0.1491\n",
                        "Seed: 43, Epoch: 015, Loss: 0.0580, Val AP: 0.1564, Test AP: 0.1543\n",
                        "Seed: 43, Epoch: 016, Loss: 0.0577, Val AP: 0.1594, Test AP: 0.1564\n",
                        "Seed: 43, Epoch: 017, Loss: 0.0574, Val AP: 0.1663, Test AP: 0.1647\n",
                        "Seed: 43, Epoch: 018, Loss: 0.0570, Val AP: 0.1710, Test AP: 0.1708\n",
                        "Seed: 43, Epoch: 019, Loss: 0.0567, Val AP: 0.1836, Test AP: 0.1839\n",
                        "Seed: 43, Epoch: 020, Loss: 0.0563, Val AP: 0.1916, Test AP: 0.1923\n",
                        "Seed: 43, Epoch: 021, Loss: 0.0561, Val AP: 0.1746, Test AP: 0.1755\n",
                        "Seed: 43, Epoch: 022, Loss: 0.0558, Val AP: 0.1936, Test AP: 0.1937\n",
                        "Seed: 43, Epoch: 023, Loss: 0.0556, Val AP: 0.1897, Test AP: 0.1898\n",
                        "Seed: 43, Epoch: 024, Loss: 0.0553, Val AP: 0.2090, Test AP: 0.2087\n",
                        "Seed: 43, Epoch: 025, Loss: 0.0551, Val AP: 0.1915, Test AP: 0.1934\n",
                        "Seed: 43, Epoch: 026, Loss: 0.0550, Val AP: 0.1968, Test AP: 0.1968\n",
                        "Seed: 43, Epoch: 027, Loss: 0.0547, Val AP: 0.1955, Test AP: 0.1975\n",
                        "Seed: 43, Epoch: 028, Loss: 0.0546, Val AP: 0.1841, Test AP: 0.1888\n",
                        "Seed: 43, Epoch: 029, Loss: 0.0546, Val AP: 0.2112, Test AP: 0.2130\n",
                        "Seed: 43, Epoch: 030, Loss: 0.0543, Val AP: 0.1961, Test AP: 0.1992\n",
                        "Seed: 43, Epoch: 031, Loss: 0.0544, Val AP: 0.2032, Test AP: 0.2040\n",
                        "Seed: 43, Epoch: 032, Loss: 0.0542, Val AP: 0.2223, Test AP: 0.2253\n",
                        "Seed: 43, Epoch: 033, Loss: 0.0540, Val AP: 0.2179, Test AP: 0.2207\n",
                        "Seed: 43, Epoch: 034, Loss: 0.0540, Val AP: 0.2211, Test AP: 0.2209\n",
                        "Seed: 43, Epoch: 035, Loss: 0.0538, Val AP: 0.2007, Test AP: 0.2028\n",
                        "Seed: 43, Epoch: 036, Loss: 0.0538, Val AP: 0.2217, Test AP: 0.2241\n",
                        "Seed: 43, Epoch: 037, Loss: 0.0535, Val AP: 0.2245, Test AP: 0.2284\n",
                        "Seed: 43, Epoch: 038, Loss: 0.0535, Val AP: 0.2290, Test AP: 0.2324\n",
                        "Seed: 43, Epoch: 039, Loss: 0.0532, Val AP: 0.2275, Test AP: 0.2316\n",
                        "Seed: 43, Epoch: 040, Loss: 0.0533, Val AP: 0.2194, Test AP: 0.2204\n",
                        "Seed: 43, Epoch: 041, Loss: 0.0533, Val AP: 0.2429, Test AP: 0.2463\n",
                        "Seed: 43, Epoch: 042, Loss: 0.0530, Val AP: 0.2286, Test AP: 0.2304\n",
                        "Seed: 43, Epoch: 043, Loss: 0.0529, Val AP: 0.2373, Test AP: 0.2410\n",
                        "Seed: 43, Epoch: 044, Loss: 0.0528, Val AP: 0.2233, Test AP: 0.2267\n",
                        "Seed: 43, Epoch: 045, Loss: 0.0527, Val AP: 0.2446, Test AP: 0.2462\n",
                        "Seed: 43, Epoch: 046, Loss: 0.0528, Val AP: 0.2271, Test AP: 0.2304\n",
                        "Seed: 43, Epoch: 047, Loss: 0.0526, Val AP: 0.2410, Test AP: 0.2455\n",
                        "Seed: 43, Epoch: 048, Loss: 0.0526, Val AP: 0.2212, Test AP: 0.2255\n",
                        "Seed: 43, Epoch: 049, Loss: 0.0526, Val AP: 0.2346, Test AP: 0.2375\n",
                        "Seed: 43, Epoch: 050, Loss: 0.0523, Val AP: 0.2482, Test AP: 0.2488\n",
                        "Seed: 43, Epoch: 051, Loss: 0.0524, Val AP: 0.2289, Test AP: 0.2303\n",
                        "Seed: 43, Epoch: 052, Loss: 0.0523, Val AP: 0.2320, Test AP: 0.2361\n",
                        "Seed: 43, Epoch: 053, Loss: 0.0523, Val AP: 0.2201, Test AP: 0.2194\n",
                        "Seed: 43, Epoch: 054, Loss: 0.0522, Val AP: 0.2309, Test AP: 0.2354\n",
                        "Seed: 43, Epoch: 055, Loss: 0.0522, Val AP: 0.2432, Test AP: 0.2464\n",
                        "Seed: 43, Epoch: 056, Loss: 0.0520, Val AP: 0.2617, Test AP: 0.2617\n",
                        "Seed: 43, Epoch: 057, Loss: 0.0519, Val AP: 0.2357, Test AP: 0.2393\n",
                        "Seed: 43, Epoch: 058, Loss: 0.0520, Val AP: 0.2391, Test AP: 0.2404\n",
                        "Seed: 43, Epoch: 059, Loss: 0.0518, Val AP: 0.2461, Test AP: 0.2483\n",
                        "Seed: 43, Epoch: 060, Loss: 0.0517, Val AP: 0.2500, Test AP: 0.2525\n",
                        "Seed: 43, Epoch: 061, Loss: 0.0517, Val AP: 0.2522, Test AP: 0.2515\n",
                        "Seed: 43, Epoch: 062, Loss: 0.0516, Val AP: 0.2569, Test AP: 0.2594\n",
                        "Seed: 43, Epoch: 063, Loss: 0.0516, Val AP: 0.2418, Test AP: 0.2415\n",
                        "Seed: 43, Epoch: 064, Loss: 0.0517, Val AP: 0.2587, Test AP: 0.2595\n",
                        "Seed: 43, Epoch: 065, Loss: 0.0514, Val AP: 0.2384, Test AP: 0.2444\n",
                        "Seed: 43, Epoch: 066, Loss: 0.0515, Val AP: 0.2612, Test AP: 0.2614\n",
                        "Seed: 43, Epoch: 067, Loss: 0.0513, Val AP: 0.2519, Test AP: 0.2539\n",
                        "Seed: 43, Epoch: 068, Loss: 0.0513, Val AP: 0.2455, Test AP: 0.2490\n",
                        "Seed: 43, Epoch: 069, Loss: 0.0513, Val AP: 0.2544, Test AP: 0.2579\n",
                        "Seed: 43, Epoch: 070, Loss: 0.0512, Val AP: 0.2530, Test AP: 0.2555\n",
                        "Seed: 43, Epoch: 071, Loss: 0.0510, Val AP: 0.2583, Test AP: 0.2589\n",
                        "Seed: 43, Epoch: 072, Loss: 0.0511, Val AP: 0.2455, Test AP: 0.2490\n",
                        "Seed: 43, Epoch: 073, Loss: 0.0512, Val AP: 0.2638, Test AP: 0.2644\n",
                        "Seed: 43, Epoch: 074, Loss: 0.0509, Val AP: 0.2552, Test AP: 0.2588\n",
                        "Seed: 43, Epoch: 075, Loss: 0.0509, Val AP: 0.2472, Test AP: 0.2484\n",
                        "Seed: 43, Epoch: 076, Loss: 0.0508, Val AP: 0.2572, Test AP: 0.2581\n",
                        "Seed: 43, Epoch: 077, Loss: 0.0510, Val AP: 0.2584, Test AP: 0.2607\n",
                        "Seed: 43, Epoch: 078, Loss: 0.0507, Val AP: 0.2720, Test AP: 0.2718\n",
                        "Seed: 43, Epoch: 079, Loss: 0.0507, Val AP: 0.2581, Test AP: 0.2608\n",
                        "Seed: 43, Epoch: 080, Loss: 0.0507, Val AP: 0.2540, Test AP: 0.2567\n",
                        "Seed: 43, Epoch: 081, Loss: 0.0508, Val AP: 0.2560, Test AP: 0.2553\n",
                        "Seed: 43, Epoch: 082, Loss: 0.0507, Val AP: 0.2717, Test AP: 0.2730\n",
                        "Seed: 43, Epoch: 083, Loss: 0.0505, Val AP: 0.2469, Test AP: 0.2477\n",
                        "Seed: 43, Epoch: 084, Loss: 0.0507, Val AP: 0.2100, Test AP: 0.2120\n",
                        "Seed: 43, Epoch: 085, Loss: 0.0508, Val AP: 0.2495, Test AP: 0.2521\n",
                        "Seed: 43, Epoch: 086, Loss: 0.0506, Val AP: 0.2541, Test AP: 0.2561\n",
                        "Seed: 43, Epoch: 087, Loss: 0.0504, Val AP: 0.2311, Test AP: 0.2347\n",
                        "Seed: 43, Epoch: 088, Loss: 0.0507, Val AP: 0.2618, Test AP: 0.2639\n",
                        "Seed: 43, Epoch: 089, Loss: 0.0503, Val AP: 0.2544, Test AP: 0.2563\n",
                        "Seed: 43, Epoch: 090, Loss: 0.0503, Val AP: 0.2493, Test AP: 0.2529\n",
                        "Seed: 43, Epoch: 091, Loss: 0.0506, Val AP: 0.2586, Test AP: 0.2624\n",
                        "Seed: 43, Epoch: 092, Loss: 0.0504, Val AP: 0.2609, Test AP: 0.2639\n",
                        "Seed: 43, Epoch: 093, Loss: 0.0502, Val AP: 0.2618, Test AP: 0.2655\n",
                        "Seed: 43, Epoch: 094, Loss: 0.0502, Val AP: 0.2620, Test AP: 0.2645\n",
                        "Seed: 43, Epoch: 095, Loss: 0.0502, Val AP: 0.2658, Test AP: 0.2698\n",
                        "Seed: 43, Epoch: 096, Loss: 0.0501, Val AP: 0.2683, Test AP: 0.2721\n",
                        "Seed: 43, Epoch: 097, Loss: 0.0501, Val AP: 0.2638, Test AP: 0.2684\n",
                        "Seed: 43, Epoch: 098, Loss: 0.0501, Val AP: 0.2640, Test AP: 0.2683\n",
                        "Seed: 43, Epoch: 099, Loss: 0.0504, Val AP: 0.2566, Test AP: 0.2619\n",
                        "Seed: 43, Epoch: 100, Loss: 0.0502, Val AP: 0.2638, Test AP: 0.2657\n",
                        "Seed: 43, Epoch: 101, Loss: 0.0500, Val AP: 0.2665, Test AP: 0.2710\n",
                        "Seed: 43, Epoch: 102, Loss: 0.0498, Val AP: 0.2665, Test AP: 0.2702\n",
                        "Seed: 43, Epoch: 103, Loss: 0.0500, Val AP: 0.2681, Test AP: 0.2717\n",
                        "Seed: 43, Epoch: 104, Loss: 0.0499, Val AP: 0.2539, Test AP: 0.2584\n",
                        "Seed: 43, Epoch: 105, Loss: 0.0502, Val AP: 0.2753, Test AP: 0.2772\n",
                        "Seed: 43, Epoch: 106, Loss: 0.0498, Val AP: 0.2555, Test AP: 0.2602\n",
                        "Seed: 43, Epoch: 107, Loss: 0.0501, Val AP: 0.2630, Test AP: 0.2702\n",
                        "Seed: 43, Epoch: 108, Loss: 0.0499, Val AP: 0.2252, Test AP: 0.2310\n",
                        "Seed: 43, Epoch: 109, Loss: 0.0500, Val AP: 0.2692, Test AP: 0.2709\n",
                        "Seed: 43, Epoch: 110, Loss: 0.0495, Val AP: 0.2612, Test AP: 0.2655\n",
                        "Seed: 43, Epoch: 111, Loss: 0.0497, Val AP: 0.2692, Test AP: 0.2745\n",
                        "Seed: 43, Epoch: 112, Loss: 0.0496, Val AP: 0.2745, Test AP: 0.2742\n",
                        "Seed: 43, Epoch: 113, Loss: 0.0495, Val AP: 0.2673, Test AP: 0.2713\n",
                        "Seed: 43, Epoch: 114, Loss: 0.0497, Val AP: 0.2553, Test AP: 0.2590\n",
                        "Seed: 43, Epoch: 115, Loss: 0.0499, Val AP: 0.2466, Test AP: 0.2504\n",
                        "Seed: 43, Epoch: 116, Loss: 0.0496, Val AP: 0.2444, Test AP: 0.2508\n",
                        "Seed: 43, Epoch: 117, Loss: 0.0498, Val AP: 0.2641, Test AP: 0.2665\n",
                        "Seed: 43, Epoch: 118, Loss: 0.0498, Val AP: 0.2714, Test AP: 0.2760\n",
                        "Seed: 43, Epoch: 119, Loss: 0.0494, Val AP: 0.2560, Test AP: 0.2604\n",
                        "Seed: 43, Epoch: 120, Loss: 0.0493, Val AP: 0.2625, Test AP: 0.2667\n",
                        "Seed: 43, Epoch: 121, Loss: 0.0494, Val AP: 0.2657, Test AP: 0.2696\n",
                        "Seed: 43, Epoch: 122, Loss: 0.0492, Val AP: 0.2533, Test AP: 0.2566\n",
                        "Seed: 43, Epoch: 123, Loss: 0.0495, Val AP: 0.2494, Test AP: 0.2547\n",
                        "Seed: 43, Epoch: 124, Loss: 0.0500, Val AP: 0.2236, Test AP: 0.2283\n",
                        "Seed: 43, Epoch: 125, Loss: 0.0499, Val AP: 0.2659, Test AP: 0.2689\n",
                        "Seed: 43, Epoch: 126, Loss: 0.0495, Val AP: 0.2772, Test AP: 0.2800\n",
                        "Seed: 43, Epoch: 127, Loss: 0.0492, Val AP: 0.2601, Test AP: 0.2643\n",
                        "Seed: 43, Epoch: 128, Loss: 0.0494, Val AP: 0.2360, Test AP: 0.2406\n",
                        "Seed: 43, Epoch: 129, Loss: 0.0494, Val AP: 0.2521, Test AP: 0.2569\n",
                        "Seed: 43, Epoch: 130, Loss: 0.0494, Val AP: 0.2639, Test AP: 0.2673\n",
                        "Seed: 43, Epoch: 131, Loss: 0.0494, Val AP: 0.2676, Test AP: 0.2708\n",
                        "Seed: 43, Epoch: 132, Loss: 0.0490, Val AP: 0.2465, Test AP: 0.2506\n",
                        "Seed: 43, Epoch: 133, Loss: 0.0495, Val AP: 0.2721, Test AP: 0.2757\n",
                        "Seed: 43, Epoch: 134, Loss: 0.0494, Val AP: 0.2657, Test AP: 0.2689\n",
                        "Seed: 43, Epoch: 135, Loss: 0.0493, Val AP: 0.2742, Test AP: 0.2781\n",
                        "Seed: 43, Epoch: 136, Loss: 0.0489, Val AP: 0.2758, Test AP: 0.2789\n",
                        "Seed: 43, Epoch: 137, Loss: 0.0490, Val AP: 0.2690, Test AP: 0.2743\n",
                        "Seed: 43, Epoch: 138, Loss: 0.0491, Val AP: 0.2489, Test AP: 0.2528\n",
                        "Seed: 43, Epoch: 139, Loss: 0.0494, Val AP: 0.2641, Test AP: 0.2697\n",
                        "Seed: 43, Epoch: 140, Loss: 0.0490, Val AP: 0.2690, Test AP: 0.2721\n",
                        "Seed: 43, Epoch: 141, Loss: 0.0490, Val AP: 0.2608, Test AP: 0.2661\n",
                        "Seed: 43, Epoch: 142, Loss: 0.0494, Val AP: 0.2328, Test AP: 0.2375\n",
                        "Seed: 43, Epoch: 143, Loss: 0.0496, Val AP: 0.2646, Test AP: 0.2672\n",
                        "Seed: 43, Epoch: 144, Loss: 0.0489, Val AP: 0.2722, Test AP: 0.2769\n",
                        "Seed: 43, Epoch: 145, Loss: 0.0492, Val AP: 0.2714, Test AP: 0.2757\n",
                        "Seed: 43, Epoch: 146, Loss: 0.0487, Val AP: 0.2656, Test AP: 0.2691\n",
                        "Seed: 43, Epoch: 147, Loss: 0.0490, Val AP: 0.2616, Test AP: 0.2653\n",
                        "Seed: 43, Epoch: 148, Loss: 0.0490, Val AP: 0.2685, Test AP: 0.2735\n",
                        "Seed: 43, Epoch: 149, Loss: 0.0488, Val AP: 0.2661, Test AP: 0.2704\n",
                        "Seed: 43, Epoch: 150, Loss: 0.0488, Val AP: 0.2565, Test AP: 0.2636\n",
                        "Seed: 44, Epoch: 001, Loss: 0.0640, Val AP: 0.1166, Test AP: 0.1244\n",
                        "Seed: 44, Epoch: 002, Loss: 0.0604, Val AP: 0.1181, Test AP: 0.1242\n",
                        "Seed: 44, Epoch: 003, Loss: 0.0602, Val AP: 0.1180, Test AP: 0.1241\n",
                        "Seed: 44, Epoch: 004, Loss: 0.0602, Val AP: 0.1167, Test AP: 0.1210\n",
                        "Seed: 44, Epoch: 005, Loss: 0.0601, Val AP: 0.1153, Test AP: 0.1192\n",
                        "Seed: 44, Epoch: 006, Loss: 0.0600, Val AP: 0.1175, Test AP: 0.1235\n",
                        "Seed: 44, Epoch: 007, Loss: 0.0599, Val AP: 0.1160, Test AP: 0.1206\n",
                        "Seed: 44, Epoch: 008, Loss: 0.0598, Val AP: 0.1154, Test AP: 0.1208\n",
                        "Seed: 44, Epoch: 009, Loss: 0.0597, Val AP: 0.1182, Test AP: 0.1249\n",
                        "Seed: 44, Epoch: 010, Loss: 0.0597, Val AP: 0.1054, Test AP: 0.1072\n",
                        "Seed: 44, Epoch: 011, Loss: 0.0595, Val AP: 0.1269, Test AP: 0.1328\n",
                        "Seed: 44, Epoch: 012, Loss: 0.0592, Val AP: 0.1284, Test AP: 0.1327\n",
                        "Seed: 44, Epoch: 013, Loss: 0.0590, Val AP: 0.1271, Test AP: 0.1342\n",
                        "Seed: 44, Epoch: 014, Loss: 0.0586, Val AP: 0.1508, Test AP: 0.1589\n",
                        "Seed: 44, Epoch: 015, Loss: 0.0579, Val AP: 0.1565, Test AP: 0.1613\n",
                        "Seed: 44, Epoch: 016, Loss: 0.0576, Val AP: 0.1725, Test AP: 0.1789\n",
                        "Seed: 44, Epoch: 017, Loss: 0.0571, Val AP: 0.1689, Test AP: 0.1770\n",
                        "Seed: 44, Epoch: 018, Loss: 0.0567, Val AP: 0.1696, Test AP: 0.1766\n",
                        "Seed: 44, Epoch: 019, Loss: 0.0566, Val AP: 0.1696, Test AP: 0.1797\n",
                        "Seed: 44, Epoch: 020, Loss: 0.0563, Val AP: 0.1778, Test AP: 0.1824\n",
                        "Seed: 44, Epoch: 021, Loss: 0.0559, Val AP: 0.1644, Test AP: 0.1667\n",
                        "Seed: 44, Epoch: 022, Loss: 0.0557, Val AP: 0.1949, Test AP: 0.1969\n",
                        "Seed: 44, Epoch: 023, Loss: 0.0554, Val AP: 0.1870, Test AP: 0.1903\n",
                        "Seed: 44, Epoch: 024, Loss: 0.0554, Val AP: 0.1637, Test AP: 0.1670\n",
                        "Seed: 44, Epoch: 025, Loss: 0.0553, Val AP: 0.1941, Test AP: 0.1970\n",
                        "Seed: 44, Epoch: 026, Loss: 0.0549, Val AP: 0.1779, Test AP: 0.1796\n",
                        "Seed: 44, Epoch: 027, Loss: 0.0547, Val AP: 0.1977, Test AP: 0.2011\n",
                        "Seed: 44, Epoch: 028, Loss: 0.0544, Val AP: 0.1951, Test AP: 0.1949\n",
                        "Seed: 44, Epoch: 029, Loss: 0.0543, Val AP: 0.1855, Test AP: 0.1848\n",
                        "Seed: 44, Epoch: 030, Loss: 0.0540, Val AP: 0.1964, Test AP: 0.1975\n",
                        "Seed: 44, Epoch: 031, Loss: 0.0539, Val AP: 0.2014, Test AP: 0.2049\n",
                        "Seed: 44, Epoch: 032, Loss: 0.0537, Val AP: 0.2244, Test AP: 0.2272\n",
                        "Seed: 44, Epoch: 033, Loss: 0.0535, Val AP: 0.2286, Test AP: 0.2307\n",
                        "Seed: 44, Epoch: 034, Loss: 0.0534, Val AP: 0.2268, Test AP: 0.2263\n",
                        "Seed: 44, Epoch: 035, Loss: 0.0531, Val AP: 0.2333, Test AP: 0.2336\n",
                        "Seed: 44, Epoch: 036, Loss: 0.0529, Val AP: 0.2145, Test AP: 0.2129\n",
                        "Seed: 44, Epoch: 037, Loss: 0.0528, Val AP: 0.2101, Test AP: 0.2158\n",
                        "Seed: 44, Epoch: 038, Loss: 0.0528, Val AP: 0.2421, Test AP: 0.2453\n",
                        "Seed: 44, Epoch: 039, Loss: 0.0526, Val AP: 0.2264, Test AP: 0.2269\n",
                        "Seed: 44, Epoch: 040, Loss: 0.0524, Val AP: 0.2299, Test AP: 0.2303\n",
                        "Seed: 44, Epoch: 041, Loss: 0.0524, Val AP: 0.2349, Test AP: 0.2357\n",
                        "Seed: 44, Epoch: 042, Loss: 0.0522, Val AP: 0.2228, Test AP: 0.2234\n",
                        "Seed: 44, Epoch: 043, Loss: 0.0522, Val AP: 0.2525, Test AP: 0.2541\n",
                        "Seed: 44, Epoch: 044, Loss: 0.0519, Val AP: 0.2394, Test AP: 0.2419\n",
                        "Seed: 44, Epoch: 045, Loss: 0.0518, Val AP: 0.2438, Test AP: 0.2464\n",
                        "Seed: 44, Epoch: 046, Loss: 0.0518, Val AP: 0.2470, Test AP: 0.2495\n",
                        "Seed: 44, Epoch: 047, Loss: 0.0517, Val AP: 0.2558, Test AP: 0.2605\n",
                        "Seed: 44, Epoch: 048, Loss: 0.0518, Val AP: 0.2114, Test AP: 0.2142\n",
                        "Seed: 44, Epoch: 049, Loss: 0.0517, Val AP: 0.2625, Test AP: 0.2650\n",
                        "Seed: 44, Epoch: 050, Loss: 0.0513, Val AP: 0.2468, Test AP: 0.2481\n",
                        "Seed: 44, Epoch: 051, Loss: 0.0512, Val AP: 0.2455, Test AP: 0.2460\n",
                        "Seed: 44, Epoch: 052, Loss: 0.0512, Val AP: 0.2612, Test AP: 0.2634\n",
                        "Seed: 44, Epoch: 053, Loss: 0.0510, Val AP: 0.2215, Test AP: 0.2221\n",
                        "Seed: 44, Epoch: 054, Loss: 0.0512, Val AP: 0.2572, Test AP: 0.2569\n",
                        "Seed: 44, Epoch: 055, Loss: 0.0508, Val AP: 0.2455, Test AP: 0.2456\n",
                        "Seed: 44, Epoch: 056, Loss: 0.0510, Val AP: 0.2595, Test AP: 0.2597\n",
                        "Seed: 44, Epoch: 057, Loss: 0.0509, Val AP: 0.2407, Test AP: 0.2398\n",
                        "Seed: 44, Epoch: 058, Loss: 0.0508, Val AP: 0.2640, Test AP: 0.2659\n",
                        "Seed: 44, Epoch: 059, Loss: 0.0505, Val AP: 0.2470, Test AP: 0.2456\n",
                        "Seed: 44, Epoch: 060, Loss: 0.0505, Val AP: 0.2696, Test AP: 0.2670\n",
                        "Seed: 44, Epoch: 061, Loss: 0.0504, Val AP: 0.2684, Test AP: 0.2672\n",
                        "Seed: 44, Epoch: 062, Loss: 0.0506, Val AP: 0.2516, Test AP: 0.2524\n",
                        "Seed: 44, Epoch: 063, Loss: 0.0503, Val AP: 0.2443, Test AP: 0.2437\n",
                        "Seed: 44, Epoch: 064, Loss: 0.0502, Val AP: 0.2562, Test AP: 0.2585\n",
                        "Seed: 44, Epoch: 065, Loss: 0.0500, Val AP: 0.2592, Test AP: 0.2614\n",
                        "Seed: 44, Epoch: 066, Loss: 0.0502, Val AP: 0.2704, Test AP: 0.2713\n",
                        "Seed: 44, Epoch: 067, Loss: 0.0499, Val AP: 0.2433, Test AP: 0.2417\n",
                        "Seed: 44, Epoch: 068, Loss: 0.0500, Val AP: 0.2582, Test AP: 0.2579\n",
                        "Seed: 44, Epoch: 069, Loss: 0.0502, Val AP: 0.2502, Test AP: 0.2467\n",
                        "Seed: 44, Epoch: 070, Loss: 0.0500, Val AP: 0.2713, Test AP: 0.2703\n",
                        "Seed: 44, Epoch: 071, Loss: 0.0500, Val AP: 0.2596, Test AP: 0.2577\n",
                        "Seed: 44, Epoch: 072, Loss: 0.0499, Val AP: 0.2261, Test AP: 0.2257\n",
                        "Seed: 44, Epoch: 073, Loss: 0.0500, Val AP: 0.2589, Test AP: 0.2588\n",
                        "Seed: 44, Epoch: 074, Loss: 0.0499, Val AP: 0.2714, Test AP: 0.2676\n",
                        "Seed: 44, Epoch: 075, Loss: 0.0494, Val AP: 0.2430, Test AP: 0.2399\n",
                        "Seed: 44, Epoch: 076, Loss: 0.0495, Val AP: 0.2693, Test AP: 0.2684\n",
                        "Seed: 44, Epoch: 077, Loss: 0.0494, Val AP: 0.2555, Test AP: 0.2558\n",
                        "Seed: 44, Epoch: 078, Loss: 0.0495, Val AP: 0.2609, Test AP: 0.2604\n",
                        "Seed: 44, Epoch: 079, Loss: 0.0494, Val AP: 0.2842, Test AP: 0.2832\n",
                        "Seed: 44, Epoch: 080, Loss: 0.0495, Val AP: 0.2739, Test AP: 0.2729\n",
                        "Seed: 44, Epoch: 081, Loss: 0.0496, Val AP: 0.2259, Test AP: 0.2211\n",
                        "Seed: 44, Epoch: 082, Loss: 0.0498, Val AP: 0.2638, Test AP: 0.2635\n",
                        "Seed: 44, Epoch: 083, Loss: 0.0493, Val AP: 0.2796, Test AP: 0.2807\n",
                        "Seed: 44, Epoch: 084, Loss: 0.0493, Val AP: 0.2579, Test AP: 0.2581\n",
                        "Seed: 44, Epoch: 085, Loss: 0.0495, Val AP: 0.2758, Test AP: 0.2781\n",
                        "Seed: 44, Epoch: 086, Loss: 0.0495, Val AP: 0.2836, Test AP: 0.2853\n",
                        "Seed: 44, Epoch: 087, Loss: 0.0490, Val AP: 0.2624, Test AP: 0.2614\n",
                        "Seed: 44, Epoch: 088, Loss: 0.0490, Val AP: 0.2596, Test AP: 0.2580\n",
                        "Seed: 44, Epoch: 089, Loss: 0.0488, Val AP: 0.2666, Test AP: 0.2682\n",
                        "Seed: 44, Epoch: 090, Loss: 0.0491, Val AP: 0.2650, Test AP: 0.2636\n",
                        "Seed: 44, Epoch: 091, Loss: 0.0490, Val AP: 0.2599, Test AP: 0.2578\n",
                        "Seed: 44, Epoch: 092, Loss: 0.0487, Val AP: 0.2796, Test AP: 0.2819\n",
                        "Seed: 44, Epoch: 093, Loss: 0.0487, Val AP: 0.2671, Test AP: 0.2669\n",
                        "Seed: 44, Epoch: 094, Loss: 0.0487, Val AP: 0.2688, Test AP: 0.2666\n",
                        "Seed: 44, Epoch: 095, Loss: 0.0487, Val AP: 0.2661, Test AP: 0.2674\n",
                        "Seed: 44, Epoch: 096, Loss: 0.0488, Val AP: 0.2640, Test AP: 0.2636\n",
                        "Seed: 44, Epoch: 097, Loss: 0.0485, Val AP: 0.2890, Test AP: 0.2907\n",
                        "Seed: 44, Epoch: 098, Loss: 0.0484, Val AP: 0.2530, Test AP: 0.2524\n",
                        "Seed: 44, Epoch: 099, Loss: 0.0488, Val AP: 0.2498, Test AP: 0.2483\n",
                        "Seed: 44, Epoch: 100, Loss: 0.0492, Val AP: 0.2775, Test AP: 0.2773\n",
                        "Seed: 44, Epoch: 101, Loss: 0.0483, Val AP: 0.2707, Test AP: 0.2701\n",
                        "Seed: 44, Epoch: 102, Loss: 0.0485, Val AP: 0.2768, Test AP: 0.2749\n",
                        "Seed: 44, Epoch: 103, Loss: 0.0484, Val AP: 0.2729, Test AP: 0.2731\n",
                        "Seed: 44, Epoch: 104, Loss: 0.0483, Val AP: 0.2874, Test AP: 0.2881\n",
                        "Seed: 44, Epoch: 105, Loss: 0.0484, Val AP: 0.2702, Test AP: 0.2697\n",
                        "Seed: 44, Epoch: 106, Loss: 0.0483, Val AP: 0.2712, Test AP: 0.2720\n",
                        "Seed: 44, Epoch: 107, Loss: 0.0483, Val AP: 0.2724, Test AP: 0.2693\n",
                        "Seed: 44, Epoch: 108, Loss: 0.0484, Val AP: 0.2665, Test AP: 0.2657\n",
                        "Seed: 44, Epoch: 109, Loss: 0.0482, Val AP: 0.2548, Test AP: 0.2517\n",
                        "Seed: 44, Epoch: 110, Loss: 0.0484, Val AP: 0.2677, Test AP: 0.2680\n",
                        "Seed: 44, Epoch: 111, Loss: 0.0481, Val AP: 0.2556, Test AP: 0.2540\n",
                        "Seed: 44, Epoch: 112, Loss: 0.0483, Val AP: 0.2675, Test AP: 0.2681\n",
                        "Seed: 44, Epoch: 113, Loss: 0.0480, Val AP: 0.2640, Test AP: 0.2627\n",
                        "Seed: 44, Epoch: 114, Loss: 0.0479, Val AP: 0.2348, Test AP: 0.2340\n",
                        "Seed: 44, Epoch: 115, Loss: 0.0485, Val AP: 0.2654, Test AP: 0.2629\n",
                        "Seed: 44, Epoch: 116, Loss: 0.0482, Val AP: 0.2602, Test AP: 0.2606\n",
                        "Seed: 44, Epoch: 117, Loss: 0.0481, Val AP: 0.2681, Test AP: 0.2672\n",
                        "Seed: 44, Epoch: 118, Loss: 0.0478, Val AP: 0.2675, Test AP: 0.2696\n",
                        "Seed: 44, Epoch: 119, Loss: 0.0481, Val AP: 0.2897, Test AP: 0.2903\n",
                        "Seed: 44, Epoch: 120, Loss: 0.0474, Val AP: 0.2756, Test AP: 0.2785\n",
                        "Seed: 44, Epoch: 121, Loss: 0.0475, Val AP: 0.2824, Test AP: 0.2831\n",
                        "Seed: 44, Epoch: 122, Loss: 0.0476, Val AP: 0.2756, Test AP: 0.2767\n",
                        "Seed: 44, Epoch: 123, Loss: 0.0479, Val AP: 0.2679, Test AP: 0.2690\n",
                        "Seed: 44, Epoch: 124, Loss: 0.0479, Val AP: 0.2862, Test AP: 0.2868\n",
                        "Seed: 44, Epoch: 125, Loss: 0.0473, Val AP: 0.2819, Test AP: 0.2817\n",
                        "Seed: 44, Epoch: 126, Loss: 0.0475, Val AP: 0.2530, Test AP: 0.2542\n",
                        "Seed: 44, Epoch: 127, Loss: 0.0476, Val AP: 0.2665, Test AP: 0.2668\n",
                        "Seed: 44, Epoch: 128, Loss: 0.0475, Val AP: 0.2814, Test AP: 0.2822\n",
                        "Seed: 44, Epoch: 129, Loss: 0.0479, Val AP: 0.2714, Test AP: 0.2744\n",
                        "Seed: 44, Epoch: 130, Loss: 0.0472, Val AP: 0.2849, Test AP: 0.2856\n",
                        "Seed: 44, Epoch: 131, Loss: 0.0471, Val AP: 0.2828, Test AP: 0.2832\n",
                        "Seed: 44, Epoch: 132, Loss: 0.0473, Val AP: 0.2620, Test AP: 0.2613\n",
                        "Seed: 44, Epoch: 133, Loss: 0.0473, Val AP: 0.2754, Test AP: 0.2754\n",
                        "Seed: 44, Epoch: 134, Loss: 0.0472, Val AP: 0.2756, Test AP: 0.2763\n",
                        "Seed: 44, Epoch: 135, Loss: 0.0473, Val AP: 0.2799, Test AP: 0.2785\n",
                        "Seed: 44, Epoch: 136, Loss: 0.0473, Val AP: 0.2778, Test AP: 0.2759\n",
                        "Seed: 44, Epoch: 137, Loss: 0.0470, Val AP: 0.2742, Test AP: 0.2750\n",
                        "Seed: 44, Epoch: 138, Loss: 0.0476, Val AP: 0.2804, Test AP: 0.2820\n",
                        "Seed: 44, Epoch: 139, Loss: 0.0476, Val AP: 0.2783, Test AP: 0.2783\n",
                        "Seed: 44, Epoch: 140, Loss: 0.0470, Val AP: 0.2674, Test AP: 0.2662\n",
                        "Seed: 44, Epoch: 141, Loss: 0.0473, Val AP: 0.2560, Test AP: 0.2544\n",
                        "Seed: 44, Epoch: 142, Loss: 0.0474, Val AP: 0.2711, Test AP: 0.2715\n",
                        "Seed: 44, Epoch: 143, Loss: 0.0474, Val AP: 0.2827, Test AP: 0.2803\n",
                        "Seed: 44, Epoch: 144, Loss: 0.0470, Val AP: 0.2896, Test AP: 0.2893\n",
                        "Seed: 44, Epoch: 145, Loss: 0.0469, Val AP: 0.2792, Test AP: 0.2788\n",
                        "Seed: 44, Epoch: 146, Loss: 0.0470, Val AP: 0.2816, Test AP: 0.2830\n",
                        "Seed: 44, Epoch: 147, Loss: 0.0470, Val AP: 0.2737, Test AP: 0.2733\n",
                        "Seed: 44, Epoch: 148, Loss: 0.0469, Val AP: 0.2782, Test AP: 0.2796\n",
                        "Seed: 44, Epoch: 149, Loss: 0.0470, Val AP: 0.2658, Test AP: 0.2653\n",
                        "Seed: 44, Epoch: 150, Loss: 0.0469, Val AP: 0.2819, Test AP: 0.2831\n",
                        "Average Time: 5927.06 seconds\n",
                        "Var Time: 814108.01 seconds\n",
                        "Average Memory: 8133.33 MB\n",
                        "Average Best Val AP: 0.2356\n",
                        "Std Best Test AP: 0.0671\n",
                        "Average Test AP: 0.2378\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nclass HierarchicalGCN_CGI(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_CGI, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, improved = True)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = CGIPool(hidden_channels, ratio=0.9)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, improved = True)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = CGIPool(hidden_channels, ratio=0.9)\n        self.conv3 = GCNConv(hidden_channels, out_channels, improved = True)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        init_x = data.x\n        x = x.float()\n        init_x = init_x.float()\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, perm = self.pool1(x, edge_index, None, batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, perm = self.pool2(x, edge_index, None, batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False)\n    model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### KMISPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom typing import Callable, Optional, Tuple, Union\nimport torch\nfrom torch.nn import Module\nfrom torch_scatter import scatter, scatter_add, scatter_min\nfrom torch_sparse import SparseTensor, remove_diag\nfrom torch_geometric.nn.aggr import Aggregation\nfrom torch_geometric.nn.dense import Linear\nfrom torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\nScorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\nimport torch\nfrom torch_scatter import scatter_max, scatter_min\nfrom torch_geometric.typing import Adj, OptTensor, SparseTensor, Tensor\nfrom typing import Callable, Optional, Tuple, Union\nimport torch\nfrom torch.nn import Module\nfrom torch_scatter import scatter, scatter_add, scatter_min\nfrom torch_sparse import SparseTensor, remove_diag\nfrom torch_geometric.nn.aggr import Aggregation\nfrom torch_geometric.nn.dense import Linear\nfrom torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\nScorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\nimport torch\nfrom torch_scatter import scatter_max, scatter_min\nfrom torch_geometric.typing import Adj, OptTensor, SparseTensor, Tensor\ndef maximal_independent_set(edge_index: Adj, k: int = 1,\n                            perm: OptTensor = None) -> Tensor:\n    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n    returned set.\n    The algorithm greedily selects the nodes in their canonical order. If a\n    permutation :obj:`perm` is provided, the nodes are extracted following\n    that permutation instead.\n    This method follows `Blelloch's Alogirithm\n    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n    higher values of :math:`k`.\n    Args:\n        edge_index (Tensor or SparseTensor): The graph connectivity.\n        k (int): The :math:`k` value (defaults to 1).\n        perm (LongTensor, optional): Permutation vector. Must be of size\n            :obj:`(n,)` (defaults to :obj:`None`).\n    :rtype: :class:`ByteTensor`\n    \"\"\"\n    if isinstance(edge_index, SparseTensor):\n        row, col, _ = edge_index.coo()\n        device = edge_index.device()\n        n = edge_index.size(0)\n    else:\n        row, col = edge_index[0], edge_index[1]\n        device = row.device\n        n = edge_index.max().item() + 1\n    if perm is None:\n        rank = torch.arange(n, dtype=torch.long, device=device)\n    else:\n        rank = torch.zeros_like(perm)\n        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n    mis = torch.zeros(n, dtype=torch.bool, device=device)\n    mask = mis.clone()\n    min_rank = rank.clone()\n    while not mask.all():\n        for _ in range(k):\n            min_neigh = torch.full_like(min_rank, fill_value=n)\n            scatter_min(min_rank[row], col, out=min_neigh)\n            torch.minimum(min_neigh, min_rank, out=min_rank)  \n        mis = mis | torch.eq(rank, min_rank)\n        mask = mis.clone().byte()\n        for _ in range(k):\n            max_neigh = torch.full_like(mask, fill_value=0)\n            scatter_max(mask[row], col, out=max_neigh)\n            torch.maximum(max_neigh, mask, out=mask)  \n        mask = mask.to(dtype=torch.bool)\n        min_rank = rank.clone()\n        min_rank[mask] = n\n    return mis\ndef maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n                                    perm: OptTensor = None) -> PairTensor:\n    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n    clustering of a graph, as defined in `\"Generalizing Downsampling from\n    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n    The algorithm greedily selects the nodes in their canonical order. If a\n    permutation :obj:`perm` is provided, the nodes are extracted following\n    that permutation instead.\n    This method returns both the :math:`k`-MIS and the clustering, where the\n    :math:`c`-th cluster refers to the :math:`c`-th element of the\n    :math:`k`-MIS.\n    Args:\n        edge_index (Tensor or SparseTensor): The graph connectivity.\n        k (int): The :math:`k` value (defaults to 1).\n        perm (LongTensor, optional): Permutation vector. Must be of size\n            :obj:`(n,)` (defaults to :obj:`None`).\n    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n    \"\"\"\n    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n    n, device = mis.size(0), mis.device\n    if isinstance(edge_index, SparseTensor):\n        row, col, _ = edge_index.coo()\n    else:\n        row, col = edge_index[0], edge_index[1]\n    if perm is None:\n        rank = torch.arange(n, dtype=torch.long, device=device)\n    else:\n        rank = torch.zeros_like(perm)\n        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n    rank_mis = rank[mis]\n    min_rank[mis] = rank_mis\n    for _ in range(k):\n        min_neigh = torch.full_like(min_rank, fill_value=n)\n        scatter_min(min_rank[row], col, out=min_neigh)\n        torch.minimum(min_neigh, min_rank, out=min_rank)\n    _, clusters = torch.unique(min_rank, return_inverse=True)\n    perm = torch.argsort(rank_mis)\n    return mis, perm[clusters]\nclass KMISPooling(Module):\n    _heuristics = {None, 'greedy', 'w-greedy'}\n    _passthroughs = {None, 'before', 'after'}\n    _scorers = {\n        'linear',\n        'random',\n        'constant',\n        'canonical',\n        'first',\n        'last',\n    }\n    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n                 scorer: Union[Scorer, str] = 'linear',\n                 score_heuristic: Optional[str] = 'greedy',\n                 score_passthrough: Optional[str] = 'before',\n                 aggr_x: Optional[Union[str, Aggregation]] = None,\n                 aggr_edge: str = 'sum',\n                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n                 remove_self_loops: bool = True) -> None:\n        super(KMISPooling, self).__init__()\n        assert score_heuristic in self._heuristics, \\\n            \"Unrecognized `score_heuristic` value.\"\n        assert score_passthrough in self._passthroughs, \\\n            \"Unrecognized `score_passthrough` value.\"\n        if not callable(scorer):\n            assert scorer in self._scorers, \\\n                \"Unrecognized `scorer` value.\"\n        self.k = k\n        self.scorer = scorer\n        self.score_heuristic = score_heuristic\n        self.score_passthrough = score_passthrough\n        self.aggr_x = aggr_x\n        self.aggr_edge = aggr_edge\n        self.aggr_score = aggr_score\n        self.remove_self_loops = remove_self_loops\n        if scorer == 'linear':\n            assert self.score_passthrough is not None, \\\n                \"`'score_passthrough'` must not be `None`\" \\\n                \" when using `'linear'` scorer\"\n            self.lin = Linear(in_features=in_channels, out_features=1)\n    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n        if self.score_heuristic is None:\n            return x\n        row, col, _ = adj.coo()\n        x = x.view(-1)\n        if self.score_heuristic == 'greedy':\n            k_sums = torch.ones_like(x)\n        else:\n            k_sums = x.clone()\n        for _ in range(self.k):\n            scatter_add(k_sums[row], col, out=k_sums)\n        return x / k_sums\n    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n                batch: OptTensor = None) -> Tensor:\n        if self.scorer == 'linear':\n            return self.lin(x).sigmoid()\n        if self.scorer == 'random':\n            return torch.rand((x.size(0), 1), device=x.device)\n        if self.scorer == 'constant':\n            return torch.ones((x.size(0), 1), device=x.device)\n        if self.scorer == 'canonical':\n            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n        if self.scorer == 'first':\n            return x[..., [0]]\n        if self.scorer == 'last':\n            return x[..., [-1]]\n        return self.scorer(x, edge_index, edge_attr, batch)\n    def forward(self, x: Tensor, edge_index: Adj,\n                edge_attr: OptTensor = None,\n                batch: OptTensor = None) \\\n            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n        \"\"\"\"\"\"\n        edge_index = edge_index.long()\n        adj, n = edge_index, x.size(0)\n        if not isinstance(edge_index, SparseTensor):\n            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n        score = self._scorer(x, edge_index, edge_attr, batch)\n        updated_score = self._apply_heuristic(score, adj)\n        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n        row, col, val = adj.coo()\n        c = mis.sum()\n        if val is None:\n            val = torch.ones_like(row, dtype=torch.float)\n        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n                           is_sorted=False,\n                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n        if self.remove_self_loops:\n            adj = remove_diag(adj)\n        if self.score_passthrough == 'before':\n            x = self.aggr_score(x, score)\n        if self.aggr_x is None:\n            x = x[mis]\n        elif isinstance(self.aggr_x, str):\n            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n                        reduce=self.aggr_x)\n        else:\n            x = self.aggr_x(x, cluster, dim_size=c)\n        if self.score_passthrough == 'after':\n            x = self.aggr_score(x, score[mis])\n        if isinstance(edge_index, SparseTensor):\n            edge_index, edge_attr = adj, None\n        else:\n            row, col, edge_attr = adj.coo()\n            edge_index = torch.stack([row, col])\n        if batch is not None:\n            batch = batch[mis]\n        return x, edge_index, edge_attr, batch, mis, cluster\n    def __repr__(self):\n        if self.scorer == 'linear':\n            channels = f\"in_channels={self.lin.in_channels}, \"\n        else:\n            channels = \"\"\n        return f'{self.__class__.__name__}({channels}k={self.k})'"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0629, Val AP: 0.1179, Test AP: 0.1196\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0602, Val AP: 0.1155, Test AP: 0.1192\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0601, Val AP: 0.1131, Test AP: 0.1162\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0599, Val AP: 0.1199, Test AP: 0.1217\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0596, Val AP: 0.1223, Test AP: 0.1230\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0593, Val AP: 0.1272, Test AP: 0.1281\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0590, Val AP: 0.1327, Test AP: 0.1351\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0587, Val AP: 0.1438, Test AP: 0.1463\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0583, Val AP: 0.1493, Test AP: 0.1524\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0580, Val AP: 0.1426, Test AP: 0.1448\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0577, Val AP: 0.1562, Test AP: 0.1593\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0574, Val AP: 0.1520, Test AP: 0.1550\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0572, Val AP: 0.1563, Test AP: 0.1595\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0569, Val AP: 0.1635, Test AP: 0.1665\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0567, Val AP: 0.1651, Test AP: 0.1687\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0565, Val AP: 0.1601, Test AP: 0.1622\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0564, Val AP: 0.1573, Test AP: 0.1602\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0562, Val AP: 0.1736, Test AP: 0.1766\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0561, Val AP: 0.1762, Test AP: 0.1795\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0559, Val AP: 0.1903, Test AP: 0.1942\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0558, Val AP: 0.1635, Test AP: 0.1644\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0557, Val AP: 0.1847, Test AP: 0.1883\n",
                        "Seed: 42, Epoch: 023, Loss: 0.0555, Val AP: 0.1780, Test AP: 0.1800\n",
                        "Seed: 42, Epoch: 024, Loss: 0.0555, Val AP: 0.1853, Test AP: 0.1876\n",
                        "Seed: 42, Epoch: 025, Loss: 0.0553, Val AP: 0.1883, Test AP: 0.1924\n",
                        "Seed: 42, Epoch: 026, Loss: 0.0550, Val AP: 0.1665, Test AP: 0.1690\n",
                        "Seed: 42, Epoch: 027, Loss: 0.0551, Val AP: 0.2082, Test AP: 0.2116\n",
                        "Seed: 42, Epoch: 028, Loss: 0.0549, Val AP: 0.1895, Test AP: 0.1927\n",
                        "Seed: 42, Epoch: 029, Loss: 0.0548, Val AP: 0.1985, Test AP: 0.2013\n",
                        "Seed: 42, Epoch: 030, Loss: 0.0547, Val AP: 0.1949, Test AP: 0.1981\n",
                        "Seed: 42, Epoch: 031, Loss: 0.0546, Val AP: 0.2020, Test AP: 0.2064\n",
                        "Seed: 42, Epoch: 032, Loss: 0.0544, Val AP: 0.1935, Test AP: 0.1971\n",
                        "Seed: 42, Epoch: 033, Loss: 0.0544, Val AP: 0.1846, Test AP: 0.1901\n",
                        "Seed: 42, Epoch: 034, Loss: 0.0543, Val AP: 0.1965, Test AP: 0.2002\n",
                        "Seed: 42, Epoch: 035, Loss: 0.0542, Val AP: 0.1993, Test AP: 0.2027\n",
                        "Seed: 42, Epoch: 036, Loss: 0.0541, Val AP: 0.2116, Test AP: 0.2156\n",
                        "Seed: 42, Epoch: 037, Loss: 0.0540, Val AP: 0.1941, Test AP: 0.1977\n",
                        "Seed: 42, Epoch: 038, Loss: 0.0541, Val AP: 0.2154, Test AP: 0.2189\n",
                        "Seed: 42, Epoch: 039, Loss: 0.0539, Val AP: 0.1995, Test AP: 0.2024\n",
                        "Seed: 42, Epoch: 040, Loss: 0.0539, Val AP: 0.2178, Test AP: 0.2220\n",
                        "Seed: 42, Epoch: 041, Loss: 0.0539, Val AP: 0.2239, Test AP: 0.2289\n",
                        "Seed: 42, Epoch: 042, Loss: 0.0538, Val AP: 0.1891, Test AP: 0.1903\n",
                        "Seed: 42, Epoch: 043, Loss: 0.0537, Val AP: 0.2060, Test AP: 0.2105\n",
                        "Seed: 42, Epoch: 044, Loss: 0.0537, Val AP: 0.2293, Test AP: 0.2350\n",
                        "Seed: 42, Epoch: 045, Loss: 0.0536, Val AP: 0.2182, Test AP: 0.2210\n",
                        "Seed: 42, Epoch: 046, Loss: 0.0535, Val AP: 0.2329, Test AP: 0.2388\n",
                        "Seed: 42, Epoch: 047, Loss: 0.0534, Val AP: 0.2072, Test AP: 0.2124\n",
                        "Seed: 42, Epoch: 048, Loss: 0.0536, Val AP: 0.2188, Test AP: 0.2228\n",
                        "Seed: 42, Epoch: 049, Loss: 0.0533, Val AP: 0.2230, Test AP: 0.2286\n",
                        "Seed: 42, Epoch: 050, Loss: 0.0534, Val AP: 0.2228, Test AP: 0.2249\n",
                        "Seed: 42, Epoch: 051, Loss: 0.0533, Val AP: 0.2171, Test AP: 0.2234\n",
                        "Seed: 42, Epoch: 052, Loss: 0.0533, Val AP: 0.2239, Test AP: 0.2280\n",
                        "Seed: 42, Epoch: 053, Loss: 0.0532, Val AP: 0.2101, Test AP: 0.2129\n",
                        "Seed: 42, Epoch: 054, Loss: 0.0532, Val AP: 0.2204, Test AP: 0.2227\n",
                        "Seed: 42, Epoch: 055, Loss: 0.0530, Val AP: 0.2254, Test AP: 0.2287\n",
                        "Seed: 42, Epoch: 056, Loss: 0.0530, Val AP: 0.2293, Test AP: 0.2355\n",
                        "Seed: 42, Epoch: 057, Loss: 0.0530, Val AP: 0.2222, Test AP: 0.2274\n",
                        "Seed: 42, Epoch: 058, Loss: 0.0531, Val AP: 0.2225, Test AP: 0.2260\n",
                        "Seed: 42, Epoch: 059, Loss: 0.0529, Val AP: 0.2241, Test AP: 0.2302\n",
                        "Seed: 42, Epoch: 060, Loss: 0.0529, Val AP: 0.2183, Test AP: 0.2219\n",
                        "Seed: 42, Epoch: 061, Loss: 0.0528, Val AP: 0.2217, Test AP: 0.2285\n",
                        "Seed: 42, Epoch: 062, Loss: 0.0530, Val AP: 0.2317, Test AP: 0.2357\n",
                        "Seed: 42, Epoch: 063, Loss: 0.0528, Val AP: 0.2230, Test AP: 0.2274\n",
                        "Seed: 42, Epoch: 064, Loss: 0.0529, Val AP: 0.2346, Test AP: 0.2412\n",
                        "Seed: 42, Epoch: 065, Loss: 0.0528, Val AP: 0.2354, Test AP: 0.2423\n",
                        "Seed: 42, Epoch: 066, Loss: 0.0527, Val AP: 0.2186, Test AP: 0.2220\n",
                        "Seed: 42, Epoch: 067, Loss: 0.0526, Val AP: 0.2356, Test AP: 0.2417\n",
                        "Seed: 42, Epoch: 068, Loss: 0.0526, Val AP: 0.2362, Test AP: 0.2425\n",
                        "Seed: 42, Epoch: 069, Loss: 0.0526, Val AP: 0.2374, Test AP: 0.2420\n",
                        "Seed: 42, Epoch: 070, Loss: 0.0527, Val AP: 0.2236, Test AP: 0.2286\n",
                        "Seed: 42, Epoch: 071, Loss: 0.0526, Val AP: 0.2382, Test AP: 0.2442\n",
                        "Seed: 42, Epoch: 072, Loss: 0.0525, Val AP: 0.2410, Test AP: 0.2481\n",
                        "Seed: 42, Epoch: 073, Loss: 0.0525, Val AP: 0.2279, Test AP: 0.2324\n",
                        "Seed: 42, Epoch: 074, Loss: 0.0525, Val AP: 0.2312, Test AP: 0.2367\n",
                        "Seed: 42, Epoch: 075, Loss: 0.0523, Val AP: 0.2255, Test AP: 0.2296\n",
                        "Seed: 42, Epoch: 076, Loss: 0.0524, Val AP: 0.2258, Test AP: 0.2304\n",
                        "Seed: 42, Epoch: 077, Loss: 0.0524, Val AP: 0.2243, Test AP: 0.2306\n",
                        "Seed: 42, Epoch: 078, Loss: 0.0524, Val AP: 0.2347, Test AP: 0.2403\n",
                        "Seed: 42, Epoch: 079, Loss: 0.0525, Val AP: 0.2460, Test AP: 0.2519\n",
                        "Seed: 42, Epoch: 080, Loss: 0.0523, Val AP: 0.2236, Test AP: 0.2269\n",
                        "Seed: 42, Epoch: 081, Loss: 0.0523, Val AP: 0.2464, Test AP: 0.2525\n",
                        "Seed: 42, Epoch: 082, Loss: 0.0523, Val AP: 0.2410, Test AP: 0.2479\n",
                        "Seed: 42, Epoch: 083, Loss: 0.0522, Val AP: 0.2429, Test AP: 0.2496\n",
                        "Seed: 42, Epoch: 084, Loss: 0.0522, Val AP: 0.2293, Test AP: 0.2341\n",
                        "Seed: 42, Epoch: 085, Loss: 0.0522, Val AP: 0.2481, Test AP: 0.2545\n",
                        "Seed: 42, Epoch: 086, Loss: 0.0522, Val AP: 0.2397, Test AP: 0.2462\n",
                        "Seed: 42, Epoch: 087, Loss: 0.0521, Val AP: 0.2373, Test AP: 0.2421\n",
                        "Seed: 42, Epoch: 088, Loss: 0.0522, Val AP: 0.2334, Test AP: 0.2384\n",
                        "Seed: 42, Epoch: 089, Loss: 0.0522, Val AP: 0.2504, Test AP: 0.2571\n",
                        "Seed: 42, Epoch: 090, Loss: 0.0521, Val AP: 0.2335, Test AP: 0.2392\n",
                        "Seed: 42, Epoch: 091, Loss: 0.0524, Val AP: 0.2317, Test AP: 0.2370\n",
                        "Seed: 42, Epoch: 092, Loss: 0.0521, Val AP: 0.2435, Test AP: 0.2478\n",
                        "Seed: 42, Epoch: 093, Loss: 0.0521, Val AP: 0.2487, Test AP: 0.2564\n",
                        "Seed: 42, Epoch: 094, Loss: 0.0520, Val AP: 0.2435, Test AP: 0.2483\n",
                        "Seed: 42, Epoch: 095, Loss: 0.0521, Val AP: 0.2544, Test AP: 0.2605\n",
                        "Seed: 42, Epoch: 096, Loss: 0.0520, Val AP: 0.2314, Test AP: 0.2366\n",
                        "Seed: 42, Epoch: 097, Loss: 0.0521, Val AP: 0.2352, Test AP: 0.2393\n",
                        "Seed: 42, Epoch: 098, Loss: 0.0520, Val AP: 0.2312, Test AP: 0.2354\n",
                        "Seed: 42, Epoch: 099, Loss: 0.0520, Val AP: 0.2564, Test AP: 0.2621\n",
                        "Seed: 42, Epoch: 100, Loss: 0.0519, Val AP: 0.2177, Test AP: 0.2242\n",
                        "Seed: 42, Epoch: 101, Loss: 0.0523, Val AP: 0.2478, Test AP: 0.2536\n",
                        "Seed: 42, Epoch: 102, Loss: 0.0519, Val AP: 0.2380, Test AP: 0.2413\n",
                        "Seed: 42, Epoch: 103, Loss: 0.0520, Val AP: 0.2494, Test AP: 0.2559\n",
                        "Seed: 42, Epoch: 104, Loss: 0.0519, Val AP: 0.2402, Test AP: 0.2463\n",
                        "Seed: 42, Epoch: 105, Loss: 0.0519, Val AP: 0.2410, Test AP: 0.2466\n",
                        "Seed: 42, Epoch: 106, Loss: 0.0519, Val AP: 0.2433, Test AP: 0.2481\n",
                        "Seed: 42, Epoch: 107, Loss: 0.0518, Val AP: 0.2512, Test AP: 0.2571\n",
                        "Seed: 42, Epoch: 108, Loss: 0.0517, Val AP: 0.2521, Test AP: 0.2555\n",
                        "Seed: 42, Epoch: 109, Loss: 0.0517, Val AP: 0.2413, Test AP: 0.2460\n",
                        "Seed: 42, Epoch: 110, Loss: 0.0519, Val AP: 0.2281, Test AP: 0.2345\n",
                        "Seed: 42, Epoch: 111, Loss: 0.0521, Val AP: 0.2421, Test AP: 0.2468\n",
                        "Seed: 42, Epoch: 112, Loss: 0.0517, Val AP: 0.2375, Test AP: 0.2419\n",
                        "Seed: 42, Epoch: 113, Loss: 0.0518, Val AP: 0.2346, Test AP: 0.2383\n",
                        "Seed: 42, Epoch: 114, Loss: 0.0518, Val AP: 0.2172, Test AP: 0.2223\n",
                        "Seed: 42, Epoch: 115, Loss: 0.0519, Val AP: 0.2360, Test AP: 0.2414\n",
                        "Seed: 42, Epoch: 116, Loss: 0.0517, Val AP: 0.2546, Test AP: 0.2608\n",
                        "Seed: 42, Epoch: 117, Loss: 0.0517, Val AP: 0.2430, Test AP: 0.2462\n",
                        "Seed: 42, Epoch: 118, Loss: 0.0517, Val AP: 0.2589, Test AP: 0.2634\n",
                        "Seed: 42, Epoch: 119, Loss: 0.0517, Val AP: 0.2343, Test AP: 0.2377\n",
                        "Seed: 42, Epoch: 120, Loss: 0.0517, Val AP: 0.2414, Test AP: 0.2482\n",
                        "Seed: 42, Epoch: 121, Loss: 0.0516, Val AP: 0.2191, Test AP: 0.2228\n",
                        "Seed: 42, Epoch: 122, Loss: 0.0517, Val AP: 0.2415, Test AP: 0.2467\n",
                        "Seed: 42, Epoch: 123, Loss: 0.0516, Val AP: 0.2388, Test AP: 0.2418\n",
                        "Seed: 42, Epoch: 124, Loss: 0.0516, Val AP: 0.2363, Test AP: 0.2408\n",
                        "Seed: 42, Epoch: 125, Loss: 0.0515, Val AP: 0.2451, Test AP: 0.2502\n",
                        "Seed: 42, Epoch: 126, Loss: 0.0516, Val AP: 0.2361, Test AP: 0.2417\n",
                        "Seed: 42, Epoch: 127, Loss: 0.0517, Val AP: 0.2476, Test AP: 0.2509\n",
                        "Seed: 42, Epoch: 128, Loss: 0.0515, Val AP: 0.2312, Test AP: 0.2353\n",
                        "Seed: 42, Epoch: 129, Loss: 0.0516, Val AP: 0.2471, Test AP: 0.2523\n",
                        "Seed: 42, Epoch: 130, Loss: 0.0516, Val AP: 0.2527, Test AP: 0.2583\n",
                        "Seed: 42, Epoch: 131, Loss: 0.0515, Val AP: 0.2498, Test AP: 0.2532\n",
                        "Seed: 42, Epoch: 132, Loss: 0.0515, Val AP: 0.2297, Test AP: 0.2325\n",
                        "Seed: 42, Epoch: 133, Loss: 0.0515, Val AP: 0.2560, Test AP: 0.2625\n",
                        "Seed: 42, Epoch: 134, Loss: 0.0515, Val AP: 0.2613, Test AP: 0.2679\n",
                        "Seed: 42, Epoch: 135, Loss: 0.0514, Val AP: 0.2511, Test AP: 0.2571\n",
                        "Seed: 42, Epoch: 136, Loss: 0.0515, Val AP: 0.2627, Test AP: 0.2679\n",
                        "Seed: 42, Epoch: 137, Loss: 0.0515, Val AP: 0.2452, Test AP: 0.2500\n",
                        "Seed: 42, Epoch: 138, Loss: 0.0515, Val AP: 0.2529, Test AP: 0.2589\n",
                        "Seed: 42, Epoch: 139, Loss: 0.0514, Val AP: 0.2406, Test AP: 0.2471\n",
                        "Seed: 42, Epoch: 140, Loss: 0.0514, Val AP: 0.2595, Test AP: 0.2663\n",
                        "Seed: 42, Epoch: 141, Loss: 0.0514, Val AP: 0.2441, Test AP: 0.2498\n",
                        "Seed: 42, Epoch: 142, Loss: 0.0514, Val AP: 0.2482, Test AP: 0.2540\n",
                        "Seed: 42, Epoch: 143, Loss: 0.0514, Val AP: 0.2410, Test AP: 0.2454\n",
                        "Seed: 42, Epoch: 144, Loss: 0.0514, Val AP: 0.2470, Test AP: 0.2545\n",
                        "Seed: 42, Epoch: 145, Loss: 0.0515, Val AP: 0.2444, Test AP: 0.2490\n",
                        "Seed: 42, Epoch: 146, Loss: 0.0514, Val AP: 0.2383, Test AP: 0.2431\n",
                        "Seed: 42, Epoch: 147, Loss: 0.0514, Val AP: 0.2538, Test AP: 0.2603\n",
                        "Seed: 42, Epoch: 148, Loss: 0.0513, Val AP: 0.2321, Test AP: 0.2363\n",
                        "Seed: 42, Epoch: 149, Loss: 0.0514, Val AP: 0.2554, Test AP: 0.2608\n",
                        "Seed: 42, Epoch: 150, Loss: 0.0513, Val AP: 0.2532, Test AP: 0.2582\n",
                        "Seed: 43, Epoch: 001, Loss: 0.0620, Val AP: 0.1228, Test AP: 0.1204\n",
                        "Seed: 43, Epoch: 002, Loss: 0.0602, Val AP: 0.1166, Test AP: 0.1152\n",
                        "Seed: 43, Epoch: 003, Loss: 0.0600, Val AP: 0.1055, Test AP: 0.1055\n",
                        "Seed: 43, Epoch: 004, Loss: 0.0596, Val AP: 0.1227, Test AP: 0.1199\n",
                        "Seed: 43, Epoch: 005, Loss: 0.0594, Val AP: 0.1309, Test AP: 0.1270\n",
                        "Seed: 43, Epoch: 006, Loss: 0.0591, Val AP: 0.1304, Test AP: 0.1262\n",
                        "Seed: 43, Epoch: 007, Loss: 0.0588, Val AP: 0.1406, Test AP: 0.1352\n",
                        "Seed: 43, Epoch: 008, Loss: 0.0586, Val AP: 0.1442, Test AP: 0.1410\n",
                        "Seed: 43, Epoch: 009, Loss: 0.0583, Val AP: 0.1502, Test AP: 0.1470\n",
                        "Seed: 43, Epoch: 010, Loss: 0.0579, Val AP: 0.1473, Test AP: 0.1445\n",
                        "Seed: 43, Epoch: 011, Loss: 0.0576, Val AP: 0.1540, Test AP: 0.1491\n",
                        "Seed: 43, Epoch: 012, Loss: 0.0573, Val AP: 0.1706, Test AP: 0.1682\n",
                        "Seed: 43, Epoch: 013, Loss: 0.0571, Val AP: 0.1650, Test AP: 0.1652\n",
                        "Seed: 43, Epoch: 014, Loss: 0.0569, Val AP: 0.1668, Test AP: 0.1609\n",
                        "Seed: 43, Epoch: 015, Loss: 0.0566, Val AP: 0.1660, Test AP: 0.1636\n",
                        "Seed: 43, Epoch: 016, Loss: 0.0564, Val AP: 0.1736, Test AP: 0.1698\n",
                        "Seed: 43, Epoch: 017, Loss: 0.0563, Val AP: 0.1762, Test AP: 0.1745\n",
                        "Seed: 43, Epoch: 018, Loss: 0.0561, Val AP: 0.1756, Test AP: 0.1746\n",
                        "Seed: 43, Epoch: 019, Loss: 0.0559, Val AP: 0.1807, Test AP: 0.1795\n",
                        "Seed: 43, Epoch: 020, Loss: 0.0558, Val AP: 0.1865, Test AP: 0.1827\n",
                        "Seed: 43, Epoch: 021, Loss: 0.0556, Val AP: 0.1862, Test AP: 0.1873\n",
                        "Seed: 43, Epoch: 022, Loss: 0.0556, Val AP: 0.1647, Test AP: 0.1623\n",
                        "Seed: 43, Epoch: 023, Loss: 0.0555, Val AP: 0.1843, Test AP: 0.1849\n",
                        "Seed: 43, Epoch: 024, Loss: 0.0552, Val AP: 0.1841, Test AP: 0.1833\n",
                        "Seed: 43, Epoch: 025, Loss: 0.0552, Val AP: 0.2019, Test AP: 0.2018\n",
                        "Seed: 43, Epoch: 026, Loss: 0.0549, Val AP: 0.1980, Test AP: 0.1976\n",
                        "Seed: 43, Epoch: 027, Loss: 0.0551, Val AP: 0.2021, Test AP: 0.2020\n",
                        "Seed: 43, Epoch: 028, Loss: 0.0547, Val AP: 0.1655, Test AP: 0.1640\n",
                        "Seed: 43, Epoch: 029, Loss: 0.0547, Val AP: 0.1893, Test AP: 0.1907\n",
                        "Seed: 43, Epoch: 030, Loss: 0.0547, Val AP: 0.2109, Test AP: 0.2127\n",
                        "Seed: 43, Epoch: 031, Loss: 0.0543, Val AP: 0.2184, Test AP: 0.2191\n",
                        "Seed: 43, Epoch: 032, Loss: 0.0543, Val AP: 0.2100, Test AP: 0.2126\n",
                        "Seed: 43, Epoch: 033, Loss: 0.0542, Val AP: 0.2206, Test AP: 0.2216\n",
                        "Seed: 43, Epoch: 034, Loss: 0.0541, Val AP: 0.2287, Test AP: 0.2284\n",
                        "Seed: 43, Epoch: 035, Loss: 0.0540, Val AP: 0.2130, Test AP: 0.2138\n",
                        "Seed: 43, Epoch: 036, Loss: 0.0539, Val AP: 0.2289, Test AP: 0.2275\n",
                        "Seed: 43, Epoch: 037, Loss: 0.0539, Val AP: 0.2102, Test AP: 0.2114\n",
                        "Seed: 43, Epoch: 038, Loss: 0.0538, Val AP: 0.2309, Test AP: 0.2314\n",
                        "Seed: 43, Epoch: 039, Loss: 0.0537, Val AP: 0.2030, Test AP: 0.2054\n",
                        "Seed: 43, Epoch: 040, Loss: 0.0537, Val AP: 0.2102, Test AP: 0.2134\n",
                        "Seed: 43, Epoch: 041, Loss: 0.0535, Val AP: 0.2126, Test AP: 0.2113\n",
                        "Seed: 43, Epoch: 042, Loss: 0.0537, Val AP: 0.2230, Test AP: 0.2239\n",
                        "Seed: 43, Epoch: 043, Loss: 0.0536, Val AP: 0.2151, Test AP: 0.2177\n",
                        "Seed: 43, Epoch: 044, Loss: 0.0535, Val AP: 0.2167, Test AP: 0.2198\n",
                        "Seed: 43, Epoch: 045, Loss: 0.0533, Val AP: 0.2079, Test AP: 0.2105\n",
                        "Seed: 43, Epoch: 046, Loss: 0.0534, Val AP: 0.2306, Test AP: 0.2338\n",
                        "Seed: 43, Epoch: 047, Loss: 0.0533, Val AP: 0.2378, Test AP: 0.2395\n",
                        "Seed: 43, Epoch: 048, Loss: 0.0532, Val AP: 0.2238, Test AP: 0.2267\n",
                        "Seed: 43, Epoch: 049, Loss: 0.0534, Val AP: 0.2277, Test AP: 0.2306\n",
                        "Seed: 43, Epoch: 050, Loss: 0.0531, Val AP: 0.2203, Test AP: 0.2242\n",
                        "Seed: 43, Epoch: 051, Loss: 0.0530, Val AP: 0.2177, Test AP: 0.2223\n",
                        "Seed: 43, Epoch: 052, Loss: 0.0530, Val AP: 0.2288, Test AP: 0.2326\n",
                        "Seed: 43, Epoch: 053, Loss: 0.0530, Val AP: 0.2081, Test AP: 0.2102\n",
                        "Seed: 43, Epoch: 054, Loss: 0.0530, Val AP: 0.2169, Test AP: 0.2204\n",
                        "Seed: 43, Epoch: 055, Loss: 0.0530, Val AP: 0.2380, Test AP: 0.2424\n",
                        "Seed: 43, Epoch: 056, Loss: 0.0530, Val AP: 0.2403, Test AP: 0.2443\n",
                        "Seed: 43, Epoch: 057, Loss: 0.0528, Val AP: 0.2398, Test AP: 0.2445\n",
                        "Seed: 43, Epoch: 058, Loss: 0.0529, Val AP: 0.2180, Test AP: 0.2228\n",
                        "Seed: 43, Epoch: 059, Loss: 0.0528, Val AP: 0.2415, Test AP: 0.2436\n",
                        "Seed: 43, Epoch: 060, Loss: 0.0528, Val AP: 0.2423, Test AP: 0.2438\n",
                        "Seed: 43, Epoch: 061, Loss: 0.0528, Val AP: 0.2441, Test AP: 0.2467\n",
                        "Seed: 43, Epoch: 062, Loss: 0.0527, Val AP: 0.2194, Test AP: 0.2226\n",
                        "Seed: 43, Epoch: 063, Loss: 0.0528, Val AP: 0.2272, Test AP: 0.2231\n",
                        "Seed: 43, Epoch: 064, Loss: 0.0527, Val AP: 0.2337, Test AP: 0.2345\n",
                        "Seed: 43, Epoch: 065, Loss: 0.0527, Val AP: 0.2392, Test AP: 0.2426\n",
                        "Seed: 43, Epoch: 066, Loss: 0.0527, Val AP: 0.2438, Test AP: 0.2448\n",
                        "Seed: 43, Epoch: 067, Loss: 0.0527, Val AP: 0.2490, Test AP: 0.2502\n",
                        "Seed: 43, Epoch: 068, Loss: 0.0525, Val AP: 0.2400, Test AP: 0.2415\n",
                        "Seed: 43, Epoch: 069, Loss: 0.0527, Val AP: 0.2256, Test AP: 0.2299\n",
                        "Seed: 43, Epoch: 070, Loss: 0.0525, Val AP: 0.2302, Test AP: 0.2317\n",
                        "Seed: 43, Epoch: 071, Loss: 0.0525, Val AP: 0.2423, Test AP: 0.2432\n",
                        "Seed: 43, Epoch: 072, Loss: 0.0524, Val AP: 0.2295, Test AP: 0.2323\n",
                        "Seed: 43, Epoch: 073, Loss: 0.0525, Val AP: 0.2484, Test AP: 0.2493\n",
                        "Seed: 43, Epoch: 074, Loss: 0.0525, Val AP: 0.2310, Test AP: 0.2342\n",
                        "Seed: 43, Epoch: 075, Loss: 0.0524, Val AP: 0.2370, Test AP: 0.2379\n",
                        "Seed: 43, Epoch: 076, Loss: 0.0524, Val AP: 0.2419, Test AP: 0.2397\n",
                        "Seed: 43, Epoch: 077, Loss: 0.0523, Val AP: 0.2347, Test AP: 0.2368\n",
                        "Seed: 43, Epoch: 078, Loss: 0.0523, Val AP: 0.2368, Test AP: 0.2399\n",
                        "Seed: 43, Epoch: 079, Loss: 0.0523, Val AP: 0.2352, Test AP: 0.2376\n",
                        "Seed: 43, Epoch: 080, Loss: 0.0522, Val AP: 0.2239, Test AP: 0.2266\n",
                        "Seed: 43, Epoch: 081, Loss: 0.0522, Val AP: 0.2410, Test AP: 0.2431\n",
                        "Seed: 43, Epoch: 082, Loss: 0.0522, Val AP: 0.2536, Test AP: 0.2526\n",
                        "Seed: 43, Epoch: 083, Loss: 0.0522, Val AP: 0.2281, Test AP: 0.2321\n",
                        "Seed: 43, Epoch: 084, Loss: 0.0522, Val AP: 0.2481, Test AP: 0.2507\n",
                        "Seed: 43, Epoch: 085, Loss: 0.0521, Val AP: 0.2335, Test AP: 0.2353\n",
                        "Seed: 43, Epoch: 086, Loss: 0.0521, Val AP: 0.2438, Test AP: 0.2444\n",
                        "Seed: 43, Epoch: 087, Loss: 0.0521, Val AP: 0.2443, Test AP: 0.2490\n",
                        "Seed: 43, Epoch: 088, Loss: 0.0520, Val AP: 0.2441, Test AP: 0.2454\n",
                        "Seed: 43, Epoch: 089, Loss: 0.0521, Val AP: 0.2393, Test AP: 0.2437\n",
                        "Seed: 43, Epoch: 090, Loss: 0.0520, Val AP: 0.2410, Test AP: 0.2445\n",
                        "Seed: 43, Epoch: 091, Loss: 0.0520, Val AP: 0.2282, Test AP: 0.2314\n",
                        "Seed: 43, Epoch: 092, Loss: 0.0520, Val AP: 0.2398, Test AP: 0.2422\n",
                        "Seed: 43, Epoch: 093, Loss: 0.0520, Val AP: 0.2370, Test AP: 0.2397\n",
                        "Seed: 43, Epoch: 094, Loss: 0.0523, Val AP: 0.2571, Test AP: 0.2588\n",
                        "Seed: 43, Epoch: 095, Loss: 0.0519, Val AP: 0.2544, Test AP: 0.2558\n",
                        "Seed: 43, Epoch: 096, Loss: 0.0522, Val AP: 0.2499, Test AP: 0.2547\n",
                        "Seed: 43, Epoch: 097, Loss: 0.0519, Val AP: 0.2348, Test AP: 0.2384\n",
                        "Seed: 43, Epoch: 098, Loss: 0.0520, Val AP: 0.2495, Test AP: 0.2526\n",
                        "Seed: 43, Epoch: 099, Loss: 0.0521, Val AP: 0.2544, Test AP: 0.2555\n",
                        "Seed: 43, Epoch: 100, Loss: 0.0519, Val AP: 0.2213, Test AP: 0.2220\n",
                        "Seed: 43, Epoch: 101, Loss: 0.0522, Val AP: 0.2532, Test AP: 0.2523\n",
                        "Seed: 43, Epoch: 102, Loss: 0.0517, Val AP: 0.2644, Test AP: 0.2658\n",
                        "Seed: 43, Epoch: 103, Loss: 0.0518, Val AP: 0.2375, Test AP: 0.2417\n",
                        "Seed: 43, Epoch: 104, Loss: 0.0518, Val AP: 0.2461, Test AP: 0.2477\n",
                        "Seed: 43, Epoch: 105, Loss: 0.0518, Val AP: 0.2566, Test AP: 0.2582\n",
                        "Seed: 43, Epoch: 106, Loss: 0.0517, Val AP: 0.2540, Test AP: 0.2571\n",
                        "Seed: 43, Epoch: 107, Loss: 0.0517, Val AP: 0.2659, Test AP: 0.2691\n",
                        "Seed: 43, Epoch: 108, Loss: 0.0517, Val AP: 0.2398, Test AP: 0.2437\n",
                        "Seed: 43, Epoch: 109, Loss: 0.0517, Val AP: 0.2563, Test AP: 0.2587\n",
                        "Seed: 43, Epoch: 110, Loss: 0.0518, Val AP: 0.2350, Test AP: 0.2391\n",
                        "Seed: 43, Epoch: 111, Loss: 0.0517, Val AP: 0.2507, Test AP: 0.2516\n",
                        "Seed: 43, Epoch: 112, Loss: 0.0516, Val AP: 0.2547, Test AP: 0.2580\n",
                        "Seed: 43, Epoch: 113, Loss: 0.0516, Val AP: 0.2528, Test AP: 0.2572\n",
                        "Seed: 43, Epoch: 114, Loss: 0.0516, Val AP: 0.2476, Test AP: 0.2488\n",
                        "Seed: 43, Epoch: 115, Loss: 0.0517, Val AP: 0.2467, Test AP: 0.2483\n",
                        "Seed: 43, Epoch: 116, Loss: 0.0516, Val AP: 0.2587, Test AP: 0.2616\n",
                        "Seed: 43, Epoch: 117, Loss: 0.0516, Val AP: 0.2634, Test AP: 0.2628\n",
                        "Seed: 43, Epoch: 118, Loss: 0.0516, Val AP: 0.2309, Test AP: 0.2337\n",
                        "Seed: 43, Epoch: 119, Loss: 0.0516, Val AP: 0.2507, Test AP: 0.2557\n",
                        "Seed: 43, Epoch: 120, Loss: 0.0515, Val AP: 0.2509, Test AP: 0.2531\n",
                        "Seed: 43, Epoch: 121, Loss: 0.0515, Val AP: 0.2517, Test AP: 0.2519\n",
                        "Seed: 43, Epoch: 122, Loss: 0.0517, Val AP: 0.2483, Test AP: 0.2530\n",
                        "Seed: 43, Epoch: 123, Loss: 0.0516, Val AP: 0.2427, Test AP: 0.2486\n",
                        "Seed: 43, Epoch: 124, Loss: 0.0515, Val AP: 0.2478, Test AP: 0.2508\n",
                        "Seed: 43, Epoch: 125, Loss: 0.0514, Val AP: 0.2584, Test AP: 0.2608\n",
                        "Seed: 43, Epoch: 126, Loss: 0.0515, Val AP: 0.2685, Test AP: 0.2706\n",
                        "Seed: 43, Epoch: 127, Loss: 0.0515, Val AP: 0.2377, Test AP: 0.2433\n",
                        "Seed: 43, Epoch: 128, Loss: 0.0515, Val AP: 0.2577, Test AP: 0.2605\n",
                        "Seed: 43, Epoch: 129, Loss: 0.0515, Val AP: 0.2540, Test AP: 0.2582\n",
                        "Seed: 43, Epoch: 130, Loss: 0.0516, Val AP: 0.2562, Test AP: 0.2601\n",
                        "Seed: 43, Epoch: 131, Loss: 0.0514, Val AP: 0.2317, Test AP: 0.2340\n",
                        "Seed: 43, Epoch: 132, Loss: 0.0515, Val AP: 0.2503, Test AP: 0.2544\n",
                        "Seed: 43, Epoch: 133, Loss: 0.0514, Val AP: 0.2519, Test AP: 0.2557\n",
                        "Seed: 43, Epoch: 134, Loss: 0.0514, Val AP: 0.2575, Test AP: 0.2620\n",
                        "Seed: 43, Epoch: 135, Loss: 0.0514, Val AP: 0.2479, Test AP: 0.2496\n",
                        "Seed: 43, Epoch: 136, Loss: 0.0514, Val AP: 0.2606, Test AP: 0.2635\n",
                        "Seed: 43, Epoch: 137, Loss: 0.0513, Val AP: 0.2525, Test AP: 0.2575\n",
                        "Seed: 43, Epoch: 138, Loss: 0.0514, Val AP: 0.2589, Test AP: 0.2597\n",
                        "Seed: 43, Epoch: 139, Loss: 0.0513, Val AP: 0.2692, Test AP: 0.2727\n",
                        "Seed: 43, Epoch: 140, Loss: 0.0514, Val AP: 0.2591, Test AP: 0.2633\n",
                        "Seed: 43, Epoch: 141, Loss: 0.0515, Val AP: 0.2383, Test AP: 0.2426\n",
                        "Seed: 43, Epoch: 142, Loss: 0.0512, Val AP: 0.2562, Test AP: 0.2594\n",
                        "Seed: 43, Epoch: 143, Loss: 0.0513, Val AP: 0.2408, Test AP: 0.2447\n",
                        "Seed: 43, Epoch: 144, Loss: 0.0517, Val AP: 0.2562, Test AP: 0.2596\n",
                        "Seed: 43, Epoch: 145, Loss: 0.0513, Val AP: 0.2708, Test AP: 0.2721\n",
                        "Seed: 43, Epoch: 146, Loss: 0.0513, Val AP: 0.2627, Test AP: 0.2667\n",
                        "Seed: 43, Epoch: 147, Loss: 0.0512, Val AP: 0.2463, Test AP: 0.2505\n",
                        "Seed: 43, Epoch: 148, Loss: 0.0513, Val AP: 0.2652, Test AP: 0.2697\n",
                        "Seed: 43, Epoch: 149, Loss: 0.0512, Val AP: 0.2545, Test AP: 0.2582\n",
                        "Seed: 43, Epoch: 150, Loss: 0.0513, Val AP: 0.2505, Test AP: 0.2540\n",
                        "Seed: 44, Epoch: 001, Loss: 0.0630, Val AP: 0.1197, Test AP: 0.1196\n",
                        "Seed: 44, Epoch: 002, Loss: 0.0602, Val AP: 0.1208, Test AP: 0.1209\n",
                        "Seed: 44, Epoch: 003, Loss: 0.0600, Val AP: 0.1162, Test AP: 0.1159\n",
                        "Seed: 44, Epoch: 004, Loss: 0.0597, Val AP: 0.1296, Test AP: 0.1292\n",
                        "Seed: 44, Epoch: 005, Loss: 0.0594, Val AP: 0.1318, Test AP: 0.1312\n",
                        "Seed: 44, Epoch: 006, Loss: 0.0590, Val AP: 0.1132, Test AP: 0.1135\n",
                        "Seed: 44, Epoch: 007, Loss: 0.0590, Val AP: 0.1311, Test AP: 0.1322\n",
                        "Seed: 44, Epoch: 008, Loss: 0.0586, Val AP: 0.1388, Test AP: 0.1383\n",
                        "Seed: 44, Epoch: 009, Loss: 0.0582, Val AP: 0.1384, Test AP: 0.1393\n",
                        "Seed: 44, Epoch: 010, Loss: 0.0580, Val AP: 0.1556, Test AP: 0.1561\n",
                        "Seed: 44, Epoch: 011, Loss: 0.0578, Val AP: 0.1558, Test AP: 0.1560\n",
                        "Seed: 44, Epoch: 012, Loss: 0.0575, Val AP: 0.1594, Test AP: 0.1609\n",
                        "Seed: 44, Epoch: 013, Loss: 0.0573, Val AP: 0.1684, Test AP: 0.1696\n",
                        "Seed: 44, Epoch: 014, Loss: 0.0570, Val AP: 0.1683, Test AP: 0.1698\n",
                        "Seed: 44, Epoch: 015, Loss: 0.0568, Val AP: 0.1687, Test AP: 0.1700\n",
                        "Seed: 44, Epoch: 016, Loss: 0.0569, Val AP: 0.1800, Test AP: 0.1807\n",
                        "Seed: 44, Epoch: 017, Loss: 0.0565, Val AP: 0.1698, Test AP: 0.1713\n",
                        "Seed: 44, Epoch: 018, Loss: 0.0564, Val AP: 0.1828, Test AP: 0.1842\n",
                        "Seed: 44, Epoch: 019, Loss: 0.0562, Val AP: 0.1670, Test AP: 0.1670\n",
                        "Seed: 44, Epoch: 020, Loss: 0.0562, Val AP: 0.1841, Test AP: 0.1844\n",
                        "Seed: 44, Epoch: 021, Loss: 0.0560, Val AP: 0.1830, Test AP: 0.1839\n",
                        "Seed: 44, Epoch: 022, Loss: 0.0558, Val AP: 0.1835, Test AP: 0.1836\n",
                        "Seed: 44, Epoch: 023, Loss: 0.0557, Val AP: 0.1778, Test AP: 0.1772\n",
                        "Seed: 44, Epoch: 024, Loss: 0.0557, Val AP: 0.1835, Test AP: 0.1834\n",
                        "Seed: 44, Epoch: 025, Loss: 0.0553, Val AP: 0.1781, Test AP: 0.1791\n",
                        "Seed: 44, Epoch: 026, Loss: 0.0554, Val AP: 0.1735, Test AP: 0.1752\n",
                        "Seed: 44, Epoch: 027, Loss: 0.0552, Val AP: 0.2005, Test AP: 0.2000\n",
                        "Seed: 44, Epoch: 028, Loss: 0.0551, Val AP: 0.2046, Test AP: 0.2043\n",
                        "Seed: 44, Epoch: 029, Loss: 0.0549, Val AP: 0.2090, Test AP: 0.2081\n",
                        "Seed: 44, Epoch: 030, Loss: 0.0548, Val AP: 0.1934, Test AP: 0.1927\n",
                        "Seed: 44, Epoch: 031, Loss: 0.0547, Val AP: 0.2037, Test AP: 0.2024\n",
                        "Seed: 44, Epoch: 032, Loss: 0.0546, Val AP: 0.1927, Test AP: 0.1909\n",
                        "Seed: 44, Epoch: 033, Loss: 0.0545, Val AP: 0.2005, Test AP: 0.1995\n",
                        "Seed: 44, Epoch: 034, Loss: 0.0544, Val AP: 0.2138, Test AP: 0.2114\n",
                        "Seed: 44, Epoch: 035, Loss: 0.0544, Val AP: 0.2165, Test AP: 0.2144\n",
                        "Seed: 44, Epoch: 036, Loss: 0.0542, Val AP: 0.1904, Test AP: 0.1923\n",
                        "Seed: 44, Epoch: 037, Loss: 0.0541, Val AP: 0.2176, Test AP: 0.2167\n",
                        "Seed: 44, Epoch: 038, Loss: 0.0541, Val AP: 0.2216, Test AP: 0.2201\n",
                        "Seed: 44, Epoch: 039, Loss: 0.0541, Val AP: 0.2104, Test AP: 0.2096\n",
                        "Seed: 44, Epoch: 040, Loss: 0.0540, Val AP: 0.2144, Test AP: 0.2121\n",
                        "Seed: 44, Epoch: 041, Loss: 0.0539, Val AP: 0.2196, Test AP: 0.2181\n",
                        "Seed: 44, Epoch: 042, Loss: 0.0538, Val AP: 0.2198, Test AP: 0.2191\n",
                        "Seed: 44, Epoch: 043, Loss: 0.0538, Val AP: 0.2079, Test AP: 0.2073\n",
                        "Seed: 44, Epoch: 044, Loss: 0.0537, Val AP: 0.2302, Test AP: 0.2304\n",
                        "Seed: 44, Epoch: 045, Loss: 0.0537, Val AP: 0.2185, Test AP: 0.2171\n",
                        "Seed: 44, Epoch: 046, Loss: 0.0537, Val AP: 0.2205, Test AP: 0.2197\n",
                        "Seed: 44, Epoch: 047, Loss: 0.0536, Val AP: 0.2300, Test AP: 0.2296\n",
                        "Seed: 44, Epoch: 048, Loss: 0.0535, Val AP: 0.2224, Test AP: 0.2244\n",
                        "Seed: 44, Epoch: 049, Loss: 0.0534, Val AP: 0.2126, Test AP: 0.2130\n",
                        "Seed: 44, Epoch: 050, Loss: 0.0536, Val AP: 0.2311, Test AP: 0.2313\n",
                        "Seed: 44, Epoch: 051, Loss: 0.0535, Val AP: 0.2150, Test AP: 0.2127\n",
                        "Seed: 44, Epoch: 052, Loss: 0.0534, Val AP: 0.2422, Test AP: 0.2409\n",
                        "Seed: 44, Epoch: 053, Loss: 0.0533, Val AP: 0.2301, Test AP: 0.2300\n",
                        "Seed: 44, Epoch: 054, Loss: 0.0532, Val AP: 0.1978, Test AP: 0.1983\n",
                        "Seed: 44, Epoch: 055, Loss: 0.0532, Val AP: 0.2248, Test AP: 0.2252\n",
                        "Seed: 44, Epoch: 056, Loss: 0.0531, Val AP: 0.2310, Test AP: 0.2330\n",
                        "Seed: 44, Epoch: 057, Loss: 0.0532, Val AP: 0.2357, Test AP: 0.2350\n",
                        "Seed: 44, Epoch: 058, Loss: 0.0531, Val AP: 0.2305, Test AP: 0.2303\n",
                        "Seed: 44, Epoch: 059, Loss: 0.0531, Val AP: 0.2280, Test AP: 0.2262\n",
                        "Seed: 44, Epoch: 060, Loss: 0.0531, Val AP: 0.2244, Test AP: 0.2261\n",
                        "Seed: 44, Epoch: 061, Loss: 0.0530, Val AP: 0.2349, Test AP: 0.2351\n",
                        "Seed: 44, Epoch: 062, Loss: 0.0529, Val AP: 0.2299, Test AP: 0.2304\n",
                        "Seed: 44, Epoch: 063, Loss: 0.0529, Val AP: 0.2383, Test AP: 0.2395\n",
                        "Seed: 44, Epoch: 064, Loss: 0.0529, Val AP: 0.2475, Test AP: 0.2459\n",
                        "Seed: 44, Epoch: 065, Loss: 0.0528, Val AP: 0.2390, Test AP: 0.2383\n",
                        "Seed: 44, Epoch: 066, Loss: 0.0528, Val AP: 0.2207, Test AP: 0.2221\n",
                        "Seed: 44, Epoch: 067, Loss: 0.0528, Val AP: 0.2429, Test AP: 0.2426\n",
                        "Seed: 44, Epoch: 068, Loss: 0.0528, Val AP: 0.2306, Test AP: 0.2298\n",
                        "Seed: 44, Epoch: 069, Loss: 0.0528, Val AP: 0.2214, Test AP: 0.2219\n",
                        "Seed: 44, Epoch: 070, Loss: 0.0527, Val AP: 0.2460, Test AP: 0.2462\n",
                        "Seed: 44, Epoch: 071, Loss: 0.0526, Val AP: 0.2394, Test AP: 0.2389\n",
                        "Seed: 44, Epoch: 072, Loss: 0.0527, Val AP: 0.2333, Test AP: 0.2309\n",
                        "Seed: 44, Epoch: 073, Loss: 0.0527, Val AP: 0.2525, Test AP: 0.2510\n",
                        "Seed: 44, Epoch: 074, Loss: 0.0527, Val AP: 0.2403, Test AP: 0.2398\n",
                        "Seed: 44, Epoch: 075, Loss: 0.0525, Val AP: 0.2413, Test AP: 0.2410\n",
                        "Seed: 44, Epoch: 076, Loss: 0.0526, Val AP: 0.2412, Test AP: 0.2414\n",
                        "Seed: 44, Epoch: 077, Loss: 0.0525, Val AP: 0.2496, Test AP: 0.2496\n",
                        "Seed: 44, Epoch: 078, Loss: 0.0526, Val AP: 0.2404, Test AP: 0.2403\n",
                        "Seed: 44, Epoch: 079, Loss: 0.0525, Val AP: 0.2418, Test AP: 0.2414\n",
                        "Seed: 44, Epoch: 080, Loss: 0.0525, Val AP: 0.2388, Test AP: 0.2402\n",
                        "Seed: 44, Epoch: 081, Loss: 0.0525, Val AP: 0.2447, Test AP: 0.2460\n",
                        "Seed: 44, Epoch: 082, Loss: 0.0523, Val AP: 0.2337, Test AP: 0.2345\n",
                        "Seed: 44, Epoch: 083, Loss: 0.0524, Val AP: 0.2383, Test AP: 0.2390\n",
                        "Seed: 44, Epoch: 084, Loss: 0.0524, Val AP: 0.2490, Test AP: 0.2498\n",
                        "Seed: 44, Epoch: 085, Loss: 0.0523, Val AP: 0.2484, Test AP: 0.2485\n",
                        "Seed: 44, Epoch: 086, Loss: 0.0523, Val AP: 0.2470, Test AP: 0.2475\n",
                        "Seed: 44, Epoch: 087, Loss: 0.0524, Val AP: 0.2502, Test AP: 0.2493\n",
                        "Seed: 44, Epoch: 088, Loss: 0.0523, Val AP: 0.2402, Test AP: 0.2412\n",
                        "Seed: 44, Epoch: 089, Loss: 0.0523, Val AP: 0.2520, Test AP: 0.2529\n",
                        "Seed: 44, Epoch: 090, Loss: 0.0522, Val AP: 0.2502, Test AP: 0.2523\n",
                        "Seed: 44, Epoch: 091, Loss: 0.0525, Val AP: 0.2206, Test AP: 0.2232\n",
                        "Seed: 44, Epoch: 092, Loss: 0.0524, Val AP: 0.2480, Test AP: 0.2512\n",
                        "Seed: 44, Epoch: 093, Loss: 0.0524, Val AP: 0.2560, Test AP: 0.2558\n",
                        "Seed: 44, Epoch: 094, Loss: 0.0523, Val AP: 0.2407, Test AP: 0.2405\n",
                        "Seed: 44, Epoch: 095, Loss: 0.0522, Val AP: 0.2424, Test AP: 0.2439\n",
                        "Seed: 44, Epoch: 096, Loss: 0.0522, Val AP: 0.2390, Test AP: 0.2389\n",
                        "Seed: 44, Epoch: 097, Loss: 0.0523, Val AP: 0.2344, Test AP: 0.2384\n",
                        "Seed: 44, Epoch: 098, Loss: 0.0523, Val AP: 0.2259, Test AP: 0.2278\n",
                        "Seed: 44, Epoch: 099, Loss: 0.0522, Val AP: 0.2345, Test AP: 0.2347\n",
                        "Seed: 44, Epoch: 100, Loss: 0.0523, Val AP: 0.2387, Test AP: 0.2403\n",
                        "Seed: 44, Epoch: 101, Loss: 0.0523, Val AP: 0.2470, Test AP: 0.2478\n",
                        "Seed: 44, Epoch: 102, Loss: 0.0521, Val AP: 0.2278, Test AP: 0.2305\n",
                        "Seed: 44, Epoch: 103, Loss: 0.0524, Val AP: 0.2090, Test AP: 0.2126\n",
                        "Seed: 44, Epoch: 104, Loss: 0.0522, Val AP: 0.2469, Test AP: 0.2488\n",
                        "Seed: 44, Epoch: 105, Loss: 0.0522, Val AP: 0.2411, Test AP: 0.2420\n",
                        "Seed: 44, Epoch: 106, Loss: 0.0521, Val AP: 0.2425, Test AP: 0.2417\n",
                        "Seed: 44, Epoch: 107, Loss: 0.0521, Val AP: 0.2445, Test AP: 0.2455\n",
                        "Seed: 44, Epoch: 108, Loss: 0.0521, Val AP: 0.2367, Test AP: 0.2387\n",
                        "Seed: 44, Epoch: 109, Loss: 0.0522, Val AP: 0.2442, Test AP: 0.2470\n",
                        "Seed: 44, Epoch: 110, Loss: 0.0521, Val AP: 0.2515, Test AP: 0.2516\n",
                        "Seed: 44, Epoch: 111, Loss: 0.0519, Val AP: 0.2493, Test AP: 0.2495\n",
                        "Seed: 44, Epoch: 112, Loss: 0.0520, Val AP: 0.2432, Test AP: 0.2451\n",
                        "Seed: 44, Epoch: 113, Loss: 0.0521, Val AP: 0.2605, Test AP: 0.2616\n",
                        "Seed: 44, Epoch: 114, Loss: 0.0522, Val AP: 0.2472, Test AP: 0.2491\n",
                        "Seed: 44, Epoch: 115, Loss: 0.0520, Val AP: 0.2453, Test AP: 0.2487\n",
                        "Seed: 44, Epoch: 116, Loss: 0.0520, Val AP: 0.2377, Test AP: 0.2411\n",
                        "Seed: 44, Epoch: 117, Loss: 0.0520, Val AP: 0.2504, Test AP: 0.2526\n",
                        "Seed: 44, Epoch: 118, Loss: 0.0520, Val AP: 0.2608, Test AP: 0.2617\n",
                        "Seed: 44, Epoch: 119, Loss: 0.0520, Val AP: 0.2567, Test AP: 0.2556\n",
                        "Seed: 44, Epoch: 120, Loss: 0.0519, Val AP: 0.2538, Test AP: 0.2557\n",
                        "Seed: 44, Epoch: 121, Loss: 0.0520, Val AP: 0.2558, Test AP: 0.2579\n",
                        "Seed: 44, Epoch: 122, Loss: 0.0519, Val AP: 0.2398, Test AP: 0.2407\n",
                        "Seed: 44, Epoch: 123, Loss: 0.0519, Val AP: 0.2479, Test AP: 0.2496\n",
                        "Seed: 44, Epoch: 124, Loss: 0.0519, Val AP: 0.2333, Test AP: 0.2339\n",
                        "Seed: 44, Epoch: 125, Loss: 0.0520, Val AP: 0.2497, Test AP: 0.2499\n",
                        "Seed: 44, Epoch: 126, Loss: 0.0518, Val AP: 0.2632, Test AP: 0.2654\n",
                        "Seed: 44, Epoch: 127, Loss: 0.0519, Val AP: 0.2613, Test AP: 0.2625\n",
                        "Seed: 44, Epoch: 128, Loss: 0.0519, Val AP: 0.2582, Test AP: 0.2596\n",
                        "Seed: 44, Epoch: 129, Loss: 0.0519, Val AP: 0.2437, Test AP: 0.2455\n",
                        "Seed: 44, Epoch: 130, Loss: 0.0518, Val AP: 0.2505, Test AP: 0.2524\n",
                        "Seed: 44, Epoch: 131, Loss: 0.0519, Val AP: 0.2531, Test AP: 0.2533\n",
                        "Seed: 44, Epoch: 132, Loss: 0.0520, Val AP: 0.2512, Test AP: 0.2522\n",
                        "Seed: 44, Epoch: 133, Loss: 0.0517, Val AP: 0.2547, Test AP: 0.2540\n",
                        "Seed: 44, Epoch: 134, Loss: 0.0518, Val AP: 0.2415, Test AP: 0.2440\n",
                        "Seed: 44, Epoch: 135, Loss: 0.0517, Val AP: 0.2581, Test AP: 0.2603\n",
                        "Seed: 44, Epoch: 136, Loss: 0.0518, Val AP: 0.2444, Test AP: 0.2451\n",
                        "Seed: 44, Epoch: 137, Loss: 0.0517, Val AP: 0.2295, Test AP: 0.2288\n",
                        "Seed: 44, Epoch: 138, Loss: 0.0518, Val AP: 0.2405, Test AP: 0.2402\n",
                        "Seed: 44, Epoch: 139, Loss: 0.0518, Val AP: 0.2406, Test AP: 0.2423\n",
                        "Seed: 44, Epoch: 140, Loss: 0.0518, Val AP: 0.2518, Test AP: 0.2530\n",
                        "Seed: 44, Epoch: 141, Loss: 0.0518, Val AP: 0.2531, Test AP: 0.2537\n",
                        "Seed: 44, Epoch: 142, Loss: 0.0519, Val AP: 0.2371, Test AP: 0.2369\n",
                        "Seed: 44, Epoch: 143, Loss: 0.0519, Val AP: 0.2445, Test AP: 0.2457\n",
                        "Seed: 44, Epoch: 144, Loss: 0.0518, Val AP: 0.2620, Test AP: 0.2621\n",
                        "Seed: 44, Epoch: 145, Loss: 0.0518, Val AP: 0.2584, Test AP: 0.2587\n",
                        "Seed: 44, Epoch: 146, Loss: 0.0517, Val AP: 0.2545, Test AP: 0.2553\n",
                        "Seed: 44, Epoch: 147, Loss: 0.0517, Val AP: 0.2463, Test AP: 0.2490\n",
                        "Seed: 44, Epoch: 148, Loss: 0.0517, Val AP: 0.2462, Test AP: 0.2471\n",
                        "Seed: 44, Epoch: 149, Loss: 0.0517, Val AP: 0.2458, Test AP: 0.2469\n",
                        "Seed: 44, Epoch: 150, Loss: 0.0517, Val AP: 0.2321, Test AP: 0.2348\n",
                        "Average Time: 5316.69 seconds\n",
                        "Var Time: 1743.18 seconds\n",
                        "Average Memory: 3430.00 MB\n",
                        "Average Best Val AP: 0.2656\n",
                        "Std Best Test AP: 0.0028\n",
                        "Average Test AP: 0.2685\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nclass HierarchicalGCN_KMIS(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_KMIS, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, improved = True)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = KMISPooling(512, k=1, aggr_x='sum')\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, improved = True)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = KMISPooling(512, k=1, aggr_x='sum')\n        self.conv3 = GCNConv(hidden_channels, out_channels, improved = True)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        init_x = data.x\n        x = x.float()\n        init_x = init_x.float()\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch=batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, batch=batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False)\n    model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### GSAPool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom torch_geometric.nn import GCNConv\nfrom torch.nn import Parameter\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Union, Optional, Callable\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax\nimport math\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, GCNConv, GATConv, ChebConv, GraphConv\ndef uniform(size, tensor):\n    if tensor is not None:\n        bound = 1.0 / math.sqrt(size)\n        tensor.data.uniform_(-bound, bound)\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ),\n                             torch.finfo(x.dtype).min)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        if isinstance(ratio, int):\n            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n            k = torch.min(k, num_nodes)\n        else:\n            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\nclass GSAPool(torch.nn.Module):\n    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n                        min_score=None, multiplier=1,\n                        non_linearity=torch.tanh,\n                        cus_drop_ratio =0):\n        super(GSAPool,self).__init__()\n        self.in_channels = in_channels\n        self.ratio = pooling_ratio\n        self.alpha = alpha\n        self.sbtl_layer = GCNConv(in_channels,1)\n        self.fbtl_layer = nn.Linear(in_channels, 1)\n        self.fusion = GCNConv(in_channels,in_channels)\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.fusion_flag = 0\n        self.non_linearity = non_linearity\n        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n    def conv_selection(self, conv, in_channels, conv_type=0):\n        if(conv_type == 0):\n            out_channels = 1\n        elif(conv_type == 1):\n            out_channels = in_channels\n        if(conv == \"GCNConv\"):\n            return GCNConv(in_channels,out_channels)\n        elif(conv == \"ChebConv\"):\n            return ChebConv(in_channels,out_channels,1)\n        elif(conv == \"GCNConv\"):\n            return GCNConv(in_channels,out_channels)\n        elif(conv == \"GATConv\"):\n            return GATConv(in_channels,out_channels, heads=1, concat=True)\n        elif(conv == \"GraphConv\"):\n            return GraphConv(in_channels,out_channels)\n        else:\n            raise ValueError\n    def forward(self, x, edge_index, edge_attr=None, batch=None):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x = x.unsqueeze(-1) if x.dim() == 1 else x\n        score_s = self.sbtl_layer(x,edge_index).squeeze()\n        score_f = self.fbtl_layer(x).squeeze()\n        score = score_s*self.alpha + score_f*(1-self.alpha)\n        score = score.unsqueeze(-1) if score.dim()==0 else score\n        if self.min_score is None:\n            score = self.non_linearity(score)\n        else:\n            score = softmax(score, batch)\n        sc = self.dropout(score)\n        perm = topk(sc, self.ratio, batch)\n        if(self.fusion_flag == 1):\n            x = self.fusion(x, edge_index)\n        x_ae = x[perm]\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n        batch = batch[perm]\n        edge_index, edge_attr = filter_adj(\n            edge_index, edge_attr, perm, num_nodes=score.size(0))\n        return x, edge_index, edge_attr, batch, perm, x_ae"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0628, Val AP: 0.1183, Test AP: 0.1179\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0604, Val AP: 0.1160, Test AP: 0.1169\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0601, Val AP: 0.1245, Test AP: 0.1246\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0596, Val AP: 0.1165, Test AP: 0.1168\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0594, Val AP: 0.1340, Test AP: 0.1329\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0591, Val AP: 0.1343, Test AP: 0.1331\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0588, Val AP: 0.1402, Test AP: 0.1383\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0585, Val AP: 0.1444, Test AP: 0.1424\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0582, Val AP: 0.1556, Test AP: 0.1543\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0578, Val AP: 0.1632, Test AP: 0.1615\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0576, Val AP: 0.1514, Test AP: 0.1513\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0574, Val AP: 0.1614, Test AP: 0.1610\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0571, Val AP: 0.1496, Test AP: 0.1485\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0570, Val AP: 0.1703, Test AP: 0.1726\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0567, Val AP: 0.1764, Test AP: 0.1775\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0565, Val AP: 0.1651, Test AP: 0.1646\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0563, Val AP: 0.1786, Test AP: 0.1795\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0561, Val AP: 0.1835, Test AP: 0.1847\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0559, Val AP: 0.1827, Test AP: 0.1854\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0558, Val AP: 0.1949, Test AP: 0.1963\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0555, Val AP: 0.1722, Test AP: 0.1709\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0555, Val AP: 0.1865, Test AP: 0.1867\n",
                        "Seed: 42, Epoch: 023, Loss: 0.0554, Val AP: 0.1975, Test AP: 0.1990\n",
                        "Seed: 42, Epoch: 024, Loss: 0.0551, Val AP: 0.1903, Test AP: 0.1894\n",
                        "Seed: 42, Epoch: 025, Loss: 0.0550, Val AP: 0.2031, Test AP: 0.2013\n",
                        "Seed: 42, Epoch: 026, Loss: 0.0549, Val AP: 0.2204, Test AP: 0.2191\n",
                        "Seed: 42, Epoch: 027, Loss: 0.0547, Val AP: 0.1994, Test AP: 0.2001\n",
                        "Seed: 42, Epoch: 028, Loss: 0.0546, Val AP: 0.2241, Test AP: 0.2249\n",
                        "Seed: 42, Epoch: 029, Loss: 0.0546, Val AP: 0.2092, Test AP: 0.2106\n",
                        "Seed: 42, Epoch: 030, Loss: 0.0544, Val AP: 0.2061, Test AP: 0.2078\n",
                        "Seed: 42, Epoch: 031, Loss: 0.0543, Val AP: 0.2230, Test AP: 0.2220\n",
                        "Seed: 42, Epoch: 032, Loss: 0.0542, Val AP: 0.2101, Test AP: 0.2096\n",
                        "Seed: 42, Epoch: 033, Loss: 0.0543, Val AP: 0.2114, Test AP: 0.2120\n",
                        "Seed: 42, Epoch: 034, Loss: 0.0541, Val AP: 0.2156, Test AP: 0.2165\n",
                        "Seed: 42, Epoch: 035, Loss: 0.0540, Val AP: 0.2093, Test AP: 0.2100\n",
                        "Seed: 42, Epoch: 036, Loss: 0.0539, Val AP: 0.2304, Test AP: 0.2287\n",
                        "Seed: 42, Epoch: 037, Loss: 0.0538, Val AP: 0.2266, Test AP: 0.2235\n",
                        "Seed: 42, Epoch: 038, Loss: 0.0537, Val AP: 0.2157, Test AP: 0.2154\n",
                        "Seed: 42, Epoch: 039, Loss: 0.0536, Val AP: 0.2348, Test AP: 0.2338\n",
                        "Seed: 42, Epoch: 040, Loss: 0.0536, Val AP: 0.2302, Test AP: 0.2289\n",
                        "Seed: 42, Epoch: 041, Loss: 0.0536, Val AP: 0.2385, Test AP: 0.2389\n",
                        "Seed: 42, Epoch: 042, Loss: 0.0534, Val AP: 0.2254, Test AP: 0.2243\n",
                        "Seed: 42, Epoch: 043, Loss: 0.0534, Val AP: 0.2290, Test AP: 0.2280\n",
                        "Seed: 42, Epoch: 044, Loss: 0.0534, Val AP: 0.2174, Test AP: 0.2163\n",
                        "Seed: 42, Epoch: 045, Loss: 0.0533, Val AP: 0.2113, Test AP: 0.2085\n",
                        "Seed: 42, Epoch: 046, Loss: 0.0535, Val AP: 0.2357, Test AP: 0.2355\n",
                        "Seed: 42, Epoch: 047, Loss: 0.0531, Val AP: 0.2290, Test AP: 0.2271\n",
                        "Seed: 42, Epoch: 048, Loss: 0.0532, Val AP: 0.2341, Test AP: 0.2321\n",
                        "Seed: 42, Epoch: 049, Loss: 0.0531, Val AP: 0.2423, Test AP: 0.2423\n",
                        "Seed: 42, Epoch: 050, Loss: 0.0530, Val AP: 0.2349, Test AP: 0.2356\n",
                        "Seed: 42, Epoch: 051, Loss: 0.0529, Val AP: 0.2395, Test AP: 0.2378\n",
                        "Seed: 42, Epoch: 052, Loss: 0.0528, Val AP: 0.2462, Test AP: 0.2431\n",
                        "Seed: 42, Epoch: 053, Loss: 0.0528, Val AP: 0.2224, Test AP: 0.2213\n",
                        "Seed: 42, Epoch: 054, Loss: 0.0528, Val AP: 0.2484, Test AP: 0.2463\n",
                        "Seed: 42, Epoch: 055, Loss: 0.0528, Val AP: 0.2470, Test AP: 0.2458\n",
                        "Seed: 42, Epoch: 056, Loss: 0.0527, Val AP: 0.2394, Test AP: 0.2376\n",
                        "Seed: 42, Epoch: 057, Loss: 0.0526, Val AP: 0.2412, Test AP: 0.2387\n",
                        "Seed: 42, Epoch: 058, Loss: 0.0526, Val AP: 0.2461, Test AP: 0.2444\n",
                        "Seed: 42, Epoch: 059, Loss: 0.0525, Val AP: 0.2433, Test AP: 0.2419\n",
                        "Seed: 42, Epoch: 060, Loss: 0.0525, Val AP: 0.2286, Test AP: 0.2276\n",
                        "Seed: 42, Epoch: 061, Loss: 0.0525, Val AP: 0.2281, Test AP: 0.2242\n",
                        "Seed: 42, Epoch: 062, Loss: 0.0526, Val AP: 0.2537, Test AP: 0.2523\n",
                        "Seed: 42, Epoch: 063, Loss: 0.0523, Val AP: 0.2384, Test AP: 0.2351\n",
                        "Seed: 42, Epoch: 064, Loss: 0.0523, Val AP: 0.2474, Test AP: 0.2463\n",
                        "Seed: 42, Epoch: 065, Loss: 0.0522, Val AP: 0.2502, Test AP: 0.2492\n",
                        "Seed: 42, Epoch: 066, Loss: 0.0521, Val AP: 0.2519, Test AP: 0.2493\n",
                        "Seed: 42, Epoch: 067, Loss: 0.0522, Val AP: 0.2241, Test AP: 0.2224\n",
                        "Seed: 42, Epoch: 068, Loss: 0.0522, Val AP: 0.2386, Test AP: 0.2352\n",
                        "Seed: 42, Epoch: 069, Loss: 0.0522, Val AP: 0.2499, Test AP: 0.2463\n",
                        "Seed: 42, Epoch: 070, Loss: 0.0521, Val AP: 0.2628, Test AP: 0.2605\n",
                        "Seed: 42, Epoch: 071, Loss: 0.0520, Val AP: 0.2503, Test AP: 0.2477\n",
                        "Seed: 42, Epoch: 072, Loss: 0.0520, Val AP: 0.2511, Test AP: 0.2500\n",
                        "Seed: 42, Epoch: 073, Loss: 0.0519, Val AP: 0.2418, Test AP: 0.2391\n",
                        "Seed: 42, Epoch: 074, Loss: 0.0520, Val AP: 0.2680, Test AP: 0.2641\n",
                        "Seed: 42, Epoch: 075, Loss: 0.0519, Val AP: 0.2656, Test AP: 0.2629\n",
                        "Seed: 42, Epoch: 076, Loss: 0.0519, Val AP: 0.2553, Test AP: 0.2531\n",
                        "Seed: 42, Epoch: 077, Loss: 0.0518, Val AP: 0.2418, Test AP: 0.2390\n",
                        "Seed: 42, Epoch: 078, Loss: 0.0517, Val AP: 0.2617, Test AP: 0.2595\n",
                        "Seed: 42, Epoch: 079, Loss: 0.0518, Val AP: 0.2600, Test AP: 0.2581\n",
                        "Seed: 42, Epoch: 080, Loss: 0.0517, Val AP: 0.2501, Test AP: 0.2460\n",
                        "Seed: 42, Epoch: 081, Loss: 0.0517, Val AP: 0.2531, Test AP: 0.2518\n",
                        "Seed: 42, Epoch: 082, Loss: 0.0516, Val AP: 0.2621, Test AP: 0.2606\n",
                        "Seed: 42, Epoch: 083, Loss: 0.0516, Val AP: 0.2617, Test AP: 0.2593\n",
                        "Seed: 42, Epoch: 084, Loss: 0.0515, Val AP: 0.2536, Test AP: 0.2516\n",
                        "Seed: 42, Epoch: 085, Loss: 0.0515, Val AP: 0.2679, Test AP: 0.2642\n",
                        "Seed: 42, Epoch: 086, Loss: 0.0516, Val AP: 0.2293, Test AP: 0.2288\n",
                        "Seed: 42, Epoch: 087, Loss: 0.0516, Val AP: 0.2552, Test AP: 0.2530\n",
                        "Seed: 42, Epoch: 088, Loss: 0.0514, Val AP: 0.2541, Test AP: 0.2512\n",
                        "Seed: 42, Epoch: 089, Loss: 0.0515, Val AP: 0.2650, Test AP: 0.2634\n",
                        "Seed: 42, Epoch: 090, Loss: 0.0514, Val AP: 0.2750, Test AP: 0.2704\n",
                        "Seed: 42, Epoch: 091, Loss: 0.0513, Val AP: 0.2663, Test AP: 0.2623\n",
                        "Seed: 42, Epoch: 092, Loss: 0.0513, Val AP: 0.2519, Test AP: 0.2480\n",
                        "Seed: 42, Epoch: 093, Loss: 0.0515, Val AP: 0.2589, Test AP: 0.2574\n",
                        "Seed: 42, Epoch: 094, Loss: 0.0514, Val AP: 0.2755, Test AP: 0.2724\n",
                        "Seed: 42, Epoch: 095, Loss: 0.0512, Val AP: 0.2584, Test AP: 0.2540\n",
                        "Seed: 42, Epoch: 096, Loss: 0.0513, Val AP: 0.2477, Test AP: 0.2448\n",
                        "Seed: 42, Epoch: 097, Loss: 0.0515, Val AP: 0.2734, Test AP: 0.2717\n",
                        "Seed: 42, Epoch: 098, Loss: 0.0512, Val AP: 0.2700, Test AP: 0.2666\n",
                        "Seed: 42, Epoch: 099, Loss: 0.0513, Val AP: 0.2665, Test AP: 0.2658\n",
                        "Seed: 42, Epoch: 100, Loss: 0.0512, Val AP: 0.2710, Test AP: 0.2661\n",
                        "Seed: 42, Epoch: 101, Loss: 0.0512, Val AP: 0.2682, Test AP: 0.2686\n",
                        "Seed: 42, Epoch: 102, Loss: 0.0510, Val AP: 0.2733, Test AP: 0.2711\n",
                        "Seed: 42, Epoch: 103, Loss: 0.0511, Val AP: 0.2785, Test AP: 0.2757\n",
                        "Seed: 42, Epoch: 104, Loss: 0.0511, Val AP: 0.2494, Test AP: 0.2457\n",
                        "Seed: 42, Epoch: 105, Loss: 0.0511, Val AP: 0.2623, Test AP: 0.2601\n",
                        "Seed: 42, Epoch: 106, Loss: 0.0510, Val AP: 0.2544, Test AP: 0.2542\n",
                        "Seed: 42, Epoch: 107, Loss: 0.0510, Val AP: 0.2635, Test AP: 0.2610\n",
                        "Seed: 42, Epoch: 108, Loss: 0.0511, Val AP: 0.2791, Test AP: 0.2759\n",
                        "Seed: 42, Epoch: 109, Loss: 0.0510, Val AP: 0.2601, Test AP: 0.2552\n",
                        "Seed: 42, Epoch: 110, Loss: 0.0510, Val AP: 0.2639, Test AP: 0.2615\n",
                        "Seed: 42, Epoch: 111, Loss: 0.0511, Val AP: 0.2749, Test AP: 0.2709\n",
                        "Seed: 42, Epoch: 112, Loss: 0.0509, Val AP: 0.2659, Test AP: 0.2631\n",
                        "Seed: 42, Epoch: 113, Loss: 0.0509, Val AP: 0.2409, Test AP: 0.2390\n",
                        "Seed: 42, Epoch: 114, Loss: 0.0509, Val AP: 0.2757, Test AP: 0.2731\n",
                        "Seed: 42, Epoch: 115, Loss: 0.0508, Val AP: 0.2717, Test AP: 0.2703\n",
                        "Seed: 42, Epoch: 116, Loss: 0.0508, Val AP: 0.2709, Test AP: 0.2687\n",
                        "Seed: 42, Epoch: 117, Loss: 0.0508, Val AP: 0.2813, Test AP: 0.2790\n",
                        "Seed: 42, Epoch: 118, Loss: 0.0508, Val AP: 0.2709, Test AP: 0.2675\n",
                        "Seed: 42, Epoch: 119, Loss: 0.0507, Val AP: 0.2797, Test AP: 0.2792\n",
                        "Seed: 42, Epoch: 120, Loss: 0.0507, Val AP: 0.2810, Test AP: 0.2780\n",
                        "Seed: 42, Epoch: 121, Loss: 0.0506, Val AP: 0.2825, Test AP: 0.2808\n",
                        "Seed: 42, Epoch: 122, Loss: 0.0507, Val AP: 0.2783, Test AP: 0.2738\n",
                        "Seed: 42, Epoch: 123, Loss: 0.0507, Val AP: 0.2679, Test AP: 0.2669\n",
                        "Seed: 42, Epoch: 124, Loss: 0.0507, Val AP: 0.2547, Test AP: 0.2527\n",
                        "Seed: 42, Epoch: 125, Loss: 0.0507, Val AP: 0.2753, Test AP: 0.2745\n",
                        "Seed: 42, Epoch: 126, Loss: 0.0506, Val AP: 0.2793, Test AP: 0.2729\n",
                        "Seed: 42, Epoch: 127, Loss: 0.0506, Val AP: 0.2613, Test AP: 0.2594\n",
                        "Seed: 42, Epoch: 128, Loss: 0.0505, Val AP: 0.2760, Test AP: 0.2742\n",
                        "Seed: 42, Epoch: 129, Loss: 0.0506, Val AP: 0.2711, Test AP: 0.2678\n",
                        "Seed: 42, Epoch: 130, Loss: 0.0505, Val AP: 0.2823, Test AP: 0.2796\n",
                        "Seed: 42, Epoch: 131, Loss: 0.0506, Val AP: 0.2761, Test AP: 0.2713\n",
                        "Seed: 42, Epoch: 132, Loss: 0.0505, Val AP: 0.2712, Test AP: 0.2688\n",
                        "Seed: 42, Epoch: 133, Loss: 0.0504, Val AP: 0.2793, Test AP: 0.2770\n",
                        "Seed: 42, Epoch: 134, Loss: 0.0506, Val AP: 0.2724, Test AP: 0.2715\n",
                        "Seed: 42, Epoch: 135, Loss: 0.0504, Val AP: 0.2810, Test AP: 0.2788\n",
                        "Seed: 42, Epoch: 136, Loss: 0.0505, Val AP: 0.2506, Test AP: 0.2486\n",
                        "Seed: 42, Epoch: 137, Loss: 0.0506, Val AP: 0.2715, Test AP: 0.2678\n",
                        "Seed: 42, Epoch: 138, Loss: 0.0506, Val AP: 0.2625, Test AP: 0.2610\n",
                        "Seed: 42, Epoch: 139, Loss: 0.0504, Val AP: 0.2870, Test AP: 0.2837\n",
                        "Seed: 42, Epoch: 140, Loss: 0.0504, Val AP: 0.2737, Test AP: 0.2708\n",
                        "Seed: 42, Epoch: 141, Loss: 0.0504, Val AP: 0.2744, Test AP: 0.2722\n",
                        "Seed: 42, Epoch: 142, Loss: 0.0503, Val AP: 0.2774, Test AP: 0.2756\n",
                        "Seed: 42, Epoch: 143, Loss: 0.0503, Val AP: 0.2784, Test AP: 0.2759\n",
                        "Seed: 42, Epoch: 144, Loss: 0.0503, Val AP: 0.2607, Test AP: 0.2593\n",
                        "Seed: 42, Epoch: 145, Loss: 0.0502, Val AP: 0.2481, Test AP: 0.2464\n",
                        "Seed: 42, Epoch: 146, Loss: 0.0504, Val AP: 0.2612, Test AP: 0.2588\n",
                        "Seed: 42, Epoch: 147, Loss: 0.0503, Val AP: 0.2811, Test AP: 0.2803\n",
                        "Seed: 42, Epoch: 148, Loss: 0.0502, Val AP: 0.2712, Test AP: 0.2687\n",
                        "Seed: 42, Epoch: 149, Loss: 0.0503, Val AP: 0.2736, Test AP: 0.2743\n",
                        "Seed: 42, Epoch: 150, Loss: 0.0502, Val AP: 0.2739, Test AP: 0.2716\n",
                        "Seed: 43, Epoch: 001, Loss: 0.0626, Val AP: 0.1182, Test AP: 0.1244\n",
                        "Seed: 43, Epoch: 002, Loss: 0.0602, Val AP: 0.1145, Test AP: 0.1177\n",
                        "Seed: 43, Epoch: 003, Loss: 0.0598, Val AP: 0.1154, Test AP: 0.1186\n",
                        "Seed: 43, Epoch: 004, Loss: 0.0596, Val AP: 0.1284, Test AP: 0.1335\n",
                        "Seed: 43, Epoch: 005, Loss: 0.0593, Val AP: 0.1287, Test AP: 0.1337\n",
                        "Seed: 43, Epoch: 006, Loss: 0.0590, Val AP: 0.1325, Test AP: 0.1377\n",
                        "Seed: 43, Epoch: 007, Loss: 0.0588, Val AP: 0.1373, Test AP: 0.1405\n",
                        "Seed: 43, Epoch: 008, Loss: 0.0586, Val AP: 0.1397, Test AP: 0.1428\n",
                        "Seed: 43, Epoch: 009, Loss: 0.0583, Val AP: 0.1396, Test AP: 0.1437\n",
                        "Seed: 43, Epoch: 010, Loss: 0.0581, Val AP: 0.1427, Test AP: 0.1446\n",
                        "Seed: 43, Epoch: 011, Loss: 0.0580, Val AP: 0.1494, Test AP: 0.1542\n",
                        "Seed: 43, Epoch: 012, Loss: 0.0575, Val AP: 0.1571, Test AP: 0.1607\n",
                        "Seed: 43, Epoch: 013, Loss: 0.0572, Val AP: 0.1493, Test AP: 0.1536\n",
                        "Seed: 43, Epoch: 014, Loss: 0.0570, Val AP: 0.1681, Test AP: 0.1712\n",
                        "Seed: 43, Epoch: 015, Loss: 0.0568, Val AP: 0.1744, Test AP: 0.1780\n",
                        "Seed: 43, Epoch: 016, Loss: 0.0566, Val AP: 0.1707, Test AP: 0.1725\n",
                        "Seed: 43, Epoch: 017, Loss: 0.0563, Val AP: 0.1784, Test AP: 0.1802\n",
                        "Seed: 43, Epoch: 018, Loss: 0.0561, Val AP: 0.1897, Test AP: 0.1921\n",
                        "Seed: 43, Epoch: 019, Loss: 0.0559, Val AP: 0.1723, Test AP: 0.1758\n",
                        "Seed: 43, Epoch: 020, Loss: 0.0558, Val AP: 0.1928, Test AP: 0.1956\n",
                        "Seed: 43, Epoch: 021, Loss: 0.0555, Val AP: 0.1984, Test AP: 0.1998\n",
                        "Seed: 43, Epoch: 022, Loss: 0.0555, Val AP: 0.1927, Test AP: 0.1950\n",
                        "Seed: 43, Epoch: 023, Loss: 0.0552, Val AP: 0.1887, Test AP: 0.1910\n",
                        "Seed: 43, Epoch: 024, Loss: 0.0552, Val AP: 0.2047, Test AP: 0.2090\n",
                        "Seed: 43, Epoch: 025, Loss: 0.0550, Val AP: 0.1910, Test AP: 0.1928\n",
                        "Seed: 43, Epoch: 026, Loss: 0.0552, Val AP: 0.1848, Test AP: 0.1870\n",
                        "Seed: 43, Epoch: 027, Loss: 0.0550, Val AP: 0.1919, Test AP: 0.1946\n",
                        "Seed: 43, Epoch: 028, Loss: 0.0547, Val AP: 0.2100, Test AP: 0.2135\n",
                        "Seed: 43, Epoch: 029, Loss: 0.0546, Val AP: 0.2046, Test AP: 0.2081\n",
                        "Seed: 43, Epoch: 030, Loss: 0.0546, Val AP: 0.1995, Test AP: 0.2026\n",
                        "Seed: 43, Epoch: 031, Loss: 0.0544, Val AP: 0.2028, Test AP: 0.2054\n",
                        "Seed: 43, Epoch: 032, Loss: 0.0545, Val AP: 0.2131, Test AP: 0.2165\n",
                        "Seed: 43, Epoch: 033, Loss: 0.0543, Val AP: 0.2063, Test AP: 0.2108\n",
                        "Seed: 43, Epoch: 034, Loss: 0.0542, Val AP: 0.2029, Test AP: 0.2081\n",
                        "Seed: 43, Epoch: 035, Loss: 0.0542, Val AP: 0.2199, Test AP: 0.2240\n",
                        "Seed: 43, Epoch: 036, Loss: 0.0541, Val AP: 0.2132, Test AP: 0.2186\n",
                        "Seed: 43, Epoch: 037, Loss: 0.0539, Val AP: 0.2121, Test AP: 0.2176\n",
                        "Seed: 43, Epoch: 038, Loss: 0.0540, Val AP: 0.2127, Test AP: 0.2172\n",
                        "Seed: 43, Epoch: 039, Loss: 0.0539, Val AP: 0.2260, Test AP: 0.2289\n",
                        "Seed: 43, Epoch: 040, Loss: 0.0538, Val AP: 0.2169, Test AP: 0.2209\n",
                        "Seed: 43, Epoch: 041, Loss: 0.0538, Val AP: 0.2251, Test AP: 0.2267\n",
                        "Seed: 43, Epoch: 042, Loss: 0.0538, Val AP: 0.2147, Test AP: 0.2175\n",
                        "Seed: 43, Epoch: 043, Loss: 0.0537, Val AP: 0.2273, Test AP: 0.2309\n",
                        "Seed: 43, Epoch: 044, Loss: 0.0535, Val AP: 0.2179, Test AP: 0.2211\n",
                        "Seed: 43, Epoch: 045, Loss: 0.0536, Val AP: 0.2244, Test AP: 0.2283\n",
                        "Seed: 43, Epoch: 046, Loss: 0.0535, Val AP: 0.2182, Test AP: 0.2223\n",
                        "Seed: 43, Epoch: 047, Loss: 0.0535, Val AP: 0.1979, Test AP: 0.2010\n",
                        "Seed: 43, Epoch: 048, Loss: 0.0535, Val AP: 0.2173, Test AP: 0.2190\n",
                        "Seed: 43, Epoch: 049, Loss: 0.0533, Val AP: 0.2277, Test AP: 0.2313\n",
                        "Seed: 43, Epoch: 050, Loss: 0.0532, Val AP: 0.2237, Test AP: 0.2276\n",
                        "Seed: 43, Epoch: 051, Loss: 0.0532, Val AP: 0.2344, Test AP: 0.2394\n",
                        "Seed: 43, Epoch: 052, Loss: 0.0532, Val AP: 0.2306, Test AP: 0.2335\n",
                        "Seed: 43, Epoch: 053, Loss: 0.0532, Val AP: 0.2332, Test AP: 0.2355\n",
                        "Seed: 43, Epoch: 054, Loss: 0.0531, Val AP: 0.2404, Test AP: 0.2422\n",
                        "Seed: 43, Epoch: 055, Loss: 0.0531, Val AP: 0.2356, Test AP: 0.2406\n",
                        "Seed: 43, Epoch: 056, Loss: 0.0530, Val AP: 0.2372, Test AP: 0.2403\n",
                        "Seed: 43, Epoch: 057, Loss: 0.0531, Val AP: 0.2428, Test AP: 0.2461\n",
                        "Seed: 43, Epoch: 058, Loss: 0.0530, Val AP: 0.2244, Test AP: 0.2291\n",
                        "Seed: 43, Epoch: 059, Loss: 0.0529, Val AP: 0.2296, Test AP: 0.2338\n",
                        "Seed: 43, Epoch: 060, Loss: 0.0529, Val AP: 0.2320, Test AP: 0.2334\n",
                        "Seed: 43, Epoch: 061, Loss: 0.0530, Val AP: 0.2251, Test AP: 0.2275\n",
                        "Seed: 43, Epoch: 062, Loss: 0.0529, Val AP: 0.2283, Test AP: 0.2302\n",
                        "Seed: 43, Epoch: 063, Loss: 0.0527, Val AP: 0.2409, Test AP: 0.2426\n",
                        "Seed: 43, Epoch: 064, Loss: 0.0527, Val AP: 0.2306, Test AP: 0.2320\n",
                        "Seed: 43, Epoch: 065, Loss: 0.0527, Val AP: 0.2316, Test AP: 0.2341\n",
                        "Seed: 43, Epoch: 066, Loss: 0.0528, Val AP: 0.2403, Test AP: 0.2430\n",
                        "Seed: 43, Epoch: 067, Loss: 0.0527, Val AP: 0.2373, Test AP: 0.2405\n",
                        "Seed: 43, Epoch: 068, Loss: 0.0527, Val AP: 0.2439, Test AP: 0.2476\n",
                        "Seed: 43, Epoch: 069, Loss: 0.0526, Val AP: 0.2396, Test AP: 0.2422\n",
                        "Seed: 43, Epoch: 070, Loss: 0.0527, Val AP: 0.2398, Test AP: 0.2421\n",
                        "Seed: 43, Epoch: 071, Loss: 0.0525, Val AP: 0.2201, Test AP: 0.2224\n",
                        "Seed: 43, Epoch: 072, Loss: 0.0526, Val AP: 0.2425, Test AP: 0.2449\n",
                        "Seed: 43, Epoch: 073, Loss: 0.0525, Val AP: 0.2452, Test AP: 0.2478\n",
                        "Seed: 43, Epoch: 074, Loss: 0.0523, Val AP: 0.2386, Test AP: 0.2402\n",
                        "Seed: 43, Epoch: 075, Loss: 0.0524, Val AP: 0.2434, Test AP: 0.2463\n",
                        "Seed: 43, Epoch: 076, Loss: 0.0524, Val AP: 0.2423, Test AP: 0.2456\n",
                        "Seed: 43, Epoch: 077, Loss: 0.0523, Val AP: 0.2400, Test AP: 0.2415\n",
                        "Seed: 43, Epoch: 078, Loss: 0.0523, Val AP: 0.2519, Test AP: 0.2542\n",
                        "Seed: 43, Epoch: 079, Loss: 0.0523, Val AP: 0.2349, Test AP: 0.2378\n",
                        "Seed: 43, Epoch: 080, Loss: 0.0522, Val AP: 0.2319, Test AP: 0.2350\n",
                        "Seed: 43, Epoch: 081, Loss: 0.0523, Val AP: 0.2405, Test AP: 0.2417\n",
                        "Seed: 43, Epoch: 082, Loss: 0.0521, Val AP: 0.2499, Test AP: 0.2529\n",
                        "Seed: 43, Epoch: 083, Loss: 0.0521, Val AP: 0.2578, Test AP: 0.2612\n",
                        "Seed: 43, Epoch: 084, Loss: 0.0521, Val AP: 0.2472, Test AP: 0.2500\n",
                        "Seed: 43, Epoch: 085, Loss: 0.0521, Val AP: 0.2413, Test AP: 0.2454\n",
                        "Seed: 43, Epoch: 086, Loss: 0.0522, Val AP: 0.2500, Test AP: 0.2523\n",
                        "Seed: 43, Epoch: 087, Loss: 0.0521, Val AP: 0.2435, Test AP: 0.2441\n",
                        "Seed: 43, Epoch: 088, Loss: 0.0521, Val AP: 0.2432, Test AP: 0.2456\n",
                        "Seed: 43, Epoch: 089, Loss: 0.0520, Val AP: 0.2468, Test AP: 0.2483\n",
                        "Seed: 43, Epoch: 090, Loss: 0.0519, Val AP: 0.2461, Test AP: 0.2496\n",
                        "Seed: 43, Epoch: 091, Loss: 0.0519, Val AP: 0.2491, Test AP: 0.2527\n",
                        "Seed: 43, Epoch: 092, Loss: 0.0520, Val AP: 0.2520, Test AP: 0.2549\n",
                        "Seed: 43, Epoch: 093, Loss: 0.0521, Val AP: 0.2536, Test AP: 0.2571\n",
                        "Seed: 43, Epoch: 094, Loss: 0.0519, Val AP: 0.2519, Test AP: 0.2556\n",
                        "Seed: 43, Epoch: 095, Loss: 0.0519, Val AP: 0.2475, Test AP: 0.2501\n",
                        "Seed: 43, Epoch: 096, Loss: 0.0519, Val AP: 0.2507, Test AP: 0.2523\n",
                        "Seed: 43, Epoch: 097, Loss: 0.0518, Val AP: 0.2390, Test AP: 0.2434\n",
                        "Seed: 43, Epoch: 098, Loss: 0.0518, Val AP: 0.2475, Test AP: 0.2505\n",
                        "Seed: 43, Epoch: 099, Loss: 0.0519, Val AP: 0.2455, Test AP: 0.2485\n",
                        "Seed: 43, Epoch: 100, Loss: 0.0518, Val AP: 0.2473, Test AP: 0.2496\n",
                        "Seed: 43, Epoch: 101, Loss: 0.0518, Val AP: 0.2557, Test AP: 0.2583\n",
                        "Seed: 43, Epoch: 102, Loss: 0.0520, Val AP: 0.2491, Test AP: 0.2512\n",
                        "Seed: 43, Epoch: 103, Loss: 0.0518, Val AP: 0.2450, Test AP: 0.2468\n",
                        "Seed: 43, Epoch: 104, Loss: 0.0517, Val AP: 0.2509, Test AP: 0.2526\n",
                        "Seed: 43, Epoch: 105, Loss: 0.0517, Val AP: 0.2577, Test AP: 0.2601\n",
                        "Seed: 43, Epoch: 106, Loss: 0.0516, Val AP: 0.2604, Test AP: 0.2621\n",
                        "Seed: 43, Epoch: 107, Loss: 0.0517, Val AP: 0.2543, Test AP: 0.2561\n",
                        "Seed: 43, Epoch: 108, Loss: 0.0515, Val AP: 0.2466, Test AP: 0.2482\n",
                        "Seed: 43, Epoch: 109, Loss: 0.0516, Val AP: 0.2430, Test AP: 0.2459\n",
                        "Seed: 43, Epoch: 110, Loss: 0.0514, Val AP: 0.2447, Test AP: 0.2482\n",
                        "Seed: 43, Epoch: 111, Loss: 0.0514, Val AP: 0.2466, Test AP: 0.2499\n",
                        "Seed: 43, Epoch: 112, Loss: 0.0517, Val AP: 0.2593, Test AP: 0.2605\n",
                        "Seed: 43, Epoch: 113, Loss: 0.0516, Val AP: 0.2489, Test AP: 0.2490\n",
                        "Seed: 43, Epoch: 114, Loss: 0.0516, Val AP: 0.2509, Test AP: 0.2511\n",
                        "Seed: 43, Epoch: 115, Loss: 0.0515, Val AP: 0.2502, Test AP: 0.2521\n",
                        "Seed: 43, Epoch: 116, Loss: 0.0515, Val AP: 0.2611, Test AP: 0.2630\n",
                        "Seed: 43, Epoch: 117, Loss: 0.0515, Val AP: 0.2610, Test AP: 0.2643\n",
                        "Seed: 43, Epoch: 118, Loss: 0.0515, Val AP: 0.2570, Test AP: 0.2586\n",
                        "Seed: 43, Epoch: 119, Loss: 0.0516, Val AP: 0.2476, Test AP: 0.2469\n",
                        "Seed: 43, Epoch: 120, Loss: 0.0514, Val AP: 0.2534, Test AP: 0.2549\n",
                        "Seed: 43, Epoch: 121, Loss: 0.0515, Val AP: 0.2503, Test AP: 0.2524\n",
                        "Seed: 43, Epoch: 122, Loss: 0.0513, Val AP: 0.2630, Test AP: 0.2649\n",
                        "Seed: 43, Epoch: 123, Loss: 0.0512, Val AP: 0.2419, Test AP: 0.2423\n",
                        "Seed: 43, Epoch: 124, Loss: 0.0513, Val AP: 0.2631, Test AP: 0.2651\n",
                        "Seed: 43, Epoch: 125, Loss: 0.0512, Val AP: 0.2586, Test AP: 0.2612\n",
                        "Seed: 43, Epoch: 126, Loss: 0.0512, Val AP: 0.2680, Test AP: 0.2705\n",
                        "Seed: 43, Epoch: 127, Loss: 0.0513, Val AP: 0.2541, Test AP: 0.2581\n",
                        "Seed: 43, Epoch: 128, Loss: 0.0511, Val AP: 0.2436, Test AP: 0.2473\n",
                        "Seed: 43, Epoch: 129, Loss: 0.0513, Val AP: 0.2605, Test AP: 0.2638\n",
                        "Seed: 43, Epoch: 130, Loss: 0.0512, Val AP: 0.2472, Test AP: 0.2499\n",
                        "Seed: 43, Epoch: 131, Loss: 0.0512, Val AP: 0.2631, Test AP: 0.2642\n",
                        "Seed: 43, Epoch: 132, Loss: 0.0511, Val AP: 0.2579, Test AP: 0.2589\n",
                        "Seed: 43, Epoch: 133, Loss: 0.0511, Val AP: 0.2555, Test AP: 0.2566\n",
                        "Seed: 43, Epoch: 134, Loss: 0.0512, Val AP: 0.2452, Test AP: 0.2471\n",
                        "Seed: 43, Epoch: 135, Loss: 0.0510, Val AP: 0.2563, Test AP: 0.2595\n",
                        "Seed: 43, Epoch: 136, Loss: 0.0511, Val AP: 0.2424, Test AP: 0.2435\n",
                        "Seed: 43, Epoch: 137, Loss: 0.0510, Val AP: 0.2620, Test AP: 0.2643\n",
                        "Seed: 43, Epoch: 138, Loss: 0.0510, Val AP: 0.2611, Test AP: 0.2620\n",
                        "Seed: 43, Epoch: 139, Loss: 0.0513, Val AP: 0.2517, Test AP: 0.2540\n",
                        "Seed: 43, Epoch: 140, Loss: 0.0512, Val AP: 0.2648, Test AP: 0.2677\n",
                        "Seed: 43, Epoch: 141, Loss: 0.0510, Val AP: 0.2642, Test AP: 0.2655\n",
                        "Seed: 43, Epoch: 142, Loss: 0.0510, Val AP: 0.2719, Test AP: 0.2737\n",
                        "Seed: 43, Epoch: 143, Loss: 0.0510, Val AP: 0.2691, Test AP: 0.2721\n",
                        "Seed: 43, Epoch: 144, Loss: 0.0509, Val AP: 0.2663, Test AP: 0.2694\n",
                        "Seed: 43, Epoch: 145, Loss: 0.0510, Val AP: 0.2667, Test AP: 0.2687\n",
                        "Seed: 43, Epoch: 146, Loss: 0.0508, Val AP: 0.2551, Test AP: 0.2569\n",
                        "Seed: 43, Epoch: 147, Loss: 0.0510, Val AP: 0.2521, Test AP: 0.2545\n",
                        "Seed: 43, Epoch: 148, Loss: 0.0509, Val AP: 0.2483, Test AP: 0.2497\n",
                        "Seed: 43, Epoch: 149, Loss: 0.0510, Val AP: 0.2518, Test AP: 0.2537\n",
                        "Seed: 43, Epoch: 150, Loss: 0.0509, Val AP: 0.2540, Test AP: 0.2567\n",
                        "Seed: 44, Epoch: 001, Loss: 0.0620, Val AP: 0.1207, Test AP: 0.1185\n",
                        "Seed: 44, Epoch: 002, Loss: 0.0602, Val AP: 0.1177, Test AP: 0.1166\n",
                        "Seed: 44, Epoch: 003, Loss: 0.0599, Val AP: 0.1173, Test AP: 0.1161\n",
                        "Seed: 44, Epoch: 004, Loss: 0.0595, Val AP: 0.1303, Test AP: 0.1279\n",
                        "Seed: 44, Epoch: 005, Loss: 0.0594, Val AP: 0.1339, Test AP: 0.1304\n",
                        "Seed: 44, Epoch: 006, Loss: 0.0591, Val AP: 0.1340, Test AP: 0.1323\n",
                        "Seed: 44, Epoch: 007, Loss: 0.0589, Val AP: 0.1335, Test AP: 0.1306\n",
                        "Seed: 44, Epoch: 008, Loss: 0.0588, Val AP: 0.1402, Test AP: 0.1370\n",
                        "Seed: 44, Epoch: 009, Loss: 0.0585, Val AP: 0.1458, Test AP: 0.1435\n",
                        "Seed: 44, Epoch: 010, Loss: 0.0584, Val AP: 0.1459, Test AP: 0.1417\n",
                        "Seed: 44, Epoch: 011, Loss: 0.0582, Val AP: 0.1457, Test AP: 0.1407\n",
                        "Seed: 44, Epoch: 012, Loss: 0.0579, Val AP: 0.1455, Test AP: 0.1419\n",
                        "Seed: 44, Epoch: 013, Loss: 0.0576, Val AP: 0.1578, Test AP: 0.1530\n",
                        "Seed: 44, Epoch: 014, Loss: 0.0573, Val AP: 0.1466, Test AP: 0.1416\n",
                        "Seed: 44, Epoch: 015, Loss: 0.0570, Val AP: 0.1664, Test AP: 0.1607\n",
                        "Seed: 44, Epoch: 016, Loss: 0.0568, Val AP: 0.1691, Test AP: 0.1644\n",
                        "Seed: 44, Epoch: 017, Loss: 0.0566, Val AP: 0.1770, Test AP: 0.1725\n",
                        "Seed: 44, Epoch: 018, Loss: 0.0564, Val AP: 0.1827, Test AP: 0.1779\n",
                        "Seed: 44, Epoch: 019, Loss: 0.0562, Val AP: 0.1769, Test AP: 0.1731\n",
                        "Seed: 44, Epoch: 020, Loss: 0.0561, Val AP: 0.1863, Test AP: 0.1818\n",
                        "Seed: 44, Epoch: 021, Loss: 0.0559, Val AP: 0.1928, Test AP: 0.1878\n",
                        "Seed: 44, Epoch: 022, Loss: 0.0558, Val AP: 0.1849, Test AP: 0.1819\n",
                        "Seed: 44, Epoch: 023, Loss: 0.0556, Val AP: 0.1849, Test AP: 0.1824\n",
                        "Seed: 44, Epoch: 024, Loss: 0.0556, Val AP: 0.1928, Test AP: 0.1903\n",
                        "Seed: 44, Epoch: 025, Loss: 0.0555, Val AP: 0.1964, Test AP: 0.1936\n",
                        "Seed: 44, Epoch: 026, Loss: 0.0553, Val AP: 0.1897, Test AP: 0.1868\n",
                        "Seed: 44, Epoch: 027, Loss: 0.0552, Val AP: 0.1943, Test AP: 0.1908\n",
                        "Seed: 44, Epoch: 028, Loss: 0.0552, Val AP: 0.1933, Test AP: 0.1921\n",
                        "Seed: 44, Epoch: 029, Loss: 0.0550, Val AP: 0.1738, Test AP: 0.1718\n",
                        "Seed: 44, Epoch: 030, Loss: 0.0551, Val AP: 0.1956, Test AP: 0.1925\n",
                        "Seed: 44, Epoch: 031, Loss: 0.0549, Val AP: 0.2010, Test AP: 0.1963\n",
                        "Seed: 44, Epoch: 032, Loss: 0.0547, Val AP: 0.1961, Test AP: 0.1936\n",
                        "Seed: 44, Epoch: 033, Loss: 0.0547, Val AP: 0.1997, Test AP: 0.1949\n",
                        "Seed: 44, Epoch: 034, Loss: 0.0547, Val AP: 0.1944, Test AP: 0.1910\n",
                        "Seed: 44, Epoch: 035, Loss: 0.0545, Val AP: 0.1848, Test AP: 0.1805\n",
                        "Seed: 44, Epoch: 036, Loss: 0.0546, Val AP: 0.1981, Test AP: 0.1946\n",
                        "Seed: 44, Epoch: 037, Loss: 0.0545, Val AP: 0.1922, Test AP: 0.1890\n",
                        "Seed: 44, Epoch: 038, Loss: 0.0544, Val AP: 0.2055, Test AP: 0.2009\n",
                        "Seed: 44, Epoch: 039, Loss: 0.0544, Val AP: 0.1911, Test AP: 0.1856\n",
                        "Seed: 44, Epoch: 040, Loss: 0.0544, Val AP: 0.2168, Test AP: 0.2111\n",
                        "Seed: 44, Epoch: 041, Loss: 0.0543, Val AP: 0.1910, Test AP: 0.1845\n",
                        "Seed: 44, Epoch: 042, Loss: 0.0543, Val AP: 0.1711, Test AP: 0.1661\n",
                        "Seed: 44, Epoch: 043, Loss: 0.0543, Val AP: 0.2018, Test AP: 0.1996\n",
                        "Seed: 44, Epoch: 044, Loss: 0.0541, Val AP: 0.1988, Test AP: 0.1962\n",
                        "Seed: 44, Epoch: 045, Loss: 0.0542, Val AP: 0.2098, Test AP: 0.2035\n",
                        "Seed: 44, Epoch: 046, Loss: 0.0541, Val AP: 0.2079, Test AP: 0.2049\n",
                        "Seed: 44, Epoch: 047, Loss: 0.0540, Val AP: 0.2185, Test AP: 0.2122\n",
                        "Seed: 44, Epoch: 048, Loss: 0.0542, Val AP: 0.2169, Test AP: 0.2112\n",
                        "Seed: 44, Epoch: 049, Loss: 0.0540, Val AP: 0.2057, Test AP: 0.2026\n",
                        "Seed: 44, Epoch: 050, Loss: 0.0540, Val AP: 0.2148, Test AP: 0.2107\n",
                        "Seed: 44, Epoch: 051, Loss: 0.0542, Val AP: 0.2101, Test AP: 0.2068\n",
                        "Seed: 44, Epoch: 052, Loss: 0.0539, Val AP: 0.2163, Test AP: 0.2131\n",
                        "Seed: 44, Epoch: 053, Loss: 0.0538, Val AP: 0.2011, Test AP: 0.1990\n",
                        "Seed: 44, Epoch: 054, Loss: 0.0538, Val AP: 0.2053, Test AP: 0.2029\n",
                        "Seed: 44, Epoch: 055, Loss: 0.0538, Val AP: 0.2280, Test AP: 0.2236\n",
                        "Seed: 44, Epoch: 056, Loss: 0.0537, Val AP: 0.2182, Test AP: 0.2148\n",
                        "Seed: 44, Epoch: 057, Loss: 0.0536, Val AP: 0.2153, Test AP: 0.2128\n",
                        "Seed: 44, Epoch: 058, Loss: 0.0536, Val AP: 0.2129, Test AP: 0.2090\n",
                        "Seed: 44, Epoch: 059, Loss: 0.0537, Val AP: 0.2231, Test AP: 0.2218\n",
                        "Seed: 44, Epoch: 060, Loss: 0.0535, Val AP: 0.2236, Test AP: 0.2215\n",
                        "Seed: 44, Epoch: 061, Loss: 0.0536, Val AP: 0.1987, Test AP: 0.1984\n",
                        "Seed: 44, Epoch: 062, Loss: 0.0535, Val AP: 0.2015, Test AP: 0.1991\n",
                        "Seed: 44, Epoch: 063, Loss: 0.0536, Val AP: 0.2176, Test AP: 0.2164\n",
                        "Seed: 44, Epoch: 064, Loss: 0.0535, Val AP: 0.2357, Test AP: 0.2314\n",
                        "Seed: 44, Epoch: 065, Loss: 0.0533, Val AP: 0.2237, Test AP: 0.2192\n",
                        "Seed: 44, Epoch: 066, Loss: 0.0534, Val AP: 0.2321, Test AP: 0.2258\n",
                        "Seed: 44, Epoch: 067, Loss: 0.0534, Val AP: 0.2267, Test AP: 0.2245\n",
                        "Seed: 44, Epoch: 068, Loss: 0.0534, Val AP: 0.2269, Test AP: 0.2238\n",
                        "Seed: 44, Epoch: 069, Loss: 0.0532, Val AP: 0.2266, Test AP: 0.2241\n",
                        "Seed: 44, Epoch: 070, Loss: 0.0533, Val AP: 0.2203, Test AP: 0.2176\n",
                        "Seed: 44, Epoch: 071, Loss: 0.0532, Val AP: 0.2255, Test AP: 0.2220\n",
                        "Seed: 44, Epoch: 072, Loss: 0.0532, Val AP: 0.2343, Test AP: 0.2302\n",
                        "Seed: 44, Epoch: 073, Loss: 0.0531, Val AP: 0.2204, Test AP: 0.2204\n",
                        "Seed: 44, Epoch: 074, Loss: 0.0532, Val AP: 0.2312, Test AP: 0.2268\n",
                        "Seed: 44, Epoch: 075, Loss: 0.0534, Val AP: 0.2246, Test AP: 0.2223\n",
                        "Seed: 44, Epoch: 076, Loss: 0.0531, Val AP: 0.2005, Test AP: 0.1996\n",
                        "Seed: 44, Epoch: 077, Loss: 0.0533, Val AP: 0.2397, Test AP: 0.2367\n",
                        "Seed: 44, Epoch: 078, Loss: 0.0530, Val AP: 0.2345, Test AP: 0.2310\n",
                        "Seed: 44, Epoch: 079, Loss: 0.0530, Val AP: 0.2142, Test AP: 0.2136\n",
                        "Seed: 44, Epoch: 080, Loss: 0.0529, Val AP: 0.2257, Test AP: 0.2235\n",
                        "Seed: 44, Epoch: 081, Loss: 0.0530, Val AP: 0.2296, Test AP: 0.2270\n",
                        "Seed: 44, Epoch: 082, Loss: 0.0530, Val AP: 0.2359, Test AP: 0.2335\n",
                        "Seed: 44, Epoch: 083, Loss: 0.0530, Val AP: 0.2353, Test AP: 0.2333\n",
                        "Seed: 44, Epoch: 084, Loss: 0.0529, Val AP: 0.2224, Test AP: 0.2211\n",
                        "Seed: 44, Epoch: 085, Loss: 0.0529, Val AP: 0.2405, Test AP: 0.2370\n",
                        "Seed: 44, Epoch: 086, Loss: 0.0529, Val AP: 0.2210, Test AP: 0.2210\n",
                        "Seed: 44, Epoch: 087, Loss: 0.0529, Val AP: 0.2299, Test AP: 0.2288\n",
                        "Seed: 44, Epoch: 088, Loss: 0.0529, Val AP: 0.2362, Test AP: 0.2335\n",
                        "Seed: 44, Epoch: 089, Loss: 0.0528, Val AP: 0.2235, Test AP: 0.2235\n",
                        "Seed: 44, Epoch: 090, Loss: 0.0528, Val AP: 0.2323, Test AP: 0.2288\n",
                        "Seed: 44, Epoch: 091, Loss: 0.0529, Val AP: 0.2133, Test AP: 0.2119\n",
                        "Seed: 44, Epoch: 092, Loss: 0.0530, Val AP: 0.2372, Test AP: 0.2342\n",
                        "Seed: 44, Epoch: 093, Loss: 0.0528, Val AP: 0.2411, Test AP: 0.2382\n",
                        "Seed: 44, Epoch: 094, Loss: 0.0526, Val AP: 0.2330, Test AP: 0.2310\n",
                        "Seed: 44, Epoch: 095, Loss: 0.0527, Val AP: 0.2279, Test AP: 0.2262\n",
                        "Seed: 44, Epoch: 096, Loss: 0.0527, Val AP: 0.2376, Test AP: 0.2346\n",
                        "Seed: 44, Epoch: 097, Loss: 0.0527, Val AP: 0.2319, Test AP: 0.2302\n",
                        "Seed: 44, Epoch: 098, Loss: 0.0526, Val AP: 0.2321, Test AP: 0.2316\n",
                        "Seed: 44, Epoch: 099, Loss: 0.0527, Val AP: 0.2326, Test AP: 0.2316\n",
                        "Seed: 44, Epoch: 100, Loss: 0.0527, Val AP: 0.2256, Test AP: 0.2236\n",
                        "Seed: 44, Epoch: 101, Loss: 0.0527, Val AP: 0.2202, Test AP: 0.2199\n",
                        "Seed: 44, Epoch: 102, Loss: 0.0525, Val AP: 0.2382, Test AP: 0.2368\n",
                        "Seed: 44, Epoch: 103, Loss: 0.0527, Val AP: 0.2399, Test AP: 0.2372\n",
                        "Seed: 44, Epoch: 104, Loss: 0.0525, Val AP: 0.2440, Test AP: 0.2411\n",
                        "Seed: 44, Epoch: 105, Loss: 0.0525, Val AP: 0.2270, Test AP: 0.2237\n",
                        "Seed: 44, Epoch: 106, Loss: 0.0525, Val AP: 0.2369, Test AP: 0.2355\n",
                        "Seed: 44, Epoch: 107, Loss: 0.0524, Val AP: 0.2464, Test AP: 0.2458\n",
                        "Seed: 44, Epoch: 108, Loss: 0.0524, Val AP: 0.2432, Test AP: 0.2407\n",
                        "Seed: 44, Epoch: 109, Loss: 0.0525, Val AP: 0.2321, Test AP: 0.2319\n",
                        "Seed: 44, Epoch: 110, Loss: 0.0524, Val AP: 0.2343, Test AP: 0.2320\n",
                        "Seed: 44, Epoch: 111, Loss: 0.0525, Val AP: 0.2404, Test AP: 0.2388\n",
                        "Seed: 44, Epoch: 112, Loss: 0.0524, Val AP: 0.2352, Test AP: 0.2333\n",
                        "Seed: 44, Epoch: 113, Loss: 0.0524, Val AP: 0.2474, Test AP: 0.2448\n",
                        "Seed: 44, Epoch: 114, Loss: 0.0525, Val AP: 0.2168, Test AP: 0.2156\n",
                        "Seed: 44, Epoch: 115, Loss: 0.0524, Val AP: 0.2387, Test AP: 0.2372\n",
                        "Seed: 44, Epoch: 116, Loss: 0.0523, Val AP: 0.2309, Test AP: 0.2294\n",
                        "Seed: 44, Epoch: 117, Loss: 0.0524, Val AP: 0.2235, Test AP: 0.2229\n",
                        "Seed: 44, Epoch: 118, Loss: 0.0525, Val AP: 0.2350, Test AP: 0.2330\n",
                        "Seed: 44, Epoch: 119, Loss: 0.0524, Val AP: 0.2493, Test AP: 0.2445\n",
                        "Seed: 44, Epoch: 120, Loss: 0.0524, Val AP: 0.2373, Test AP: 0.2365\n",
                        "Seed: 44, Epoch: 121, Loss: 0.0523, Val AP: 0.2362, Test AP: 0.2341\n",
                        "Seed: 44, Epoch: 122, Loss: 0.0524, Val AP: 0.2316, Test AP: 0.2311\n",
                        "Seed: 44, Epoch: 123, Loss: 0.0524, Val AP: 0.2442, Test AP: 0.2421\n",
                        "Seed: 44, Epoch: 124, Loss: 0.0523, Val AP: 0.2419, Test AP: 0.2413\n",
                        "Seed: 44, Epoch: 125, Loss: 0.0523, Val AP: 0.2430, Test AP: 0.2415\n",
                        "Seed: 44, Epoch: 126, Loss: 0.0522, Val AP: 0.2482, Test AP: 0.2453\n",
                        "Seed: 44, Epoch: 127, Loss: 0.0522, Val AP: 0.2345, Test AP: 0.2343\n",
                        "Seed: 44, Epoch: 128, Loss: 0.0523, Val AP: 0.2433, Test AP: 0.2418\n",
                        "Seed: 44, Epoch: 129, Loss: 0.0521, Val AP: 0.2431, Test AP: 0.2408\n",
                        "Seed: 44, Epoch: 130, Loss: 0.0522, Val AP: 0.2490, Test AP: 0.2478\n",
                        "Seed: 44, Epoch: 131, Loss: 0.0522, Val AP: 0.2367, Test AP: 0.2357\n",
                        "Seed: 44, Epoch: 132, Loss: 0.0521, Val AP: 0.2363, Test AP: 0.2352\n",
                        "Seed: 44, Epoch: 133, Loss: 0.0523, Val AP: 0.2345, Test AP: 0.2321\n",
                        "Seed: 44, Epoch: 134, Loss: 0.0521, Val AP: 0.2396, Test AP: 0.2390\n",
                        "Seed: 44, Epoch: 135, Loss: 0.0521, Val AP: 0.2428, Test AP: 0.2409\n",
                        "Seed: 44, Epoch: 136, Loss: 0.0520, Val AP: 0.2497, Test AP: 0.2462\n",
                        "Seed: 44, Epoch: 137, Loss: 0.0521, Val AP: 0.2451, Test AP: 0.2423\n",
                        "Seed: 44, Epoch: 138, Loss: 0.0521, Val AP: 0.2417, Test AP: 0.2396\n",
                        "Seed: 44, Epoch: 139, Loss: 0.0521, Val AP: 0.2396, Test AP: 0.2374\n",
                        "Seed: 44, Epoch: 140, Loss: 0.0520, Val AP: 0.2552, Test AP: 0.2512\n",
                        "Seed: 44, Epoch: 141, Loss: 0.0521, Val AP: 0.2350, Test AP: 0.2336\n",
                        "Seed: 44, Epoch: 142, Loss: 0.0520, Val AP: 0.2487, Test AP: 0.2463\n",
                        "Seed: 44, Epoch: 143, Loss: 0.0521, Val AP: 0.2533, Test AP: 0.2499\n",
                        "Seed: 44, Epoch: 144, Loss: 0.0520, Val AP: 0.2515, Test AP: 0.2499\n",
                        "Seed: 44, Epoch: 145, Loss: 0.0520, Val AP: 0.2403, Test AP: 0.2382\n",
                        "Seed: 44, Epoch: 146, Loss: 0.0520, Val AP: 0.2432, Test AP: 0.2407\n",
                        "Seed: 44, Epoch: 147, Loss: 0.0520, Val AP: 0.2402, Test AP: 0.2376\n",
                        "Seed: 44, Epoch: 148, Loss: 0.0519, Val AP: 0.2354, Test AP: 0.2346\n",
                        "Seed: 44, Epoch: 149, Loss: 0.0520, Val AP: 0.2436, Test AP: 0.2409\n",
                        "Seed: 44, Epoch: 150, Loss: 0.0520, Val AP: 0.2388, Test AP: 0.2382\n",
                        "Average Time: 8839.86 seconds\n",
                        "Var Time: 6176.02 seconds\n",
                        "Average Memory: 4142.00 MB\n",
                        "Average Best Val AP: 0.2714\n",
                        "Std Best Test AP: 0.0136\n",
                        "Average Test AP: 0.2695\n"
                    ]
                }
            ],
            "source": "import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nclass HierarchicalGCN_GSA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_GSA, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, improved = True)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = GSAPool(512, pooling_ratio=0.9, alpha = 0.6, cus_drop_ratio = 0)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, improved = True)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = GSAPool(512, pooling_ratio=0.9, alpha = 0.6, cus_drop_ratio = 0)\n        self.conv3 = GCNConv(hidden_channels, out_channels, improved = True)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        init_x = data.x\n        x = x.float()\n        init_x = init_x.float()\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, perm_1, x_ae1 = self.pool1(x, edge_index, None, batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, perm_1, x_ae1 = self.pool2(x, edge_index, None, batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False)\n    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### HGPSLPool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "tensor([0.5344, 0.0000, 0.0000, 0.4656, 0.0613, 0.0000, 0.0000, 0.0000, 0.5640,\n",
                        "        0.3748])\n"
                    ]
                }
            ],
            "source": "from torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom torch_geometric.nn import GCNConv\nfrom torch.nn import Parameter\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom typing import Union, Optional, Callable\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax\nimport math\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, GCNConv, GATConv, ChebConv, GraphConv\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\nfrom torch_scatter import scatter_add\nfrom torch_sparse import spspmm, coalesce\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nfrom torch_scatter import scatter_add, scatter_max\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = torch.nonzero(x > scores_min).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n            num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_weight, perm, num_nodes=None):\n        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n        mask = perm.new_full((num_nodes, ), -1)\n        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n        mask[perm] = i\n        row, col = edge_index\n        row, col = mask[row], mask[col]\n        mask = (row >= 0) & (col >= 0)\n        row, col = row[mask], col[mask]\n        if edge_weight is not None:\n            edge_weight = edge_weight[mask]\n        return torch.stack([row, col], dim=0), edge_weight\ndef scatter_sort(x, batch, fill_value=-1e16):\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n    dense_x[index] = x\n    dense_x = dense_x.view(batch_size, max_num_nodes)\n    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n    sorted_x = sorted_x.view(-1)\n    filled_index = sorted_x != fill_value\n    sorted_x = sorted_x[filled_index]\n    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n    return sorted_x, cumsum_sorted_x\ndef _make_ix_like(batch):\n    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n    idx = torch.cat(idx, dim=0)\n    return idx\ndef _threshold_and_support(x, batch):\n    \"\"\"Sparsemax building block: compute the threshold\n    Args:\n        x: input tensor to apply the sparsemax\n        batch: group indicators\n    Returns:\n        the threshold value\n    \"\"\"\n    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n    sorted_input, input_cumsum = scatter_sort(x, batch)\n    input_cumsum = input_cumsum - 1.0\n    rhos = _make_ix_like(batch).to(x.dtype)\n    support = rhos * sorted_input > input_cumsum\n    support_size = scatter_add(support.to(batch.dtype), batch)\n    idx = support_size + cum_num_nodes - 1\n    mask = idx < 0\n    idx[mask] = 0\n    tau = input_cumsum.gather(0, idx)\n    tau /= support_size.to(x.dtype)\n    return tau, support_size\nclass SparsemaxFunction(Function):\n    @staticmethod\n    def forward(ctx, x, batch):\n        \"\"\"sparsemax: normalizing sparse transform\n        Parameters:\n            ctx: context object\n            x (Tensor): shape (N, )\n            batch: group indicator\n        Returns:\n            output (Tensor): same shape as input\n        \"\"\"\n        max_val, _ = scatter_max(x, batch)\n        x -= max_val[batch]\n        tau, supp_size = _threshold_and_support(x, batch)\n        output = torch.clamp(x - tau[batch], min=0)\n        ctx.save_for_backward(supp_size, output, batch)\n        return output\n    @staticmethod\n    def backward(ctx, grad_output):\n        supp_size, output, batch = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[output == 0] = 0\n        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n        return grad_input, None\nsparsemax = SparsemaxFunction.apply\nclass Sparsemax(nn.Module):\n    def __init__(self):\n        super(Sparsemax, self).__init__()\n    def forward(self, x, batch):\n        return sparsemax(x, batch)\nif __name__ == '__main__':\n    sparse_attention = Sparsemax()\n    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n    res = sparse_attention(input_x, input_batch)\n    print(res)\nclass TwoHopNeighborhood(object):\n    def __call__(self, data):\n        edge_index, edge_attr = data.edge_index, data.edge_attr\n        n = data.num_nodes\n        fill = 1e16\n        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n        edge_index = torch.cat([edge_index, index], dim=1)\n        if edge_attr is None:\n            data.edge_index, _ = coalesce(edge_index, None, n, n)\n        else:\n            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n            value = value.expand(-1, *list(edge_attr.size())[1:])\n            edge_attr = torch.cat([edge_attr, value], dim=0)\n            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min', fill_value=fill)\n            edge_attr[edge_attr >= fill] = 0\n            data.edge_attr = edge_attr\n        return data\n    def __repr__(self):\n        return '{}()'.format(self.__class__.__name__)\nclass GCN(MessagePassing):\n    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n        super(GCN, self).__init__(aggr='add', **kwargs)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n        nn.init.xavier_uniform_(self.weight.data)\n        if bias:\n            self.bias = Parameter(torch.Tensor(out_channels))\n            nn.init.zeros_(self.bias.data)\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n    def reset_parameters(self):\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight=None):\n        x = torch.matmul(x, self.weight)\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        if self.bias is not None:\n            aggr_out = aggr_out + self.bias\n        return aggr_out\n    def __repr__(self):\n        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) \n        edge_index = edge_index.type(torch.long)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass HGPSLPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n        super(HGPSLPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.sample = sample\n        self.sparse = sparse\n        self.sl = sl\n        self.negative_slop = negative_slop\n        self.lamb = lamb\n        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n        nn.init.xavier_uniform_(self.att.data)\n        self.sparse_attention = Sparsemax()\n        self.neighbor_augment = TwoHopNeighborhood()\n        self.calc_information_score = NodeInformationScore()\n    def forward(self, x, edge_index, edge_attr, batch):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        original_x = x\n        perm = topk(score, self.ratio, batch)\n        x = x[perm]\n        batch = batch[perm]\n        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        if self.sl is False:\n            return x, induced_edge_index, induced_edge_attr, batch\n        if self.sample:\n            k_hop = 3\n            if edge_attr is None:\n                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n            for _ in range(k_hop - 1):\n                hop_data = self.neighbor_augment(hop_data)\n            hop_edge_index = hop_data.edge_index\n            hop_edge_attr = hop_data.edge_attr\n            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            adj[row, col] = weights\n            new_edge_index, weights = dense_to_sparse(adj)\n            row, col = new_edge_index\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        else:\n            if edge_attr is None:\n                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n                                               device=induced_edge_index.device)\n            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n            cum_num_nodes = num_nodes.cumsum(dim=0)\n            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n            new_edge_index, _ = dense_to_sparse(adj)\n            row, col = new_edge_index\n            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n            weights = F.leaky_relu(weights, self.negative_slop)\n            adj[row, col] = weights\n            induced_row, induced_col = induced_edge_index\n            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n            weights = adj[row, col]\n            if self.sparse:\n                new_edge_attr = self.sparse_attention(weights, row)\n            else:\n                new_edge_attr = softmax(weights, row, num_nodes=x.size(0))\n            adj[row, col] = new_edge_attr\n            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n            del adj\n            torch.cuda.empty_cache()\n        return x, new_edge_index, new_edge_attr, batch"
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0612, Val AP: 0.1202, Test AP: 0.1205\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0599, Val AP: 0.1202, Test AP: 0.1199\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0594, Val AP: 0.1389, Test AP: 0.1392\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0590, Val AP: 0.1405, Test AP: 0.1412\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0587, Val AP: 0.1402, Test AP: 0.1404\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0584, Val AP: 0.1404, Test AP: 0.1393\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0581, Val AP: 0.1557, Test AP: 0.1555\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0578, Val AP: 0.1526, Test AP: 0.1530\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0577, Val AP: 0.1616, Test AP: 0.1607\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0576, Val AP: 0.1534, Test AP: 0.1524\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0573, Val AP: 0.1612, Test AP: 0.1602\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0571, Val AP: 0.1707, Test AP: 0.1697\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0569, Val AP: 0.1798, Test AP: 0.1793\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0568, Val AP: 0.1805, Test AP: 0.1781\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0566, Val AP: 0.1752, Test AP: 0.1732\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0564, Val AP: 0.1772, Test AP: 0.1750\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0563, Val AP: 0.1862, Test AP: 0.1829\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0562, Val AP: 0.1892, Test AP: 0.1858\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0561, Val AP: 0.1908, Test AP: 0.1892\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0560, Val AP: 0.1876, Test AP: 0.1843\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0559, Val AP: 0.1916, Test AP: 0.1896\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0558, Val AP: 0.1648, Test AP: 0.1647\n",
                        "Seed: 42, Epoch: 023, Loss: 0.0558, Val AP: 0.1985, Test AP: 0.1953\n",
                        "Seed: 42, Epoch: 024, Loss: 0.0557, Val AP: 0.1750, Test AP: 0.1722\n",
                        "Seed: 42, Epoch: 025, Loss: 0.0556, Val AP: 0.1891, Test AP: 0.1880\n",
                        "Seed: 42, Epoch: 026, Loss: 0.0556, Val AP: 0.1919, Test AP: 0.1893\n",
                        "Seed: 42, Epoch: 027, Loss: 0.0554, Val AP: 0.1970, Test AP: 0.1922\n",
                        "Seed: 42, Epoch: 028, Loss: 0.0554, Val AP: 0.2036, Test AP: 0.1996\n",
                        "Seed: 42, Epoch: 029, Loss: 0.0554, Val AP: 0.1948, Test AP: 0.1910\n",
                        "Seed: 42, Epoch: 030, Loss: 0.0553, Val AP: 0.1848, Test AP: 0.1812\n",
                        "Seed: 42, Epoch: 031, Loss: 0.0552, Val AP: 0.2025, Test AP: 0.1997\n",
                        "Seed: 42, Epoch: 032, Loss: 0.0552, Val AP: 0.1935, Test AP: 0.1916\n",
                        "Seed: 42, Epoch: 033, Loss: 0.0552, Val AP: 0.1949, Test AP: 0.1920\n",
                        "Seed: 42, Epoch: 034, Loss: 0.0551, Val AP: 0.1869, Test AP: 0.1870\n",
                        "Seed: 42, Epoch: 035, Loss: 0.0551, Val AP: 0.2013, Test AP: 0.1979\n",
                        "Seed: 42, Epoch: 036, Loss: 0.0550, Val AP: 0.1939, Test AP: 0.1898\n",
                        "Seed: 42, Epoch: 037, Loss: 0.0550, Val AP: 0.1901, Test AP: 0.1875\n",
                        "Seed: 42, Epoch: 038, Loss: 0.0550, Val AP: 0.1895, Test AP: 0.1874\n",
                        "Seed: 42, Epoch: 039, Loss: 0.0549, Val AP: 0.2033, Test AP: 0.2001\n",
                        "Seed: 42, Epoch: 040, Loss: 0.0549, Val AP: 0.1930, Test AP: 0.1894\n",
                        "Seed: 42, Epoch: 041, Loss: 0.0548, Val AP: 0.1937, Test AP: 0.1925\n",
                        "Seed: 42, Epoch: 042, Loss: 0.0548, Val AP: 0.1976, Test AP: 0.1950\n",
                        "Seed: 42, Epoch: 043, Loss: 0.0547, Val AP: 0.2018, Test AP: 0.1979\n",
                        "Seed: 42, Epoch: 044, Loss: 0.0547, Val AP: 0.2068, Test AP: 0.2037\n",
                        "Seed: 42, Epoch: 045, Loss: 0.0546, Val AP: 0.1799, Test AP: 0.1749\n",
                        "Seed: 42, Epoch: 046, Loss: 0.0548, Val AP: 0.2082, Test AP: 0.2047\n",
                        "Seed: 42, Epoch: 047, Loss: 0.0546, Val AP: 0.2058, Test AP: 0.2023\n",
                        "Seed: 42, Epoch: 048, Loss: 0.0546, Val AP: 0.2169, Test AP: 0.2142\n",
                        "Seed: 42, Epoch: 049, Loss: 0.0546, Val AP: 0.2114, Test AP: 0.2085\n",
                        "Seed: 42, Epoch: 050, Loss: 0.0545, Val AP: 0.2064, Test AP: 0.2036\n",
                        "Seed: 42, Epoch: 051, Loss: 0.0545, Val AP: 0.2168, Test AP: 0.2151\n",
                        "Seed: 42, Epoch: 052, Loss: 0.0545, Val AP: 0.2117, Test AP: 0.2072\n",
                        "Seed: 42, Epoch: 053, Loss: 0.0545, Val AP: 0.2034, Test AP: 0.2018\n",
                        "Seed: 42, Epoch: 054, Loss: 0.0544, Val AP: 0.2124, Test AP: 0.2117\n",
                        "Seed: 42, Epoch: 055, Loss: 0.0544, Val AP: 0.2181, Test AP: 0.2154\n",
                        "Seed: 42, Epoch: 056, Loss: 0.0544, Val AP: 0.2117, Test AP: 0.2080\n",
                        "Seed: 42, Epoch: 057, Loss: 0.0543, Val AP: 0.2129, Test AP: 0.2090\n",
                        "Seed: 42, Epoch: 058, Loss: 0.0544, Val AP: 0.2062, Test AP: 0.2014\n",
                        "Seed: 42, Epoch: 059, Loss: 0.0543, Val AP: 0.2179, Test AP: 0.2139\n",
                        "Seed: 42, Epoch: 060, Loss: 0.0543, Val AP: 0.2009, Test AP: 0.1966\n",
                        "Seed: 42, Epoch: 061, Loss: 0.0543, Val AP: 0.2115, Test AP: 0.2078\n",
                        "Seed: 42, Epoch: 062, Loss: 0.0543, Val AP: 0.2180, Test AP: 0.2147\n",
                        "Seed: 42, Epoch: 063, Loss: 0.0543, Val AP: 0.2138, Test AP: 0.2116\n",
                        "Seed: 42, Epoch: 064, Loss: 0.0542, Val AP: 0.2043, Test AP: 0.2007\n",
                        "Seed: 42, Epoch: 065, Loss: 0.0542, Val AP: 0.2113, Test AP: 0.2083\n",
                        "Seed: 42, Epoch: 066, Loss: 0.0542, Val AP: 0.2144, Test AP: 0.2100\n",
                        "Seed: 42, Epoch: 067, Loss: 0.0542, Val AP: 0.2056, Test AP: 0.2019\n",
                        "Seed: 42, Epoch: 068, Loss: 0.0541, Val AP: 0.2220, Test AP: 0.2179\n",
                        "Seed: 42, Epoch: 069, Loss: 0.0542, Val AP: 0.2169, Test AP: 0.2142\n",
                        "Seed: 42, Epoch: 070, Loss: 0.0542, Val AP: 0.1954, Test AP: 0.1910\n",
                        "Seed: 42, Epoch: 071, Loss: 0.0541, Val AP: 0.2163, Test AP: 0.2141\n",
                        "Seed: 42, Epoch: 072, Loss: 0.0541, Val AP: 0.2191, Test AP: 0.2156\n",
                        "Seed: 42, Epoch: 073, Loss: 0.0541, Val AP: 0.2099, Test AP: 0.2058\n",
                        "Seed: 42, Epoch: 074, Loss: 0.0540, Val AP: 0.2182, Test AP: 0.2144\n",
                        "Seed: 42, Epoch: 075, Loss: 0.0540, Val AP: 0.2178, Test AP: 0.2144\n",
                        "Seed: 42, Epoch: 076, Loss: 0.0541, Val AP: 0.2198, Test AP: 0.2179\n",
                        "Seed: 42, Epoch: 077, Loss: 0.0541, Val AP: 0.2195, Test AP: 0.2158\n",
                        "Seed: 42, Epoch: 078, Loss: 0.0539, Val AP: 0.2194, Test AP: 0.2156\n",
                        "Seed: 42, Epoch: 079, Loss: 0.0539, Val AP: 0.2140, Test AP: 0.2102\n",
                        "Seed: 42, Epoch: 080, Loss: 0.0540, Val AP: 0.2117, Test AP: 0.2076\n",
                        "Seed: 42, Epoch: 081, Loss: 0.0540, Val AP: 0.2076, Test AP: 0.2045\n",
                        "Seed: 42, Epoch: 082, Loss: 0.0540, Val AP: 0.2201, Test AP: 0.2177\n",
                        "Seed: 42, Epoch: 083, Loss: 0.0539, Val AP: 0.1996, Test AP: 0.1964\n",
                        "Seed: 42, Epoch: 084, Loss: 0.0539, Val AP: 0.2164, Test AP: 0.2129\n",
                        "Seed: 42, Epoch: 085, Loss: 0.0539, Val AP: 0.2162, Test AP: 0.2131\n",
                        "Seed: 42, Epoch: 086, Loss: 0.0540, Val AP: 0.2191, Test AP: 0.2163\n",
                        "Seed: 42, Epoch: 087, Loss: 0.0539, Val AP: 0.2119, Test AP: 0.2067\n",
                        "Seed: 42, Epoch: 088, Loss: 0.0539, Val AP: 0.2261, Test AP: 0.2219\n",
                        "Seed: 42, Epoch: 089, Loss: 0.0539, Val AP: 0.2145, Test AP: 0.2104\n",
                        "Seed: 42, Epoch: 090, Loss: 0.0538, Val AP: 0.2213, Test AP: 0.2195\n",
                        "Seed: 42, Epoch: 091, Loss: 0.0538, Val AP: 0.2173, Test AP: 0.2135\n",
                        "Seed: 42, Epoch: 092, Loss: 0.0538, Val AP: 0.2245, Test AP: 0.2197\n",
                        "Seed: 42, Epoch: 093, Loss: 0.0537, Val AP: 0.2181, Test AP: 0.2138\n",
                        "Seed: 42, Epoch: 094, Loss: 0.0538, Val AP: 0.2246, Test AP: 0.2203\n",
                        "Seed: 42, Epoch: 095, Loss: 0.0538, Val AP: 0.2093, Test AP: 0.2043\n",
                        "Seed: 42, Epoch: 096, Loss: 0.0537, Val AP: 0.2113, Test AP: 0.2059\n",
                        "Seed: 42, Epoch: 097, Loss: 0.0538, Val AP: 0.2141, Test AP: 0.2119\n",
                        "Seed: 42, Epoch: 098, Loss: 0.0538, Val AP: 0.2137, Test AP: 0.2098\n",
                        "Seed: 42, Epoch: 099, Loss: 0.0537, Val AP: 0.2276, Test AP: 0.2237\n",
                        "Seed: 42, Epoch: 100, Loss: 0.0537, Val AP: 0.2109, Test AP: 0.2060\n",
                        "Seed: 42, Epoch: 101, Loss: 0.0537, Val AP: 0.2178, Test AP: 0.2135\n",
                        "Seed: 42, Epoch: 102, Loss: 0.0537, Val AP: 0.2228, Test AP: 0.2189\n",
                        "Seed: 42, Epoch: 103, Loss: 0.0537, Val AP: 0.2095, Test AP: 0.2052\n",
                        "Seed: 42, Epoch: 104, Loss: 0.0537, Val AP: 0.2213, Test AP: 0.2166\n",
                        "Seed: 42, Epoch: 105, Loss: 0.0536, Val AP: 0.2170, Test AP: 0.2134\n",
                        "Seed: 42, Epoch: 106, Loss: 0.0537, Val AP: 0.2148, Test AP: 0.2113\n",
                        "Seed: 42, Epoch: 107, Loss: 0.0537, Val AP: 0.2150, Test AP: 0.2131\n",
                        "Seed: 42, Epoch: 108, Loss: 0.0536, Val AP: 0.2226, Test AP: 0.2174\n",
                        "Seed: 42, Epoch: 109, Loss: 0.0536, Val AP: 0.2153, Test AP: 0.2120\n",
                        "Seed: 42, Epoch: 110, Loss: 0.0536, Val AP: 0.2213, Test AP: 0.2178\n",
                        "Seed: 42, Epoch: 111, Loss: 0.0536, Val AP: 0.2183, Test AP: 0.2145\n",
                        "Seed: 42, Epoch: 112, Loss: 0.0536, Val AP: 0.2183, Test AP: 0.2162\n",
                        "Seed: 42, Epoch: 113, Loss: 0.0536, Val AP: 0.2135, Test AP: 0.2088\n",
                        "Seed: 42, Epoch: 114, Loss: 0.0536, Val AP: 0.2172, Test AP: 0.2119\n",
                        "Seed: 42, Epoch: 115, Loss: 0.0536, Val AP: 0.2060, Test AP: 0.2019\n",
                        "Seed: 42, Epoch: 116, Loss: 0.0536, Val AP: 0.2173, Test AP: 0.2139\n",
                        "Seed: 42, Epoch: 117, Loss: 0.0536, Val AP: 0.2207, Test AP: 0.2163\n",
                        "Seed: 42, Epoch: 118, Loss: 0.0536, Val AP: 0.2182, Test AP: 0.2145\n",
                        "Seed: 42, Epoch: 119, Loss: 0.0535, Val AP: 0.2184, Test AP: 0.2141\n",
                        "Seed: 42, Epoch: 120, Loss: 0.0536, Val AP: 0.2145, Test AP: 0.2103\n",
                        "Seed: 42, Epoch: 121, Loss: 0.0535, Val AP: 0.2155, Test AP: 0.2112\n",
                        "Seed: 42, Epoch: 122, Loss: 0.0535, Val AP: 0.2180, Test AP: 0.2141\n",
                        "Seed: 42, Epoch: 123, Loss: 0.0535, Val AP: 0.2230, Test AP: 0.2200\n",
                        "Seed: 42, Epoch: 124, Loss: 0.0535, Val AP: 0.2158, Test AP: 0.2107\n",
                        "Seed: 42, Epoch: 125, Loss: 0.0535, Val AP: 0.2226, Test AP: 0.2190\n",
                        "Seed: 42, Epoch: 126, Loss: 0.0535, Val AP: 0.2248, Test AP: 0.2213\n",
                        "Seed: 42, Epoch: 127, Loss: 0.0535, Val AP: 0.2243, Test AP: 0.2200\n",
                        "Seed: 42, Epoch: 128, Loss: 0.0534, Val AP: 0.2244, Test AP: 0.2211\n",
                        "Seed: 42, Epoch: 129, Loss: 0.0535, Val AP: 0.2220, Test AP: 0.2187\n",
                        "Seed: 42, Epoch: 130, Loss: 0.0535, Val AP: 0.2118, Test AP: 0.2080\n",
                        "Seed: 42, Epoch: 131, Loss: 0.0535, Val AP: 0.2280, Test AP: 0.2238\n",
                        "Seed: 42, Epoch: 132, Loss: 0.0535, Val AP: 0.2228, Test AP: 0.2172\n",
                        "Seed: 42, Epoch: 133, Loss: 0.0535, Val AP: 0.2177, Test AP: 0.2133\n",
                        "Seed: 42, Epoch: 134, Loss: 0.0534, Val AP: 0.2304, Test AP: 0.2255\n",
                        "Seed: 42, Epoch: 135, Loss: 0.0534, Val AP: 0.2314, Test AP: 0.2271\n",
                        "Seed: 42, Epoch: 136, Loss: 0.0534, Val AP: 0.2198, Test AP: 0.2135\n",
                        "Seed: 42, Epoch: 137, Loss: 0.0534, Val AP: 0.2107, Test AP: 0.2064\n",
                        "Seed: 42, Epoch: 138, Loss: 0.0535, Val AP: 0.2158, Test AP: 0.2129\n",
                        "Seed: 42, Epoch: 139, Loss: 0.0535, Val AP: 0.2247, Test AP: 0.2205\n",
                        "Seed: 42, Epoch: 140, Loss: 0.0534, Val AP: 0.2165, Test AP: 0.2119\n",
                        "Seed: 42, Epoch: 141, Loss: 0.0534, Val AP: 0.2328, Test AP: 0.2295\n",
                        "Seed: 42, Epoch: 142, Loss: 0.0534, Val AP: 0.2290, Test AP: 0.2256\n",
                        "Seed: 42, Epoch: 143, Loss: 0.0534, Val AP: 0.2299, Test AP: 0.2256\n",
                        "Seed: 42, Epoch: 144, Loss: 0.0534, Val AP: 0.2291, Test AP: 0.2253\n",
                        "Seed: 42, Epoch: 145, Loss: 0.0534, Val AP: 0.2123, Test AP: 0.2054\n",
                        "Seed: 42, Epoch: 146, Loss: 0.0533, Val AP: 0.2252, Test AP: 0.2194\n",
                        "Seed: 42, Epoch: 147, Loss: 0.0534, Val AP: 0.2266, Test AP: 0.2216\n",
                        "Seed: 42, Epoch: 148, Loss: 0.0534, Val AP: 0.2155, Test AP: 0.2106\n",
                        "Seed: 42, Epoch: 149, Loss: 0.0534, Val AP: 0.2241, Test AP: 0.2189\n",
                        "Seed: 42, Epoch: 150, Loss: 0.0534, Val AP: 0.2165, Test AP: 0.2128\n",
                        "Seed: 43, Epoch: 001, Loss: 0.0612, Val AP: 0.1206, Test AP: 0.1197\n",
                        "Seed: 43, Epoch: 002, Loss: 0.0597, Val AP: 0.1350, Test AP: 0.1339\n",
                        "Seed: 43, Epoch: 003, Loss: 0.0591, Val AP: 0.1397, Test AP: 0.1382\n",
                        "Seed: 43, Epoch: 004, Loss: 0.0588, Val AP: 0.1413, Test AP: 0.1392\n",
                        "Seed: 43, Epoch: 005, Loss: 0.0585, Val AP: 0.1482, Test AP: 0.1472\n",
                        "Seed: 43, Epoch: 006, Loss: 0.0582, Val AP: 0.1538, Test AP: 0.1538\n",
                        "Seed: 43, Epoch: 007, Loss: 0.0579, Val AP: 0.1485, Test AP: 0.1475\n",
                        "Seed: 43, Epoch: 008, Loss: 0.0577, Val AP: 0.1687, Test AP: 0.1670\n",
                        "Seed: 43, Epoch: 009, Loss: 0.0574, Val AP: 0.1663, Test AP: 0.1650\n",
                        "Seed: 43, Epoch: 010, Loss: 0.0573, Val AP: 0.1699, Test AP: 0.1673\n",
                        "Seed: 43, Epoch: 011, Loss: 0.0571, Val AP: 0.1634, Test AP: 0.1607\n",
                        "Seed: 43, Epoch: 012, Loss: 0.0570, Val AP: 0.1798, Test AP: 0.1769\n",
                        "Seed: 43, Epoch: 013, Loss: 0.0568, Val AP: 0.1822, Test AP: 0.1787\n",
                        "Seed: 43, Epoch: 014, Loss: 0.0566, Val AP: 0.1815, Test AP: 0.1782\n",
                        "Seed: 43, Epoch: 015, Loss: 0.0565, Val AP: 0.1818, Test AP: 0.1764\n",
                        "Seed: 43, Epoch: 016, Loss: 0.0564, Val AP: 0.1814, Test AP: 0.1785\n",
                        "Seed: 43, Epoch: 017, Loss: 0.0562, Val AP: 0.1942, Test AP: 0.1891\n",
                        "Seed: 43, Epoch: 018, Loss: 0.0561, Val AP: 0.1968, Test AP: 0.1940\n",
                        "Seed: 43, Epoch: 019, Loss: 0.0560, Val AP: 0.1860, Test AP: 0.1819\n",
                        "Seed: 43, Epoch: 020, Loss: 0.0558, Val AP: 0.1905, Test AP: 0.1861\n",
                        "Seed: 43, Epoch: 021, Loss: 0.0558, Val AP: 0.1915, Test AP: 0.1871\n",
                        "Seed: 43, Epoch: 022, Loss: 0.0556, Val AP: 0.1996, Test AP: 0.1955\n",
                        "Seed: 43, Epoch: 023, Loss: 0.0556, Val AP: 0.1897, Test AP: 0.1853\n",
                        "Seed: 43, Epoch: 024, Loss: 0.0555, Val AP: 0.1983, Test AP: 0.1936\n",
                        "Seed: 43, Epoch: 025, Loss: 0.0554, Val AP: 0.2042, Test AP: 0.2006\n",
                        "Seed: 43, Epoch: 026, Loss: 0.0554, Val AP: 0.2098, Test AP: 0.2048\n",
                        "Seed: 43, Epoch: 027, Loss: 0.0553, Val AP: 0.2085, Test AP: 0.2032\n",
                        "Seed: 43, Epoch: 028, Loss: 0.0552, Val AP: 0.1988, Test AP: 0.1942\n",
                        "Seed: 43, Epoch: 029, Loss: 0.0552, Val AP: 0.1941, Test AP: 0.1883\n",
                        "Seed: 43, Epoch: 030, Loss: 0.0551, Val AP: 0.2140, Test AP: 0.2088\n",
                        "Seed: 43, Epoch: 031, Loss: 0.0551, Val AP: 0.1891, Test AP: 0.1841\n",
                        "Seed: 43, Epoch: 032, Loss: 0.0550, Val AP: 0.1997, Test AP: 0.1926\n",
                        "Seed: 43, Epoch: 033, Loss: 0.0550, Val AP: 0.2001, Test AP: 0.1934\n",
                        "Seed: 43, Epoch: 034, Loss: 0.0549, Val AP: 0.2157, Test AP: 0.2105\n",
                        "Seed: 43, Epoch: 035, Loss: 0.0548, Val AP: 0.2038, Test AP: 0.1979\n",
                        "Seed: 43, Epoch: 036, Loss: 0.0548, Val AP: 0.2077, Test AP: 0.2021\n",
                        "Seed: 43, Epoch: 037, Loss: 0.0548, Val AP: 0.2111, Test AP: 0.2066\n",
                        "Seed: 43, Epoch: 038, Loss: 0.0547, Val AP: 0.2185, Test AP: 0.2125\n",
                        "Seed: 43, Epoch: 039, Loss: 0.0547, Val AP: 0.2135, Test AP: 0.2076\n",
                        "Seed: 43, Epoch: 040, Loss: 0.0547, Val AP: 0.2112, Test AP: 0.2063\n",
                        "Seed: 43, Epoch: 041, Loss: 0.0546, Val AP: 0.1942, Test AP: 0.1884\n",
                        "Seed: 43, Epoch: 042, Loss: 0.0546, Val AP: 0.2145, Test AP: 0.2102\n",
                        "Seed: 43, Epoch: 043, Loss: 0.0546, Val AP: 0.1951, Test AP: 0.1888\n",
                        "Seed: 43, Epoch: 044, Loss: 0.0545, Val AP: 0.2087, Test AP: 0.2037\n",
                        "Seed: 43, Epoch: 045, Loss: 0.0545, Val AP: 0.2160, Test AP: 0.2078\n",
                        "Seed: 43, Epoch: 046, Loss: 0.0545, Val AP: 0.2025, Test AP: 0.1965\n",
                        "Seed: 43, Epoch: 047, Loss: 0.0545, Val AP: 0.2104, Test AP: 0.2047\n",
                        "Seed: 43, Epoch: 048, Loss: 0.0544, Val AP: 0.2250, Test AP: 0.2181\n",
                        "Seed: 43, Epoch: 049, Loss: 0.0544, Val AP: 0.2123, Test AP: 0.2058\n",
                        "Seed: 43, Epoch: 050, Loss: 0.0543, Val AP: 0.2197, Test AP: 0.2119\n",
                        "Seed: 43, Epoch: 051, Loss: 0.0543, Val AP: 0.2129, Test AP: 0.2061\n",
                        "Seed: 43, Epoch: 052, Loss: 0.0543, Val AP: 0.2077, Test AP: 0.2029\n",
                        "Seed: 43, Epoch: 053, Loss: 0.0543, Val AP: 0.2182, Test AP: 0.2112\n",
                        "Seed: 43, Epoch: 054, Loss: 0.0542, Val AP: 0.2223, Test AP: 0.2171\n",
                        "Seed: 43, Epoch: 055, Loss: 0.0542, Val AP: 0.2128, Test AP: 0.2068\n",
                        "Seed: 43, Epoch: 056, Loss: 0.0542, Val AP: 0.2242, Test AP: 0.2180\n",
                        "Seed: 43, Epoch: 057, Loss: 0.0541, Val AP: 0.2119, Test AP: 0.2060\n",
                        "Seed: 43, Epoch: 058, Loss: 0.0542, Val AP: 0.2172, Test AP: 0.2102\n",
                        "Seed: 43, Epoch: 059, Loss: 0.0541, Val AP: 0.2083, Test AP: 0.2025\n",
                        "Seed: 43, Epoch: 060, Loss: 0.0541, Val AP: 0.2249, Test AP: 0.2182\n",
                        "Seed: 43, Epoch: 061, Loss: 0.0540, Val AP: 0.2154, Test AP: 0.2086\n",
                        "Seed: 43, Epoch: 062, Loss: 0.0540, Val AP: 0.2216, Test AP: 0.2143\n",
                        "Seed: 43, Epoch: 063, Loss: 0.0540, Val AP: 0.2238, Test AP: 0.2176\n",
                        "Seed: 43, Epoch: 064, Loss: 0.0540, Val AP: 0.2271, Test AP: 0.2194\n",
                        "Seed: 43, Epoch: 065, Loss: 0.0540, Val AP: 0.2219, Test AP: 0.2152\n",
                        "Seed: 43, Epoch: 066, Loss: 0.0540, Val AP: 0.2121, Test AP: 0.2058\n",
                        "Seed: 43, Epoch: 067, Loss: 0.0539, Val AP: 0.2181, Test AP: 0.2118\n",
                        "Seed: 43, Epoch: 068, Loss: 0.0539, Val AP: 0.2231, Test AP: 0.2159\n",
                        "Seed: 43, Epoch: 069, Loss: 0.0539, Val AP: 0.2118, Test AP: 0.2043\n",
                        "Seed: 43, Epoch: 070, Loss: 0.0539, Val AP: 0.2014, Test AP: 0.1954\n",
                        "Seed: 43, Epoch: 071, Loss: 0.0539, Val AP: 0.2080, Test AP: 0.2019\n",
                        "Seed: 43, Epoch: 072, Loss: 0.0539, Val AP: 0.2157, Test AP: 0.2104\n",
                        "Seed: 43, Epoch: 073, Loss: 0.0538, Val AP: 0.2178, Test AP: 0.2107\n",
                        "Seed: 43, Epoch: 074, Loss: 0.0539, Val AP: 0.2259, Test AP: 0.2183\n",
                        "Seed: 43, Epoch: 075, Loss: 0.0537, Val AP: 0.2250, Test AP: 0.2182\n",
                        "Seed: 43, Epoch: 076, Loss: 0.0537, Val AP: 0.2092, Test AP: 0.2020\n",
                        "Seed: 43, Epoch: 077, Loss: 0.0538, Val AP: 0.2195, Test AP: 0.2122\n",
                        "Seed: 43, Epoch: 078, Loss: 0.0538, Val AP: 0.2303, Test AP: 0.2235\n",
                        "Seed: 43, Epoch: 079, Loss: 0.0537, Val AP: 0.2244, Test AP: 0.2165\n",
                        "Seed: 43, Epoch: 080, Loss: 0.0538, Val AP: 0.2269, Test AP: 0.2186\n",
                        "Seed: 43, Epoch: 081, Loss: 0.0537, Val AP: 0.2286, Test AP: 0.2216\n",
                        "Seed: 43, Epoch: 082, Loss: 0.0537, Val AP: 0.2300, Test AP: 0.2233\n",
                        "Seed: 43, Epoch: 083, Loss: 0.0537, Val AP: 0.2187, Test AP: 0.2109\n",
                        "Seed: 43, Epoch: 084, Loss: 0.0537, Val AP: 0.2093, Test AP: 0.2024\n",
                        "Seed: 43, Epoch: 085, Loss: 0.0536, Val AP: 0.2183, Test AP: 0.2113\n",
                        "Seed: 43, Epoch: 086, Loss: 0.0536, Val AP: 0.2177, Test AP: 0.2107\n",
                        "Seed: 43, Epoch: 087, Loss: 0.0536, Val AP: 0.2301, Test AP: 0.2239\n",
                        "Seed: 43, Epoch: 088, Loss: 0.0536, Val AP: 0.2233, Test AP: 0.2160\n",
                        "Seed: 43, Epoch: 089, Loss: 0.0536, Val AP: 0.2264, Test AP: 0.2197\n",
                        "Seed: 43, Epoch: 090, Loss: 0.0537, Val AP: 0.2387, Test AP: 0.2329\n",
                        "Seed: 43, Epoch: 091, Loss: 0.0536, Val AP: 0.2293, Test AP: 0.2242\n",
                        "Seed: 43, Epoch: 092, Loss: 0.0535, Val AP: 0.2259, Test AP: 0.2198\n",
                        "Seed: 43, Epoch: 093, Loss: 0.0535, Val AP: 0.2276, Test AP: 0.2218\n",
                        "Seed: 43, Epoch: 094, Loss: 0.0536, Val AP: 0.2181, Test AP: 0.2116\n",
                        "Seed: 43, Epoch: 095, Loss: 0.0535, Val AP: 0.2254, Test AP: 0.2173\n",
                        "Seed: 43, Epoch: 096, Loss: 0.0535, Val AP: 0.2194, Test AP: 0.2126\n",
                        "Seed: 43, Epoch: 097, Loss: 0.0535, Val AP: 0.2186, Test AP: 0.2113\n",
                        "Seed: 43, Epoch: 098, Loss: 0.0535, Val AP: 0.2243, Test AP: 0.2166\n",
                        "Seed: 43, Epoch: 099, Loss: 0.0535, Val AP: 0.2254, Test AP: 0.2183\n",
                        "Seed: 43, Epoch: 100, Loss: 0.0535, Val AP: 0.2369, Test AP: 0.2303\n",
                        "Seed: 43, Epoch: 101, Loss: 0.0535, Val AP: 0.2246, Test AP: 0.2171\n",
                        "Seed: 43, Epoch: 102, Loss: 0.0535, Val AP: 0.2171, Test AP: 0.2104\n",
                        "Seed: 43, Epoch: 103, Loss: 0.0535, Val AP: 0.2310, Test AP: 0.2232\n",
                        "Seed: 43, Epoch: 104, Loss: 0.0535, Val AP: 0.2251, Test AP: 0.2180\n",
                        "Seed: 43, Epoch: 105, Loss: 0.0534, Val AP: 0.2144, Test AP: 0.2093\n",
                        "Seed: 43, Epoch: 106, Loss: 0.0534, Val AP: 0.2231, Test AP: 0.2157\n",
                        "Seed: 43, Epoch: 107, Loss: 0.0535, Val AP: 0.2249, Test AP: 0.2189\n",
                        "Seed: 43, Epoch: 108, Loss: 0.0535, Val AP: 0.2243, Test AP: 0.2199\n",
                        "Seed: 43, Epoch: 109, Loss: 0.0535, Val AP: 0.2190, Test AP: 0.2118\n",
                        "Seed: 43, Epoch: 110, Loss: 0.0535, Val AP: 0.2196, Test AP: 0.2106\n",
                        "Seed: 43, Epoch: 111, Loss: 0.0534, Val AP: 0.2297, Test AP: 0.2218\n",
                        "Seed: 43, Epoch: 112, Loss: 0.0534, Val AP: 0.2306, Test AP: 0.2223\n",
                        "Seed: 43, Epoch: 113, Loss: 0.0534, Val AP: 0.2258, Test AP: 0.2176\n",
                        "Seed: 43, Epoch: 114, Loss: 0.0534, Val AP: 0.2217, Test AP: 0.2150\n",
                        "Seed: 43, Epoch: 115, Loss: 0.0533, Val AP: 0.2251, Test AP: 0.2168\n",
                        "Seed: 43, Epoch: 116, Loss: 0.0533, Val AP: 0.2247, Test AP: 0.2166\n",
                        "Seed: 43, Epoch: 117, Loss: 0.0533, Val AP: 0.2328, Test AP: 0.2256\n",
                        "Seed: 43, Epoch: 118, Loss: 0.0534, Val AP: 0.2179, Test AP: 0.2115\n",
                        "Seed: 43, Epoch: 119, Loss: 0.0533, Val AP: 0.2251, Test AP: 0.2168\n",
                        "Seed: 43, Epoch: 120, Loss: 0.0533, Val AP: 0.2325, Test AP: 0.2252\n",
                        "Seed: 43, Epoch: 121, Loss: 0.0533, Val AP: 0.2244, Test AP: 0.2178\n",
                        "Seed: 43, Epoch: 122, Loss: 0.0533, Val AP: 0.2190, Test AP: 0.2123\n",
                        "Seed: 43, Epoch: 123, Loss: 0.0533, Val AP: 0.2214, Test AP: 0.2132\n",
                        "Seed: 43, Epoch: 124, Loss: 0.0533, Val AP: 0.2247, Test AP: 0.2164\n",
                        "Seed: 43, Epoch: 125, Loss: 0.0533, Val AP: 0.2175, Test AP: 0.2121\n",
                        "Seed: 43, Epoch: 126, Loss: 0.0533, Val AP: 0.2299, Test AP: 0.2237\n",
                        "Seed: 43, Epoch: 127, Loss: 0.0532, Val AP: 0.2141, Test AP: 0.2079\n",
                        "Seed: 43, Epoch: 128, Loss: 0.0533, Val AP: 0.2222, Test AP: 0.2146\n",
                        "Seed: 43, Epoch: 129, Loss: 0.0532, Val AP: 0.2186, Test AP: 0.2116\n",
                        "Seed: 43, Epoch: 130, Loss: 0.0533, Val AP: 0.2241, Test AP: 0.2178\n",
                        "Seed: 43, Epoch: 131, Loss: 0.0532, Val AP: 0.2326, Test AP: 0.2267\n",
                        "Seed: 43, Epoch: 132, Loss: 0.0532, Val AP: 0.2327, Test AP: 0.2262\n",
                        "Seed: 43, Epoch: 133, Loss: 0.0532, Val AP: 0.2223, Test AP: 0.2144\n",
                        "Seed: 43, Epoch: 134, Loss: 0.0532, Val AP: 0.2298, Test AP: 0.2238\n",
                        "Seed: 43, Epoch: 135, Loss: 0.0532, Val AP: 0.2271, Test AP: 0.2192\n",
                        "Seed: 43, Epoch: 136, Loss: 0.0532, Val AP: 0.2284, Test AP: 0.2209\n",
                        "Seed: 43, Epoch: 137, Loss: 0.0533, Val AP: 0.2319, Test AP: 0.2238\n",
                        "Seed: 43, Epoch: 138, Loss: 0.0533, Val AP: 0.2305, Test AP: 0.2242\n",
                        "Seed: 43, Epoch: 139, Loss: 0.0532, Val AP: 0.2278, Test AP: 0.2206\n",
                        "Seed: 43, Epoch: 140, Loss: 0.0531, Val AP: 0.2269, Test AP: 0.2195\n",
                        "Early stopping at epoch 140 for seed 43\n",
                        "Seed: 44, Epoch: 001, Loss: 0.0611, Val AP: 0.1181, Test AP: 0.1194\n",
                        "Seed: 44, Epoch: 002, Loss: 0.0599, Val AP: 0.1351, Test AP: 0.1372\n",
                        "Seed: 44, Epoch: 003, Loss: 0.0593, Val AP: 0.1407, Test AP: 0.1416\n",
                        "Seed: 44, Epoch: 004, Loss: 0.0591, Val AP: 0.1408, Test AP: 0.1422\n",
                        "Seed: 44, Epoch: 005, Loss: 0.0588, Val AP: 0.1479, Test AP: 0.1480\n",
                        "Seed: 44, Epoch: 006, Loss: 0.0586, Val AP: 0.1460, Test AP: 0.1460\n",
                        "Seed: 44, Epoch: 007, Loss: 0.0582, Val AP: 0.1538, Test AP: 0.1527\n",
                        "Seed: 44, Epoch: 008, Loss: 0.0581, Val AP: 0.1577, Test AP: 0.1573\n",
                        "Seed: 44, Epoch: 009, Loss: 0.0579, Val AP: 0.1602, Test AP: 0.1597\n",
                        "Seed: 44, Epoch: 010, Loss: 0.0577, Val AP: 0.1657, Test AP: 0.1653\n",
                        "Seed: 44, Epoch: 011, Loss: 0.0575, Val AP: 0.1688, Test AP: 0.1673\n",
                        "Seed: 44, Epoch: 012, Loss: 0.0574, Val AP: 0.1611, Test AP: 0.1611\n",
                        "Seed: 44, Epoch: 013, Loss: 0.0572, Val AP: 0.1762, Test AP: 0.1751\n",
                        "Seed: 44, Epoch: 014, Loss: 0.0572, Val AP: 0.1575, Test AP: 0.1566\n",
                        "Seed: 44, Epoch: 015, Loss: 0.0570, Val AP: 0.1668, Test AP: 0.1658\n",
                        "Seed: 44, Epoch: 016, Loss: 0.0569, Val AP: 0.1780, Test AP: 0.1760\n",
                        "Seed: 44, Epoch: 017, Loss: 0.0568, Val AP: 0.1847, Test AP: 0.1819\n",
                        "Seed: 44, Epoch: 018, Loss: 0.0567, Val AP: 0.1815, Test AP: 0.1810\n",
                        "Seed: 44, Epoch: 019, Loss: 0.0565, Val AP: 0.1816, Test AP: 0.1802\n",
                        "Seed: 44, Epoch: 020, Loss: 0.0564, Val AP: 0.1643, Test AP: 0.1616\n",
                        "Seed: 44, Epoch: 021, Loss: 0.0563, Val AP: 0.1769, Test AP: 0.1760\n",
                        "Seed: 44, Epoch: 022, Loss: 0.0562, Val AP: 0.1845, Test AP: 0.1835\n",
                        "Seed: 44, Epoch: 023, Loss: 0.0561, Val AP: 0.1898, Test AP: 0.1875\n",
                        "Seed: 44, Epoch: 024, Loss: 0.0560, Val AP: 0.1712, Test AP: 0.1711\n",
                        "Seed: 44, Epoch: 025, Loss: 0.0560, Val AP: 0.1902, Test AP: 0.1878\n",
                        "Seed: 44, Epoch: 026, Loss: 0.0560, Val AP: 0.1793, Test AP: 0.1770\n",
                        "Seed: 44, Epoch: 027, Loss: 0.0559, Val AP: 0.1851, Test AP: 0.1845\n",
                        "Seed: 44, Epoch: 028, Loss: 0.0558, Val AP: 0.1871, Test AP: 0.1871\n",
                        "Seed: 44, Epoch: 029, Loss: 0.0557, Val AP: 0.1992, Test AP: 0.1992\n",
                        "Seed: 44, Epoch: 030, Loss: 0.0557, Val AP: 0.1863, Test AP: 0.1873\n",
                        "Seed: 44, Epoch: 031, Loss: 0.0556, Val AP: 0.1951, Test AP: 0.1956\n",
                        "Seed: 44, Epoch: 032, Loss: 0.0556, Val AP: 0.1928, Test AP: 0.1926\n",
                        "Seed: 44, Epoch: 033, Loss: 0.0555, Val AP: 0.2008, Test AP: 0.1979\n",
                        "Seed: 44, Epoch: 034, Loss: 0.0554, Val AP: 0.1753, Test AP: 0.1746\n",
                        "Seed: 44, Epoch: 035, Loss: 0.0555, Val AP: 0.1823, Test AP: 0.1837\n",
                        "Seed: 44, Epoch: 036, Loss: 0.0554, Val AP: 0.1888, Test AP: 0.1888\n",
                        "Seed: 44, Epoch: 037, Loss: 0.0553, Val AP: 0.2000, Test AP: 0.2003\n",
                        "Seed: 44, Epoch: 038, Loss: 0.0553, Val AP: 0.2042, Test AP: 0.2034\n",
                        "Seed: 44, Epoch: 039, Loss: 0.0553, Val AP: 0.2043, Test AP: 0.2049\n",
                        "Seed: 44, Epoch: 040, Loss: 0.0552, Val AP: 0.1917, Test AP: 0.1909\n",
                        "Seed: 44, Epoch: 041, Loss: 0.0552, Val AP: 0.2016, Test AP: 0.2010\n",
                        "Seed: 44, Epoch: 042, Loss: 0.0552, Val AP: 0.1833, Test AP: 0.1839\n",
                        "Seed: 44, Epoch: 043, Loss: 0.0551, Val AP: 0.2004, Test AP: 0.2005\n",
                        "Seed: 44, Epoch: 044, Loss: 0.0551, Val AP: 0.1926, Test AP: 0.1943\n",
                        "Seed: 44, Epoch: 045, Loss: 0.0551, Val AP: 0.1930, Test AP: 0.1948\n",
                        "Seed: 44, Epoch: 046, Loss: 0.0551, Val AP: 0.1970, Test AP: 0.1954\n",
                        "Seed: 44, Epoch: 047, Loss: 0.0550, Val AP: 0.2131, Test AP: 0.2098\n",
                        "Seed: 44, Epoch: 048, Loss: 0.0550, Val AP: 0.2068, Test AP: 0.2048\n",
                        "Seed: 44, Epoch: 049, Loss: 0.0550, Val AP: 0.2066, Test AP: 0.2051\n",
                        "Seed: 44, Epoch: 050, Loss: 0.0550, Val AP: 0.2085, Test AP: 0.2071\n",
                        "Seed: 44, Epoch: 051, Loss: 0.0549, Val AP: 0.1821, Test AP: 0.1825\n",
                        "Seed: 44, Epoch: 052, Loss: 0.0549, Val AP: 0.2008, Test AP: 0.2006\n",
                        "Seed: 44, Epoch: 053, Loss: 0.0549, Val AP: 0.1880, Test AP: 0.1881\n",
                        "Seed: 44, Epoch: 054, Loss: 0.0548, Val AP: 0.2027, Test AP: 0.2024\n",
                        "Seed: 44, Epoch: 055, Loss: 0.0548, Val AP: 0.2114, Test AP: 0.2103\n",
                        "Seed: 44, Epoch: 056, Loss: 0.0548, Val AP: 0.1982, Test AP: 0.1985\n",
                        "Seed: 44, Epoch: 057, Loss: 0.0548, Val AP: 0.2047, Test AP: 0.2042\n",
                        "Seed: 44, Epoch: 058, Loss: 0.0547, Val AP: 0.1989, Test AP: 0.1982\n",
                        "Seed: 44, Epoch: 059, Loss: 0.0547, Val AP: 0.1923, Test AP: 0.1911\n",
                        "Seed: 44, Epoch: 060, Loss: 0.0547, Val AP: 0.1983, Test AP: 0.1984\n",
                        "Seed: 44, Epoch: 061, Loss: 0.0546, Val AP: 0.2051, Test AP: 0.2042\n",
                        "Seed: 44, Epoch: 062, Loss: 0.0547, Val AP: 0.2113, Test AP: 0.2092\n",
                        "Seed: 44, Epoch: 063, Loss: 0.0547, Val AP: 0.1932, Test AP: 0.1940\n",
                        "Seed: 44, Epoch: 064, Loss: 0.0547, Val AP: 0.1988, Test AP: 0.1982\n",
                        "Seed: 44, Epoch: 065, Loss: 0.0546, Val AP: 0.1994, Test AP: 0.1986\n",
                        "Seed: 44, Epoch: 066, Loss: 0.0547, Val AP: 0.2072, Test AP: 0.2062\n",
                        "Seed: 44, Epoch: 067, Loss: 0.0546, Val AP: 0.2049, Test AP: 0.2058\n",
                        "Seed: 44, Epoch: 068, Loss: 0.0546, Val AP: 0.2132, Test AP: 0.2112\n",
                        "Seed: 44, Epoch: 069, Loss: 0.0546, Val AP: 0.2044, Test AP: 0.2034\n",
                        "Seed: 44, Epoch: 070, Loss: 0.0545, Val AP: 0.1991, Test AP: 0.1992\n",
                        "Seed: 44, Epoch: 071, Loss: 0.0545, Val AP: 0.2122, Test AP: 0.2108\n",
                        "Seed: 44, Epoch: 072, Loss: 0.0545, Val AP: 0.1913, Test AP: 0.1893\n",
                        "Seed: 44, Epoch: 073, Loss: 0.0546, Val AP: 0.2146, Test AP: 0.2127\n",
                        "Seed: 44, Epoch: 074, Loss: 0.0545, Val AP: 0.2127, Test AP: 0.2123\n",
                        "Seed: 44, Epoch: 075, Loss: 0.0545, Val AP: 0.2123, Test AP: 0.2112\n",
                        "Seed: 44, Epoch: 076, Loss: 0.0544, Val AP: 0.2009, Test AP: 0.2017\n",
                        "Seed: 44, Epoch: 077, Loss: 0.0545, Val AP: 0.2129, Test AP: 0.2133\n",
                        "Seed: 44, Epoch: 078, Loss: 0.0544, Val AP: 0.1989, Test AP: 0.1998\n",
                        "Seed: 44, Epoch: 079, Loss: 0.0544, Val AP: 0.2047, Test AP: 0.2052\n",
                        "Seed: 44, Epoch: 080, Loss: 0.0545, Val AP: 0.2120, Test AP: 0.2108\n",
                        "Seed: 44, Epoch: 081, Loss: 0.0544, Val AP: 0.2078, Test AP: 0.2072\n",
                        "Seed: 44, Epoch: 082, Loss: 0.0544, Val AP: 0.2049, Test AP: 0.2055\n",
                        "Seed: 44, Epoch: 083, Loss: 0.0544, Val AP: 0.2011, Test AP: 0.2020\n",
                        "Seed: 44, Epoch: 084, Loss: 0.0544, Val AP: 0.2066, Test AP: 0.2055\n",
                        "Seed: 44, Epoch: 085, Loss: 0.0544, Val AP: 0.2188, Test AP: 0.2181\n",
                        "Seed: 44, Epoch: 086, Loss: 0.0544, Val AP: 0.2014, Test AP: 0.2017\n",
                        "Seed: 44, Epoch: 087, Loss: 0.0543, Val AP: 0.2157, Test AP: 0.2155\n",
                        "Seed: 44, Epoch: 088, Loss: 0.0543, Val AP: 0.2187, Test AP: 0.2182\n",
                        "Seed: 44, Epoch: 089, Loss: 0.0543, Val AP: 0.2154, Test AP: 0.2153\n",
                        "Seed: 44, Epoch: 090, Loss: 0.0543, Val AP: 0.2069, Test AP: 0.2064\n",
                        "Seed: 44, Epoch: 091, Loss: 0.0543, Val AP: 0.2111, Test AP: 0.2097\n",
                        "Seed: 44, Epoch: 092, Loss: 0.0543, Val AP: 0.2119, Test AP: 0.2140\n",
                        "Seed: 44, Epoch: 093, Loss: 0.0543, Val AP: 0.1960, Test AP: 0.1981\n",
                        "Seed: 44, Epoch: 094, Loss: 0.0543, Val AP: 0.2087, Test AP: 0.2094\n",
                        "Seed: 44, Epoch: 095, Loss: 0.0542, Val AP: 0.2038, Test AP: 0.2054\n",
                        "Seed: 44, Epoch: 096, Loss: 0.0542, Val AP: 0.2061, Test AP: 0.2078\n",
                        "Seed: 44, Epoch: 097, Loss: 0.0542, Val AP: 0.2081, Test AP: 0.2085\n",
                        "Seed: 44, Epoch: 098, Loss: 0.0543, Val AP: 0.2088, Test AP: 0.2078\n",
                        "Seed: 44, Epoch: 099, Loss: 0.0542, Val AP: 0.2215, Test AP: 0.2207\n",
                        "Seed: 44, Epoch: 100, Loss: 0.0542, Val AP: 0.2039, Test AP: 0.2046\n",
                        "Seed: 44, Epoch: 101, Loss: 0.0542, Val AP: 0.2037, Test AP: 0.2047\n",
                        "Seed: 44, Epoch: 102, Loss: 0.0542, Val AP: 0.2209, Test AP: 0.2206\n",
                        "Seed: 44, Epoch: 103, Loss: 0.0541, Val AP: 0.2081, Test AP: 0.2072\n",
                        "Seed: 44, Epoch: 104, Loss: 0.0542, Val AP: 0.2133, Test AP: 0.2137\n",
                        "Seed: 44, Epoch: 105, Loss: 0.0542, Val AP: 0.2186, Test AP: 0.2167\n",
                        "Seed: 44, Epoch: 106, Loss: 0.0541, Val AP: 0.2176, Test AP: 0.2173\n",
                        "Seed: 44, Epoch: 107, Loss: 0.0541, Val AP: 0.2070, Test AP: 0.2074\n",
                        "Seed: 44, Epoch: 108, Loss: 0.0541, Val AP: 0.2042, Test AP: 0.2054\n",
                        "Seed: 44, Epoch: 109, Loss: 0.0541, Val AP: 0.2149, Test AP: 0.2138\n",
                        "Seed: 44, Epoch: 110, Loss: 0.0541, Val AP: 0.2076, Test AP: 0.2073\n",
                        "Seed: 44, Epoch: 111, Loss: 0.0541, Val AP: 0.2080, Test AP: 0.2088\n",
                        "Seed: 44, Epoch: 112, Loss: 0.0541, Val AP: 0.2147, Test AP: 0.2154\n",
                        "Seed: 44, Epoch: 113, Loss: 0.0541, Val AP: 0.2012, Test AP: 0.2023\n",
                        "Seed: 44, Epoch: 114, Loss: 0.0541, Val AP: 0.2009, Test AP: 0.2017\n",
                        "Seed: 44, Epoch: 115, Loss: 0.0541, Val AP: 0.2152, Test AP: 0.2145\n",
                        "Seed: 44, Epoch: 116, Loss: 0.0540, Val AP: 0.2078, Test AP: 0.2072\n",
                        "Seed: 44, Epoch: 117, Loss: 0.0540, Val AP: 0.2122, Test AP: 0.2120\n",
                        "Seed: 44, Epoch: 118, Loss: 0.0540, Val AP: 0.2189, Test AP: 0.2184\n",
                        "Seed: 44, Epoch: 119, Loss: 0.0540, Val AP: 0.2094, Test AP: 0.2099\n",
                        "Seed: 44, Epoch: 120, Loss: 0.0540, Val AP: 0.2128, Test AP: 0.2134\n",
                        "Seed: 44, Epoch: 121, Loss: 0.0541, Val AP: 0.2092, Test AP: 0.2104\n",
                        "Seed: 44, Epoch: 122, Loss: 0.0541, Val AP: 0.2128, Test AP: 0.2102\n",
                        "Seed: 44, Epoch: 123, Loss: 0.0540, Val AP: 0.2075, Test AP: 0.2063\n",
                        "Seed: 44, Epoch: 124, Loss: 0.0540, Val AP: 0.2158, Test AP: 0.2161\n",
                        "Seed: 44, Epoch: 125, Loss: 0.0540, Val AP: 0.2115, Test AP: 0.2118\n",
                        "Seed: 44, Epoch: 126, Loss: 0.0540, Val AP: 0.2194, Test AP: 0.2186\n",
                        "Seed: 44, Epoch: 127, Loss: 0.0540, Val AP: 0.2219, Test AP: 0.2209\n",
                        "Seed: 44, Epoch: 128, Loss: 0.0540, Val AP: 0.2129, Test AP: 0.2134\n",
                        "Seed: 44, Epoch: 129, Loss: 0.0540, Val AP: 0.2113, Test AP: 0.2098\n",
                        "Seed: 44, Epoch: 130, Loss: 0.0540, Val AP: 0.2109, Test AP: 0.2095\n",
                        "Seed: 44, Epoch: 131, Loss: 0.0539, Val AP: 0.2157, Test AP: 0.2165\n",
                        "Seed: 44, Epoch: 132, Loss: 0.0539, Val AP: 0.2122, Test AP: 0.2127\n",
                        "Seed: 44, Epoch: 133, Loss: 0.0540, Val AP: 0.2100, Test AP: 0.2108\n",
                        "Seed: 44, Epoch: 134, Loss: 0.0540, Val AP: 0.2112, Test AP: 0.2114\n",
                        "Seed: 44, Epoch: 135, Loss: 0.0540, Val AP: 0.2020, Test AP: 0.2036\n",
                        "Seed: 44, Epoch: 136, Loss: 0.0539, Val AP: 0.2161, Test AP: 0.2147\n",
                        "Seed: 44, Epoch: 137, Loss: 0.0539, Val AP: 0.2118, Test AP: 0.2083\n",
                        "Seed: 44, Epoch: 138, Loss: 0.0539, Val AP: 0.2113, Test AP: 0.2102\n",
                        "Seed: 44, Epoch: 139, Loss: 0.0539, Val AP: 0.2161, Test AP: 0.2130\n",
                        "Seed: 44, Epoch: 140, Loss: 0.0539, Val AP: 0.2141, Test AP: 0.2130\n",
                        "Seed: 44, Epoch: 141, Loss: 0.0539, Val AP: 0.2093, Test AP: 0.2102\n",
                        "Seed: 44, Epoch: 142, Loss: 0.0539, Val AP: 0.2188, Test AP: 0.2192\n",
                        "Seed: 44, Epoch: 143, Loss: 0.0539, Val AP: 0.2107, Test AP: 0.2083\n",
                        "Seed: 44, Epoch: 144, Loss: 0.0539, Val AP: 0.2087, Test AP: 0.2087\n",
                        "Seed: 44, Epoch: 145, Loss: 0.0539, Val AP: 0.2132, Test AP: 0.2126\n",
                        "Seed: 44, Epoch: 146, Loss: 0.0540, Val AP: 0.2143, Test AP: 0.2145\n",
                        "Seed: 44, Epoch: 147, Loss: 0.0539, Val AP: 0.2041, Test AP: 0.2052\n",
                        "Seed: 44, Epoch: 148, Loss: 0.0539, Val AP: 0.2149, Test AP: 0.2137\n",
                        "Seed: 44, Epoch: 149, Loss: 0.0539, Val AP: 0.2116, Test AP: 0.2135\n",
                        "Seed: 44, Epoch: 150, Loss: 0.0538, Val AP: 0.2098, Test AP: 0.2089\n",
                        "Average Time: 23383.05 seconds\n",
                        "Var Time: 502771.97 seconds\n",
                        "Average Memory: 5881.33 MB\n",
                        "Average Best Val AP: 0.2311\n",
                        "Std Best Test AP: 0.0051\n",
                        "Average Test AP: 0.2278\n"
                    ]
                }
            ],
            "source": "class HierarchicalGCN_HGPSL(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_HGPSL, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, improved = True)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = HGPSLPool(hidden_channels, ratio=0.9, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, improved = True)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = HGPSLPool(hidden_channels, ratio=0.9, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2)\n        self.conv3 = GCNConv(hidden_channels, out_channels, improved = True)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, data):\n        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n        edge_attr=None\n        init_x = data.x\n        x = x.float()\n        init_x = init_x.float()\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool1(x, edge_index, edge_attr=edge_attr, batch=batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch = self.pool2(x, edge_index, edge_attr=edge_attr, batch=batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        out = model(data)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n    model = HierarchicalGCN_HGPSL(in_channels=dataset_sparse.num_features, hidden_channels=512,out_channels=256, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### AsymCheegerCutPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nmax_nodes = 100\ndataset_dense = PygGraphPropPredDataset(root='/data/XXX/Pooling', name=\"ogbg-molpcba\", transform=T.ToSparseTensor())\nevaluator = Evaluator(\"ogbg-molpcba\")\neval_metric = dataset_dense.eval_metric\ntrain_ratio = 0.8\nval_ratio = 0.1\ntest_ratio = 0.1\ndataset_dense = dataset_dense.shuffle()\nnum_total = len(dataset_dense)\nnum_train = int(num_total * train_ratio)\nnum_val = int(num_total * val_ratio)\nnum_test = num_total - num_train - num_val\ntrain_dataset = dataset_dense[:num_train]\nval_dataset = dataset_dense[num_train:num_train + num_val]\ntest_dataset = dataset_dense[num_train + num_val:]\ntrain_loader = DenseDataLoader(train_dataset, batch_size=2048, shuffle=True)\nvalid_loader = DenseDataLoader(val_dataset, batch_size=2048, shuffle=False)\ntest_loader = DenseDataLoader(test_dataset, batch_size=2048, shuffle=False)"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "from typing import List, Optional, Tuple, Union\nimport math\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.nn.models.mlp import Linear\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import BatchNorm\nclass AsymCheegerCutPool(torch.nn.Module):\n    r\"\"\"\n    The asymmetric cheeger cut pooling layer from the `\"Total Variation Graph Neural Networks\"\n    <https://arxiv.org/abs/2211.06218>`_ paper.\n    Args:\n        k (int):\n            Number of clusters or output nodes\n        mlp_channels (int, list of int):\n            Number of hidden units for each hidden layer in the MLP used to\n            compute cluster assignments. First integer must match the number\n            of input channels.\n        mlp_activation (any):\n            Activation function between hidden layers of the MLP.\n            Must be compatible with `torch_geometric.nn.resolver`.\n        return_selection (bool):\n            Whether to return selection matrix. Cannot not  be False\n            if `return_pooled_graph` is False. (default: :obj:`False`)\n        return_pooled_graph (bool):\n            Whether to return pooled node features and adjacency.\n            Cannot be False if `return_selection` is False. (default: :obj:`True`)\n        bias (bool):\n            whether to add a bias term to the MLP layers. (default: :obj:`True`)\n        totvar_coeff (float):\n            Coefficient for graph total variation loss component. (default: :obj:`1.0`)\n        balance_coeff (float):\n            Coefficient for asymmetric norm loss component. (default: :obj:`1.0`)\n    \"\"\"\n    def __init__(self,\n                 k: int,\n                 mlp_channels: Union[int, List[int]],\n                 mlp_activation=\"relu\",\n                 return_selection: bool = False,\n                 return_pooled_graph: bool = True,\n                 bias: bool = True,\n                 totvar_coeff: float = 1.0,\n                 balance_coeff: float = 1.0,\n                 ):\n        super().__init__()\n        if not return_selection and not return_pooled_graph:\n            raise ValueError(\"return_selection and return_pooled_graph can not both be False\")\n        if isinstance(mlp_channels, int):\n            mlp_channels = [mlp_channels]\n        act = activation_resolver(mlp_activation)\n        in_channels = mlp_channels[0]\n        self.mlp = torch.nn.Sequential()\n        for channels in mlp_channels[1:]:\n            self.mlp.append(Linear(in_channels, channels, bias=bias))\n            in_channels = channels\n            self.mlp.append(act)\n        self.mlp.append(Linear(in_channels, k))\n        self.k = k\n        self.return_selection = return_selection\n        self.return_pooled_graph = return_pooled_graph\n        self.totvar_coeff = totvar_coeff\n        self.balance_coeff = balance_coeff\n        self.reset_parameters()\n    def reset_parameters(self):\n        for layer in self.mlp:\n            if isinstance(layer, Linear):\n                torch.nn.init.xavier_uniform(layer.weight)\n                torch.nn.init.zeros_(layer.bias)\n    def forward(\n        self,\n        x: Tensor,\n        adj: Tensor,\n        mask: Optional[Tensor] = None,\n    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n        r\"\"\"\n        Args:\n            x (Tensor):\n                Node feature tensor :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times F}`\n                with batch-size :math:`B`, (maximum) number of nodes :math:`N` for each graph,\n                and feature dimension :math:`F`. Note that the cluster assignment matrix\n                :math:`\\mathbf{S} \\in \\mathbb{R}^{B \\times N \\times C}` is\n                being created within this method.\n            adj (Tensor):\n                Adjacency tensor :math:`\\mathbf{A} \\in \\mathbb{R}^{B \\times N \\times N}`.\n            mask (BoolTensor, optional):\n                Mask matrix :math:`\\mathbf{M} \\in {\\{ 0, 1 \\}}^{B \\times N}`\n                indicating the valid nodes for each graph. (default: :obj:`None`)\n        :rtype: (:class:`Tensor`, :class:`Tensor`, :class:`Tensor`,\n            :class:`Tensor`, :class:`Tensor`, :class:`Tensor`)\n        \"\"\"\n        x = x.unsqueeze(0) if x.dim() == 2 else x\n        adj = adj.unsqueeze(0) if adj.dim() == 2 else adj\n        s = self.mlp(x)\n        s = torch.softmax(s, dim=-1)\n        batch_size, n_nodes, _ = x.size()\n        if mask is not None:\n            mask = mask.view(batch_size, n_nodes, 1).to(x.dtype)\n            x, s = x * mask, s * mask\n        if self.return_pooled_graph:\n            x_pool = torch.matmul(s.transpose(1, 2), x)\n            adj_pool = torch.matmul(torch.matmul(s.transpose(1, 2), adj), s)\n        tv_loss = self.totvar_coeff*torch.mean(self.totvar_loss(adj, s))\n        bal_loss = self.balance_coeff*torch.mean(self.balance_loss(s))\n        if self.return_selection and self.return_pooled_graph:\n            return s, x_pool, adj_pool, tv_loss, bal_loss\n        elif self.return_selection and not self.return_pooled_graph:\n            return s, tv_loss, bal_loss\n        else:\n            return x_pool, adj_pool, tv_loss, bal_loss\n    def totvar_loss(self, adj, s):\n        l1_norm = torch.sum(torch.abs(s[..., None, :] - s[:, None, ...]), dim=-1)\n        loss = torch.sum(adj * l1_norm, dim=(-1, -2))\n        n_edges = torch.count_nonzero(adj, dim=(-1, -2))\n        loss *= 1 / (2 * n_edges)\n        return loss\n    def balance_loss(self, s):\n        n_nodes = s.size()[-2]\n        idx = int(math.floor(n_nodes / self.k))\n        quant = torch.sort(s, dim=-2, descending=True)[0][:, idx, :] \n        loss = s - torch.unsqueeze(quant, dim=1)\n        loss = (loss >= 0) * (self.k - 1) * loss + (loss < 0) * loss * -1\n        loss = torch.sum(loss, dim=(-1, -2)) \n        loss = 1 / (n_nodes * (self.k - 1)) * (n_nodes * (self.k - 1) - loss)\n        return loss"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[4], line 293\u001b[0m\n\u001b[1;32m    290\u001b[0m epochs_no_improve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m151\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m    294\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m test(valid_loader)  \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m    295\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test(test_loader)  \u001b[38;5;66;03m# \u001b[39;00m\n",
                        "Cell \u001b[0;32mIn[4], line 173\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    171\u001b[0m num_g_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": "from torch_geometric.nn import SAGEConv\nimport os.path as osp\nimport time\nfrom math import ceil\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_sparse import SparseTensor\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, dense_diff_pool\nN = 150\nmp_layers = 1\nmp_channels = 64\nmp_activation = \"relu\"\ndelta_coeff = 2.0\nmlp_hidden_layers = 1\nmlp_hidden_channels = 128\nmlp_activation = \"relu\"\ntotvar_coeff = 0.5\nbalance_coeff = 0.5\nepochs = 100\nbatch_size = 16\nlearning_rate = 5e-4\nl2_reg_val = 0\npatience = 10\nclass GNN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, normalize)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, normalize)\n        self.conv3 = GCNConv(hidden_channels, out_channels, normalize)\n        if lin:\n            self.lin = torch.nn.Linear(out_channels, out_channels)\n        else:\n            self.lin = None\n    def forward(self, x, adj_t, mask=None):\n        adj_t = adj_t.to(torch.float32)\n        x = self.conv1(x, adj_t, mask).relu()\n        x = self.conv2(x, adj_t, mask).relu()\n        x = self.conv3(x, adj_t, mask).relu()\n        if self.lin is not None:\n            x = self.lin(x).relu()\n        return x\nclass Net_AsymCheegerCut(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        num_nodes = 64\n        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n        self.gnn1_embed = GCNConv(dataset_dense.num_features, 64)\n        num_nodes = 64\n        self.gnn2_pool = GNN(64, 64, num_nodes)\n        self.gnn2_embed = GCNConv(64, 64)\n        self.gnn3_embed = GCNConv(64, 64)\n        self.lin1 = torch.nn.Linear(64, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n        self.pool1 = AsymCheegerCutPool(int(N//2),\n                           mlp_channels=[mp_channels] +\n                                [mlp_hidden_channels for _ in range(mlp_hidden_layers)],\n                           mlp_activation=mlp_activation,\n                           totvar_coeff=totvar_coeff,\n                           balance_coeff=balance_coeff,\n                           return_selection=False,\n                           return_pooled_graph=True)\n        self.pool2 = AsymCheegerCutPool(int(N//2),\n                           mlp_channels=[mp_channels] +\n                                [mlp_hidden_channels for _ in range(mlp_hidden_layers)],\n                           mlp_activation=mlp_activation,\n                           totvar_coeff=totvar_coeff,\n                           balance_coeff=balance_coeff,\n                           return_selection=False,\n                           return_pooled_graph=True)\n    def forward(self, x, adj_t, batch):\n        adj_t = adj_t.to(torch.float32)\n        s = self.gnn1_pool(x, adj_t)\n        x = self.gnn1_embed(x, adj_t)\n        x = F.relu(x)\n        if isinstance(adj_t, SparseTensor):\n            adj_t_dense = adj_t.to_dense()\n            adj_t_dense = adj_t_dense.unsqueeze(0) if adj_t_dense.dim() == 2 else adj_t_dense\n        else:\n            adj_t_dense = adj_t.unsqueeze(0) if adj_t.dim() == 2 else adj_t\n            x, adj_t, tv1, bal1 = self.pool1(x, adj_t_dense)\n        s = self.gnn2_pool(x, adj_t)\n        x = self.gnn2_embed(x, adj_t)\n        x = F.relu(x)\n        if isinstance(adj_t, SparseTensor):\n            adj_t_dense = adj_t.to_dense()\n            adj_t_dense = adj_t_dense.unsqueeze(0) if adj_t_dense.dim() == 2 else adj_t_dense\n        else:\n            adj_t_dense = adj_t.unsqueeze(0) if adj_t.dim() == 2 else adj_t\n            x, adj_t, tv1, bal1 = self.pool2(x, adj_t_dense, mask=None)\n        x = self.gnn3_embed(x, adj_t)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Net_AsymCheegerCut().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        x, adj_t = data.x, data.adj_t\n        x, adj_t, batch = data.x.to(torch.float32), data.adj_t.to(torch.float32), data.batch\n        out = model(x, adj_t, batch)\n        pred = out\n        is_labeled = data.y == data.y \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        x, adj_t = data.x, data.adj_t\n        x, adj_t, batch = data.x.to(torch.float32), data.adj_t.to(torch.float32), data.batch\n        out = model(x, adj_t, batch)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_dense = dataset_dense.shuffle()\n    num_total = len(dataset_dense)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_dense[:num_train]\n    val_dataset = dataset_dense[num_train:num_train + num_val]\n    test_dataset = dataset_dense[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n    model = Net_AsymCheegerCut().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### DifferencePool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0608, Val AP: 0.1209, Test AP: 0.1243\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0598, Val AP: 0.1313, Test AP: 0.1376\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0593, Val AP: 0.1331, Test AP: 0.1386\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0590, Val AP: 0.1361, Test AP: 0.1400\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0586, Val AP: 0.1411, Test AP: 0.1452\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0583, Val AP: 0.1555, Test AP: 0.1612\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0580, Val AP: 0.1575, Test AP: 0.1617\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0577, Val AP: 0.1665, Test AP: 0.1697\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0574, Val AP: 0.1764, Test AP: 0.1792\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0572, Val AP: 0.1623, Test AP: 0.1645\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0569, Val AP: 0.1792, Test AP: 0.1819\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0567, Val AP: 0.1696, Test AP: 0.1729\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0565, Val AP: 0.1593, Test AP: 0.1621\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0562, Val AP: 0.1826, Test AP: 0.1846\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0561, Val AP: 0.1890, Test AP: 0.1920\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0560, Val AP: 0.1954, Test AP: 0.1981\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0558, Val AP: 0.1925, Test AP: 0.1952\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0557, Val AP: 0.1864, Test AP: 0.1874\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0556, Val AP: 0.1932, Test AP: 0.1960\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0555, Val AP: 0.1989, Test AP: 0.2005\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0553, Val AP: 0.2048, Test AP: 0.2068\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0552, Val AP: 0.2100, Test AP: 0.2124\n",
                        "Seed: 42, Epoch: 023, Loss: 0.0552, Val AP: 0.2092, Test AP: 0.2123\n",
                        "Seed: 42, Epoch: 024, Loss: 0.0550, Val AP: 0.1935, Test AP: 0.1956\n",
                        "Seed: 42, Epoch: 025, Loss: 0.0550, Val AP: 0.2039, Test AP: 0.2075\n",
                        "Seed: 42, Epoch: 026, Loss: 0.0549, Val AP: 0.2105, Test AP: 0.2138\n",
                        "Seed: 42, Epoch: 027, Loss: 0.0548, Val AP: 0.2051, Test AP: 0.2091\n",
                        "Seed: 42, Epoch: 028, Loss: 0.0547, Val AP: 0.1999, Test AP: 0.2027\n",
                        "Seed: 42, Epoch: 029, Loss: 0.0547, Val AP: 0.2122, Test AP: 0.2141\n",
                        "Seed: 42, Epoch: 030, Loss: 0.0546, Val AP: 0.2095, Test AP: 0.2121\n",
                        "Seed: 42, Epoch: 031, Loss: 0.0546, Val AP: 0.1996, Test AP: 0.2029\n",
                        "Seed: 42, Epoch: 032, Loss: 0.0545, Val AP: 0.2151, Test AP: 0.2179\n",
                        "Seed: 42, Epoch: 033, Loss: 0.0544, Val AP: 0.2145, Test AP: 0.2183\n",
                        "Seed: 42, Epoch: 034, Loss: 0.0544, Val AP: 0.1826, Test AP: 0.1835\n",
                        "Seed: 42, Epoch: 035, Loss: 0.0544, Val AP: 0.2164, Test AP: 0.2207\n",
                        "Seed: 42, Epoch: 036, Loss: 0.0543, Val AP: 0.2168, Test AP: 0.2197\n",
                        "Seed: 42, Epoch: 037, Loss: 0.0543, Val AP: 0.1958, Test AP: 0.1992\n",
                        "Seed: 42, Epoch: 038, Loss: 0.0543, Val AP: 0.2096, Test AP: 0.2130\n",
                        "Seed: 42, Epoch: 039, Loss: 0.0542, Val AP: 0.2081, Test AP: 0.2110\n",
                        "Seed: 42, Epoch: 040, Loss: 0.0541, Val AP: 0.2142, Test AP: 0.2157\n",
                        "Seed: 42, Epoch: 041, Loss: 0.0541, Val AP: 0.2290, Test AP: 0.2315\n",
                        "Seed: 42, Epoch: 042, Loss: 0.0540, Val AP: 0.2237, Test AP: 0.2284\n",
                        "Seed: 42, Epoch: 043, Loss: 0.0540, Val AP: 0.2180, Test AP: 0.2195\n",
                        "Seed: 42, Epoch: 044, Loss: 0.0541, Val AP: 0.2104, Test AP: 0.2138\n",
                        "Seed: 42, Epoch: 045, Loss: 0.0540, Val AP: 0.2203, Test AP: 0.2233\n",
                        "Seed: 42, Epoch: 046, Loss: 0.0539, Val AP: 0.2251, Test AP: 0.2304\n",
                        "Seed: 42, Epoch: 047, Loss: 0.0539, Val AP: 0.2176, Test AP: 0.2204\n",
                        "Seed: 42, Epoch: 048, Loss: 0.0539, Val AP: 0.2225, Test AP: 0.2251\n",
                        "Seed: 42, Epoch: 049, Loss: 0.0539, Val AP: 0.2012, Test AP: 0.2010\n",
                        "Seed: 42, Epoch: 050, Loss: 0.0538, Val AP: 0.2246, Test AP: 0.2269\n",
                        "Seed: 42, Epoch: 051, Loss: 0.0538, Val AP: 0.2246, Test AP: 0.2279\n",
                        "Seed: 42, Epoch: 052, Loss: 0.0538, Val AP: 0.2067, Test AP: 0.2093\n",
                        "Seed: 42, Epoch: 053, Loss: 0.0538, Val AP: 0.2283, Test AP: 0.2321\n",
                        "Seed: 42, Epoch: 054, Loss: 0.0538, Val AP: 0.2308, Test AP: 0.2331\n",
                        "Seed: 42, Epoch: 055, Loss: 0.0538, Val AP: 0.2120, Test AP: 0.2138\n",
                        "Seed: 42, Epoch: 056, Loss: 0.0537, Val AP: 0.2289, Test AP: 0.2301\n",
                        "Seed: 42, Epoch: 057, Loss: 0.0538, Val AP: 0.2306, Test AP: 0.2337\n",
                        "Seed: 42, Epoch: 058, Loss: 0.0537, Val AP: 0.2373, Test AP: 0.2418\n",
                        "Seed: 42, Epoch: 059, Loss: 0.0536, Val AP: 0.2207, Test AP: 0.2238\n",
                        "Seed: 42, Epoch: 060, Loss: 0.0536, Val AP: 0.2211, Test AP: 0.2253\n",
                        "Seed: 42, Epoch: 061, Loss: 0.0536, Val AP: 0.2300, Test AP: 0.2327\n",
                        "Seed: 42, Epoch: 062, Loss: 0.0535, Val AP: 0.2244, Test AP: 0.2265\n",
                        "Seed: 42, Epoch: 063, Loss: 0.0536, Val AP: 0.2230, Test AP: 0.2260\n",
                        "Seed: 42, Epoch: 064, Loss: 0.0536, Val AP: 0.2271, Test AP: 0.2317\n",
                        "Seed: 42, Epoch: 065, Loss: 0.0536, Val AP: 0.2323, Test AP: 0.2342\n",
                        "Seed: 42, Epoch: 066, Loss: 0.0535, Val AP: 0.2226, Test AP: 0.2268\n",
                        "Seed: 42, Epoch: 067, Loss: 0.0535, Val AP: 0.2163, Test AP: 0.2192\n",
                        "Seed: 42, Epoch: 068, Loss: 0.0535, Val AP: 0.2121, Test AP: 0.2153\n",
                        "Seed: 42, Epoch: 069, Loss: 0.0535, Val AP: 0.2203, Test AP: 0.2244\n",
                        "Seed: 42, Epoch: 070, Loss: 0.0535, Val AP: 0.2189, Test AP: 0.2228\n",
                        "Seed: 42, Epoch: 071, Loss: 0.0535, Val AP: 0.2252, Test AP: 0.2282\n",
                        "Seed: 42, Epoch: 072, Loss: 0.0535, Val AP: 0.2347, Test AP: 0.2395\n",
                        "Seed: 42, Epoch: 073, Loss: 0.0534, Val AP: 0.2152, Test AP: 0.2191\n",
                        "Seed: 42, Epoch: 074, Loss: 0.0534, Val AP: 0.2179, Test AP: 0.2217\n",
                        "Seed: 42, Epoch: 075, Loss: 0.0534, Val AP: 0.2144, Test AP: 0.2173\n",
                        "Seed: 42, Epoch: 076, Loss: 0.0534, Val AP: 0.2199, Test AP: 0.2240\n",
                        "Seed: 42, Epoch: 077, Loss: 0.0534, Val AP: 0.2322, Test AP: 0.2343\n",
                        "Seed: 42, Epoch: 078, Loss: 0.0534, Val AP: 0.2218, Test AP: 0.2258\n",
                        "Seed: 42, Epoch: 079, Loss: 0.0534, Val AP: 0.2240, Test AP: 0.2283\n",
                        "Seed: 42, Epoch: 080, Loss: 0.0533, Val AP: 0.2333, Test AP: 0.2377\n",
                        "Seed: 42, Epoch: 081, Loss: 0.0533, Val AP: 0.2220, Test AP: 0.2280\n",
                        "Seed: 42, Epoch: 082, Loss: 0.0533, Val AP: 0.2256, Test AP: 0.2294\n",
                        "Seed: 42, Epoch: 083, Loss: 0.0533, Val AP: 0.2344, Test AP: 0.2370\n",
                        "Seed: 42, Epoch: 084, Loss: 0.0533, Val AP: 0.2256, Test AP: 0.2287\n",
                        "Seed: 42, Epoch: 085, Loss: 0.0533, Val AP: 0.2284, Test AP: 0.2336\n",
                        "Seed: 42, Epoch: 086, Loss: 0.0533, Val AP: 0.2315, Test AP: 0.2349\n",
                        "Seed: 42, Epoch: 087, Loss: 0.0533, Val AP: 0.2214, Test AP: 0.2260\n",
                        "Seed: 42, Epoch: 088, Loss: 0.0533, Val AP: 0.2321, Test AP: 0.2351\n",
                        "Seed: 42, Epoch: 089, Loss: 0.0532, Val AP: 0.2305, Test AP: 0.2333\n",
                        "Seed: 42, Epoch: 090, Loss: 0.0533, Val AP: 0.2328, Test AP: 0.2361\n",
                        "Seed: 42, Epoch: 091, Loss: 0.0532, Val AP: 0.2351, Test AP: 0.2386\n",
                        "Seed: 42, Epoch: 092, Loss: 0.0532, Val AP: 0.2257, Test AP: 0.2297\n",
                        "Seed: 42, Epoch: 093, Loss: 0.0532, Val AP: 0.2438, Test AP: 0.2472\n",
                        "Seed: 42, Epoch: 094, Loss: 0.0533, Val AP: 0.2301, Test AP: 0.2338\n",
                        "Seed: 42, Epoch: 095, Loss: 0.0532, Val AP: 0.2308, Test AP: 0.2339\n",
                        "Seed: 42, Epoch: 096, Loss: 0.0532, Val AP: 0.2240, Test AP: 0.2281\n",
                        "Seed: 42, Epoch: 097, Loss: 0.0532, Val AP: 0.2448, Test AP: 0.2509\n",
                        "Seed: 42, Epoch: 098, Loss: 0.0532, Val AP: 0.2389, Test AP: 0.2443\n",
                        "Seed: 42, Epoch: 099, Loss: 0.0532, Val AP: 0.2258, Test AP: 0.2285\n",
                        "Seed: 42, Epoch: 100, Loss: 0.0532, Val AP: 0.2326, Test AP: 0.2368\n",
                        "Seed: 42, Epoch: 101, Loss: 0.0532, Val AP: 0.2331, Test AP: 0.2367\n",
                        "Seed: 42, Epoch: 102, Loss: 0.0532, Val AP: 0.2299, Test AP: 0.2322\n",
                        "Seed: 42, Epoch: 103, Loss: 0.0531, Val AP: 0.2304, Test AP: 0.2354\n",
                        "Seed: 42, Epoch: 104, Loss: 0.0531, Val AP: 0.2336, Test AP: 0.2378\n",
                        "Seed: 42, Epoch: 105, Loss: 0.0532, Val AP: 0.2317, Test AP: 0.2347\n",
                        "Seed: 42, Epoch: 106, Loss: 0.0531, Val AP: 0.2176, Test AP: 0.2219\n",
                        "Seed: 42, Epoch: 107, Loss: 0.0531, Val AP: 0.2448, Test AP: 0.2490\n",
                        "Seed: 42, Epoch: 108, Loss: 0.0531, Val AP: 0.2279, Test AP: 0.2323\n",
                        "Seed: 42, Epoch: 109, Loss: 0.0531, Val AP: 0.2429, Test AP: 0.2472\n",
                        "Seed: 42, Epoch: 110, Loss: 0.0532, Val AP: 0.2357, Test AP: 0.2381\n",
                        "Seed: 42, Epoch: 111, Loss: 0.0531, Val AP: 0.2407, Test AP: 0.2443\n",
                        "Seed: 42, Epoch: 112, Loss: 0.0531, Val AP: 0.2457, Test AP: 0.2489\n",
                        "Seed: 42, Epoch: 113, Loss: 0.0531, Val AP: 0.2319, Test AP: 0.2344\n",
                        "Seed: 42, Epoch: 114, Loss: 0.0531, Val AP: 0.2380, Test AP: 0.2415\n",
                        "Seed: 42, Epoch: 115, Loss: 0.0530, Val AP: 0.2367, Test AP: 0.2408\n",
                        "Seed: 42, Epoch: 116, Loss: 0.0531, Val AP: 0.2385, Test AP: 0.2419\n",
                        "Seed: 42, Epoch: 117, Loss: 0.0530, Val AP: 0.2192, Test AP: 0.2212\n",
                        "Seed: 42, Epoch: 118, Loss: 0.0530, Val AP: 0.2384, Test AP: 0.2409\n",
                        "Seed: 42, Epoch: 119, Loss: 0.0530, Val AP: 0.2422, Test AP: 0.2450\n",
                        "Seed: 42, Epoch: 120, Loss: 0.0531, Val AP: 0.2400, Test AP: 0.2439\n",
                        "Seed: 42, Epoch: 121, Loss: 0.0530, Val AP: 0.2399, Test AP: 0.2436\n",
                        "Seed: 42, Epoch: 122, Loss: 0.0530, Val AP: 0.2312, Test AP: 0.2345\n",
                        "Seed: 42, Epoch: 123, Loss: 0.0531, Val AP: 0.2227, Test AP: 0.2260\n",
                        "Seed: 42, Epoch: 124, Loss: 0.0530, Val AP: 0.2373, Test AP: 0.2406\n",
                        "Seed: 42, Epoch: 125, Loss: 0.0530, Val AP: 0.2426, Test AP: 0.2444\n",
                        "Seed: 42, Epoch: 126, Loss: 0.0529, Val AP: 0.2223, Test AP: 0.2263\n",
                        "Seed: 42, Epoch: 127, Loss: 0.0529, Val AP: 0.2425, Test AP: 0.2438\n",
                        "Seed: 42, Epoch: 128, Loss: 0.0530, Val AP: 0.2309, Test AP: 0.2338\n",
                        "Seed: 42, Epoch: 129, Loss: 0.0530, Val AP: 0.2331, Test AP: 0.2360\n",
                        "Seed: 42, Epoch: 130, Loss: 0.0529, Val AP: 0.2359, Test AP: 0.2390\n",
                        "Seed: 42, Epoch: 131, Loss: 0.0529, Val AP: 0.2361, Test AP: 0.2392\n",
                        "Seed: 42, Epoch: 132, Loss: 0.0529, Val AP: 0.2348, Test AP: 0.2386\n",
                        "Seed: 42, Epoch: 133, Loss: 0.0529, Val AP: 0.2385, Test AP: 0.2419\n",
                        "Seed: 42, Epoch: 134, Loss: 0.0529, Val AP: 0.2344, Test AP: 0.2392\n",
                        "Seed: 42, Epoch: 135, Loss: 0.0529, Val AP: 0.2345, Test AP: 0.2377\n",
                        "Seed: 42, Epoch: 136, Loss: 0.0529, Val AP: 0.2337, Test AP: 0.2347\n",
                        "Seed: 42, Epoch: 137, Loss: 0.0530, Val AP: 0.2494, Test AP: 0.2510\n",
                        "Seed: 42, Epoch: 138, Loss: 0.0529, Val AP: 0.2308, Test AP: 0.2332\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import SAGEConv\nimport os.path as osp\nimport time\nfrom math import ceil\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_sparse import SparseTensor\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, dense_diff_pool\nclass GNN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, normalize)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, normalize)\n        self.conv3 = GCNConv(hidden_channels, out_channels, normalize)\n        if lin:\n            self.lin = torch.nn.Linear(out_channels, out_channels)\n        else:\n            self.lin = None\n    def forward(self, x, adj_t, mask=None):\n        adj_t = adj_t.to(torch.float32)\n        x = self.conv1(x, adj_t, mask).relu()\n        x = self.conv2(x, adj_t, mask).relu()\n        x = self.conv3(x, adj_t, mask).relu()\n        if self.lin is not None:\n            x = self.lin(x).relu()\n        return x\nclass Net_Diff(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        num_nodes = 64\n        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n        self.gnn1_embed = GCNConv(dataset_dense.num_features, 64)\n        num_nodes = 64\n        self.gnn2_pool = GNN(64, 64, num_nodes)\n        self.gnn2_embed = GCNConv(64, 64)\n        self.gnn3_embed = GCNConv(64, 64)\n        self.lin1 = torch.nn.Linear(64, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, x, adj_t, batch):\n        adj_t = adj_t.to(torch.float32)\n        s = self.gnn1_pool(x, adj_t)\n        x = self.gnn1_embed(x, adj_t)\n        x = F.relu(x)\n        if isinstance(adj_t, SparseTensor):\n            adj_t_dense = adj_t.to_dense()\n            adj_t_dense = adj_t_dense.unsqueeze(0) if adj_t_dense.dim() == 2 else adj_t_dense\n        else:\n            adj_t_dense = adj_t.unsqueeze(0) if adj_t.dim() == 2 else adj_t\n            x, adj_t, tv1, bal1 = dense_diff_pool(x, adj_t_dense, s)\n        s = self.gnn2_pool(x, adj_t)\n        x = self.gnn2_embed(x, adj_t)\n        x = F.relu(x)\n        if isinstance(adj_t, SparseTensor):\n            adj_t_dense = adj_t.to_dense()\n            adj_t_dense = adj_t_dense.unsqueeze(0) if adj_t_dense.dim() == 2 else adj_t_dense\n        else:\n            adj_t_dense = adj_t.unsqueeze(0) if adj_t.dim() == 2 else adj_t\n            x, adj_t, tv1, bal1 = dense_diff_pool(x, adj_t_dense)\n        x = self.gnn3_embed(x, adj_t)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Net_Diff().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        x, adj_t = data.x, data.adj_t\n        x, adj_t, batch = data.x.to(torch.float32), data.adj_t.to(torch.float32), data.batch\n        out = model(x, adj_t, batch)\n        pred = out\n        is_labeled = data.y == data.y \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        x, adj_t = data.x, data.adj_t\n        x, adj_t, batch = data.x.to(torch.float32), data.adj_t.to(torch.float32), data.batch\n        out = model(x, adj_t, batch)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_dense = dataset_dense.shuffle()\n    num_total = len(dataset_dense)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_dense[:num_train]\n    val_dataset = dataset_dense[num_train:num_train + num_val]\n    test_dataset = dataset_dense[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n    model = Net_Diff().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### MincutPool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0607, Val AP: 0.1209, Test AP: 0.1236\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0597, Val AP: 0.1313, Test AP: 0.1354\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0591, Val AP: 0.1362, Test AP: 0.1396\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0586, Val AP: 0.1409, Test AP: 0.1449\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0582, Val AP: 0.1476, Test AP: 0.1531\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0579, Val AP: 0.1485, Test AP: 0.1526\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0575, Val AP: 0.1663, Test AP: 0.1708\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0572, Val AP: 0.1612, Test AP: 0.1642\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0569, Val AP: 0.1615, Test AP: 0.1645\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0567, Val AP: 0.1700, Test AP: 0.1732\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0564, Val AP: 0.1797, Test AP: 0.1819\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0563, Val AP: 0.1785, Test AP: 0.1805\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0561, Val AP: 0.1845, Test AP: 0.1876\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0559, Val AP: 0.1881, Test AP: 0.1886\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0557, Val AP: 0.1926, Test AP: 0.1931\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0555, Val AP: 0.1940, Test AP: 0.1975\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0554, Val AP: 0.2027, Test AP: 0.2055\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0552, Val AP: 0.1985, Test AP: 0.2012\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0551, Val AP: 0.2033, Test AP: 0.2050\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0550, Val AP: 0.2010, Test AP: 0.2035\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0549, Val AP: 0.2002, Test AP: 0.2006\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0548, Val AP: 0.2035, Test AP: 0.2061\n",
                        "Seed: 42, Epoch: 023, Loss: 0.0548, Val AP: 0.2079, Test AP: 0.2102\n",
                        "Seed: 42, Epoch: 024, Loss: 0.0547, Val AP: 0.2063, Test AP: 0.2082\n",
                        "Seed: 42, Epoch: 025, Loss: 0.0546, Val AP: 0.2079, Test AP: 0.2089\n",
                        "Seed: 42, Epoch: 026, Loss: 0.0545, Val AP: 0.2081, Test AP: 0.2096\n",
                        "Seed: 42, Epoch: 027, Loss: 0.0545, Val AP: 0.2190, Test AP: 0.2206\n",
                        "Seed: 42, Epoch: 028, Loss: 0.0544, Val AP: 0.2140, Test AP: 0.2152\n",
                        "Seed: 42, Epoch: 029, Loss: 0.0543, Val AP: 0.1981, Test AP: 0.1989\n",
                        "Seed: 42, Epoch: 030, Loss: 0.0543, Val AP: 0.2142, Test AP: 0.2148\n",
                        "Seed: 42, Epoch: 031, Loss: 0.0542, Val AP: 0.2158, Test AP: 0.2179\n",
                        "Seed: 42, Epoch: 032, Loss: 0.0542, Val AP: 0.2102, Test AP: 0.2116\n",
                        "Seed: 42, Epoch: 033, Loss: 0.0542, Val AP: 0.2163, Test AP: 0.2183\n",
                        "Seed: 42, Epoch: 034, Loss: 0.0541, Val AP: 0.2235, Test AP: 0.2257\n",
                        "Seed: 42, Epoch: 035, Loss: 0.0540, Val AP: 0.2133, Test AP: 0.2142\n",
                        "Seed: 42, Epoch: 036, Loss: 0.0540, Val AP: 0.2165, Test AP: 0.2181\n",
                        "Seed: 42, Epoch: 037, Loss: 0.0540, Val AP: 0.2174, Test AP: 0.2177\n",
                        "Seed: 42, Epoch: 038, Loss: 0.0540, Val AP: 0.2211, Test AP: 0.2215\n",
                        "Seed: 42, Epoch: 039, Loss: 0.0539, Val AP: 0.2249, Test AP: 0.2272\n",
                        "Seed: 42, Epoch: 040, Loss: 0.0538, Val AP: 0.2195, Test AP: 0.2214\n",
                        "Seed: 42, Epoch: 041, Loss: 0.0538, Val AP: 0.2072, Test AP: 0.2086\n",
                        "Seed: 42, Epoch: 042, Loss: 0.0538, Val AP: 0.2255, Test AP: 0.2265\n",
                        "Seed: 42, Epoch: 043, Loss: 0.0537, Val AP: 0.2309, Test AP: 0.2319\n",
                        "Seed: 42, Epoch: 044, Loss: 0.0537, Val AP: 0.2330, Test AP: 0.2332\n",
                        "Seed: 42, Epoch: 045, Loss: 0.0537, Val AP: 0.2251, Test AP: 0.2261\n",
                        "Seed: 42, Epoch: 046, Loss: 0.0537, Val AP: 0.2155, Test AP: 0.2158\n",
                        "Seed: 42, Epoch: 047, Loss: 0.0536, Val AP: 0.2254, Test AP: 0.2263\n",
                        "Seed: 42, Epoch: 048, Loss: 0.0535, Val AP: 0.2131, Test AP: 0.2134\n",
                        "Seed: 42, Epoch: 049, Loss: 0.0535, Val AP: 0.2095, Test AP: 0.2109\n",
                        "Seed: 42, Epoch: 050, Loss: 0.0535, Val AP: 0.2257, Test AP: 0.2273\n",
                        "Seed: 42, Epoch: 051, Loss: 0.0535, Val AP: 0.2327, Test AP: 0.2332\n",
                        "Seed: 42, Epoch: 052, Loss: 0.0535, Val AP: 0.2212, Test AP: 0.2205\n",
                        "Seed: 42, Epoch: 053, Loss: 0.0535, Val AP: 0.2230, Test AP: 0.2233\n",
                        "Seed: 42, Epoch: 054, Loss: 0.0534, Val AP: 0.2107, Test AP: 0.2116\n",
                        "Seed: 42, Epoch: 055, Loss: 0.0534, Val AP: 0.2231, Test AP: 0.2222\n",
                        "Seed: 42, Epoch: 056, Loss: 0.0534, Val AP: 0.2147, Test AP: 0.2138\n",
                        "Seed: 42, Epoch: 057, Loss: 0.0534, Val AP: 0.2311, Test AP: 0.2314\n",
                        "Seed: 42, Epoch: 058, Loss: 0.0533, Val AP: 0.2349, Test AP: 0.2343\n",
                        "Seed: 42, Epoch: 059, Loss: 0.0533, Val AP: 0.2326, Test AP: 0.2327\n",
                        "Seed: 42, Epoch: 060, Loss: 0.0533, Val AP: 0.2275, Test AP: 0.2298\n",
                        "Seed: 42, Epoch: 061, Loss: 0.0533, Val AP: 0.2272, Test AP: 0.2280\n",
                        "Seed: 42, Epoch: 062, Loss: 0.0533, Val AP: 0.2303, Test AP: 0.2286\n",
                        "Seed: 42, Epoch: 063, Loss: 0.0533, Val AP: 0.2341, Test AP: 0.2341\n",
                        "Seed: 42, Epoch: 064, Loss: 0.0532, Val AP: 0.2220, Test AP: 0.2205\n",
                        "Seed: 42, Epoch: 065, Loss: 0.0533, Val AP: 0.2289, Test AP: 0.2282\n",
                        "Seed: 42, Epoch: 066, Loss: 0.0532, Val AP: 0.2141, Test AP: 0.2160\n",
                        "Seed: 42, Epoch: 067, Loss: 0.0532, Val AP: 0.2318, Test AP: 0.2334\n",
                        "Seed: 42, Epoch: 068, Loss: 0.0531, Val AP: 0.2388, Test AP: 0.2362\n",
                        "Seed: 42, Epoch: 069, Loss: 0.0532, Val AP: 0.2299, Test AP: 0.2300\n",
                        "Seed: 42, Epoch: 070, Loss: 0.0532, Val AP: 0.2324, Test AP: 0.2313\n",
                        "Seed: 42, Epoch: 071, Loss: 0.0531, Val AP: 0.2285, Test AP: 0.2284\n",
                        "Seed: 42, Epoch: 072, Loss: 0.0531, Val AP: 0.2366, Test AP: 0.2353\n",
                        "Seed: 42, Epoch: 073, Loss: 0.0531, Val AP: 0.2371, Test AP: 0.2366\n",
                        "Seed: 42, Epoch: 074, Loss: 0.0531, Val AP: 0.2405, Test AP: 0.2394\n",
                        "Seed: 42, Epoch: 075, Loss: 0.0531, Val AP: 0.2275, Test AP: 0.2292\n",
                        "Seed: 42, Epoch: 076, Loss: 0.0531, Val AP: 0.2272, Test AP: 0.2262\n",
                        "Seed: 42, Epoch: 077, Loss: 0.0531, Val AP: 0.2305, Test AP: 0.2289\n",
                        "Seed: 42, Epoch: 078, Loss: 0.0530, Val AP: 0.2278, Test AP: 0.2244\n",
                        "Seed: 42, Epoch: 079, Loss: 0.0530, Val AP: 0.2299, Test AP: 0.2276\n",
                        "Seed: 42, Epoch: 080, Loss: 0.0531, Val AP: 0.2303, Test AP: 0.2315\n",
                        "Seed: 42, Epoch: 081, Loss: 0.0530, Val AP: 0.2352, Test AP: 0.2342\n",
                        "Seed: 42, Epoch: 082, Loss: 0.0530, Val AP: 0.2406, Test AP: 0.2406\n",
                        "Seed: 42, Epoch: 083, Loss: 0.0530, Val AP: 0.2296, Test AP: 0.2299\n",
                        "Seed: 42, Epoch: 084, Loss: 0.0529, Val AP: 0.2375, Test AP: 0.2368\n",
                        "Seed: 42, Epoch: 085, Loss: 0.0529, Val AP: 0.2370, Test AP: 0.2375\n",
                        "Seed: 42, Epoch: 086, Loss: 0.0529, Val AP: 0.2416, Test AP: 0.2409\n",
                        "Seed: 42, Epoch: 087, Loss: 0.0529, Val AP: 0.2377, Test AP: 0.2366\n",
                        "Seed: 42, Epoch: 088, Loss: 0.0530, Val AP: 0.2268, Test AP: 0.2263\n",
                        "Seed: 42, Epoch: 089, Loss: 0.0529, Val AP: 0.2364, Test AP: 0.2365\n",
                        "Seed: 42, Epoch: 090, Loss: 0.0529, Val AP: 0.2209, Test AP: 0.2176\n",
                        "Seed: 42, Epoch: 091, Loss: 0.0530, Val AP: 0.2366, Test AP: 0.2346\n",
                        "Seed: 42, Epoch: 092, Loss: 0.0528, Val AP: 0.2301, Test AP: 0.2319\n",
                        "Seed: 42, Epoch: 093, Loss: 0.0529, Val AP: 0.2397, Test AP: 0.2380\n",
                        "Seed: 42, Epoch: 094, Loss: 0.0529, Val AP: 0.2400, Test AP: 0.2396\n",
                        "Seed: 42, Epoch: 095, Loss: 0.0528, Val AP: 0.2367, Test AP: 0.2368\n",
                        "Seed: 42, Epoch: 096, Loss: 0.0528, Val AP: 0.2488, Test AP: 0.2482\n",
                        "Seed: 42, Epoch: 097, Loss: 0.0528, Val AP: 0.2162, Test AP: 0.2153\n",
                        "Seed: 42, Epoch: 098, Loss: 0.0528, Val AP: 0.2382, Test AP: 0.2398\n",
                        "Seed: 42, Epoch: 099, Loss: 0.0528, Val AP: 0.2229, Test AP: 0.2244\n",
                        "Seed: 42, Epoch: 100, Loss: 0.0527, Val AP: 0.2371, Test AP: 0.2385\n",
                        "Seed: 42, Epoch: 101, Loss: 0.0528, Val AP: 0.2394, Test AP: 0.2394\n",
                        "Seed: 42, Epoch: 102, Loss: 0.0527, Val AP: 0.2317, Test AP: 0.2308\n",
                        "Seed: 42, Epoch: 103, Loss: 0.0528, Val AP: 0.2394, Test AP: 0.2379\n",
                        "Seed: 42, Epoch: 104, Loss: 0.0527, Val AP: 0.2332, Test AP: 0.2327\n",
                        "Seed: 42, Epoch: 105, Loss: 0.0527, Val AP: 0.2420, Test AP: 0.2408\n",
                        "Seed: 42, Epoch: 106, Loss: 0.0527, Val AP: 0.2392, Test AP: 0.2378\n",
                        "Seed: 42, Epoch: 107, Loss: 0.0528, Val AP: 0.2324, Test AP: 0.2318\n",
                        "Seed: 42, Epoch: 108, Loss: 0.0528, Val AP: 0.2363, Test AP: 0.2356\n",
                        "Seed: 42, Epoch: 109, Loss: 0.0528, Val AP: 0.2337, Test AP: 0.2327\n",
                        "Seed: 42, Epoch: 110, Loss: 0.0528, Val AP: 0.2483, Test AP: 0.2472\n",
                        "Seed: 42, Epoch: 111, Loss: 0.0527, Val AP: 0.2356, Test AP: 0.2338\n",
                        "Seed: 42, Epoch: 112, Loss: 0.0527, Val AP: 0.2336, Test AP: 0.2334\n",
                        "Seed: 42, Epoch: 113, Loss: 0.0527, Val AP: 0.2447, Test AP: 0.2432\n",
                        "Seed: 42, Epoch: 114, Loss: 0.0527, Val AP: 0.2465, Test AP: 0.2453\n",
                        "Seed: 42, Epoch: 115, Loss: 0.0527, Val AP: 0.2233, Test AP: 0.2203\n",
                        "Seed: 42, Epoch: 116, Loss: 0.0527, Val AP: 0.2382, Test AP: 0.2366\n",
                        "Seed: 42, Epoch: 117, Loss: 0.0527, Val AP: 0.2413, Test AP: 0.2401\n",
                        "Seed: 42, Epoch: 118, Loss: 0.0527, Val AP: 0.2331, Test AP: 0.2334\n",
                        "Seed: 42, Epoch: 119, Loss: 0.0527, Val AP: 0.2483, Test AP: 0.2468\n",
                        "Seed: 42, Epoch: 120, Loss: 0.0526, Val AP: 0.2392, Test AP: 0.2360\n",
                        "Seed: 42, Epoch: 121, Loss: 0.0526, Val AP: 0.2381, Test AP: 0.2392\n",
                        "Seed: 42, Epoch: 122, Loss: 0.0527, Val AP: 0.2331, Test AP: 0.2334\n",
                        "Seed: 42, Epoch: 123, Loss: 0.0526, Val AP: 0.2406, Test AP: 0.2401\n",
                        "Seed: 42, Epoch: 124, Loss: 0.0526, Val AP: 0.2434, Test AP: 0.2427\n",
                        "Seed: 42, Epoch: 125, Loss: 0.0527, Val AP: 0.2400, Test AP: 0.2396\n",
                        "Seed: 42, Epoch: 126, Loss: 0.0527, Val AP: 0.2338, Test AP: 0.2335\n",
                        "Seed: 42, Epoch: 127, Loss: 0.0526, Val AP: 0.2419, Test AP: 0.2415\n",
                        "Seed: 42, Epoch: 128, Loss: 0.0526, Val AP: 0.2354, Test AP: 0.2355\n",
                        "Seed: 42, Epoch: 129, Loss: 0.0526, Val AP: 0.2372, Test AP: 0.2378\n",
                        "Seed: 42, Epoch: 130, Loss: 0.0526, Val AP: 0.2388, Test AP: 0.2390\n",
                        "Seed: 42, Epoch: 131, Loss: 0.0526, Val AP: 0.2333, Test AP: 0.2322\n",
                        "Seed: 42, Epoch: 132, Loss: 0.0525, Val AP: 0.2413, Test AP: 0.2383\n",
                        "Seed: 42, Epoch: 133, Loss: 0.0526, Val AP: 0.2446, Test AP: 0.2439\n",
                        "Seed: 42, Epoch: 134, Loss: 0.0526, Val AP: 0.2364, Test AP: 0.2375\n",
                        "Seed: 42, Epoch: 135, Loss: 0.0526, Val AP: 0.2411, Test AP: 0.2415\n",
                        "Seed: 42, Epoch: 136, Loss: 0.0526, Val AP: 0.2404, Test AP: 0.2402\n",
                        "Seed: 42, Epoch: 137, Loss: 0.0526, Val AP: 0.2426, Test AP: 0.2419\n",
                        "Seed: 42, Epoch: 138, Loss: 0.0526, Val AP: 0.2412, Test AP: 0.2396\n",
                        "Seed: 42, Epoch: 139, Loss: 0.0525, Val AP: 0.2517, Test AP: 0.2508\n",
                        "Seed: 42, Epoch: 140, Loss: 0.0525, Val AP: 0.2471, Test AP: 0.2477\n",
                        "Seed: 42, Epoch: 141, Loss: 0.0525, Val AP: 0.2373, Test AP: 0.2361\n",
                        "Seed: 42, Epoch: 142, Loss: 0.0525, Val AP: 0.2438, Test AP: 0.2447\n",
                        "Seed: 42, Epoch: 143, Loss: 0.0525, Val AP: 0.2419, Test AP: 0.2407\n",
                        "Seed: 42, Epoch: 144, Loss: 0.0526, Val AP: 0.2541, Test AP: 0.2535\n",
                        "Seed: 42, Epoch: 145, Loss: 0.0525, Val AP: 0.2419, Test AP: 0.2406\n",
                        "Seed: 42, Epoch: 146, Loss: 0.0525, Val AP: 0.2404, Test AP: 0.2397\n",
                        "Seed: 42, Epoch: 147, Loss: 0.0525, Val AP: 0.2325, Test AP: 0.2317\n",
                        "Seed: 42, Epoch: 148, Loss: 0.0525, Val AP: 0.2500, Test AP: 0.2482\n",
                        "Seed: 42, Epoch: 149, Loss: 0.0525, Val AP: 0.2237, Test AP: 0.2249\n",
                        "Seed: 42, Epoch: 150, Loss: 0.0525, Val AP: 0.2375, Test AP: 0.2353\n",
                        "Seed: 43, Epoch: 001, Loss: 0.0610, Val AP: 0.1227, Test AP: 0.1213\n",
                        "Seed: 43, Epoch: 002, Loss: 0.0600, Val AP: 0.1257, Test AP: 0.1237\n",
                        "Seed: 43, Epoch: 003, Loss: 0.0595, Val AP: 0.1403, Test AP: 0.1387\n",
                        "Seed: 43, Epoch: 004, Loss: 0.0591, Val AP: 0.1396, Test AP: 0.1381\n",
                        "Seed: 43, Epoch: 005, Loss: 0.0588, Val AP: 0.1446, Test AP: 0.1421\n",
                        "Seed: 43, Epoch: 006, Loss: 0.0586, Val AP: 0.1488, Test AP: 0.1470\n",
                        "Seed: 43, Epoch: 007, Loss: 0.0584, Val AP: 0.1560, Test AP: 0.1557\n",
                        "Seed: 43, Epoch: 008, Loss: 0.0581, Val AP: 0.1576, Test AP: 0.1576\n",
                        "Seed: 43, Epoch: 009, Loss: 0.0579, Val AP: 0.1527, Test AP: 0.1535\n",
                        "Seed: 43, Epoch: 010, Loss: 0.0576, Val AP: 0.1590, Test AP: 0.1594\n",
                        "Seed: 43, Epoch: 011, Loss: 0.0574, Val AP: 0.1632, Test AP: 0.1654\n",
                        "Seed: 43, Epoch: 012, Loss: 0.0572, Val AP: 0.1698, Test AP: 0.1708\n",
                        "Seed: 43, Epoch: 013, Loss: 0.0569, Val AP: 0.1705, Test AP: 0.1726\n",
                        "Seed: 43, Epoch: 014, Loss: 0.0567, Val AP: 0.1718, Test AP: 0.1726\n",
                        "Seed: 43, Epoch: 015, Loss: 0.0566, Val AP: 0.1817, Test AP: 0.1831\n",
                        "Seed: 43, Epoch: 016, Loss: 0.0564, Val AP: 0.1669, Test AP: 0.1679\n",
                        "Seed: 43, Epoch: 017, Loss: 0.0563, Val AP: 0.1827, Test AP: 0.1836\n",
                        "Seed: 43, Epoch: 018, Loss: 0.0561, Val AP: 0.1807, Test AP: 0.1825\n",
                        "Seed: 43, Epoch: 019, Loss: 0.0560, Val AP: 0.1808, Test AP: 0.1828\n",
                        "Seed: 43, Epoch: 020, Loss: 0.0559, Val AP: 0.1822, Test AP: 0.1857\n",
                        "Seed: 43, Epoch: 021, Loss: 0.0557, Val AP: 0.1942, Test AP: 0.1965\n",
                        "Seed: 43, Epoch: 022, Loss: 0.0556, Val AP: 0.1902, Test AP: 0.1924\n",
                        "Seed: 43, Epoch: 023, Loss: 0.0555, Val AP: 0.1999, Test AP: 0.2038\n",
                        "Seed: 43, Epoch: 024, Loss: 0.0555, Val AP: 0.1959, Test AP: 0.1989\n",
                        "Seed: 43, Epoch: 025, Loss: 0.0553, Val AP: 0.1937, Test AP: 0.1945\n",
                        "Seed: 43, Epoch: 026, Loss: 0.0552, Val AP: 0.1984, Test AP: 0.1996\n",
                        "Seed: 43, Epoch: 027, Loss: 0.0552, Val AP: 0.2044, Test AP: 0.2057\n",
                        "Seed: 43, Epoch: 028, Loss: 0.0551, Val AP: 0.1964, Test AP: 0.1965\n",
                        "Seed: 43, Epoch: 029, Loss: 0.0550, Val AP: 0.2076, Test AP: 0.2089\n",
                        "Seed: 43, Epoch: 030, Loss: 0.0550, Val AP: 0.1979, Test AP: 0.2002\n",
                        "Seed: 43, Epoch: 031, Loss: 0.0550, Val AP: 0.2054, Test AP: 0.2069\n",
                        "Seed: 43, Epoch: 032, Loss: 0.0549, Val AP: 0.2041, Test AP: 0.2060\n",
                        "Seed: 43, Epoch: 033, Loss: 0.0548, Val AP: 0.2134, Test AP: 0.2154\n",
                        "Seed: 43, Epoch: 034, Loss: 0.0548, Val AP: 0.1985, Test AP: 0.1996\n",
                        "Seed: 43, Epoch: 035, Loss: 0.0548, Val AP: 0.2005, Test AP: 0.2010\n",
                        "Seed: 43, Epoch: 036, Loss: 0.0547, Val AP: 0.2066, Test AP: 0.2084\n",
                        "Seed: 43, Epoch: 037, Loss: 0.0547, Val AP: 0.2062, Test AP: 0.2093\n",
                        "Seed: 43, Epoch: 038, Loss: 0.0547, Val AP: 0.2110, Test AP: 0.2129\n",
                        "Seed: 43, Epoch: 039, Loss: 0.0545, Val AP: 0.2109, Test AP: 0.2120\n",
                        "Seed: 43, Epoch: 040, Loss: 0.0545, Val AP: 0.2124, Test AP: 0.2135\n",
                        "Seed: 43, Epoch: 041, Loss: 0.0546, Val AP: 0.2090, Test AP: 0.2092\n",
                        "Seed: 43, Epoch: 042, Loss: 0.0545, Val AP: 0.2165, Test AP: 0.2188\n",
                        "Seed: 43, Epoch: 043, Loss: 0.0545, Val AP: 0.2124, Test AP: 0.2134\n",
                        "Seed: 43, Epoch: 044, Loss: 0.0544, Val AP: 0.2030, Test AP: 0.2027\n",
                        "Seed: 43, Epoch: 045, Loss: 0.0544, Val AP: 0.2191, Test AP: 0.2206\n",
                        "Seed: 43, Epoch: 046, Loss: 0.0544, Val AP: 0.2126, Test AP: 0.2134\n",
                        "Seed: 43, Epoch: 047, Loss: 0.0544, Val AP: 0.2205, Test AP: 0.2232\n",
                        "Seed: 43, Epoch: 048, Loss: 0.0544, Val AP: 0.2173, Test AP: 0.2192\n",
                        "Seed: 43, Epoch: 049, Loss: 0.0543, Val AP: 0.2112, Test AP: 0.2139\n",
                        "Seed: 43, Epoch: 050, Loss: 0.0543, Val AP: 0.2246, Test AP: 0.2265\n",
                        "Seed: 43, Epoch: 051, Loss: 0.0543, Val AP: 0.2244, Test AP: 0.2264\n",
                        "Seed: 43, Epoch: 052, Loss: 0.0543, Val AP: 0.2230, Test AP: 0.2252\n",
                        "Seed: 43, Epoch: 053, Loss: 0.0542, Val AP: 0.2232, Test AP: 0.2261\n",
                        "Seed: 43, Epoch: 054, Loss: 0.0541, Val AP: 0.2211, Test AP: 0.2225\n",
                        "Seed: 43, Epoch: 055, Loss: 0.0542, Val AP: 0.2155, Test AP: 0.2169\n",
                        "Seed: 43, Epoch: 056, Loss: 0.0541, Val AP: 0.2239, Test AP: 0.2255\n",
                        "Seed: 43, Epoch: 057, Loss: 0.0541, Val AP: 0.2298, Test AP: 0.2305\n",
                        "Seed: 43, Epoch: 058, Loss: 0.0542, Val AP: 0.2305, Test AP: 0.2339\n",
                        "Seed: 43, Epoch: 059, Loss: 0.0541, Val AP: 0.2208, Test AP: 0.2221\n",
                        "Seed: 43, Epoch: 060, Loss: 0.0540, Val AP: 0.2034, Test AP: 0.2032\n",
                        "Seed: 43, Epoch: 061, Loss: 0.0540, Val AP: 0.2167, Test AP: 0.2192\n",
                        "Seed: 43, Epoch: 062, Loss: 0.0541, Val AP: 0.2311, Test AP: 0.2331\n",
                        "Seed: 43, Epoch: 063, Loss: 0.0540, Val AP: 0.2249, Test AP: 0.2268\n",
                        "Seed: 43, Epoch: 064, Loss: 0.0540, Val AP: 0.2274, Test AP: 0.2309\n",
                        "Seed: 43, Epoch: 065, Loss: 0.0540, Val AP: 0.2125, Test AP: 0.2137\n",
                        "Seed: 43, Epoch: 066, Loss: 0.0540, Val AP: 0.2223, Test AP: 0.2246\n",
                        "Seed: 43, Epoch: 067, Loss: 0.0540, Val AP: 0.2090, Test AP: 0.2089\n",
                        "Seed: 43, Epoch: 068, Loss: 0.0540, Val AP: 0.2271, Test AP: 0.2294\n",
                        "Seed: 43, Epoch: 069, Loss: 0.0539, Val AP: 0.2304, Test AP: 0.2321\n",
                        "Seed: 43, Epoch: 070, Loss: 0.0540, Val AP: 0.2223, Test AP: 0.2235\n",
                        "Seed: 43, Epoch: 071, Loss: 0.0539, Val AP: 0.2349, Test AP: 0.2375\n",
                        "Seed: 43, Epoch: 072, Loss: 0.0539, Val AP: 0.2279, Test AP: 0.2295\n",
                        "Seed: 43, Epoch: 073, Loss: 0.0539, Val AP: 0.2189, Test AP: 0.2198\n",
                        "Seed: 43, Epoch: 074, Loss: 0.0538, Val AP: 0.2279, Test AP: 0.2299\n",
                        "Seed: 43, Epoch: 075, Loss: 0.0539, Val AP: 0.2316, Test AP: 0.2341\n",
                        "Seed: 43, Epoch: 076, Loss: 0.0539, Val AP: 0.2258, Test AP: 0.2283\n",
                        "Seed: 43, Epoch: 077, Loss: 0.0538, Val AP: 0.2181, Test AP: 0.2205\n",
                        "Seed: 43, Epoch: 078, Loss: 0.0539, Val AP: 0.2307, Test AP: 0.2321\n",
                        "Seed: 43, Epoch: 079, Loss: 0.0538, Val AP: 0.2136, Test AP: 0.2141\n",
                        "Seed: 43, Epoch: 080, Loss: 0.0538, Val AP: 0.2192, Test AP: 0.2216\n",
                        "Seed: 43, Epoch: 081, Loss: 0.0538, Val AP: 0.2240, Test AP: 0.2281\n",
                        "Seed: 43, Epoch: 082, Loss: 0.0538, Val AP: 0.2365, Test AP: 0.2401\n",
                        "Seed: 43, Epoch: 083, Loss: 0.0538, Val AP: 0.2285, Test AP: 0.2317\n",
                        "Seed: 43, Epoch: 084, Loss: 0.0538, Val AP: 0.2295, Test AP: 0.2324\n",
                        "Seed: 43, Epoch: 085, Loss: 0.0538, Val AP: 0.2054, Test AP: 0.2058\n",
                        "Seed: 43, Epoch: 086, Loss: 0.0538, Val AP: 0.2243, Test AP: 0.2276\n",
                        "Seed: 43, Epoch: 087, Loss: 0.0538, Val AP: 0.2215, Test AP: 0.2258\n",
                        "Seed: 43, Epoch: 088, Loss: 0.0538, Val AP: 0.2358, Test AP: 0.2382\n",
                        "Seed: 43, Epoch: 089, Loss: 0.0537, Val AP: 0.2334, Test AP: 0.2375\n",
                        "Seed: 43, Epoch: 090, Loss: 0.0538, Val AP: 0.2258, Test AP: 0.2287\n",
                        "Seed: 43, Epoch: 091, Loss: 0.0537, Val AP: 0.2128, Test AP: 0.2157\n",
                        "Seed: 43, Epoch: 092, Loss: 0.0538, Val AP: 0.2302, Test AP: 0.2313\n",
                        "Seed: 43, Epoch: 093, Loss: 0.0537, Val AP: 0.2203, Test AP: 0.2214\n",
                        "Seed: 43, Epoch: 094, Loss: 0.0537, Val AP: 0.2219, Test AP: 0.2230\n",
                        "Seed: 43, Epoch: 095, Loss: 0.0537, Val AP: 0.2339, Test AP: 0.2377\n",
                        "Seed: 43, Epoch: 096, Loss: 0.0537, Val AP: 0.2176, Test AP: 0.2202\n",
                        "Seed: 43, Epoch: 097, Loss: 0.0537, Val AP: 0.2293, Test AP: 0.2329\n",
                        "Seed: 43, Epoch: 098, Loss: 0.0537, Val AP: 0.2211, Test AP: 0.2234\n",
                        "Seed: 43, Epoch: 099, Loss: 0.0537, Val AP: 0.2348, Test AP: 0.2387\n",
                        "Seed: 43, Epoch: 100, Loss: 0.0537, Val AP: 0.2287, Test AP: 0.2333\n",
                        "Seed: 43, Epoch: 101, Loss: 0.0537, Val AP: 0.2210, Test AP: 0.2241\n",
                        "Seed: 43, Epoch: 102, Loss: 0.0537, Val AP: 0.2212, Test AP: 0.2238\n",
                        "Seed: 43, Epoch: 103, Loss: 0.0537, Val AP: 0.2253, Test AP: 0.2284\n",
                        "Seed: 43, Epoch: 104, Loss: 0.0536, Val AP: 0.2319, Test AP: 0.2347\n",
                        "Seed: 43, Epoch: 105, Loss: 0.0537, Val AP: 0.2230, Test AP: 0.2280\n",
                        "Seed: 43, Epoch: 106, Loss: 0.0536, Val AP: 0.2342, Test AP: 0.2388\n",
                        "Seed: 43, Epoch: 107, Loss: 0.0537, Val AP: 0.2315, Test AP: 0.2341\n",
                        "Seed: 43, Epoch: 108, Loss: 0.0536, Val AP: 0.2299, Test AP: 0.2324\n",
                        "Seed: 43, Epoch: 109, Loss: 0.0536, Val AP: 0.2296, Test AP: 0.2335\n",
                        "Seed: 43, Epoch: 110, Loss: 0.0536, Val AP: 0.2246, Test AP: 0.2272\n",
                        "Seed: 43, Epoch: 111, Loss: 0.0536, Val AP: 0.2264, Test AP: 0.2278\n",
                        "Seed: 43, Epoch: 112, Loss: 0.0536, Val AP: 0.2279, Test AP: 0.2317\n",
                        "Seed: 43, Epoch: 113, Loss: 0.0536, Val AP: 0.2272, Test AP: 0.2312\n",
                        "Seed: 43, Epoch: 114, Loss: 0.0536, Val AP: 0.2322, Test AP: 0.2353\n",
                        "Seed: 43, Epoch: 115, Loss: 0.0535, Val AP: 0.2299, Test AP: 0.2331\n",
                        "Seed: 43, Epoch: 116, Loss: 0.0536, Val AP: 0.2305, Test AP: 0.2351\n",
                        "Seed: 43, Epoch: 117, Loss: 0.0536, Val AP: 0.2347, Test AP: 0.2399\n",
                        "Seed: 43, Epoch: 118, Loss: 0.0536, Val AP: 0.2286, Test AP: 0.2307\n",
                        "Seed: 43, Epoch: 119, Loss: 0.0536, Val AP: 0.2264, Test AP: 0.2291\n",
                        "Seed: 43, Epoch: 120, Loss: 0.0536, Val AP: 0.2277, Test AP: 0.2306\n",
                        "Seed: 43, Epoch: 121, Loss: 0.0536, Val AP: 0.2325, Test AP: 0.2352\n",
                        "Seed: 43, Epoch: 122, Loss: 0.0535, Val AP: 0.2199, Test AP: 0.2219\n",
                        "Seed: 43, Epoch: 123, Loss: 0.0535, Val AP: 0.2316, Test AP: 0.2350\n",
                        "Seed: 43, Epoch: 124, Loss: 0.0535, Val AP: 0.2286, Test AP: 0.2301\n",
                        "Seed: 43, Epoch: 125, Loss: 0.0536, Val AP: 0.2240, Test AP: 0.2275\n",
                        "Seed: 43, Epoch: 126, Loss: 0.0535, Val AP: 0.2309, Test AP: 0.2345\n",
                        "Seed: 43, Epoch: 127, Loss: 0.0535, Val AP: 0.2272, Test AP: 0.2300\n",
                        "Seed: 43, Epoch: 128, Loss: 0.0535, Val AP: 0.2407, Test AP: 0.2441\n",
                        "Seed: 43, Epoch: 129, Loss: 0.0535, Val AP: 0.2336, Test AP: 0.2382\n",
                        "Seed: 43, Epoch: 130, Loss: 0.0535, Val AP: 0.2349, Test AP: 0.2369\n",
                        "Seed: 43, Epoch: 131, Loss: 0.0535, Val AP: 0.2305, Test AP: 0.2322\n",
                        "Seed: 43, Epoch: 132, Loss: 0.0535, Val AP: 0.2329, Test AP: 0.2353\n",
                        "Seed: 43, Epoch: 133, Loss: 0.0535, Val AP: 0.2308, Test AP: 0.2335\n",
                        "Seed: 43, Epoch: 134, Loss: 0.0534, Val AP: 0.2323, Test AP: 0.2360\n",
                        "Seed: 43, Epoch: 135, Loss: 0.0534, Val AP: 0.2326, Test AP: 0.2354\n",
                        "Seed: 43, Epoch: 136, Loss: 0.0535, Val AP: 0.2204, Test AP: 0.2227\n",
                        "Seed: 43, Epoch: 137, Loss: 0.0534, Val AP: 0.2202, Test AP: 0.2210\n",
                        "Seed: 43, Epoch: 138, Loss: 0.0535, Val AP: 0.2290, Test AP: 0.2320\n",
                        "Seed: 43, Epoch: 139, Loss: 0.0534, Val AP: 0.2331, Test AP: 0.2361\n",
                        "Seed: 43, Epoch: 140, Loss: 0.0534, Val AP: 0.2364, Test AP: 0.2406\n",
                        "Seed: 43, Epoch: 141, Loss: 0.0535, Val AP: 0.2206, Test AP: 0.2255\n",
                        "Seed: 43, Epoch: 142, Loss: 0.0534, Val AP: 0.2332, Test AP: 0.2355\n",
                        "Seed: 43, Epoch: 143, Loss: 0.0534, Val AP: 0.2376, Test AP: 0.2401\n",
                        "Seed: 43, Epoch: 144, Loss: 0.0534, Val AP: 0.2311, Test AP: 0.2342\n",
                        "Seed: 43, Epoch: 145, Loss: 0.0534, Val AP: 0.2244, Test AP: 0.2267\n",
                        "Seed: 43, Epoch: 146, Loss: 0.0534, Val AP: 0.2314, Test AP: 0.2333\n",
                        "Seed: 43, Epoch: 147, Loss: 0.0534, Val AP: 0.2187, Test AP: 0.2202\n",
                        "Seed: 43, Epoch: 148, Loss: 0.0534, Val AP: 0.2336, Test AP: 0.2344\n",
                        "Seed: 43, Epoch: 149, Loss: 0.0534, Val AP: 0.2409, Test AP: 0.2440\n",
                        "Seed: 43, Epoch: 150, Loss: 0.0534, Val AP: 0.2378, Test AP: 0.2392\n",
                        "Seed: 44, Epoch: 001, Loss: 0.0608, Val AP: 0.1193, Test AP: 0.1212\n",
                        "Seed: 44, Epoch: 002, Loss: 0.0600, Val AP: 0.1192, Test AP: 0.1214\n",
                        "Seed: 44, Epoch: 003, Loss: 0.0595, Val AP: 0.1320, Test AP: 0.1337\n",
                        "Seed: 44, Epoch: 004, Loss: 0.0590, Val AP: 0.1367, Test AP: 0.1388\n",
                        "Seed: 44, Epoch: 005, Loss: 0.0586, Val AP: 0.1378, Test AP: 0.1402\n",
                        "Seed: 44, Epoch: 006, Loss: 0.0582, Val AP: 0.1524, Test AP: 0.1544\n",
                        "Seed: 44, Epoch: 007, Loss: 0.0577, Val AP: 0.1599, Test AP: 0.1625\n",
                        "Seed: 44, Epoch: 008, Loss: 0.0574, Val AP: 0.1595, Test AP: 0.1616\n",
                        "Seed: 44, Epoch: 009, Loss: 0.0571, Val AP: 0.1675, Test AP: 0.1708\n",
                        "Seed: 44, Epoch: 010, Loss: 0.0568, Val AP: 0.1698, Test AP: 0.1729\n",
                        "Seed: 44, Epoch: 011, Loss: 0.0566, Val AP: 0.1705, Test AP: 0.1742\n",
                        "Seed: 44, Epoch: 012, Loss: 0.0564, Val AP: 0.1775, Test AP: 0.1820\n",
                        "Seed: 44, Epoch: 013, Loss: 0.0562, Val AP: 0.1872, Test AP: 0.1906\n",
                        "Seed: 44, Epoch: 014, Loss: 0.0560, Val AP: 0.1944, Test AP: 0.1991\n",
                        "Seed: 44, Epoch: 015, Loss: 0.0558, Val AP: 0.1858, Test AP: 0.1898\n",
                        "Seed: 44, Epoch: 016, Loss: 0.0557, Val AP: 0.1888, Test AP: 0.1921\n",
                        "Seed: 44, Epoch: 017, Loss: 0.0556, Val AP: 0.2010, Test AP: 0.2045\n",
                        "Seed: 44, Epoch: 018, Loss: 0.0554, Val AP: 0.1918, Test AP: 0.1952\n",
                        "Seed: 44, Epoch: 019, Loss: 0.0553, Val AP: 0.2024, Test AP: 0.2061\n",
                        "Seed: 44, Epoch: 020, Loss: 0.0553, Val AP: 0.1950, Test AP: 0.1989\n",
                        "Seed: 44, Epoch: 021, Loss: 0.0551, Val AP: 0.1878, Test AP: 0.1916\n",
                        "Seed: 44, Epoch: 022, Loss: 0.0550, Val AP: 0.2034, Test AP: 0.2081\n",
                        "Seed: 44, Epoch: 023, Loss: 0.0550, Val AP: 0.2076, Test AP: 0.2112\n",
                        "Seed: 44, Epoch: 024, Loss: 0.0549, Val AP: 0.2014, Test AP: 0.2049\n",
                        "Seed: 44, Epoch: 025, Loss: 0.0548, Val AP: 0.2072, Test AP: 0.2117\n",
                        "Seed: 44, Epoch: 026, Loss: 0.0547, Val AP: 0.2045, Test AP: 0.2077\n",
                        "Seed: 44, Epoch: 027, Loss: 0.0547, Val AP: 0.2161, Test AP: 0.2205\n",
                        "Seed: 44, Epoch: 028, Loss: 0.0546, Val AP: 0.2033, Test AP: 0.2076\n",
                        "Seed: 44, Epoch: 029, Loss: 0.0545, Val AP: 0.2130, Test AP: 0.2175\n",
                        "Seed: 44, Epoch: 030, Loss: 0.0545, Val AP: 0.2182, Test AP: 0.2223\n",
                        "Seed: 44, Epoch: 031, Loss: 0.0545, Val AP: 0.2062, Test AP: 0.2104\n",
                        "Seed: 44, Epoch: 032, Loss: 0.0544, Val AP: 0.2174, Test AP: 0.2230\n",
                        "Seed: 44, Epoch: 033, Loss: 0.0544, Val AP: 0.1998, Test AP: 0.2055\n",
                        "Seed: 44, Epoch: 034, Loss: 0.0543, Val AP: 0.2137, Test AP: 0.2185\n",
                        "Seed: 44, Epoch: 035, Loss: 0.0543, Val AP: 0.2102, Test AP: 0.2156\n",
                        "Seed: 44, Epoch: 036, Loss: 0.0542, Val AP: 0.2203, Test AP: 0.2251\n",
                        "Seed: 44, Epoch: 037, Loss: 0.0542, Val AP: 0.2169, Test AP: 0.2217\n",
                        "Seed: 44, Epoch: 038, Loss: 0.0542, Val AP: 0.2166, Test AP: 0.2218\n",
                        "Seed: 44, Epoch: 039, Loss: 0.0541, Val AP: 0.2169, Test AP: 0.2220\n",
                        "Seed: 44, Epoch: 040, Loss: 0.0541, Val AP: 0.2172, Test AP: 0.2221\n",
                        "Seed: 44, Epoch: 041, Loss: 0.0540, Val AP: 0.2075, Test AP: 0.2136\n",
                        "Seed: 44, Epoch: 042, Loss: 0.0541, Val AP: 0.2196, Test AP: 0.2251\n",
                        "Seed: 44, Epoch: 043, Loss: 0.0540, Val AP: 0.2096, Test AP: 0.2161\n",
                        "Seed: 44, Epoch: 044, Loss: 0.0539, Val AP: 0.2226, Test AP: 0.2278\n",
                        "Seed: 44, Epoch: 045, Loss: 0.0539, Val AP: 0.2139, Test AP: 0.2193\n",
                        "Seed: 44, Epoch: 046, Loss: 0.0539, Val AP: 0.2182, Test AP: 0.2227\n",
                        "Seed: 44, Epoch: 047, Loss: 0.0539, Val AP: 0.2062, Test AP: 0.2106\n",
                        "Seed: 44, Epoch: 048, Loss: 0.0539, Val AP: 0.2152, Test AP: 0.2206\n",
                        "Seed: 44, Epoch: 049, Loss: 0.0538, Val AP: 0.2098, Test AP: 0.2139\n",
                        "Seed: 44, Epoch: 050, Loss: 0.0538, Val AP: 0.2183, Test AP: 0.2229\n",
                        "Seed: 44, Epoch: 051, Loss: 0.0538, Val AP: 0.2184, Test AP: 0.2243\n",
                        "Seed: 44, Epoch: 052, Loss: 0.0538, Val AP: 0.2216, Test AP: 0.2259\n",
                        "Seed: 44, Epoch: 053, Loss: 0.0537, Val AP: 0.2218, Test AP: 0.2277\n",
                        "Seed: 44, Epoch: 054, Loss: 0.0537, Val AP: 0.2094, Test AP: 0.2146\n",
                        "Seed: 44, Epoch: 055, Loss: 0.0537, Val AP: 0.2283, Test AP: 0.2332\n",
                        "Seed: 44, Epoch: 056, Loss: 0.0537, Val AP: 0.2129, Test AP: 0.2170\n",
                        "Seed: 44, Epoch: 057, Loss: 0.0536, Val AP: 0.2319, Test AP: 0.2378\n",
                        "Seed: 44, Epoch: 058, Loss: 0.0536, Val AP: 0.2245, Test AP: 0.2307\n",
                        "Seed: 44, Epoch: 059, Loss: 0.0536, Val AP: 0.2062, Test AP: 0.2103\n",
                        "Seed: 44, Epoch: 060, Loss: 0.0536, Val AP: 0.2312, Test AP: 0.2368\n",
                        "Seed: 44, Epoch: 061, Loss: 0.0536, Val AP: 0.2136, Test AP: 0.2195\n",
                        "Seed: 44, Epoch: 062, Loss: 0.0535, Val AP: 0.2299, Test AP: 0.2347\n",
                        "Seed: 44, Epoch: 063, Loss: 0.0535, Val AP: 0.2209, Test AP: 0.2261\n",
                        "Seed: 44, Epoch: 064, Loss: 0.0536, Val AP: 0.2271, Test AP: 0.2340\n",
                        "Seed: 44, Epoch: 065, Loss: 0.0535, Val AP: 0.2192, Test AP: 0.2246\n",
                        "Seed: 44, Epoch: 066, Loss: 0.0535, Val AP: 0.2215, Test AP: 0.2272\n",
                        "Seed: 44, Epoch: 067, Loss: 0.0535, Val AP: 0.2288, Test AP: 0.2355\n",
                        "Seed: 44, Epoch: 068, Loss: 0.0534, Val AP: 0.2250, Test AP: 0.2308\n",
                        "Seed: 44, Epoch: 069, Loss: 0.0535, Val AP: 0.2265, Test AP: 0.2313\n",
                        "Seed: 44, Epoch: 070, Loss: 0.0534, Val AP: 0.2295, Test AP: 0.2344\n",
                        "Seed: 44, Epoch: 071, Loss: 0.0534, Val AP: 0.2301, Test AP: 0.2352\n",
                        "Seed: 44, Epoch: 072, Loss: 0.0534, Val AP: 0.2222, Test AP: 0.2271\n",
                        "Seed: 44, Epoch: 073, Loss: 0.0534, Val AP: 0.2287, Test AP: 0.2347\n",
                        "Seed: 44, Epoch: 074, Loss: 0.0534, Val AP: 0.2297, Test AP: 0.2368\n",
                        "Seed: 44, Epoch: 075, Loss: 0.0533, Val AP: 0.2351, Test AP: 0.2395\n",
                        "Seed: 44, Epoch: 076, Loss: 0.0533, Val AP: 0.2124, Test AP: 0.2169\n",
                        "Seed: 44, Epoch: 077, Loss: 0.0534, Val AP: 0.2274, Test AP: 0.2327\n",
                        "Seed: 44, Epoch: 078, Loss: 0.0533, Val AP: 0.2388, Test AP: 0.2453\n",
                        "Seed: 44, Epoch: 079, Loss: 0.0533, Val AP: 0.2195, Test AP: 0.2248\n",
                        "Seed: 44, Epoch: 080, Loss: 0.0533, Val AP: 0.2250, Test AP: 0.2302\n",
                        "Seed: 44, Epoch: 081, Loss: 0.0533, Val AP: 0.2250, Test AP: 0.2295\n",
                        "Seed: 44, Epoch: 082, Loss: 0.0533, Val AP: 0.2203, Test AP: 0.2256\n",
                        "Seed: 44, Epoch: 083, Loss: 0.0533, Val AP: 0.2160, Test AP: 0.2198\n",
                        "Seed: 44, Epoch: 084, Loss: 0.0533, Val AP: 0.2267, Test AP: 0.2318\n",
                        "Seed: 44, Epoch: 085, Loss: 0.0533, Val AP: 0.2345, Test AP: 0.2391\n",
                        "Seed: 44, Epoch: 086, Loss: 0.0532, Val AP: 0.1742, Test AP: 0.1752\n",
                        "Seed: 44, Epoch: 087, Loss: 0.0535, Val AP: 0.2211, Test AP: 0.2268\n",
                        "Seed: 44, Epoch: 088, Loss: 0.0532, Val AP: 0.2383, Test AP: 0.2446\n",
                        "Seed: 44, Epoch: 089, Loss: 0.0532, Val AP: 0.2462, Test AP: 0.2516\n",
                        "Seed: 44, Epoch: 090, Loss: 0.0532, Val AP: 0.2328, Test AP: 0.2377\n",
                        "Seed: 44, Epoch: 091, Loss: 0.0532, Val AP: 0.2354, Test AP: 0.2403\n",
                        "Seed: 44, Epoch: 092, Loss: 0.0532, Val AP: 0.2170, Test AP: 0.2212\n",
                        "Seed: 44, Epoch: 093, Loss: 0.0532, Val AP: 0.2210, Test AP: 0.2246\n",
                        "Seed: 44, Epoch: 094, Loss: 0.0531, Val AP: 0.2238, Test AP: 0.2292\n",
                        "Seed: 44, Epoch: 095, Loss: 0.0532, Val AP: 0.2111, Test AP: 0.2158\n",
                        "Seed: 44, Epoch: 096, Loss: 0.0532, Val AP: 0.2331, Test AP: 0.2380\n",
                        "Seed: 44, Epoch: 097, Loss: 0.0531, Val AP: 0.2306, Test AP: 0.2359\n",
                        "Seed: 44, Epoch: 098, Loss: 0.0531, Val AP: 0.2310, Test AP: 0.2362\n",
                        "Seed: 44, Epoch: 099, Loss: 0.0531, Val AP: 0.2333, Test AP: 0.2385\n",
                        "Seed: 44, Epoch: 100, Loss: 0.0531, Val AP: 0.2345, Test AP: 0.2395\n",
                        "Seed: 44, Epoch: 101, Loss: 0.0531, Val AP: 0.2247, Test AP: 0.2306\n",
                        "Seed: 44, Epoch: 102, Loss: 0.0531, Val AP: 0.2311, Test AP: 0.2356\n",
                        "Seed: 44, Epoch: 103, Loss: 0.0531, Val AP: 0.2344, Test AP: 0.2400\n",
                        "Seed: 44, Epoch: 104, Loss: 0.0531, Val AP: 0.2300, Test AP: 0.2360\n",
                        "Seed: 44, Epoch: 105, Loss: 0.0530, Val AP: 0.2310, Test AP: 0.2387\n",
                        "Seed: 44, Epoch: 106, Loss: 0.0531, Val AP: 0.2428, Test AP: 0.2485\n",
                        "Seed: 44, Epoch: 107, Loss: 0.0530, Val AP: 0.2341, Test AP: 0.2395\n",
                        "Seed: 44, Epoch: 108, Loss: 0.0530, Val AP: 0.2400, Test AP: 0.2476\n",
                        "Seed: 44, Epoch: 109, Loss: 0.0530, Val AP: 0.2344, Test AP: 0.2411\n",
                        "Seed: 44, Epoch: 110, Loss: 0.0530, Val AP: 0.2258, Test AP: 0.2325\n",
                        "Seed: 44, Epoch: 111, Loss: 0.0530, Val AP: 0.2327, Test AP: 0.2389\n",
                        "Seed: 44, Epoch: 112, Loss: 0.0530, Val AP: 0.2283, Test AP: 0.2340\n",
                        "Seed: 44, Epoch: 113, Loss: 0.0530, Val AP: 0.2338, Test AP: 0.2390\n",
                        "Seed: 44, Epoch: 114, Loss: 0.0530, Val AP: 0.2299, Test AP: 0.2361\n",
                        "Seed: 44, Epoch: 115, Loss: 0.0530, Val AP: 0.2338, Test AP: 0.2393\n",
                        "Seed: 44, Epoch: 116, Loss: 0.0530, Val AP: 0.2407, Test AP: 0.2462\n",
                        "Seed: 44, Epoch: 117, Loss: 0.0530, Val AP: 0.2341, Test AP: 0.2414\n",
                        "Seed: 44, Epoch: 118, Loss: 0.0529, Val AP: 0.2375, Test AP: 0.2437\n",
                        "Seed: 44, Epoch: 119, Loss: 0.0529, Val AP: 0.2332, Test AP: 0.2399\n",
                        "Seed: 44, Epoch: 120, Loss: 0.0530, Val AP: 0.2403, Test AP: 0.2469\n",
                        "Seed: 44, Epoch: 121, Loss: 0.0529, Val AP: 0.2302, Test AP: 0.2355\n",
                        "Seed: 44, Epoch: 122, Loss: 0.0529, Val AP: 0.2391, Test AP: 0.2455\n",
                        "Seed: 44, Epoch: 123, Loss: 0.0529, Val AP: 0.2336, Test AP: 0.2401\n",
                        "Seed: 44, Epoch: 124, Loss: 0.0529, Val AP: 0.2292, Test AP: 0.2346\n",
                        "Seed: 44, Epoch: 125, Loss: 0.0529, Val AP: 0.2442, Test AP: 0.2504\n",
                        "Seed: 44, Epoch: 126, Loss: 0.0529, Val AP: 0.2363, Test AP: 0.2426\n",
                        "Seed: 44, Epoch: 127, Loss: 0.0529, Val AP: 0.2403, Test AP: 0.2462\n",
                        "Seed: 44, Epoch: 128, Loss: 0.0529, Val AP: 0.2339, Test AP: 0.2398\n",
                        "Seed: 44, Epoch: 129, Loss: 0.0529, Val AP: 0.2258, Test AP: 0.2312\n",
                        "Seed: 44, Epoch: 130, Loss: 0.0529, Val AP: 0.2301, Test AP: 0.2368\n",
                        "Seed: 44, Epoch: 131, Loss: 0.0529, Val AP: 0.2448, Test AP: 0.2524\n",
                        "Seed: 44, Epoch: 132, Loss: 0.0529, Val AP: 0.2378, Test AP: 0.2442\n",
                        "Seed: 44, Epoch: 133, Loss: 0.0529, Val AP: 0.2363, Test AP: 0.2434\n",
                        "Seed: 44, Epoch: 134, Loss: 0.0528, Val AP: 0.2408, Test AP: 0.2475\n",
                        "Seed: 44, Epoch: 135, Loss: 0.0528, Val AP: 0.2396, Test AP: 0.2474\n",
                        "Seed: 44, Epoch: 136, Loss: 0.0528, Val AP: 0.2186, Test AP: 0.2242\n",
                        "Seed: 44, Epoch: 137, Loss: 0.0528, Val AP: 0.2150, Test AP: 0.2206\n",
                        "Seed: 44, Epoch: 138, Loss: 0.0529, Val AP: 0.2367, Test AP: 0.2453\n",
                        "Seed: 44, Epoch: 139, Loss: 0.0528, Val AP: 0.2410, Test AP: 0.2471\n",
                        "Early stopping at epoch 139 for seed 44\n",
                        "Average Time: 33401.96 seconds\n",
                        "Var Time: 1974702.89 seconds\n",
                        "Average Memory: 48121.33 MB\n",
                        "Average Best Val AP: 0.2471\n",
                        "Std Best Test AP: 0.0041\n",
                        "Average Test AP: 0.2497\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import SAGEConv\nimport os.path as osp\nimport time\nfrom math import ceil\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_sparse import SparseTensor\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, dense_mincut_pool\nclass GNN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, normalize)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, normalize)\n        self.conv3 = GCNConv(hidden_channels, out_channels, normalize)\n        if lin:\n            self.lin = torch.nn.Linear(out_channels, out_channels)\n        else:\n            self.lin = None\n    def forward(self, x, adj_t, mask=None):\n        adj_t = adj_t.to(torch.float32)\n        x = self.conv1(x, adj_t, mask).relu()\n        x = self.conv2(x, adj_t, mask).relu()\n        x = self.conv3(x, adj_t, mask).relu()\n        if self.lin is not None:\n            x = self.lin(x).relu()\n        return x\nclass Net_Mincut(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        num_nodes = 64\n        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n        self.gnn1_embed = GCNConv(dataset_dense.num_features, 64)\n        num_nodes = 64\n        self.gnn2_pool = GNN(64, 64, num_nodes)\n        self.gnn2_embed = GCNConv(64, 64)\n        self.gnn3_embed = GCNConv(64, 64)\n        self.lin1 = torch.nn.Linear(64, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, x, adj_t, batch):\n        adj_t = adj_t.to(torch.float32)\n        s = self.gnn1_pool(x, adj_t)\n        x = self.gnn1_embed(x, adj_t)\n        x = F.relu(x)\n        if isinstance(adj_t, SparseTensor):\n            adj_t_dense = adj_t.to_dense()\n            adj_t_dense = adj_t_dense.unsqueeze(0) if adj_t_dense.dim() == 2 else adj_t_dense\n        else:\n            adj_t_dense = adj_t.unsqueeze(0) if adj_t.dim() == 2 else adj_t\n            x, adj, l1, e1 = dense_mincut_pool(x, adj, s, temp=1.8)\n        s = self.gnn2_pool(x, adj_t)\n        x = self.gnn2_embed(x, adj_t)\n        x = F.relu(x)\n        if isinstance(adj_t, SparseTensor):\n            adj_t_dense = adj_t.to_dense()\n            adj_t_dense = adj_t_dense.unsqueeze(0) if adj_t_dense.dim() == 2 else adj_t_dense\n        else:\n            adj_t_dense = adj_t.unsqueeze(0) if adj_t.dim() == 2 else adj_t\n            x, adj, l2, e2 = dense_mincut_pool(x, adj, s, temp=1.8)\n        x = self.gnn3_embed(x, adj_t)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Net_Mincut().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        x, adj_t = data.x, data.adj_t\n        x, adj_t, batch = data.x.to(torch.float32), data.adj_t.to(torch.float32), data.batch\n        out = model(x, adj_t, batch)\n        pred = out\n        is_labeled = data.y == data.y \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        x, adj_t = data.x, data.adj_t\n        x, adj_t, batch = data.x.to(torch.float32), data.adj_t.to(torch.float32), data.batch\n        out = model(x, adj_t, batch)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_dense = dataset_dense.shuffle()\n    num_total = len(dataset_dense)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_dense[:num_train]\n    val_dataset = dataset_dense[num_train:num_train + num_val]\n    test_dataset = dataset_dense[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n    model = Net_Mincut().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### DMoNPool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0609, Val AP: 0.1182, Test AP: 0.1182\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0599, Val AP: 0.1189, Test AP: 0.1200\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0594, Val AP: 0.1316, Test AP: 0.1318\n",
                        "Seed: 42, Epoch: 004, Loss: 0.0590, Val AP: 0.1323, Test AP: 0.1325\n",
                        "Seed: 42, Epoch: 005, Loss: 0.0587, Val AP: 0.1345, Test AP: 0.1350\n",
                        "Seed: 42, Epoch: 006, Loss: 0.0584, Val AP: 0.1472, Test AP: 0.1472\n",
                        "Seed: 42, Epoch: 007, Loss: 0.0581, Val AP: 0.1544, Test AP: 0.1543\n",
                        "Seed: 42, Epoch: 008, Loss: 0.0578, Val AP: 0.1602, Test AP: 0.1610\n",
                        "Seed: 42, Epoch: 009, Loss: 0.0575, Val AP: 0.1625, Test AP: 0.1619\n",
                        "Seed: 42, Epoch: 010, Loss: 0.0573, Val AP: 0.1672, Test AP: 0.1662\n",
                        "Seed: 42, Epoch: 011, Loss: 0.0571, Val AP: 0.1719, Test AP: 0.1720\n",
                        "Seed: 42, Epoch: 012, Loss: 0.0568, Val AP: 0.1757, Test AP: 0.1756\n",
                        "Seed: 42, Epoch: 013, Loss: 0.0566, Val AP: 0.1708, Test AP: 0.1718\n",
                        "Seed: 42, Epoch: 014, Loss: 0.0564, Val AP: 0.1683, Test AP: 0.1695\n",
                        "Seed: 42, Epoch: 015, Loss: 0.0563, Val AP: 0.1828, Test AP: 0.1820\n",
                        "Seed: 42, Epoch: 016, Loss: 0.0561, Val AP: 0.1866, Test AP: 0.1860\n",
                        "Seed: 42, Epoch: 017, Loss: 0.0559, Val AP: 0.1891, Test AP: 0.1896\n",
                        "Seed: 42, Epoch: 018, Loss: 0.0558, Val AP: 0.1897, Test AP: 0.1890\n",
                        "Seed: 42, Epoch: 019, Loss: 0.0556, Val AP: 0.1935, Test AP: 0.1915\n",
                        "Seed: 42, Epoch: 020, Loss: 0.0555, Val AP: 0.1876, Test AP: 0.1859\n",
                        "Seed: 42, Epoch: 021, Loss: 0.0554, Val AP: 0.1958, Test AP: 0.1927\n",
                        "Seed: 42, Epoch: 022, Loss: 0.0553, Val AP: 0.2003, Test AP: 0.2008\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import SAGEConv\nimport os.path as osp\nimport time\nfrom math import ceil\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_sparse import SparseTensor\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nfrom torch_geometric.nn import DenseGCNConv, DMoNPooling\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, dense_mincut_pool\nclass GNN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, normalize)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, normalize)\n        self.conv3 = GCNConv(hidden_channels, out_channels, normalize)\n        if lin:\n            self.lin = torch.nn.Linear(out_channels, out_channels)\n        else:\n            self.lin = None\n    def forward(self, x, adj_t, mask=None):\n        adj_t = adj_t.to(torch.float32)\n        x = self.conv1(x, adj_t, mask).relu()\n        x = self.conv2(x, adj_t, mask).relu()\n        x = self.conv3(x, adj_t, mask).relu()\n        if self.lin is not None:\n            x = self.lin(x).relu()\n        return x\nclass Net_DMoN(torch.nn.Module):\n    def __init__(self, in_channels=dataset_dense.num_features, out_channels=64, hidden_channels=64):\n        super().__init__()\n        num_nodes = 64\n        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n        self.gnn1_embed = GCNConv(dataset_dense.num_features, 64)\n        self.pool1 = DMoNPooling([hidden_channels, hidden_channels], 8)\n        self.pool2 = DMoNPooling([hidden_channels, hidden_channels], 8)\n        num_nodes = 64\n        self.gnn2_pool = GNN(64, 64, num_nodes)\n        self.gnn2_embed = GCNConv(64, 64)\n        self.gnn3_embed = GCNConv(64, 64)\n        self.lin1 = torch.nn.Linear(64, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, x, adj_t, batch):\n        adj_t = adj_t.to(torch.float32)\n        s = self.gnn1_pool(x, adj_t)\n        x = self.gnn1_embed(x, adj_t)\n        x = F.relu(x)\n        if isinstance(adj_t, SparseTensor):\n            adj_t_dense = adj_t.to_dense()\n            adj_t_dense = adj_t_dense.unsqueeze(0) if adj_t_dense.dim() == 2 else adj_t_dense\n        else:\n            adj_t_dense = adj_t.unsqueeze(0) if adj_t.dim() == 2 else adj_t\n            _, x, adj, sp1, o1, c1 = self.pool1(x, adj)\n        s = self.gnn2_pool(x, adj_t)\n        x = self.gnn2_embed(x, adj_t)\n        x = F.relu(x)\n        if isinstance(adj_t, SparseTensor):\n            adj_t_dense = adj_t.to_dense()\n            adj_t_dense = adj_t_dense.unsqueeze(0) if adj_t_dense.dim() == 2 else adj_t_dense\n        else:\n            adj_t_dense = adj_t.unsqueeze(0) if adj_t.dim() == 2 else adj_t\n            _, x, adj, sp2, o2, c2 = self.pool2(x, adj)\n        x = self.gnn3_embed(x, adj_t)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Net_DMoN().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        x, adj_t = data.x, data.adj_t\n        x, adj_t, batch = data.x.to(torch.float32), data.adj_t.to(torch.float32), data.batch\n        out = model(x, adj_t, batch)\n        pred = out\n        is_labeled = data.y == data.y \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        x, adj_t = data.x, data.adj_t\n        x, adj_t, batch = data.x.to(torch.float32), data.adj_t.to(torch.float32), data.batch\n        out = model(x, adj_t, batch)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_dense = dataset_dense.shuffle()\n    num_total = len(dataset_dense)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_dense[:num_train]\n    val_dataset = dataset_dense[num_train:num_train + num_val]\n    test_dataset = dataset_dense[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n    model = Net_DMoN().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### JustbalancePool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nmax_nodes = 100\ndataset_dense = PygGraphPropPredDataset(root='/data/XXX/Pooling', name=\"ogbg-molpcba\", transform=T.ToSparseTensor())\nevaluator = Evaluator(\"ogbg-molpcba\")\neval_metric = dataset_dense.eval_metric\ntrain_ratio = 0.8\nval_ratio = 0.1\ntest_ratio = 0.1\ndataset_dense = dataset_dense.shuffle()\nnum_total = len(dataset_dense)\nnum_train = int(num_total * train_ratio)\nnum_val = int(num_total * val_ratio)\nnum_test = num_total - num_train - num_val\ntrain_dataset = dataset_dense[:num_train]\nval_dataset = dataset_dense[num_train:num_train + num_val]\ntest_dataset = dataset_dense[num_train + num_val:]\ntrain_loader = DenseDataLoader(train_dataset, batch_size=2048, shuffle=True)\nvalid_loader = DenseDataLoader(val_dataset, batch_size=2048, shuffle=False)\ntest_loader = DenseDataLoader(test_dataset, batch_size=2048, shuffle=False)"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "import torch\nfrom torch_geometric.nn import SAGEConv\nimport os.path as osp\nimport time\nfrom math import ceil\nfrom torch_geometric.data import Data, Batch\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_sparse import SparseTensor\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch.nn import Linear, ReLU, Dropout, LayerNorm, Module, ModuleList\nfrom sklearn.metrics import average_precision_score\nfrom torch_geometric.nn import DenseGCNConv, DMoNPooling\nimport torch\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.loader import DenseDataLoader\nfrom torch_geometric.nn import DenseGCNConv, dense_mincut_pool\nEPS = 1e-15\ndef just_balance_pool(x, adj, s, mask=None, normalize=True):\n    r\"\"\"The Just Balance pooling operator from the `\"Simplifying Clustering with\n    Graph Neural Networks\" <https://arxiv.org/abs/2207.08779>`_ paper\n    .. math::\n        \\mathbf{X}^{\\prime} &= {\\mathrm{softmax}(\\mathbf{S})}^{\\top} \\cdot\n        \\mathbf{X}\n        \\mathbf{A}^{\\prime} &= {\\mathrm{softmax}(\\mathbf{S})}^{\\top} \\cdot\n        \\mathbf{A} \\cdot \\mathrm{softmax}(\\mathbf{S})\n    based on dense learned assignments :math:`\\mathbf{S} \\in \\mathbb{R}^{B\n    \\times N \\times C}`.\n    Returns the pooled node feature matrix, the coarsened and symmetrically\n    normalized adjacency matrix and the following auxiliary objective:\n    .. math::\n        \\mathcal{L} = - {\\mathrm{Tr}(\\sqrt{\\mathbf{S}^{\\top} \\mathbf{S}})}\n    Args:\n        x (Tensor): Node feature tensor :math:`\\mathbf{X} \\in \\mathbb{R}^{B \\times N \\times F}`\n            with batch-size :math:`B`, (maximum) number of nodes :math:`N`\n            for each graph, and feature dimension :math:`F`.\n        adj (Tensor): Symmetrically normalized adjacency tensor\n            :math:`\\mathbf{A} \\in \\mathbb{R}^{B \\times N \\times N}`.\n        s (Tensor): Assignment tensor :math:`\\mathbf{S} \\in \\mathbb{R}^{B \\times N \\times C}`\n            with number of clusters :math:`C`. The softmax does not have to be\n            applied beforehand, since it is executed within this method.\n        mask (BoolTensor, optional): Mask matrix\n            :math:`\\mathbf{M} \\in {\\{ 0, 1 \\}}^{B \\times N}` indicating\n            the valid nodes for each graph. (default: :obj:`None`)\n    :rtype: (:class:`Tensor`, :class:`Tensor`, :class:`Tensor`,\n        :class:`Tensor`)\n    \"\"\"\n    x = x.unsqueeze(0) if x.dim() == 2 else x\n    adj = adj.unsqueeze(0) if adj.dim() == 2 else adj\n    s = s.unsqueeze(0) if s.dim() == 2 else s\n    (batch_size, num_nodes, _), k = x.size(), s.size(-1)\n    s = torch.softmax(s, dim=-1)\n    if mask is not None:\n        mask = mask.view(batch_size, num_nodes, 1).to(x.dtype)\n        x, s = x * mask, s * mask\n    out = torch.matmul(s.transpose(1, 2), x)\n    out_adj = torch.matmul(torch.matmul(s.transpose(1, 2), adj), s)\n    ss = torch.matmul(s.transpose(1, 2), s)\n    ss_sqrt = torch.sqrt(ss + EPS)\n    loss = torch.mean(-_rank3_trace(ss_sqrt))\n    if normalize:\n        loss = loss / torch.sqrt(torch.tensor(num_nodes * k))\n    ind = torch.arange(k, device=out_adj.device)\n    out_adj[:, ind, ind] = 0\n    d = torch.einsum('ijk->ij', out_adj)\n    d = torch.sqrt(d)[:, None] + EPS\n    out_adj = (out_adj / d) / d.transpose(1, 2)\n    return out, out_adj, loss\ndef _rank3_trace(x):\n    return torch.einsum('ijj->i', x)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.0609, Val AP: 0.1219, Test AP: 0.1238\n",
                        "Seed: 42, Epoch: 002, Loss: 0.0599, Val AP: 0.1301, Test AP: 0.1308\n",
                        "Seed: 42, Epoch: 003, Loss: 0.0593, Val AP: 0.1326, Test AP: 0.1340\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[18], line 228\u001b[0m\n\u001b[1;32m    225\u001b[0m epochs_no_improve \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m151\u001b[39m):\n\u001b[0;32m--> 228\u001b[0m     loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m    229\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m test(valid_loader)  \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m    230\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test(test_loader)  \u001b[38;5;66;03m# \u001b[39;00m\n",
                        "Cell \u001b[0;32mIn[18], line 108\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    106\u001b[0m num_g_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/site-packages/torch_geometric/data/dataset.py:290\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[1;32m    289\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices()[idx])\n\u001b[0;32m--> 290\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/site-packages/torch_geometric/transforms/base_transform.py:32\u001b[0m, in \u001b[0;36mBaseTransform.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Shallow-copy the data so that we prevent in-place data modification.\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/site-packages/torch_geometric/transforms/to_sparse_tensor.py:85\u001b[0m, in \u001b[0;36mToSparseTensor.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_edge_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     86\u001b[0m     keys\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[1;32m     87\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(value)\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/site-packages/torch_geometric/data/storage.py:835\u001b[0m, in \u001b[0;36mGlobalStorage.is_edge_attr\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_attr[AttrType\u001b[38;5;241m.\u001b[39mNODE]:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_attr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mAttrType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOTHER\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    838\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[key]\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX1/lib/python3.10/enum.py:783\u001b[0m, in \u001b[0;36mEnum.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value_\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(val, format_spec)\n\u001b[0;32m--> 783\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name_)\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__reduce_ex__\u001b[39m(\u001b[38;5;28mself\u001b[39m, proto):\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": "class GNN(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, normalize=False, lin=True):\n        super().__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels, normalize)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels, normalize)\n        self.conv3 = GCNConv(hidden_channels, out_channels, normalize)\n        if lin:\n            self.lin = torch.nn.Linear(out_channels, out_channels)\n        else:\n            self.lin = None\n    def forward(self, x, adj_t, mask=None):\n        adj_t = adj_t.to(torch.float32)\n        x = self.conv1(x, adj_t, mask).relu()\n        x = self.conv2(x, adj_t, mask).relu()\n        x = self.conv3(x, adj_t, mask).relu()\n        if self.lin is not None:\n            x = self.lin(x).relu()\n        return x\nclass Net_just_balance_pool(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        num_nodes = 64\n        self.gnn1_pool = GNN(dataset_dense.num_features, 64, num_nodes)\n        self.gnn1_embed = GCNConv(dataset_dense.num_features, 64)\n        num_nodes = 64\n        self.gnn2_pool = GNN(64, 64, num_nodes)\n        self.gnn2_embed = GCNConv(64, 64)\n        self.gnn3_embed = GCNConv(64, 64)\n        self.lin1 = torch.nn.Linear(64, 128)\n        self.lin2 = torch.nn.Linear(128, 128)\n    def forward(self, x, adj_t, batch):\n        adj_t = adj_t.to(torch.float32)\n        s = self.gnn1_pool(x, adj_t)\n        x = self.gnn1_embed(x, adj_t)\n        x = F.relu(x)\n        if isinstance(adj_t, SparseTensor):\n            adj_t_dense = adj_t.to_dense()\n            adj_t_dense = adj_t_dense.unsqueeze(0) if adj_t_dense.dim() == 2 else adj_t_dense\n        else:\n            adj_t_dense = adj_t.unsqueeze(0) if adj_t.dim() == 2 else adj_t\n            x, adj_t, tv1, bal1 = just_balance_pool(x, adj_t_dense, s)\n        s = self.gnn2_pool(x, adj_t)\n        x = self.gnn2_embed(x, adj_t)\n        x = F.relu(x)\n        if isinstance(adj_t, SparseTensor):\n            adj_t_dense = adj_t.to_dense()\n            adj_t_dense = adj_t_dense.unsqueeze(0) if adj_t_dense.dim() == 2 else adj_t_dense\n        else:\n            adj_t_dense = adj_t.unsqueeze(0) if adj_t.dim() == 2 else adj_t\n            x, adj_t, tv1, bal1 = just_balance_pool(x, adj_t_dense)\n        x = self.gnn3_embed(x, adj_t)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Net_just_balance_pool().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.005)\ncriterion = torch.nn.BCEWithLogitsLoss()\ndef train():\n    model.train()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in train_loader:\n        if data.x.shape[0] == 1 or data.batch[-1] == 0:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        x, adj_t = data.x, data.adj_t\n        x, adj_t, batch = data.x.to(torch.float32), data.adj_t.to(torch.float32), data.batch\n        out = model(x, adj_t, batch)\n        pred = out\n        is_labeled = data.y == data.y \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef test(loader):\n    model.eval()\n    y_true = []\n    y_pred = []\n    total_loss = 0.\n    num_g_total = 0.\n    for data in loader:\n        if data.x.shape[0] == 1:\n            continue\n        num_g = data.num_graphs\n        num_g_total += num_g\n        data = data.to(device)\n        x, adj_t = data.x, data.adj_t\n        x, adj_t, batch = data.x.to(torch.float32), data.adj_t.to(torch.float32), data.batch\n        out = model(x, adj_t, batch)\n        pred = out\n        is_labeled = data.y == data.y  \n        loss = torch.nn.BCEWithLogitsLoss()(pred.to(torch.float32)[is_labeled], data.y.to(torch.float32)[is_labeled])\n        total_loss += loss.item() * num_g\n        y_true.append(data.y.view(pred.shape)[is_labeled].detach().cpu())\n        y_pred.append(torch.sigmoid(pred)[is_labeled].detach().cpu())  \n    total_loss = total_loss / num_g_total\n    y_true = torch.cat(y_true, dim=0).numpy()\n    y_pred = torch.cat(y_pred, dim=0).numpy()\n    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n    metric = average_precision_score(y_true, y_pred)\n    return total_loss, metric\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 50\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_dense = dataset_dense.shuffle()\n    num_total = len(dataset_dense)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_dense[:num_train]\n    val_dataset = dataset_dense[num_train:num_train + num_val]\n    test_dataset = dataset_dense[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n    model = Net_just_balance_pool().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 151):\n        loss, _ = train()  \n        val_loss, val_acc = test(valid_loader)  \n        test_loss, test_acc = test(test_loader)  \n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AP: {val_acc:.4f}, Test AP: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val AP: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test AP: {np.std(best_test_accs):.4f}')\nprint(f'Average Test AP: {np.mean(best_test_accs):.4f}')"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "CG-ODE",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}