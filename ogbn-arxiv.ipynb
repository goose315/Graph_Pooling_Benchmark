{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": []
                }
            ],
            "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport sys\nimport torch\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom ogb.graphproppred import PygGraphPropPredDataset, Evaluator\nfrom torch_geometric.loader import DataLoader\nimport os\nimport random\nimport pandas as pd\nimport torch\nimport torch_geometric.transforms as T\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data\nfrom torch_geometric.data.datapipes import functional_transform\nfrom torch_geometric.transforms import BaseTransform\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import Actor\nfrom torch_geometric.datasets import GNNBenchmarkDataset\nfrom torch_geometric.datasets import TUDataset\nfrom sklearn.metrics import r2_score\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import MoleculeNet\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.utils import to_networkx\nfrom torch.nn import Linear\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport time\nimport psutil\nimport torch\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TopKPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport sys\nimport torch\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom ogb.nodeproppred import PygNodePropPredDataset, Evaluator\nfrom torch_geometric.loader import DataLoader\nimport os\nimport random\nimport pandas as pd\nimport torch\nimport torch_geometric.transforms as T\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data\nfrom torch_geometric.data.datapipes import functional_transform\nfrom torch_geometric.transforms import BaseTransform\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import Actor\nfrom torch_geometric.datasets import GNNBenchmarkDataset\nfrom torch_geometric.datasets import TUDataset\nfrom sklearn.metrics import r2_score\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import MoleculeNet\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.utils import to_networkx\nfrom torch.nn import Linear\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nfrom torch_geometric.datasets import GitHub\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimport time\nimport tracemalloc\nimport torch\nfrom torch_geometric.datasets import Planetoid\nimport torch_geometric.transforms as T\nfrom torch_geometric.nn import GCNConv\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport random\nfrom typing import Callable, Optional, Union\nimport time\nimport psutil\nimport torch\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nmax_nodes = 150\ndataset_sparse = PygNodePropPredDataset(root='/data/XXX/Pooling', name=\"ogbn-arxiv\")\nevaluator = Evaluator(\"ogbn-arxiv\")\neval_metric = dataset_sparse.eval_metric"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 042, Epoch: 010, Loss: 3.2028, Train Acc: 0.1639, Val Acc: 0.2573, Test Acc: 0.2359\n",
                        "Seed: 042, Epoch: 020, Loss: 2.5912, Train Acc: 0.3391, Val Acc: 0.3953, Test Acc: 0.3635\n",
                        "Seed: 042, Epoch: 030, Loss: 2.1476, Train Acc: 0.4297, Val Acc: 0.4340, Test Acc: 0.3805\n",
                        "Seed: 042, Epoch: 040, Loss: 1.9460, Train Acc: 0.4819, Val Acc: 0.4755, Test Acc: 0.4163\n",
                        "Seed: 042, Epoch: 050, Loss: 1.7602, Train Acc: 0.5206, Val Acc: 0.5038, Test Acc: 0.4446\n",
                        "Seed: 042, Epoch: 060, Loss: 1.6680, Train Acc: 0.5400, Val Acc: 0.5195, Test Acc: 0.4594\n",
                        "Seed: 042, Epoch: 070, Loss: 1.6075, Train Acc: 0.5552, Val Acc: 0.5365, Test Acc: 0.4731\n",
                        "Seed: 042, Epoch: 080, Loss: 1.5767, Train Acc: 0.5618, Val Acc: 0.5450, Test Acc: 0.4829\n",
                        "Seed: 042, Epoch: 090, Loss: 1.5513, Train Acc: 0.5672, Val Acc: 0.5433, Test Acc: 0.4763\n",
                        "Seed: 042, Epoch: 100, Loss: 1.5279, Train Acc: 0.5714, Val Acc: 0.5533, Test Acc: 0.4947\n",
                        "Seed: 042, Epoch: 110, Loss: 1.5049, Train Acc: 0.5774, Val Acc: 0.5526, Test Acc: 0.4898\n",
                        "Seed: 042, Epoch: 120, Loss: 1.4900, Train Acc: 0.5814, Val Acc: 0.5594, Test Acc: 0.4935\n",
                        "Seed: 042, Epoch: 130, Loss: 1.4785, Train Acc: 0.5852, Val Acc: 0.5669, Test Acc: 0.5010\n",
                        "Seed: 042, Epoch: 140, Loss: 1.4579, Train Acc: 0.5891, Val Acc: 0.5653, Test Acc: 0.4964\n",
                        "Seed: 042, Epoch: 150, Loss: 1.4453, Train Acc: 0.5898, Val Acc: 0.5645, Test Acc: 0.5026\n",
                        "Seed: 042, Epoch: 160, Loss: 1.4350, Train Acc: 0.5921, Val Acc: 0.5648, Test Acc: 0.4962\n",
                        "Seed: 042, Epoch: 170, Loss: 1.4240, Train Acc: 0.5954, Val Acc: 0.5666, Test Acc: 0.4999\n",
                        "Seed: 042, Epoch: 180, Loss: 1.4203, Train Acc: 0.5984, Val Acc: 0.5729, Test Acc: 0.5038\n",
                        "Seed: 042, Epoch: 190, Loss: 1.4106, Train Acc: 0.6006, Val Acc: 0.5774, Test Acc: 0.5100\n",
                        "Seed: 042, Epoch: 200, Loss: 1.4176, Train Acc: 0.5988, Val Acc: 0.5719, Test Acc: 0.5024\n",
                        "Seed: 042, Epoch: 210, Loss: 1.3991, Train Acc: 0.6026, Val Acc: 0.5779, Test Acc: 0.5123\n",
                        "Seed: 042, Epoch: 220, Loss: 1.3939, Train Acc: 0.6039, Val Acc: 0.5744, Test Acc: 0.5045\n",
                        "Seed: 042, Epoch: 230, Loss: 1.3887, Train Acc: 0.6045, Val Acc: 0.5799, Test Acc: 0.5156\n",
                        "Seed: 042, Epoch: 240, Loss: 1.3836, Train Acc: 0.6065, Val Acc: 0.5767, Test Acc: 0.5084\n",
                        "Seed: 042, Epoch: 250, Loss: 1.3846, Train Acc: 0.6047, Val Acc: 0.5790, Test Acc: 0.5153\n",
                        "Seed: 042, Epoch: 260, Loss: 1.3774, Train Acc: 0.6076, Val Acc: 0.5844, Test Acc: 0.5224\n",
                        "Seed: 042, Epoch: 270, Loss: 1.3709, Train Acc: 0.6092, Val Acc: 0.5798, Test Acc: 0.5107\n",
                        "Seed: 042, Epoch: 280, Loss: 1.3705, Train Acc: 0.6091, Val Acc: 0.5822, Test Acc: 0.5147\n",
                        "Seed: 042, Epoch: 290, Loss: 1.3733, Train Acc: 0.6079, Val Acc: 0.5848, Test Acc: 0.5252\n",
                        "Seed: 042, Epoch: 300, Loss: 1.3624, Train Acc: 0.6118, Val Acc: 0.5828, Test Acc: 0.5163\n",
                        "Seed: 042, Epoch: 310, Loss: 1.3571, Train Acc: 0.6130, Val Acc: 0.5819, Test Acc: 0.5160\n",
                        "Seed: 042, Epoch: 320, Loss: 1.3554, Train Acc: 0.6110, Val Acc: 0.5840, Test Acc: 0.5163\n",
                        "Seed: 042, Epoch: 330, Loss: 1.3524, Train Acc: 0.6140, Val Acc: 0.5853, Test Acc: 0.5180\n",
                        "Seed: 042, Epoch: 340, Loss: 1.3559, Train Acc: 0.6140, Val Acc: 0.5844, Test Acc: 0.5155\n",
                        "Seed: 042, Epoch: 350, Loss: 1.3436, Train Acc: 0.6158, Val Acc: 0.5850, Test Acc: 0.5171\n",
                        "Seed: 042, Epoch: 360, Loss: 1.3416, Train Acc: 0.6162, Val Acc: 0.5888, Test Acc: 0.5209\n",
                        "Seed: 042, Epoch: 370, Loss: 1.3395, Train Acc: 0.6168, Val Acc: 0.5862, Test Acc: 0.5175\n",
                        "Seed: 042, Epoch: 380, Loss: 1.3504, Train Acc: 0.6156, Val Acc: 0.5873, Test Acc: 0.5233\n",
                        "Seed: 042, Epoch: 390, Loss: 1.3353, Train Acc: 0.6176, Val Acc: 0.5895, Test Acc: 0.5218\n",
                        "Seed: 042, Epoch: 400, Loss: 1.3316, Train Acc: 0.6180, Val Acc: 0.5856, Test Acc: 0.5227\n",
                        "Seed: 042, Epoch: 410, Loss: 1.3432, Train Acc: 0.6142, Val Acc: 0.5818, Test Acc: 0.5206\n",
                        "Seed: 042, Epoch: 420, Loss: 1.3305, Train Acc: 0.6185, Val Acc: 0.5857, Test Acc: 0.5176\n",
                        "Seed: 042, Epoch: 430, Loss: 1.3288, Train Acc: 0.6204, Val Acc: 0.5884, Test Acc: 0.5210\n",
                        "Seed: 042, Epoch: 440, Loss: 1.3223, Train Acc: 0.6212, Val Acc: 0.5889, Test Acc: 0.5237\n",
                        "Seed: 042, Epoch: 450, Loss: 1.3214, Train Acc: 0.6210, Val Acc: 0.5878, Test Acc: 0.5205\n",
                        "Seed: 042, Epoch: 460, Loss: 1.3230, Train Acc: 0.6195, Val Acc: 0.5891, Test Acc: 0.5259\n",
                        "Seed: 042, Epoch: 470, Loss: 1.3198, Train Acc: 0.6219, Val Acc: 0.5903, Test Acc: 0.5278\n",
                        "Seed: 042, Epoch: 480, Loss: 1.3187, Train Acc: 0.6224, Val Acc: 0.5884, Test Acc: 0.5226\n",
                        "Seed: 042, Epoch: 490, Loss: 1.3134, Train Acc: 0.6221, Val Acc: 0.5919, Test Acc: 0.5287\n",
                        "Seed: 042, Epoch: 500, Loss: 1.3120, Train Acc: 0.6232, Val Acc: 0.5912, Test Acc: 0.5280\n",
                        "Seed: 123, Epoch: 010, Loss: 3.0325, Train Acc: 0.2982, Val Acc: 0.3055, Test Acc: 0.2936\n",
                        "Seed: 123, Epoch: 020, Loss: 2.4385, Train Acc: 0.3836, Val Acc: 0.4035, Test Acc: 0.3511\n",
                        "Seed: 123, Epoch: 030, Loss: 2.1034, Train Acc: 0.4492, Val Acc: 0.4451, Test Acc: 0.3879\n",
                        "Seed: 123, Epoch: 040, Loss: 1.8595, Train Acc: 0.5020, Val Acc: 0.4927, Test Acc: 0.4415\n",
                        "Seed: 123, Epoch: 050, Loss: 1.7259, Train Acc: 0.5289, Val Acc: 0.5164, Test Acc: 0.4560\n",
                        "Seed: 123, Epoch: 060, Loss: 1.6397, Train Acc: 0.5486, Val Acc: 0.5329, Test Acc: 0.4737\n",
                        "Seed: 123, Epoch: 070, Loss: 1.5848, Train Acc: 0.5596, Val Acc: 0.5356, Test Acc: 0.4699\n",
                        "Seed: 123, Epoch: 080, Loss: 1.5481, Train Acc: 0.5691, Val Acc: 0.5473, Test Acc: 0.4851\n",
                        "Seed: 123, Epoch: 090, Loss: 1.5315, Train Acc: 0.5718, Val Acc: 0.5415, Test Acc: 0.4739\n",
                        "Seed: 123, Epoch: 100, Loss: 1.4995, Train Acc: 0.5793, Val Acc: 0.5510, Test Acc: 0.4852\n",
                        "Seed: 123, Epoch: 110, Loss: 1.4809, Train Acc: 0.5833, Val Acc: 0.5565, Test Acc: 0.4931\n",
                        "Seed: 123, Epoch: 120, Loss: 1.4668, Train Acc: 0.5869, Val Acc: 0.5594, Test Acc: 0.4969\n",
                        "Seed: 123, Epoch: 130, Loss: 1.4504, Train Acc: 0.5911, Val Acc: 0.5653, Test Acc: 0.5009\n",
                        "Seed: 123, Epoch: 140, Loss: 1.4447, Train Acc: 0.5921, Val Acc: 0.5660, Test Acc: 0.5015\n",
                        "Seed: 123, Epoch: 150, Loss: 1.4320, Train Acc: 0.5953, Val Acc: 0.5693, Test Acc: 0.5048\n",
                        "Seed: 123, Epoch: 160, Loss: 1.4229, Train Acc: 0.5979, Val Acc: 0.5703, Test Acc: 0.5086\n",
                        "Seed: 123, Epoch: 170, Loss: 1.4273, Train Acc: 0.5945, Val Acc: 0.5658, Test Acc: 0.5037\n",
                        "Seed: 123, Epoch: 180, Loss: 1.4115, Train Acc: 0.5996, Val Acc: 0.5705, Test Acc: 0.5089\n",
                        "Seed: 123, Epoch: 190, Loss: 1.4000, Train Acc: 0.6024, Val Acc: 0.5731, Test Acc: 0.5083\n",
                        "Seed: 123, Epoch: 200, Loss: 1.3977, Train Acc: 0.6021, Val Acc: 0.5780, Test Acc: 0.5215\n",
                        "Seed: 123, Epoch: 210, Loss: 1.3892, Train Acc: 0.6046, Val Acc: 0.5712, Test Acc: 0.5077\n",
                        "Seed: 123, Epoch: 220, Loss: 1.3827, Train Acc: 0.6061, Val Acc: 0.5784, Test Acc: 0.5166\n",
                        "Seed: 123, Epoch: 230, Loss: 1.3808, Train Acc: 0.6061, Val Acc: 0.5804, Test Acc: 0.5159\n",
                        "Seed: 123, Epoch: 240, Loss: 1.3822, Train Acc: 0.6091, Val Acc: 0.5806, Test Acc: 0.5158\n",
                        "Seed: 123, Epoch: 250, Loss: 1.3790, Train Acc: 0.6069, Val Acc: 0.5793, Test Acc: 0.5193\n",
                        "Seed: 123, Epoch: 260, Loss: 1.3738, Train Acc: 0.6091, Val Acc: 0.5828, Test Acc: 0.5201\n",
                        "Seed: 123, Epoch: 270, Loss: 1.3623, Train Acc: 0.6113, Val Acc: 0.5814, Test Acc: 0.5181\n",
                        "Seed: 123, Epoch: 280, Loss: 1.3610, Train Acc: 0.6119, Val Acc: 0.5789, Test Acc: 0.5131\n",
                        "Seed: 123, Epoch: 290, Loss: 1.3546, Train Acc: 0.6137, Val Acc: 0.5856, Test Acc: 0.5237\n",
                        "Seed: 123, Epoch: 300, Loss: 1.3516, Train Acc: 0.6151, Val Acc: 0.5804, Test Acc: 0.5166\n",
                        "Seed: 123, Epoch: 310, Loss: 1.3678, Train Acc: 0.6102, Val Acc: 0.5746, Test Acc: 0.5083\n",
                        "Seed: 123, Epoch: 320, Loss: 1.3492, Train Acc: 0.6152, Val Acc: 0.5844, Test Acc: 0.5210\n",
                        "Seed: 123, Epoch: 330, Loss: 1.3431, Train Acc: 0.6154, Val Acc: 0.5814, Test Acc: 0.5176\n",
                        "Seed: 123, Epoch: 340, Loss: 1.3379, Train Acc: 0.6171, Val Acc: 0.5839, Test Acc: 0.5224\n",
                        "Seed: 123, Epoch: 350, Loss: 1.3389, Train Acc: 0.6167, Val Acc: 0.5851, Test Acc: 0.5231\n",
                        "Seed: 123, Epoch: 360, Loss: 1.3418, Train Acc: 0.6163, Val Acc: 0.5874, Test Acc: 0.5266\n",
                        "Seed: 123, Epoch: 370, Loss: 1.3304, Train Acc: 0.6196, Val Acc: 0.5870, Test Acc: 0.5247\n",
                        "Seed: 123, Epoch: 380, Loss: 1.3413, Train Acc: 0.6157, Val Acc: 0.5845, Test Acc: 0.5218\n",
                        "Seed: 123, Epoch: 390, Loss: 1.3263, Train Acc: 0.6200, Val Acc: 0.5871, Test Acc: 0.5239\n",
                        "Seed: 123, Epoch: 400, Loss: 1.3264, Train Acc: 0.6198, Val Acc: 0.5915, Test Acc: 0.5323\n",
                        "Seed: 123, Epoch: 410, Loss: 1.3210, Train Acc: 0.6212, Val Acc: 0.5901, Test Acc: 0.5266\n",
                        "Seed: 123, Epoch: 420, Loss: 1.3225, Train Acc: 0.6208, Val Acc: 0.5853, Test Acc: 0.5194\n",
                        "Seed: 123, Epoch: 430, Loss: 1.3226, Train Acc: 0.6204, Val Acc: 0.5877, Test Acc: 0.5269\n",
                        "Seed: 123, Epoch: 440, Loss: 1.3256, Train Acc: 0.6214, Val Acc: 0.5882, Test Acc: 0.5293\n",
                        "Seed: 123, Epoch: 450, Loss: 1.3129, Train Acc: 0.6229, Val Acc: 0.5900, Test Acc: 0.5272\n",
                        "Seed: 123, Epoch: 460, Loss: 1.3395, Train Acc: 0.6132, Val Acc: 0.5821, Test Acc: 0.5217\n",
                        "Seed: 123, Epoch: 470, Loss: 1.3102, Train Acc: 0.6213, Val Acc: 0.5842, Test Acc: 0.5251\n",
                        "Seed: 123, Epoch: 480, Loss: 1.3062, Train Acc: 0.6245, Val Acc: 0.5917, Test Acc: 0.5291\n",
                        "Seed: 123, Epoch: 490, Loss: 1.3057, Train Acc: 0.6246, Val Acc: 0.5862, Test Acc: 0.5203\n",
                        "Seed: 123, Epoch: 500, Loss: 1.3076, Train Acc: 0.6234, Val Acc: 0.5908, Test Acc: 0.5355\n",
                        "Seed: 456, Epoch: 010, Loss: 3.2165, Train Acc: 0.2190, Val Acc: 0.1738, Test Acc: 0.1989\n",
                        "Seed: 456, Epoch: 020, Loss: 2.6125, Train Acc: 0.3716, Val Acc: 0.3993, Test Acc: 0.3536\n",
                        "Seed: 456, Epoch: 030, Loss: 2.1645, Train Acc: 0.4291, Val Acc: 0.4311, Test Acc: 0.3738\n",
                        "Seed: 456, Epoch: 040, Loss: 1.9536, Train Acc: 0.4678, Val Acc: 0.4512, Test Acc: 0.4022\n",
                        "Seed: 456, Epoch: 050, Loss: 1.7592, Train Acc: 0.5202, Val Acc: 0.4987, Test Acc: 0.4325\n",
                        "Seed: 456, Epoch: 060, Loss: 1.6682, Train Acc: 0.5412, Val Acc: 0.5302, Test Acc: 0.4711\n",
                        "Seed: 456, Epoch: 070, Loss: 1.6159, Train Acc: 0.5519, Val Acc: 0.5339, Test Acc: 0.4676\n",
                        "Seed: 456, Epoch: 080, Loss: 1.5773, Train Acc: 0.5607, Val Acc: 0.5393, Test Acc: 0.4803\n",
                        "Seed: 456, Epoch: 090, Loss: 1.5489, Train Acc: 0.5685, Val Acc: 0.5479, Test Acc: 0.4819\n",
                        "Seed: 456, Epoch: 100, Loss: 1.5292, Train Acc: 0.5723, Val Acc: 0.5523, Test Acc: 0.4929\n",
                        "Seed: 456, Epoch: 110, Loss: 1.5082, Train Acc: 0.5771, Val Acc: 0.5551, Test Acc: 0.4919\n",
                        "Seed: 456, Epoch: 120, Loss: 1.4978, Train Acc: 0.5778, Val Acc: 0.5589, Test Acc: 0.4965\n",
                        "Seed: 456, Epoch: 130, Loss: 1.4834, Train Acc: 0.5855, Val Acc: 0.5624, Test Acc: 0.4997\n",
                        "Seed: 456, Epoch: 140, Loss: 1.4647, Train Acc: 0.5880, Val Acc: 0.5654, Test Acc: 0.5016\n",
                        "Seed: 456, Epoch: 150, Loss: 1.4530, Train Acc: 0.5911, Val Acc: 0.5633, Test Acc: 0.4987\n",
                        "Seed: 456, Epoch: 160, Loss: 1.4465, Train Acc: 0.5927, Val Acc: 0.5693, Test Acc: 0.5092\n",
                        "Seed: 456, Epoch: 170, Loss: 1.4375, Train Acc: 0.5937, Val Acc: 0.5676, Test Acc: 0.5073\n",
                        "Seed: 456, Epoch: 180, Loss: 1.4240, Train Acc: 0.5976, Val Acc: 0.5683, Test Acc: 0.5052\n",
                        "Seed: 456, Epoch: 190, Loss: 1.4187, Train Acc: 0.5961, Val Acc: 0.5657, Test Acc: 0.5019\n",
                        "Seed: 456, Epoch: 200, Loss: 1.4111, Train Acc: 0.5992, Val Acc: 0.5728, Test Acc: 0.5106\n",
                        "Seed: 456, Epoch: 210, Loss: 1.4059, Train Acc: 0.6019, Val Acc: 0.5719, Test Acc: 0.5064\n",
                        "Seed: 456, Epoch: 220, Loss: 1.3989, Train Acc: 0.6033, Val Acc: 0.5739, Test Acc: 0.5104\n",
                        "Seed: 456, Epoch: 230, Loss: 1.4132, Train Acc: 0.6001, Val Acc: 0.5645, Test Acc: 0.4957\n",
                        "Seed: 456, Epoch: 240, Loss: 1.3891, Train Acc: 0.6050, Val Acc: 0.5778, Test Acc: 0.5162\n",
                        "Seed: 456, Epoch: 250, Loss: 1.3997, Train Acc: 0.6018, Val Acc: 0.5706, Test Acc: 0.5109\n",
                        "Seed: 456, Epoch: 260, Loss: 1.3842, Train Acc: 0.6052, Val Acc: 0.5756, Test Acc: 0.5147\n",
                        "Seed: 456, Epoch: 270, Loss: 1.3979, Train Acc: 0.6036, Val Acc: 0.5795, Test Acc: 0.5243\n",
                        "Seed: 456, Epoch: 280, Loss: 1.3751, Train Acc: 0.6075, Val Acc: 0.5801, Test Acc: 0.5158\n",
                        "Seed: 456, Epoch: 290, Loss: 1.3675, Train Acc: 0.6090, Val Acc: 0.5797, Test Acc: 0.5144\n",
                        "Seed: 456, Epoch: 300, Loss: 1.3633, Train Acc: 0.6106, Val Acc: 0.5810, Test Acc: 0.5160\n",
                        "Seed: 456, Epoch: 310, Loss: 1.3685, Train Acc: 0.6095, Val Acc: 0.5791, Test Acc: 0.5124\n",
                        "Seed: 456, Epoch: 320, Loss: 1.3605, Train Acc: 0.6107, Val Acc: 0.5802, Test Acc: 0.5131\n",
                        "Seed: 456, Epoch: 330, Loss: 1.3559, Train Acc: 0.6121, Val Acc: 0.5819, Test Acc: 0.5172\n",
                        "Seed: 456, Epoch: 340, Loss: 1.3508, Train Acc: 0.6134, Val Acc: 0.5825, Test Acc: 0.5169\n",
                        "Seed: 456, Epoch: 350, Loss: 1.4329, Train Acc: 0.6042, Val Acc: 0.5803, Test Acc: 0.5154\n",
                        "Seed: 456, Epoch: 360, Loss: 1.3597, Train Acc: 0.6089, Val Acc: 0.5818, Test Acc: 0.5217\n",
                        "Seed: 456, Epoch: 370, Loss: 1.3494, Train Acc: 0.6139, Val Acc: 0.5828, Test Acc: 0.5190\n",
                        "Seed: 456, Epoch: 380, Loss: 1.3430, Train Acc: 0.6160, Val Acc: 0.5839, Test Acc: 0.5196\n",
                        "Seed: 456, Epoch: 390, Loss: 1.3402, Train Acc: 0.6161, Val Acc: 0.5832, Test Acc: 0.5185\n",
                        "Seed: 456, Epoch: 400, Loss: 1.3378, Train Acc: 0.6167, Val Acc: 0.5830, Test Acc: 0.5172\n",
                        "Seed: 456, Epoch: 410, Loss: 1.3342, Train Acc: 0.6178, Val Acc: 0.5842, Test Acc: 0.5195\n",
                        "Seed: 456, Epoch: 420, Loss: 1.3365, Train Acc: 0.6153, Val Acc: 0.5832, Test Acc: 0.5255\n",
                        "Seed: 456, Epoch: 430, Loss: 1.3305, Train Acc: 0.6190, Val Acc: 0.5851, Test Acc: 0.5212\n",
                        "Seed: 456, Epoch: 440, Loss: 1.3287, Train Acc: 0.6190, Val Acc: 0.5817, Test Acc: 0.5144\n",
                        "Seed: 456, Epoch: 450, Loss: 1.3380, Train Acc: 0.6152, Val Acc: 0.5826, Test Acc: 0.5220\n",
                        "Seed: 456, Epoch: 460, Loss: 1.3264, Train Acc: 0.6187, Val Acc: 0.5851, Test Acc: 0.5251\n",
                        "Seed: 456, Epoch: 470, Loss: 1.3222, Train Acc: 0.6199, Val Acc: 0.5860, Test Acc: 0.5228\n",
                        "Seed: 456, Epoch: 480, Loss: 1.3225, Train Acc: 0.6197, Val Acc: 0.5893, Test Acc: 0.5309\n",
                        "Seed: 456, Epoch: 490, Loss: 1.3221, Train Acc: 0.6202, Val Acc: 0.5837, Test Acc: 0.5203\n",
                        "Seed: 456, Epoch: 500, Loss: 1.3177, Train Acc: 0.6214, Val Acc: 0.5900, Test Acc: 0.5305\n",
                        "Seed: 42, Best Val Acc: 0.5913, Corresponding Test Acc: 0.5376, Time: 121.57s, Memory: 2.31MB, GPU Memory: 6650.66MB\n",
                        "Seed: 42, Best Val Acc: 0.5924, Corresponding Test Acc: 0.5315, Time: 61.82s, Memory: 28.71MB, GPU Memory: 5734.25MB\n",
                        "Seed: 123, Best Val Acc: 0.5942, Corresponding Test Acc: 0.5348, Time: 73.23s, Memory: 1.52MB, GPU Memory: 4877.93MB\n",
                        "Seed: 456, Best Val Acc: 0.5900, Corresponding Test Acc: 0.5305, Time: 72.16s, Memory: 1.52MB, GPU Memory: 4878.14MB\n",
                        "\n",
                        "Average Best Validation Accuracy: 0.5920\n",
                        "Average Corresponding Test Accuracy: 0.5336\n",
                        "Average Time: 82.19s\n",
                        "Average Memory Usage: 8.52MB\n",
                        "Average GPU Memory Usage: 5535.25MB\n",
                        "\n",
                        "Validation Accuracy Variance: 0.0000\n",
                        "Test Accuracy Variance: 0.0000\n",
                        "Time Variance: 536.5768\n",
                        "Memory Usage Variance: 135.9798MB\n",
                        "GPU Memory Usage Variance: 536897.2186MB\n"
                    ]
                }
            ],
            "source": "from torch_geometric.datasets import GitHub\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimport time\nimport tracemalloc\nimport torch\nfrom torch_geometric.datasets import Planetoid\nimport torch_geometric.transforms as T\nfrom torch_geometric.nn import GCNConv\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport random\nfrom typing import Callable, Optional, Union\ndataset = dataset_sparse\ngraph = dataset[0]\nnum_classes = dataset.num_classes\nin_channels = dataset.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.5, 0.5]\nclass HierarchicalGCN_TopK(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_TopK, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.sum_res = sum_res\n        self.act = act\n        channels = self.hidden_channels\n        self.pools = torch.nn.ModuleList()\n        self.down_convs = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(TopKPooling(channels, ratio=0.9))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.0, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = F.relu(x)  \n        xs = [x]\n        edge_indices = [edge_index]\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = F.relu(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            up = res\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = F.relu(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x.log_softmax(dim=-1)\ndef train(model, data, train_idx, optimizer):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index)[train_idx]\n    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n@torch.no_grad()\ndef test(model, data, split_idx, evaluator):\n    model.eval()\n    out = model(data.x, data.edge_index)\n    y_pred = out.argmax(dim=-1, keepdim=True)\n    train_acc = evaluator.eval({\n        'y_true': data.y[split_idx['train']],\n        'y_pred': y_pred[split_idx['train']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': data.y[split_idx['valid']],\n        'y_pred': y_pred[split_idx['valid']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': data.y[split_idx['test']],\n        'y_pred': y_pred[split_idx['test']],\n    })['acc']\n    return train_acc, valid_acc, test_acc\nseeds = [42, 123, 456]\nval_acc_variances = []\ntest_acc_variances = []\ntime_variances = []\nmemory_variances = []\ngpu_memory_variances = []\nfor seed in seeds:\n    graph = graph.to(device)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    val_accuracies = []\n    test_accuracies = []\n    start_time = time.time()\n    tracemalloc.start()\n    model = HierarchicalGCN_TopK(\n        in_channels=graph.num_features,\n        hidden_channels=256,\n        out_channels=dataset.num_classes,\n        depth=2,\n        act=F.relu,\n        sum_res=False\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx['train'].to(device)\n    for epoch in range(1, 501):\n        loss = train(model, graph, split_idx['train'], optimizer)\n        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n        val_accuracies.append(valid_acc)\n        test_accuracies.append(test_acc)\n        if epoch % 10 == 0:\n            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    elapsed_time = end_time - start_time\n    memory_usage = current / 10**6  \n    peak_memory_usage = peak / 10**6  \n    if torch.cuda.is_available():\n        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  \n        torch.cuda.reset_max_memory_allocated(device)\n    else:\n        gpu_memory_usage = 0\n    val_accuracies_list.append(val_accuracies)\n    times.append(elapsed_time)\n    memories.append(memory_usage)\n    gpu_memories.append(gpu_memory_usage)\n    best_val_acc = max(val_accuracies)\n    best_val_index = val_accuracies.index(best_val_acc)\n    corresponding_test_acc = test_accuracies[best_val_index]\n    results.append({\n        'seed': seed,\n        'best_val_acc': best_val_acc,\n        'corresponding_test_acc': corresponding_test_acc,\n        'elapsed_time': elapsed_time,\n        'memory_usage': memory_usage,\n        'gpu_memory_usage': gpu_memory_usage\n    })\navg_val_acc = np.mean([result['best_val_acc'] for result in results])\navg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\navg_time = np.mean(times)\navg_memory = np.mean(memories)\navg_gpu_memory = np.mean(gpu_memories)\nval_acc_variance = np.var([result['best_val_acc'] for result in results])\ntest_acc_variance = np.var([result['corresponding_test_acc'] for result in results])\ntime_variance = np.var(times)\nmemory_variance = np.var(memories)\ngpu_memory_variance = np.var(gpu_memories)\nfor result in results:\n    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\nprint(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\nprint(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\nprint(f\"Average Time: {avg_time:.2f}s\")\nprint(f\"Average Memory Usage: {avg_memory:.2f}MB\")\nprint(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")\nprint(f\"\\nValidation Accuracy Variance: {val_acc_variance:.4f}\")\nprint(f\"Test Accuracy Variance: {test_acc_variance:.4f}\")\nprint(f\"Time Variance: {time_variance:.4f}\")\nprint(f\"Memory Usage Variance: {memory_variance:.4f}MB\")\nprint(f\"GPU Memory Usage Variance: {gpu_memory_variance:.4f}MB\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### SAGPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 042, Epoch: 010, Loss: 2.9403, Train Acc: 0.2903, Val Acc: 0.3134, Test Acc: 0.2990\n",
                        "Seed: 042, Epoch: 020, Loss: 2.4534, Train Acc: 0.3768, Val Acc: 0.4007, Test Acc: 0.3550\n",
                        "Seed: 042, Epoch: 030, Loss: 2.0780, Train Acc: 0.4409, Val Acc: 0.4381, Test Acc: 0.3763\n",
                        "Seed: 042, Epoch: 040, Loss: 1.8896, Train Acc: 0.4966, Val Acc: 0.4855, Test Acc: 0.4264\n",
                        "Seed: 042, Epoch: 050, Loss: 1.7429, Train Acc: 0.5291, Val Acc: 0.5180, Test Acc: 0.4522\n",
                        "Seed: 042, Epoch: 060, Loss: 1.6436, Train Acc: 0.5456, Val Acc: 0.5330, Test Acc: 0.4698\n",
                        "Seed: 042, Epoch: 070, Loss: 1.5837, Train Acc: 0.5602, Val Acc: 0.5428, Test Acc: 0.4767\n",
                        "Seed: 042, Epoch: 080, Loss: 1.5461, Train Acc: 0.5676, Val Acc: 0.5466, Test Acc: 0.4823\n",
                        "Seed: 042, Epoch: 090, Loss: 1.5217, Train Acc: 0.5741, Val Acc: 0.5520, Test Acc: 0.4874\n",
                        "Seed: 042, Epoch: 100, Loss: 1.5000, Train Acc: 0.5785, Val Acc: 0.5563, Test Acc: 0.4945\n",
                        "Seed: 042, Epoch: 110, Loss: 1.4831, Train Acc: 0.5831, Val Acc: 0.5609, Test Acc: 0.4980\n",
                        "Seed: 042, Epoch: 120, Loss: 1.4696, Train Acc: 0.5839, Val Acc: 0.5591, Test Acc: 0.4938\n",
                        "Seed: 042, Epoch: 130, Loss: 1.4607, Train Acc: 0.5892, Val Acc: 0.5654, Test Acc: 0.5014\n",
                        "Seed: 042, Epoch: 140, Loss: 1.4439, Train Acc: 0.5925, Val Acc: 0.5689, Test Acc: 0.5054\n",
                        "Seed: 042, Epoch: 150, Loss: 1.4335, Train Acc: 0.5946, Val Acc: 0.5726, Test Acc: 0.5064\n",
                        "Seed: 042, Epoch: 160, Loss: 1.4228, Train Acc: 0.5973, Val Acc: 0.5678, Test Acc: 0.5038\n",
                        "Seed: 042, Epoch: 170, Loss: 1.4172, Train Acc: 0.6001, Val Acc: 0.5719, Test Acc: 0.5079\n",
                        "Seed: 042, Epoch: 180, Loss: 1.4078, Train Acc: 0.6006, Val Acc: 0.5760, Test Acc: 0.5146\n",
                        "Seed: 042, Epoch: 190, Loss: 1.4066, Train Acc: 0.6024, Val Acc: 0.5803, Test Acc: 0.5161\n",
                        "Seed: 042, Epoch: 200, Loss: 1.3946, Train Acc: 0.6023, Val Acc: 0.5771, Test Acc: 0.5141\n",
                        "Seed: 042, Epoch: 210, Loss: 1.3863, Train Acc: 0.6053, Val Acc: 0.5787, Test Acc: 0.5117\n",
                        "Seed: 042, Epoch: 220, Loss: 1.3853, Train Acc: 0.6065, Val Acc: 0.5790, Test Acc: 0.5130\n",
                        "Seed: 042, Epoch: 230, Loss: 1.3841, Train Acc: 0.6072, Val Acc: 0.5767, Test Acc: 0.5104\n",
                        "Seed: 042, Epoch: 240, Loss: 1.3700, Train Acc: 0.6088, Val Acc: 0.5798, Test Acc: 0.5114\n",
                        "Seed: 042, Epoch: 250, Loss: 1.3695, Train Acc: 0.6089, Val Acc: 0.5812, Test Acc: 0.5147\n",
                        "Seed: 042, Epoch: 260, Loss: 1.3682, Train Acc: 0.6111, Val Acc: 0.5818, Test Acc: 0.5166\n",
                        "Seed: 042, Epoch: 270, Loss: 1.3635, Train Acc: 0.6096, Val Acc: 0.5794, Test Acc: 0.5148\n",
                        "Seed: 042, Epoch: 280, Loss: 1.3553, Train Acc: 0.6114, Val Acc: 0.5786, Test Acc: 0.5124\n",
                        "Seed: 042, Epoch: 290, Loss: 1.3525, Train Acc: 0.6124, Val Acc: 0.5832, Test Acc: 0.5194\n",
                        "Seed: 042, Epoch: 300, Loss: 1.3510, Train Acc: 0.6137, Val Acc: 0.5852, Test Acc: 0.5191\n",
                        "Seed: 042, Epoch: 310, Loss: 1.3466, Train Acc: 0.6146, Val Acc: 0.5844, Test Acc: 0.5205\n",
                        "Seed: 042, Epoch: 320, Loss: 1.3424, Train Acc: 0.6162, Val Acc: 0.5853, Test Acc: 0.5182\n",
                        "Seed: 042, Epoch: 330, Loss: 1.3483, Train Acc: 0.6158, Val Acc: 0.5837, Test Acc: 0.5196\n",
                        "Seed: 042, Epoch: 340, Loss: 1.3381, Train Acc: 0.6165, Val Acc: 0.5875, Test Acc: 0.5272\n",
                        "Seed: 042, Epoch: 350, Loss: 1.3394, Train Acc: 0.6178, Val Acc: 0.5883, Test Acc: 0.5275\n",
                        "Seed: 042, Epoch: 360, Loss: 1.3389, Train Acc: 0.6157, Val Acc: 0.5796, Test Acc: 0.5230\n",
                        "Seed: 042, Epoch: 370, Loss: 1.3266, Train Acc: 0.6183, Val Acc: 0.5864, Test Acc: 0.5227\n",
                        "Seed: 042, Epoch: 380, Loss: 1.3327, Train Acc: 0.6183, Val Acc: 0.5871, Test Acc: 0.5231\n",
                        "Seed: 042, Epoch: 390, Loss: 1.3253, Train Acc: 0.6202, Val Acc: 0.5886, Test Acc: 0.5289\n",
                        "Seed: 042, Epoch: 400, Loss: 1.3252, Train Acc: 0.6195, Val Acc: 0.5894, Test Acc: 0.5361\n",
                        "Seed: 042, Epoch: 410, Loss: 1.3184, Train Acc: 0.6216, Val Acc: 0.5877, Test Acc: 0.5245\n",
                        "Seed: 042, Epoch: 420, Loss: 1.3470, Train Acc: 0.6094, Val Acc: 0.5764, Test Acc: 0.5119\n",
                        "Seed: 042, Epoch: 430, Loss: 1.3166, Train Acc: 0.6187, Val Acc: 0.5867, Test Acc: 0.5360\n",
                        "Seed: 042, Epoch: 440, Loss: 1.3158, Train Acc: 0.6216, Val Acc: 0.5881, Test Acc: 0.5273\n",
                        "Seed: 042, Epoch: 450, Loss: 1.3120, Train Acc: 0.6218, Val Acc: 0.5840, Test Acc: 0.5247\n",
                        "Seed: 042, Epoch: 460, Loss: 1.3191, Train Acc: 0.6229, Val Acc: 0.5862, Test Acc: 0.5248\n",
                        "Seed: 042, Epoch: 470, Loss: 1.3098, Train Acc: 0.6223, Val Acc: 0.5869, Test Acc: 0.5264\n",
                        "Seed: 042, Epoch: 480, Loss: 1.3053, Train Acc: 0.6243, Val Acc: 0.5908, Test Acc: 0.5299\n",
                        "Seed: 042, Epoch: 490, Loss: 1.3047, Train Acc: 0.6245, Val Acc: 0.5865, Test Acc: 0.5234\n",
                        "Seed: 042, Epoch: 500, Loss: 1.2998, Train Acc: 0.6252, Val Acc: 0.5926, Test Acc: 0.5330\n",
                        "Seed: 123, Epoch: 010, Loss: 2.8968, Train Acc: 0.2945, Val Acc: 0.3117, Test Acc: 0.3064\n",
                        "Seed: 123, Epoch: 020, Loss: 2.4204, Train Acc: 0.3916, Val Acc: 0.4144, Test Acc: 0.3670\n",
                        "Seed: 123, Epoch: 030, Loss: 2.0850, Train Acc: 0.4486, Val Acc: 0.4463, Test Acc: 0.3858\n",
                        "Seed: 123, Epoch: 040, Loss: 1.8441, Train Acc: 0.5078, Val Acc: 0.5019, Test Acc: 0.4400\n",
                        "Seed: 123, Epoch: 050, Loss: 1.6811, Train Acc: 0.5394, Val Acc: 0.5214, Test Acc: 0.4604\n",
                        "Seed: 123, Epoch: 060, Loss: 1.6118, Train Acc: 0.5535, Val Acc: 0.5311, Test Acc: 0.4665\n",
                        "Seed: 123, Epoch: 070, Loss: 1.5649, Train Acc: 0.5655, Val Acc: 0.5433, Test Acc: 0.4803\n",
                        "Seed: 123, Epoch: 080, Loss: 1.5338, Train Acc: 0.5721, Val Acc: 0.5530, Test Acc: 0.4915\n",
                        "Seed: 123, Epoch: 090, Loss: 1.5117, Train Acc: 0.5775, Val Acc: 0.5545, Test Acc: 0.4890\n",
                        "Seed: 123, Epoch: 100, Loss: 1.5059, Train Acc: 0.5775, Val Acc: 0.5570, Test Acc: 0.4984\n",
                        "Seed: 123, Epoch: 110, Loss: 1.4733, Train Acc: 0.5860, Val Acc: 0.5624, Test Acc: 0.4997\n",
                        "Seed: 123, Epoch: 120, Loss: 1.4585, Train Acc: 0.5890, Val Acc: 0.5619, Test Acc: 0.5011\n",
                        "Seed: 123, Epoch: 130, Loss: 1.4646, Train Acc: 0.5854, Val Acc: 0.5632, Test Acc: 0.5126\n",
                        "Seed: 123, Epoch: 140, Loss: 1.4347, Train Acc: 0.5932, Val Acc: 0.5657, Test Acc: 0.5013\n",
                        "Seed: 123, Epoch: 150, Loss: 1.4250, Train Acc: 0.5963, Val Acc: 0.5721, Test Acc: 0.5132\n",
                        "Seed: 123, Epoch: 160, Loss: 1.4231, Train Acc: 0.5990, Val Acc: 0.5716, Test Acc: 0.5101\n",
                        "Seed: 123, Epoch: 170, Loss: 1.4114, Train Acc: 0.6004, Val Acc: 0.5716, Test Acc: 0.5113\n",
                        "Seed: 123, Epoch: 180, Loss: 1.4209, Train Acc: 0.6000, Val Acc: 0.5687, Test Acc: 0.5076\n",
                        "Seed: 123, Epoch: 190, Loss: 1.3951, Train Acc: 0.6029, Val Acc: 0.5772, Test Acc: 0.5119\n",
                        "Seed: 123, Epoch: 200, Loss: 1.3933, Train Acc: 0.6043, Val Acc: 0.5762, Test Acc: 0.5121\n",
                        "Seed: 123, Epoch: 210, Loss: 1.3929, Train Acc: 0.6047, Val Acc: 0.5714, Test Acc: 0.5055\n",
                        "Seed: 123, Epoch: 220, Loss: 1.3814, Train Acc: 0.6060, Val Acc: 0.5830, Test Acc: 0.5234\n",
                        "Seed: 123, Epoch: 230, Loss: 1.3762, Train Acc: 0.6068, Val Acc: 0.5786, Test Acc: 0.5132\n",
                        "Seed: 123, Epoch: 240, Loss: 1.3803, Train Acc: 0.6048, Val Acc: 0.5810, Test Acc: 0.5267\n",
                        "Seed: 123, Epoch: 250, Loss: 1.3670, Train Acc: 0.6087, Val Acc: 0.5815, Test Acc: 0.5206\n",
                        "Seed: 123, Epoch: 260, Loss: 1.3649, Train Acc: 0.6089, Val Acc: 0.5795, Test Acc: 0.5162\n",
                        "Seed: 123, Epoch: 270, Loss: 1.3599, Train Acc: 0.6124, Val Acc: 0.5832, Test Acc: 0.5205\n",
                        "Seed: 123, Epoch: 280, Loss: 1.3539, Train Acc: 0.6130, Val Acc: 0.5818, Test Acc: 0.5181\n",
                        "Seed: 123, Epoch: 290, Loss: 1.3553, Train Acc: 0.6117, Val Acc: 0.5818, Test Acc: 0.5225\n",
                        "Seed: 123, Epoch: 300, Loss: 1.3530, Train Acc: 0.6133, Val Acc: 0.5821, Test Acc: 0.5152\n",
                        "Seed: 123, Epoch: 310, Loss: 1.3471, Train Acc: 0.6142, Val Acc: 0.5828, Test Acc: 0.5199\n",
                        "Seed: 123, Epoch: 320, Loss: 1.3418, Train Acc: 0.6156, Val Acc: 0.5826, Test Acc: 0.5189\n",
                        "Seed: 123, Epoch: 330, Loss: 1.3430, Train Acc: 0.6138, Val Acc: 0.5881, Test Acc: 0.5278\n",
                        "Seed: 123, Epoch: 340, Loss: 1.3363, Train Acc: 0.6178, Val Acc: 0.5849, Test Acc: 0.5207\n",
                        "Seed: 123, Epoch: 350, Loss: 1.3345, Train Acc: 0.6187, Val Acc: 0.5857, Test Acc: 0.5230\n",
                        "Seed: 123, Epoch: 360, Loss: 1.3301, Train Acc: 0.6182, Val Acc: 0.5862, Test Acc: 0.5214\n",
                        "Seed: 123, Epoch: 370, Loss: 1.3306, Train Acc: 0.6186, Val Acc: 0.5835, Test Acc: 0.5187\n",
                        "Seed: 123, Epoch: 380, Loss: 1.3253, Train Acc: 0.6202, Val Acc: 0.5849, Test Acc: 0.5203\n",
                        "Seed: 123, Epoch: 390, Loss: 1.3388, Train Acc: 0.6161, Val Acc: 0.5816, Test Acc: 0.5220\n",
                        "Seed: 123, Epoch: 400, Loss: 1.3262, Train Acc: 0.6202, Val Acc: 0.5835, Test Acc: 0.5164\n",
                        "Seed: 123, Epoch: 410, Loss: 1.3159, Train Acc: 0.6215, Val Acc: 0.5908, Test Acc: 0.5282\n",
                        "Seed: 123, Epoch: 420, Loss: 1.3306, Train Acc: 0.6219, Val Acc: 0.5898, Test Acc: 0.5300\n",
                        "Seed: 123, Epoch: 430, Loss: 1.3166, Train Acc: 0.6220, Val Acc: 0.5852, Test Acc: 0.5188\n",
                        "Seed: 123, Epoch: 440, Loss: 1.3115, Train Acc: 0.6228, Val Acc: 0.5919, Test Acc: 0.5292\n",
                        "Seed: 123, Epoch: 450, Loss: 1.3126, Train Acc: 0.6216, Val Acc: 0.5818, Test Acc: 0.5175\n",
                        "Seed: 123, Epoch: 460, Loss: 1.3128, Train Acc: 0.6243, Val Acc: 0.5876, Test Acc: 0.5225\n",
                        "Seed: 123, Epoch: 470, Loss: 1.3023, Train Acc: 0.6242, Val Acc: 0.5900, Test Acc: 0.5311\n",
                        "Seed: 123, Epoch: 480, Loss: 1.3063, Train Acc: 0.6232, Val Acc: 0.5876, Test Acc: 0.5264\n",
                        "Seed: 123, Epoch: 490, Loss: 1.3020, Train Acc: 0.6229, Val Acc: 0.5880, Test Acc: 0.5240\n",
                        "Seed: 123, Epoch: 500, Loss: 1.3121, Train Acc: 0.6205, Val Acc: 0.5859, Test Acc: 0.5280\n",
                        "Seed: 456, Epoch: 010, Loss: 3.0682, Train Acc: 0.2463, Val Acc: 0.2139, Test Acc: 0.1799\n",
                        "Seed: 456, Epoch: 020, Loss: 2.5427, Train Acc: 0.3507, Val Acc: 0.3606, Test Acc: 0.3320\n",
                        "Seed: 456, Epoch: 030, Loss: 2.1026, Train Acc: 0.4379, Val Acc: 0.4484, Test Acc: 0.3919\n",
                        "Seed: 456, Epoch: 040, Loss: 1.8896, Train Acc: 0.4904, Val Acc: 0.4931, Test Acc: 0.4313\n",
                        "Seed: 456, Epoch: 050, Loss: 1.7226, Train Acc: 0.5291, Val Acc: 0.5122, Test Acc: 0.4506\n",
                        "Seed: 456, Epoch: 060, Loss: 1.6422, Train Acc: 0.5457, Val Acc: 0.5250, Test Acc: 0.4610\n",
                        "Seed: 456, Epoch: 070, Loss: 1.5833, Train Acc: 0.5605, Val Acc: 0.5392, Test Acc: 0.4751\n",
                        "Seed: 456, Epoch: 080, Loss: 1.5657, Train Acc: 0.5652, Val Acc: 0.5408, Test Acc: 0.4750\n",
                        "Seed: 456, Epoch: 090, Loss: 1.5200, Train Acc: 0.5741, Val Acc: 0.5541, Test Acc: 0.4911\n",
                        "Seed: 456, Epoch: 100, Loss: 1.5008, Train Acc: 0.5753, Val Acc: 0.5495, Test Acc: 0.4808\n",
                        "Seed: 456, Epoch: 110, Loss: 1.4790, Train Acc: 0.5828, Val Acc: 0.5567, Test Acc: 0.4917\n",
                        "Seed: 456, Epoch: 120, Loss: 1.4631, Train Acc: 0.5870, Val Acc: 0.5596, Test Acc: 0.4936\n",
                        "Seed: 456, Epoch: 130, Loss: 1.4533, Train Acc: 0.5890, Val Acc: 0.5661, Test Acc: 0.5047\n",
                        "Seed: 456, Epoch: 140, Loss: 1.4389, Train Acc: 0.5925, Val Acc: 0.5669, Test Acc: 0.5052\n",
                        "Seed: 456, Epoch: 150, Loss: 1.4390, Train Acc: 0.5931, Val Acc: 0.5634, Test Acc: 0.4982\n",
                        "Seed: 456, Epoch: 160, Loss: 1.4243, Train Acc: 0.5960, Val Acc: 0.5660, Test Acc: 0.5026\n",
                        "Seed: 456, Epoch: 170, Loss: 1.4292, Train Acc: 0.5910, Val Acc: 0.5660, Test Acc: 0.5104\n",
                        "Seed: 456, Epoch: 180, Loss: 1.4114, Train Acc: 0.5989, Val Acc: 0.5686, Test Acc: 0.5066\n",
                        "Seed: 456, Epoch: 190, Loss: 1.4030, Train Acc: 0.6004, Val Acc: 0.5709, Test Acc: 0.5082\n",
                        "Seed: 456, Epoch: 200, Loss: 1.3974, Train Acc: 0.6023, Val Acc: 0.5736, Test Acc: 0.5112\n",
                        "Seed: 456, Epoch: 210, Loss: 1.3931, Train Acc: 0.6031, Val Acc: 0.5750, Test Acc: 0.5129\n",
                        "Seed: 456, Epoch: 220, Loss: 1.3877, Train Acc: 0.6044, Val Acc: 0.5726, Test Acc: 0.5073\n",
                        "Seed: 456, Epoch: 230, Loss: 1.3843, Train Acc: 0.6050, Val Acc: 0.5761, Test Acc: 0.5153\n",
                        "Seed: 456, Epoch: 240, Loss: 1.3846, Train Acc: 0.6059, Val Acc: 0.5808, Test Acc: 0.5216\n",
                        "Seed: 456, Epoch: 250, Loss: 1.3720, Train Acc: 0.6080, Val Acc: 0.5748, Test Acc: 0.5092\n",
                        "Seed: 456, Epoch: 260, Loss: 1.3697, Train Acc: 0.6097, Val Acc: 0.5778, Test Acc: 0.5149\n",
                        "Seed: 456, Epoch: 270, Loss: 1.3789, Train Acc: 0.6060, Val Acc: 0.5747, Test Acc: 0.5218\n",
                        "Seed: 456, Epoch: 280, Loss: 1.3672, Train Acc: 0.6073, Val Acc: 0.5747, Test Acc: 0.5159\n",
                        "Seed: 456, Epoch: 290, Loss: 1.3636, Train Acc: 0.6086, Val Acc: 0.5765, Test Acc: 0.5153\n",
                        "Seed: 456, Epoch: 300, Loss: 1.3569, Train Acc: 0.6112, Val Acc: 0.5832, Test Acc: 0.5236\n",
                        "Seed: 456, Epoch: 310, Loss: 1.3559, Train Acc: 0.6135, Val Acc: 0.5843, Test Acc: 0.5248\n",
                        "Seed: 456, Epoch: 320, Loss: 1.3473, Train Acc: 0.6145, Val Acc: 0.5821, Test Acc: 0.5193\n",
                        "Seed: 456, Epoch: 330, Loss: 1.3434, Train Acc: 0.6154, Val Acc: 0.5826, Test Acc: 0.5216\n",
                        "Seed: 456, Epoch: 340, Loss: 1.3448, Train Acc: 0.6140, Val Acc: 0.5783, Test Acc: 0.5170\n",
                        "Seed: 456, Epoch: 350, Loss: 1.3400, Train Acc: 0.6175, Val Acc: 0.5846, Test Acc: 0.5232\n",
                        "Seed: 456, Epoch: 360, Loss: 1.3362, Train Acc: 0.6181, Val Acc: 0.5882, Test Acc: 0.5274\n",
                        "Seed: 456, Epoch: 370, Loss: 1.3348, Train Acc: 0.6166, Val Acc: 0.5815, Test Acc: 0.5152\n",
                        "Seed: 456, Epoch: 380, Loss: 1.3426, Train Acc: 0.6167, Val Acc: 0.5802, Test Acc: 0.5151\n",
                        "Seed: 456, Epoch: 390, Loss: 1.3318, Train Acc: 0.6175, Val Acc: 0.5864, Test Acc: 0.5297\n",
                        "Seed: 456, Epoch: 400, Loss: 1.3273, Train Acc: 0.6195, Val Acc: 0.5867, Test Acc: 0.5241\n",
                        "Seed: 456, Epoch: 410, Loss: 1.3203, Train Acc: 0.6212, Val Acc: 0.5874, Test Acc: 0.5255\n",
                        "Seed: 456, Epoch: 420, Loss: 1.3170, Train Acc: 0.6219, Val Acc: 0.5874, Test Acc: 0.5241\n",
                        "Seed: 456, Epoch: 430, Loss: 1.3470, Train Acc: 0.6108, Val Acc: 0.5836, Test Acc: 0.5260\n",
                        "Seed: 456, Epoch: 440, Loss: 1.3387, Train Acc: 0.6138, Val Acc: 0.5816, Test Acc: 0.5240\n",
                        "Seed: 456, Epoch: 450, Loss: 1.3253, Train Acc: 0.6185, Val Acc: 0.5858, Test Acc: 0.5243\n",
                        "Seed: 456, Epoch: 460, Loss: 1.3153, Train Acc: 0.6229, Val Acc: 0.5907, Test Acc: 0.5293\n",
                        "Seed: 456, Epoch: 470, Loss: 1.3097, Train Acc: 0.6231, Val Acc: 0.5895, Test Acc: 0.5300\n",
                        "Seed: 456, Epoch: 480, Loss: 1.3064, Train Acc: 0.6246, Val Acc: 0.5895, Test Acc: 0.5288\n",
                        "Seed: 456, Epoch: 490, Loss: 1.3100, Train Acc: 0.6239, Val Acc: 0.5872, Test Acc: 0.5264\n",
                        "Seed: 456, Epoch: 500, Loss: 1.3025, Train Acc: 0.6250, Val Acc: 0.5896, Test Acc: 0.5326\n",
                        "Seed: 42, Best Val Acc: 0.5932, Corresponding Test Acc: 0.5318, Time: 60.65s, Memory: 1.71MB, GPU Memory: 4254.78MB\n",
                        "Seed: 123, Best Val Acc: 0.5938, Corresponding Test Acc: 0.5353, Time: 73.81s, Memory: 1.56MB, GPU Memory: 4277.71MB\n",
                        "Seed: 456, Best Val Acc: 0.5924, Corresponding Test Acc: 0.5345, Time: 60.38s, Memory: 1.56MB, GPU Memory: 4277.06MB\n",
                        "\n",
                        "Average Best Validation Accuracy: 0.5931\n",
                        "Average Corresponding Test Accuracy: 0.5339\n",
                        "Average Time: 64.95s\n",
                        "Average Memory Usage: 1.61MB\n",
                        "Average GPU Memory Usage: 4269.85MB\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import SAGPooling\ndataset = dataset_sparse\ngraph = dataset[0]\nnum_classes = dataset.num_classes\nin_channels = dataset.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.5, 0.5]\nclass HierarchicalGCN_SAG(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_SAG, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.sum_res = sum_res\n        self.act = act\n        channels = self.hidden_channels\n        self.pools = torch.nn.ModuleList()\n        self.down_convs = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(SAGPooling(channels, ratio=0.7))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.0, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = F.relu(x)  \n        xs = [x]\n        edge_indices = [edge_index]\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = F.relu(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            up = res\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = F.relu(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x.log_softmax(dim=-1)\ndef train(model, data, train_idx, optimizer):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index)[train_idx]\n    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n@torch.no_grad()\ndef test(model, data, split_idx, evaluator):\n    model.eval()\n    out = model(data.x, data.edge_index)\n    y_pred = out.argmax(dim=-1, keepdim=True)\n    train_acc = evaluator.eval({\n        'y_true': data.y[split_idx['train']],\n        'y_pred': y_pred[split_idx['train']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': data.y[split_idx['valid']],\n        'y_pred': y_pred[split_idx['valid']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': data.y[split_idx['test']],\n        'y_pred': y_pred[split_idx['test']],\n    })['acc']\n    return train_acc, valid_acc, test_acc\nseeds = [42, 123, 456]\nresults = []\nval_accuracies_list = []\ntimes = []\nmemories = []\ngpu_memories = []\nfor seed in seeds:\n    graph = graph.to(device)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    val_accuracies = []\n    test_accuracies = []\n    start_time = time.time()\n    tracemalloc.start()\n    model = HierarchicalGCN_SAG(\n        in_channels=graph.num_features,\n        hidden_channels=256,\n        out_channels=dataset.num_classes,\n        depth=2,\n        act=F.relu,\n        sum_res=False\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx['train'].to(device)\n    for epoch in range(1, 501):\n        loss = train(model, graph, split_idx['train'], optimizer)\n        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n        val_accuracies.append(valid_acc)\n        test_accuracies.append(test_acc)\n        if epoch % 10 == 0:\n            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    elapsed_time = end_time - start_time\n    memory_usage = current / 10**6  \n    peak_memory_usage = peak / 10**6  \n    if torch.cuda.is_available():\n        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  \n        torch.cuda.reset_max_memory_allocated(device)\n    else:\n        gpu_memory_usage = 0\n    val_accuracies_list.append(val_accuracies)\n    times.append(elapsed_time)\n    memories.append(memory_usage)\n    gpu_memories.append(gpu_memory_usage)\n    best_val_acc = max(val_accuracies)\n    best_val_index = val_accuracies.index(best_val_acc)\n    corresponding_test_acc = test_accuracies[best_val_index]\n    results.append({\n        'seed': seed,\n        'best_val_acc': best_val_acc,\n        'corresponding_test_acc': corresponding_test_acc,\n        'elapsed_time': elapsed_time,\n        'memory_usage': memory_usage,\n        'gpu_memory_usage': gpu_memory_usage\n    })\nfor result in results:\n    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\navg_val_acc = np.mean([result['best_val_acc'] for result in results])\navg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\navg_time = np.mean(times)\navg_memory = np.mean(memories)\navg_gpu_memory = np.mean(gpu_memories)\nprint(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\nprint(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\nprint(f\"Average Time: {avg_time:.2f}s\")\nprint(f\"Average Memory Usage: {avg_memory:.2f}MB\")\nprint(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### COPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\ndef cumsum(x: Tensor, dim: int = 0) -> Tensor:\n    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n    Args:\n        x (torch.Tensor): The input tensor.\n        dim (int, optional): The dimension to do the operation over.\n            (default: :obj:`0`)\n    Example:\n        >>> x = torch.tensor([2, 4, 1])\n        >>> cumsum(x)\n        tensor([0, 2, 6, 7])\n    \"\"\"\n    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n    out = x.new_empty(size)\n    out.narrow(dim, 0, 1).zero_()\n    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n    return out\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\ndef topk(\n    x: Tensor,\n    ratio: Optional[Union[float, int]],\n    batch: Tensor,\n    min_score: Optional[float] = None,\n    tol: float = 1e-7,\n) -> Tensor:\n    if min_score is not None:\n        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero().view(-1)\n        return perm\n    if ratio is not None:\n        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n        if ratio >= 1:\n            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n        else:\n            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n        x, x_perm = torch.sort(x.view(-1), descending=True)\n        batch = batch[x_perm]\n        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n        ptr = cumsum(num_nodes)\n        batched_arange = arange - ptr[batch]\n        mask = batched_arange < k[batch]\n        return x_perm[batch_perm[mask]]\n    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n                     \"must be specified\")\nclass GPR_prop(MessagePassing):\n    '''\n    propagation class for GPR_GNN\n    '''\n    def __init__(self, K, alpha, Init, Gamma=None, bias=True, **kwargs):\n        super(GPR_prop, self).__init__(aggr='add', **kwargs)\n        self.K = K\n        self.Init = Init\n        self.alpha = alpha\n        assert Init in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']\n        if Init == 'SGC':\n            TEMP = 0.0*np.ones(K+1)\n            TEMP[alpha] = 1.0\n        elif Init == 'PPR':\n            TEMP = alpha*(1-alpha)**np.arange(K+1)\n            TEMP[-1] = (1-alpha)**K\n        elif Init == 'NPPR':\n            TEMP = (alpha)**np.arange(K+1)\n            TEMP = TEMP/np.sum(np.abs(TEMP))\n        elif Init == 'Random':\n            bound = np.sqrt(3/(K+1))\n            TEMP = np.random.uniform(-bound, bound, K+1)\n            TEMP = TEMP/np.sum(np.abs(TEMP))\n        elif Init == 'WS':\n            TEMP = Gamma\n        self.temp = Parameter(torch.tensor(TEMP))\n    def reset_parameters(self):\n        torch.nn.init.zeros_(self.temp)\n        for k in range(self.K+1):\n            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n        self.temp.data[-1] = (1-self.alpha)**self.K\n    def forward(self, x, edge_index, edge_weight=None):\n        edge_index, norm = gcn_norm(\n            edge_index, edge_weight, num_nodes=x.size(0), dtype=x.dtype)\n        hidden = x*(self.temp[0])\n        for k in range(self.K):\n            x = self.propagate(edge_index, x=x, norm=norm)\n            gamma = self.temp[k+1]\n            hidden = hidden + gamma*x\n        return hidden\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def __repr__(self):\n        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n                                           self.temp)\nclass NodeInformationScore(MessagePassing):\n    def __init__(self, improved=False, cached=False, **kwargs):\n        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n        self.improved = improved\n        self.cached = cached\n        self.cached_result = None\n        self.cached_num_edges = None\n    @staticmethod\n    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n        if edge_weight is None:\n            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) \n        edge_index = edge_index.type(torch.long)\n        row, col = edge_index\n        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n        deg_inv_sqrt = deg.pow(-0.5)\n        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n    def forward(self, x, edge_index, edge_weight):\n        if self.cached and self.cached_result is not None:\n            if edge_index.size(1) != self.cached_num_edges:\n                raise RuntimeError(\n                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n        if not self.cached or self.cached_result is None:\n            self.cached_num_edges = edge_index.size(1)\n            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n            self.cached_result = edge_index, norm\n        edge_index, norm = self.cached_result\n        return self.propagate(edge_index, x=x, norm=norm)\n    def message(self, x_j, norm):\n        return norm.view(-1, 1) * x_j\n    def update(self, aggr_out):\n        return aggr_out\nclass graph_attention(torch.nn.Module):\n    src_nodes_dim = 0  \n    trg_nodes_dim = 1  \n    nodes_dim = 0      \n    head_dim = 1       \n    def __init__(self, num_in_features, num_out_features, num_of_heads, dropout_prob=0.6, log_attention_weights=False):\n        super().__init__()\n        self.num_of_heads = num_of_heads\n        self.num_out_features = num_out_features\n        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n        self.init_params()\n    def init_params(self):\n        \"\"\"\n        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n        Feel free to experiment - there may be better initializations depending on your problem.\n        \"\"\"\n        nn.init.xavier_uniform_(self.linear_proj.weight)\n        nn.init.xavier_uniform_(self.scoring_fn_target)\n        nn.init.xavier_uniform_(self.scoring_fn_source)\n    def forward(self, x, edge_index):\n        in_nodes_features = x  \n        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n        scores_per_edge = scores_source_lifted + scores_target_lifted\n        return torch.sigmoid(scores_per_edge)\n    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n        \"\"\"\n        Lifts i.e. duplicates certain vectors depending on the edge index.\n        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n        \"\"\"\n        src_nodes_index = edge_index[self.src_nodes_dim]\n        trg_nodes_index = edge_index[self.trg_nodes_dim]\n        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n        return scores_source, scores_target, nodes_features_matrix_proj_lifted\nclass CoPooling(torch.nn.Module):\n    def __init__(self, ratio=0.5, K=0.05, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=None):\n        super(CoPooling, self).__init__()\n        self.ratio = ratio\n        self.calc_information_score = NodeInformationScore()\n        self.edge_ratio = edge_ratio\n        self.prop1 = GPR_prop(K, alpha, Init, Gamma)\n        score_dim = 32\n        self.G_att = graph_attention(num_in_features=nhid, num_out_features=score_dim, num_of_heads=1)\n        self.weight = Parameter(torch.Tensor(2*nhid, nhid))\n        nn.init.xavier_uniform_(self.weight.data)\n        self.bias = Parameter(torch.Tensor(nhid))\n        nn.init.zeros_(self.bias.data)\n        self.reset_parameters()\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.weight.data)\n        nn.init.zeros_(self.bias.data)\n        self.prop1.reset_parameters()\n        self.G_att.init_params()\n    def forward(self, x, edge_index, edge_attr, batch=None, nodes_index=None, node_attr=None):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        ori_batch = batch.clone()\n        device = x.device\n        num_nodes = x.shape[0]\n        x_cut = self.prop1(x, edge_index) \n        attention = self.G_att(x_cut, edge_index) \n        attention = attention.sum(dim=1) \n        edge_index, attention = add_self_loops(edge_index, attention, 1.0, num_nodes) \n        edge_index_t, attention_t = transpose(edge_index, attention, num_nodes, num_nodes)\n        edge_tmp = torch.cat((edge_index, edge_index_t), 1)\n        att_tmp = torch.cat((attention, attention_t),0)\n        edge_index, attention = coalesce(edge_tmp, att_tmp, num_nodes, num_nodes, 'mean')\n        attention_np = attention.cpu().data.numpy()\n        cut_val = np.percentile(attention_np, int(100*(1-self.edge_ratio))) \n        attention = attention * (attention >= cut_val) \n        kep_idx = attention > 0.0\n        cut_edge_index, cut_edge_attr = edge_index[:, kep_idx], attention[kep_idx]\n        x_information_score = self.calc_information_score(x, cut_edge_index, cut_edge_attr)\n        score = torch.sum(torch.abs(x_information_score), dim=1)\n        perm = topk(score, self.ratio, batch)\n        x_topk = x[perm]\n        batch = batch[perm]\n        if nodes_index is not None:\n            nodes_index = nodes_index[perm]\n        if node_attr is not None:\n            node_attr = node_attr[perm]\n        if cut_edge_index is not None or cut_edge_index.nelement() != 0:\n            induced_edge_index, induced_edge_attr = filter_adj(cut_edge_index, cut_edge_attr, perm, num_nodes=num_nodes)\n        else:\n            print('All edges are cut!')\n            induced_edge_index, induced_edge_attr = cut_edge_index, cut_edge_attr\n        attention_dense = (to_dense_adj(cut_edge_index, edge_attr=cut_edge_attr, max_num_nodes=num_nodes)).squeeze()\n        x = F.relu(torch.matmul(torch.cat((x_topk, torch.matmul(attention_dense[perm],x)), 1), self.weight) + self.bias)\n        return x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "ename": "OutOfMemoryError",
                    "evalue": "CUDA out of memory. Tried to allocate 106.83 GiB. GPU 0 has a total capacty of 79.21 GiB of which 62.00 GiB is free. Process 686389 has 2.02 GiB memory in use. Process 273867 has 10.64 GiB memory in use. Including non-PyTorch memory, this process has 4.54 GiB memory in use. Of the allocated memory 2.86 GiB is allocated by PyTorch, and 333.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[8], line 144\u001b[0m\n\u001b[1;32m    141\u001b[0m train_idx \u001b[38;5;241m=\u001b[39m split_idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m501\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     train_acc, valid_acc, test_acc \u001b[38;5;241m=\u001b[39m test(model, graph, split_idx, evaluator)\n\u001b[1;32m    146\u001b[0m     val_accuracies\u001b[38;5;241m.\u001b[39mappend(valid_acc)\n",
                        "Cell \u001b[0;32mIn[8], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, train_idx, optimizer)\u001b[0m\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     79\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 80\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m[train_idx]\n\u001b[1;32m     81\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out, data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)[train_idx])\n\u001b[1;32m     82\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "Cell \u001b[0;32mIn[8], line 53\u001b[0m, in \u001b[0;36mHierarchicalGCN_CO.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     50\u001b[0m edge_indices \u001b[38;5;241m=\u001b[39m [edge_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 53\u001b[0m     x, edge_index, perm, _, batch, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpools\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_convs[i](x, edge_index)\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "Cell \u001b[0;32mIn[7], line 396\u001b[0m, in \u001b[0;36mCoPooling.forward\u001b[0;34m(self, x, edge_index, edge_attr, batch, nodes_index, node_attr)\u001b[0m\n\u001b[1;32m    393\u001b[0m     induced_edge_index, induced_edge_attr \u001b[38;5;241m=\u001b[39m cut_edge_index, cut_edge_attr\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# update node features\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m attention_dense \u001b[38;5;241m=\u001b[39m (\u001b[43mto_dense_adj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcut_edge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcut_edge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    397\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(torch\u001b[38;5;241m.\u001b[39mmatmul(torch\u001b[38;5;241m.\u001b[39mcat((x_topk, torch\u001b[38;5;241m.\u001b[39mmatmul(attention_dense[perm],x)), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX/lib/python3.10/site-packages/torch_geometric/utils/_to_dense_adj.py:97\u001b[0m, in \u001b[0;36mto_dense_adj\u001b[0;34m(edge_index, batch, edge_attr, max_num_nodes, batch_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m flattened_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m max_num_nodes \u001b[38;5;241m*\u001b[39m max_num_nodes\n\u001b[1;32m     96\u001b[0m idx \u001b[38;5;241m=\u001b[39m idx0 \u001b[38;5;241m*\u001b[39m max_num_nodes \u001b[38;5;241m*\u001b[39m max_num_nodes \u001b[38;5;241m+\u001b[39m idx1 \u001b[38;5;241m*\u001b[39m max_num_nodes \u001b[38;5;241m+\u001b[39m idx2\n\u001b[0;32m---> 97\u001b[0m adj \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflattened_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m adj \u001b[38;5;241m=\u001b[39m adj\u001b[38;5;241m.\u001b[39mview(size)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adj\n",
                        "File \u001b[0;32m~/anaconda3/envs/XXX/lib/python3.10/site-packages/torch_geometric/utils/_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
                        "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 106.83 GiB. GPU 0 has a total capacty of 79.21 GiB of which 62.00 GiB is free. Process 686389 has 2.02 GiB memory in use. Process 273867 has 10.64 GiB memory in use. Including non-PyTorch memory, this process has 4.54 GiB memory in use. Of the allocated memory 2.86 GiB is allocated by PyTorch, and 333.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndataset = dataset_sparse\ngraph = dataset[0]\nnum_classes = dataset.num_classes\nin_channels = dataset.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.5, 0.5]\nclass HierarchicalGCN_CO(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_CO, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.sum_res = sum_res\n        self.act = act\n        channels = self.hidden_channels\n        self.pools = torch.nn.ModuleList()\n        self.down_convs = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(CoPooling(ratio=0.7, K=2, edge_ratio=0.6, nhid=256, alpha=0.1, Init='Random', Gamma=1.0))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.0, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = F.relu(x)  \n        xs = [x]\n        edge_indices = [edge_index]\n        for i in range(1, self.depth + 1):\n            x, edge_index, perm, _, batch, _, _, _ = self.pools[i - 1](x, edge_index, edge_attr=None, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = F.relu(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            up = res\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = F.relu(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x.log_softmax(dim=-1)\ndef train(model, data, train_idx, optimizer):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index)[train_idx]\n    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n@torch.no_grad()\ndef test(model, data, split_idx, evaluator):\n    model.eval()\n    out = model(data.x, data.edge_index)\n    y_pred = out.argmax(dim=-1, keepdim=True)\n    train_acc = evaluator.eval({\n        'y_true': data.y[split_idx['train']],\n        'y_pred': y_pred[split_idx['train']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': data.y[split_idx['valid']],\n        'y_pred': y_pred[split_idx['valid']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': data.y[split_idx['test']],\n        'y_pred': y_pred[split_idx['test']],\n    })['acc']\n    return train_acc, valid_acc, test_acc\nseeds = [42, 123, 456]\nresults = []\nval_accuracies_list = []\ntimes = []\nmemories = []\ngpu_memories = []\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfor seed in seeds:\n    graph = graph.to(device)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    val_accuracies = []\n    test_accuracies = []\n    start_time = time.time()\n    tracemalloc.start()\n    model = HierarchicalGCN_CO(\n        in_channels=graph.num_features,\n        hidden_channels=256,\n        out_channels=dataset.num_classes,\n        depth=2,\n        act=F.relu,\n        sum_res=False\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx['train'].to(device)\n    for epoch in range(1, 501):\n        loss = train(model, graph, split_idx['train'], optimizer)\n        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n        val_accuracies.append(valid_acc)\n        test_accuracies.append(test_acc)\n        if epoch % 10 == 0:\n            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    elapsed_time = end_time - start_time\n    memory_usage = current / 10**6  \n    peak_memory_usage = peak / 10**6  \n    if torch.cuda.is_available():\n        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  \n        torch.cuda.reset_max_memory_allocated(device)\n    else:\n        gpu_memory_usage = 0\n    val_accuracies_list.append(val_accuracies)\n    times.append(elapsed_time)\n    memories.append(memory_usage)\n    gpu_memories.append(gpu_memory_usage)\n    best_val_acc = max(val_accuracies)\n    best_val_index = val_accuracies.index(best_val_acc)\n    corresponding_test_acc = test_accuracies[best_val_index]\n    results.append({\n        'seed': seed,\n        'best_val_acc': best_val_acc,\n        'corresponding_test_acc': corresponding_test_acc,\n        'elapsed_time': elapsed_time,\n        'memory_usage': memory_usage,\n        'gpu_memory_usage': gpu_memory_usage\n    })\nfor result in results:\n    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\navg_val_acc = np.mean([result['best_val_acc'] for result in results])\navg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\navg_time = np.mean(times)\navg_memory = np.mean(memories)\navg_gpu_memory = np.mean(gpu_memories)\nprint(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\nprint(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\nprint(f\"Average Time: {avg_time:.2f}s\")\nprint(f\"Average Memory Usage: {avg_memory:.2f}MB\")\nprint(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### CGIPooling"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nimport sys\nimport torch\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom ogb.nodeproppred import PygNodePropPredDataset, Evaluator\nfrom torch_geometric.loader import DataLoader\nimport os\nimport random\nimport pandas as pd\nimport torch\nimport torch_geometric.transforms as T\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data\nfrom torch_geometric.data.datapipes import functional_transform\nfrom torch_geometric.transforms import BaseTransform\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import Actor\nfrom torch_geometric.datasets import GNNBenchmarkDataset\nfrom torch_geometric.datasets import TUDataset\nfrom sklearn.metrics import r2_score\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import MoleculeNet\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.utils import to_networkx\nfrom torch.nn import Linear\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nfrom torch_geometric.datasets import GitHub\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nimport time\nimport tracemalloc\nimport torch\nfrom torch_geometric.datasets import Planetoid\nimport torch_geometric.transforms as T\nfrom torch_geometric.nn import GCNConv\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, TopKPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport random\nfrom typing import Callable, Optional, Union\nimport time\nimport psutil\nimport torch\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nmax_nodes = 150\ndataset_sparse = PygNodePropPredDataset(root='/data/XXX/Pooling', name=\"ogbn-arxiv\")\nevaluator = Evaluator(\"ogbn-arxiv\")\neval_metric = dataset_sparse.eval_metric\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom typing import Callable, Optional, Union\nimport torch\nfrom torch import Tensor\n@dataclass(init=False)\nclass SelectOutput:\n    r\"\"\"The output of the :class:`Select` method, which holds an assignment\n    from selected nodes to their respective cluster(s).\n    Args:\n        node_index (torch.Tensor): The indices of the selected nodes.\n        num_nodes (int): The number of nodes.\n        cluster_index (torch.Tensor): The indices of the clusters each node in\n            :obj:`node_index` is assigned to.\n        num_clusters (int): The number of clusters.\n        weight (torch.Tensor, optional): A weight vector, denoting the strength\n            of the assignment of a node to its cluster. (default: :obj:`None`)\n    \"\"\"\n    node_index: Tensor\n    num_nodes: int\n    cluster_index: Tensor\n    num_clusters: int\n    weight: Optional[Tensor] = None\n    def __init__(\n        self,\n        node_index: Tensor,\n        num_nodes: int,\n        cluster_index: Tensor,\n        num_clusters: int,\n        weight: Optional[Tensor] = None,\n    ):\n        if node_index.dim() != 1:\n            raise ValueError(f\"Expected 'node_index' to be one-dimensional \"\n                             f\"(got {node_index.dim()} dimensions)\")\n        if cluster_index.dim() != 1:\n            raise ValueError(f\"Expected 'cluster_index' to be one-dimensional \"\n                             f\"(got {cluster_index.dim()} dimensions)\")\n        if node_index.numel() != cluster_index.numel():\n            raise ValueError(f\"Expected 'node_index' and 'cluster_index' to \"\n                             f\"hold the same number of values (got \"\n                             f\"{node_index.numel()} and \"\n                             f\"{cluster_index.numel()} values)\")\n        if weight is not None and weight.dim() != 1:\n            raise ValueError(f\"Expected 'weight' vector to be one-dimensional \"\n                             f\"(got {weight.dim()} dimensions)\")\n        if weight is not None and weight.numel() != node_index.numel():\n            raise ValueError(f\"Expected 'weight' to hold {node_index.numel()} \"\n                             f\"values (got {weight.numel()} values)\")\n        self.node_index = node_index\n        self.num_nodes = num_nodes\n        self.cluster_index = cluster_index\n        self.num_clusters = num_clusters\n        self.weight = weight\nclass Select(torch.nn.Module):\n    r\"\"\"An abstract base class for implementing custom node selections as\n    described in the `\"Understanding Pooling in Graph Neural Networks\"\n    <https://arxiv.org/abs/1905.05178>`_ paper, which maps the nodes of an\n    input graph to supernodes in the coarsened graph.\n    Specifically, :class:`Select` returns a :class:`SelectOutput` output, which\n    holds a (sparse) mapping :math:`\\mathbf{C} \\in {[0, 1]}^{N \\times C}` that\n    assigns selected nodes to one or more of :math:`C` super nodes.\n    \"\"\"\n    def reset_parameters(self):\n        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n        pass\n    def forward(self, *args, **kwargs) -> SelectOutput:\n        raise NotImplementedError\n    def __repr__(self) -> str:\n        return f'{self.__class__.__name__}()'\ndef cumsum(x: Tensor, dim: int = 0) -> Tensor:\n    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n    Args:\n        x (torch.Tensor): The input tensor.\n        dim (int, optional): The dimension to do the operation over.\n            (default: :obj:`0`)\n    Example:\n        >>> x = torch.tensor([2, 4, 1])\n        >>> cumsum(x)\n        tensor([0, 2, 6, 7])\n    \"\"\"\n    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n    out = x.new_empty(size)\n    out.narrow(dim, 0, 1).zero_()\n    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n    return out\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\ndef topk(\n    x: Tensor,\n    ratio: Optional[Union[float, int]],\n    batch: Tensor,\n    min_score: Optional[float] = None,\n    tol: float = 1e-7,\n) -> Tensor:\n    if min_score is not None:\n        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero().view(-1)\n        return perm\n    if ratio is not None:\n        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n        if ratio >= 1:\n            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n        else:\n            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n        x, x_perm = torch.sort(x.view(-1), descending=True)\n        batch = batch[x_perm]\n        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n        ptr = cumsum(num_nodes)\n        batched_arange = arange - ptr[batch]\n        mask = batched_arange < k[batch]\n        return x_perm[batch_perm[mask]]\n    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n                     \"must be specified\")\nclass Discriminator(torch.nn.Module):\n    def __init__(self, in_channels):\n        super(Discriminator, self).__init__()\n        self.fc1 = nn.Linear(in_channels * 2, in_channels)\n        self.fc2 = nn.Linear(in_channels, 1)\n    def forward(self, x):\n        x = F.leaky_relu(self.fc1(x), 0.2)\n        x = F.sigmoid(self.fc2(x))\n        return x\nclass CGIPool(torch.nn.Module):\n    def __init__(self, in_channels, ratio=0.5, non_lin=torch.tanh):\n        super(CGIPool, self).__init__()\n        self.in_channels = in_channels\n        self.ratio = ratio\n        self.non_lin = non_lin\n        self.hidden_dim = in_channels\n        self.transform = GraphConv(in_channels, self.hidden_dim)\n        self.pp_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n        self.np_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n        self.positive_pooling = GraphConv(self.hidden_dim, 1)\n        self.negative_pooling = GraphConv(self.hidden_dim, 1)\n        self.discriminator = Discriminator(self.hidden_dim)\n        self.loss_fn = torch.nn.BCELoss()\n    def forward(self, x, edge_index, edge_attr=None, batch=None):\n        device = x.device  \n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x_transform = F.leaky_relu(self.transform(x, edge_index), 0.2)\n        x_tp = F.leaky_relu(self.pp_conv(x, edge_index), 0.2)\n        x_tn = F.leaky_relu(self.np_conv(x, edge_index), 0.2)\n        s_pp = self.positive_pooling(x_tp, edge_index).squeeze()\n        s_np = self.negative_pooling(x_tn, edge_index).squeeze()\n        perm_positive = topk(s_pp, 1, batch)\n        perm_negative = topk(s_np, 1, batch)\n        x_pp = x_transform[perm_positive] * self.non_lin(s_pp[perm_positive]).view(-1, 1)\n        x_np = x_transform[perm_negative] * self.non_lin(s_np[perm_negative]).view(-1, 1)\n        x_pp_readout = gap(x_pp, batch[perm_positive])\n        x_np_readout = gap(x_np, batch[perm_negative])\n        x_readout = gap(x_transform, batch)\n        positive_pair = torch.cat([x_pp_readout, x_readout], dim=1)\n        negative_pair = torch.cat([x_np_readout, x_readout], dim=1)\n        real = torch.ones(positive_pair.shape[0], device=device)  \n        fake = torch.zeros(negative_pair.shape[0], device=device)  \n        score = (s_pp - s_np)\n        perm = topk(score, self.ratio, batch)\n        x = x_transform[perm] * self.non_lin(score[perm]).view(-1, 1)\n        batch = batch[perm]\n        filter_edge_index, filter_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n        return x, filter_edge_index, filter_edge_attr, batch, perm"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 042, Epoch: 010, Loss: 2.8205, Train Acc: 0.2963, Val Acc: 0.3084, Test Acc: 0.2757\n",
                        "Seed: 042, Epoch: 020, Loss: 2.3297, Train Acc: 0.4035, Val Acc: 0.4122, Test Acc: 0.3680\n",
                        "Seed: 042, Epoch: 030, Loss: 2.0551, Train Acc: 0.4566, Val Acc: 0.4540, Test Acc: 0.4031\n",
                        "Seed: 042, Epoch: 040, Loss: 1.8393, Train Acc: 0.5041, Val Acc: 0.4865, Test Acc: 0.4186\n",
                        "Seed: 042, Epoch: 050, Loss: 1.6957, Train Acc: 0.5352, Val Acc: 0.5162, Test Acc: 0.4551\n",
                        "Seed: 042, Epoch: 060, Loss: 1.6249, Train Acc: 0.5521, Val Acc: 0.5399, Test Acc: 0.4818\n",
                        "Seed: 042, Epoch: 070, Loss: 1.5724, Train Acc: 0.5626, Val Acc: 0.5419, Test Acc: 0.4788\n",
                        "Seed: 042, Epoch: 080, Loss: 1.5360, Train Acc: 0.5711, Val Acc: 0.5487, Test Acc: 0.4865\n",
                        "Seed: 042, Epoch: 090, Loss: 1.5195, Train Acc: 0.5749, Val Acc: 0.5599, Test Acc: 0.4941\n",
                        "Seed: 042, Epoch: 100, Loss: 1.4943, Train Acc: 0.5811, Val Acc: 0.5610, Test Acc: 0.4968\n",
                        "Seed: 042, Epoch: 110, Loss: 1.4771, Train Acc: 0.5832, Val Acc: 0.5623, Test Acc: 0.5025\n",
                        "Seed: 042, Epoch: 120, Loss: 1.4655, Train Acc: 0.5874, Val Acc: 0.5685, Test Acc: 0.5115\n",
                        "Seed: 042, Epoch: 130, Loss: 1.4554, Train Acc: 0.5899, Val Acc: 0.5628, Test Acc: 0.4942\n",
                        "Seed: 042, Epoch: 140, Loss: 1.4415, Train Acc: 0.5932, Val Acc: 0.5702, Test Acc: 0.5112\n",
                        "Seed: 042, Epoch: 150, Loss: 1.4311, Train Acc: 0.5948, Val Acc: 0.5675, Test Acc: 0.4999\n",
                        "Seed: 042, Epoch: 160, Loss: 1.4220, Train Acc: 0.5975, Val Acc: 0.5717, Test Acc: 0.5074\n",
                        "Seed: 042, Epoch: 170, Loss: 1.4192, Train Acc: 0.5977, Val Acc: 0.5752, Test Acc: 0.5106\n",
                        "Seed: 042, Epoch: 180, Loss: 1.4095, Train Acc: 0.5975, Val Acc: 0.5691, Test Acc: 0.5060\n",
                        "Seed: 042, Epoch: 190, Loss: 1.4033, Train Acc: 0.6017, Val Acc: 0.5769, Test Acc: 0.5110\n",
                        "Seed: 042, Epoch: 200, Loss: 1.3973, Train Acc: 0.6026, Val Acc: 0.5733, Test Acc: 0.5108\n",
                        "Seed: 042, Epoch: 210, Loss: 1.3893, Train Acc: 0.6051, Val Acc: 0.5790, Test Acc: 0.5155\n",
                        "Seed: 042, Epoch: 220, Loss: 1.3953, Train Acc: 0.6028, Val Acc: 0.5728, Test Acc: 0.5110\n",
                        "Seed: 042, Epoch: 230, Loss: 1.3829, Train Acc: 0.6069, Val Acc: 0.5821, Test Acc: 0.5212\n",
                        "Seed: 042, Epoch: 240, Loss: 1.3768, Train Acc: 0.6073, Val Acc: 0.5815, Test Acc: 0.5205\n",
                        "Seed: 042, Epoch: 250, Loss: 1.3711, Train Acc: 0.6093, Val Acc: 0.5784, Test Acc: 0.5152\n",
                        "Seed: 042, Epoch: 260, Loss: 1.3792, Train Acc: 0.6052, Val Acc: 0.5768, Test Acc: 0.5169\n",
                        "Seed: 042, Epoch: 270, Loss: 1.3644, Train Acc: 0.6104, Val Acc: 0.5796, Test Acc: 0.5154\n",
                        "Seed: 042, Epoch: 280, Loss: 1.3653, Train Acc: 0.6085, Val Acc: 0.5724, Test Acc: 0.5060\n",
                        "Seed: 042, Epoch: 290, Loss: 1.3554, Train Acc: 0.6118, Val Acc: 0.5792, Test Acc: 0.5130\n",
                        "Seed: 042, Epoch: 300, Loss: 1.3540, Train Acc: 0.6122, Val Acc: 0.5835, Test Acc: 0.5207\n",
                        "Seed: 042, Epoch: 310, Loss: 1.3667, Train Acc: 0.6095, Val Acc: 0.5769, Test Acc: 0.5178\n",
                        "Seed: 042, Epoch: 320, Loss: 1.3502, Train Acc: 0.6137, Val Acc: 0.5879, Test Acc: 0.5271\n",
                        "Seed: 042, Epoch: 330, Loss: 1.3423, Train Acc: 0.6137, Val Acc: 0.5793, Test Acc: 0.5147\n",
                        "Seed: 042, Epoch: 340, Loss: 1.3554, Train Acc: 0.6111, Val Acc: 0.5814, Test Acc: 0.5199\n",
                        "Seed: 042, Epoch: 350, Loss: 1.3391, Train Acc: 0.6143, Val Acc: 0.5823, Test Acc: 0.5187\n",
                        "Seed: 042, Epoch: 360, Loss: 1.3410, Train Acc: 0.6167, Val Acc: 0.5861, Test Acc: 0.5229\n",
                        "Seed: 042, Epoch: 370, Loss: 1.3338, Train Acc: 0.6175, Val Acc: 0.5881, Test Acc: 0.5297\n",
                        "Seed: 042, Epoch: 380, Loss: 1.3267, Train Acc: 0.6192, Val Acc: 0.5857, Test Acc: 0.5221\n",
                        "Seed: 042, Epoch: 390, Loss: 1.3276, Train Acc: 0.6186, Val Acc: 0.5893, Test Acc: 0.5313\n",
                        "Seed: 042, Epoch: 400, Loss: 1.3444, Train Acc: 0.6151, Val Acc: 0.5825, Test Acc: 0.5163\n",
                        "Seed: 042, Epoch: 410, Loss: 1.3211, Train Acc: 0.6202, Val Acc: 0.5872, Test Acc: 0.5243\n",
                        "Seed: 042, Epoch: 420, Loss: 1.3172, Train Acc: 0.6207, Val Acc: 0.5852, Test Acc: 0.5220\n",
                        "Seed: 042, Epoch: 430, Loss: 1.3353, Train Acc: 0.6187, Val Acc: 0.5889, Test Acc: 0.5343\n",
                        "Seed: 042, Epoch: 440, Loss: 1.3134, Train Acc: 0.6217, Val Acc: 0.5891, Test Acc: 0.5263\n",
                        "Seed: 042, Epoch: 450, Loss: 1.3114, Train Acc: 0.6225, Val Acc: 0.5921, Test Acc: 0.5327\n",
                        "Seed: 042, Epoch: 460, Loss: 1.3092, Train Acc: 0.6225, Val Acc: 0.5832, Test Acc: 0.5197\n",
                        "Seed: 042, Epoch: 470, Loss: 1.3057, Train Acc: 0.6234, Val Acc: 0.5839, Test Acc: 0.5215\n",
                        "Seed: 042, Epoch: 480, Loss: 1.3044, Train Acc: 0.6244, Val Acc: 0.5918, Test Acc: 0.5293\n",
                        "Seed: 042, Epoch: 490, Loss: 1.3137, Train Acc: 0.6222, Val Acc: 0.5873, Test Acc: 0.5230\n",
                        "Seed: 042, Epoch: 500, Loss: 1.3031, Train Acc: 0.6248, Val Acc: 0.5876, Test Acc: 0.5246\n",
                        "Seed: 123, Epoch: 010, Loss: 2.9168, Train Acc: 0.3030, Val Acc: 0.3430, Test Acc: 0.3112\n",
                        "Seed: 123, Epoch: 020, Loss: 2.3309, Train Acc: 0.4099, Val Acc: 0.4387, Test Acc: 0.3883\n",
                        "Seed: 123, Epoch: 030, Loss: 1.9882, Train Acc: 0.4675, Val Acc: 0.4706, Test Acc: 0.4273\n",
                        "Seed: 123, Epoch: 040, Loss: 1.7906, Train Acc: 0.5208, Val Acc: 0.5099, Test Acc: 0.4518\n",
                        "Seed: 123, Epoch: 050, Loss: 1.6828, Train Acc: 0.5382, Val Acc: 0.5238, Test Acc: 0.4589\n",
                        "Seed: 123, Epoch: 060, Loss: 1.6110, Train Acc: 0.5522, Val Acc: 0.5318, Test Acc: 0.4648\n",
                        "Seed: 123, Epoch: 070, Loss: 1.5696, Train Acc: 0.5648, Val Acc: 0.5455, Test Acc: 0.4748\n",
                        "Seed: 123, Epoch: 080, Loss: 1.5321, Train Acc: 0.5719, Val Acc: 0.5512, Test Acc: 0.4846\n",
                        "Seed: 123, Epoch: 090, Loss: 1.5055, Train Acc: 0.5781, Val Acc: 0.5561, Test Acc: 0.4896\n",
                        "Seed: 123, Epoch: 100, Loss: 1.5145, Train Acc: 0.5753, Val Acc: 0.5462, Test Acc: 0.4796\n",
                        "Seed: 123, Epoch: 110, Loss: 1.4787, Train Acc: 0.5865, Val Acc: 0.5618, Test Acc: 0.4983\n",
                        "Seed: 123, Epoch: 120, Loss: 1.4565, Train Acc: 0.5913, Val Acc: 0.5699, Test Acc: 0.5052\n",
                        "Seed: 123, Epoch: 130, Loss: 1.4391, Train Acc: 0.5936, Val Acc: 0.5694, Test Acc: 0.5060\n",
                        "Seed: 123, Epoch: 140, Loss: 1.4277, Train Acc: 0.5969, Val Acc: 0.5704, Test Acc: 0.5057\n",
                        "Seed: 123, Epoch: 150, Loss: 1.4201, Train Acc: 0.5965, Val Acc: 0.5670, Test Acc: 0.5021\n",
                        "Seed: 123, Epoch: 160, Loss: 1.4164, Train Acc: 0.5987, Val Acc: 0.5698, Test Acc: 0.5058\n",
                        "Seed: 123, Epoch: 170, Loss: 1.4070, Train Acc: 0.6009, Val Acc: 0.5750, Test Acc: 0.5151\n",
                        "Seed: 123, Epoch: 180, Loss: 1.4001, Train Acc: 0.6034, Val Acc: 0.5738, Test Acc: 0.5095\n",
                        "Seed: 123, Epoch: 190, Loss: 1.3936, Train Acc: 0.6040, Val Acc: 0.5751, Test Acc: 0.5140\n",
                        "Seed: 123, Epoch: 200, Loss: 1.3868, Train Acc: 0.6048, Val Acc: 0.5762, Test Acc: 0.5107\n",
                        "Seed: 123, Epoch: 210, Loss: 1.3797, Train Acc: 0.6072, Val Acc: 0.5802, Test Acc: 0.5192\n",
                        "Seed: 123, Epoch: 220, Loss: 1.3834, Train Acc: 0.6084, Val Acc: 0.5774, Test Acc: 0.5130\n",
                        "Seed: 123, Epoch: 230, Loss: 1.3734, Train Acc: 0.6091, Val Acc: 0.5774, Test Acc: 0.5126\n",
                        "Seed: 123, Epoch: 240, Loss: 1.3649, Train Acc: 0.6105, Val Acc: 0.5792, Test Acc: 0.5157\n",
                        "Seed: 123, Epoch: 250, Loss: 1.3621, Train Acc: 0.6106, Val Acc: 0.5812, Test Acc: 0.5215\n",
                        "Seed: 123, Epoch: 260, Loss: 1.3708, Train Acc: 0.6102, Val Acc: 0.5810, Test Acc: 0.5175\n",
                        "Seed: 123, Epoch: 270, Loss: 1.3565, Train Acc: 0.6122, Val Acc: 0.5816, Test Acc: 0.5168\n",
                        "Seed: 123, Epoch: 280, Loss: 1.3526, Train Acc: 0.6127, Val Acc: 0.5836, Test Acc: 0.5213\n",
                        "Seed: 123, Epoch: 290, Loss: 1.3471, Train Acc: 0.6137, Val Acc: 0.5852, Test Acc: 0.5219\n",
                        "Seed: 123, Epoch: 300, Loss: 1.3414, Train Acc: 0.6165, Val Acc: 0.5879, Test Acc: 0.5270\n",
                        "Seed: 123, Epoch: 310, Loss: 1.3413, Train Acc: 0.6160, Val Acc: 0.5829, Test Acc: 0.5170\n",
                        "Seed: 123, Epoch: 320, Loss: 1.3418, Train Acc: 0.6145, Val Acc: 0.5830, Test Acc: 0.5156\n",
                        "Seed: 123, Epoch: 330, Loss: 1.3345, Train Acc: 0.6176, Val Acc: 0.5869, Test Acc: 0.5233\n",
                        "Seed: 123, Epoch: 340, Loss: 1.3287, Train Acc: 0.6188, Val Acc: 0.5868, Test Acc: 0.5256\n",
                        "Seed: 123, Epoch: 350, Loss: 1.3651, Train Acc: 0.6167, Val Acc: 0.5836, Test Acc: 0.5268\n",
                        "Seed: 123, Epoch: 360, Loss: 1.3515, Train Acc: 0.6005, Val Acc: 0.5512, Test Acc: 0.4888\n",
                        "Seed: 123, Epoch: 370, Loss: 1.3482, Train Acc: 0.6105, Val Acc: 0.5900, Test Acc: 0.5453\n",
                        "Seed: 123, Epoch: 380, Loss: 1.3302, Train Acc: 0.6174, Val Acc: 0.5899, Test Acc: 0.5342\n",
                        "Seed: 123, Epoch: 390, Loss: 1.3203, Train Acc: 0.6197, Val Acc: 0.5911, Test Acc: 0.5293\n",
                        "Seed: 123, Epoch: 400, Loss: 1.3171, Train Acc: 0.6206, Val Acc: 0.5878, Test Acc: 0.5230\n",
                        "Seed: 123, Epoch: 410, Loss: 1.3197, Train Acc: 0.6210, Val Acc: 0.5844, Test Acc: 0.5211\n",
                        "Seed: 123, Epoch: 420, Loss: 1.3109, Train Acc: 0.6231, Val Acc: 0.5883, Test Acc: 0.5252\n",
                        "Seed: 123, Epoch: 430, Loss: 1.3069, Train Acc: 0.6231, Val Acc: 0.5890, Test Acc: 0.5278\n",
                        "Seed: 123, Epoch: 440, Loss: 1.3160, Train Acc: 0.6190, Val Acc: 0.5871, Test Acc: 0.5259\n",
                        "Seed: 123, Epoch: 450, Loss: 1.3050, Train Acc: 0.6228, Val Acc: 0.5867, Test Acc: 0.5239\n",
                        "Seed: 123, Epoch: 460, Loss: 1.3031, Train Acc: 0.6234, Val Acc: 0.5902, Test Acc: 0.5308\n",
                        "Seed: 123, Epoch: 470, Loss: 1.3044, Train Acc: 0.6249, Val Acc: 0.5898, Test Acc: 0.5298\n",
                        "Seed: 123, Epoch: 480, Loss: 1.2993, Train Acc: 0.6248, Val Acc: 0.5905, Test Acc: 0.5311\n",
                        "Seed: 123, Epoch: 490, Loss: 1.2950, Train Acc: 0.6250, Val Acc: 0.5888, Test Acc: 0.5279\n",
                        "Seed: 123, Epoch: 500, Loss: 1.3294, Train Acc: 0.6179, Val Acc: 0.5756, Test Acc: 0.5238\n",
                        "Seed: 456, Epoch: 010, Loss: 2.8576, Train Acc: 0.2941, Val Acc: 0.3253, Test Acc: 0.3023\n",
                        "Seed: 456, Epoch: 020, Loss: 2.2614, Train Acc: 0.4116, Val Acc: 0.4415, Test Acc: 0.3923\n",
                        "Seed: 456, Epoch: 030, Loss: 2.0118, Train Acc: 0.4859, Val Acc: 0.4891, Test Acc: 0.4362\n",
                        "Seed: 456, Epoch: 040, Loss: 1.7594, Train Acc: 0.5187, Val Acc: 0.5088, Test Acc: 0.4534\n",
                        "Seed: 456, Epoch: 050, Loss: 1.6582, Train Acc: 0.5415, Val Acc: 0.5191, Test Acc: 0.4545\n",
                        "Seed: 456, Epoch: 060, Loss: 1.5920, Train Acc: 0.5594, Val Acc: 0.5415, Test Acc: 0.4790\n",
                        "Seed: 456, Epoch: 070, Loss: 1.5579, Train Acc: 0.5660, Val Acc: 0.5467, Test Acc: 0.4791\n",
                        "Seed: 456, Epoch: 080, Loss: 1.5294, Train Acc: 0.5728, Val Acc: 0.5581, Test Acc: 0.4976\n",
                        "Seed: 456, Epoch: 090, Loss: 1.5020, Train Acc: 0.5797, Val Acc: 0.5557, Test Acc: 0.4879\n",
                        "Seed: 456, Epoch: 100, Loss: 1.4789, Train Acc: 0.5856, Val Acc: 0.5601, Test Acc: 0.4942\n",
                        "Seed: 456, Epoch: 110, Loss: 1.4618, Train Acc: 0.5895, Val Acc: 0.5651, Test Acc: 0.5007\n",
                        "Seed: 456, Epoch: 120, Loss: 1.4535, Train Acc: 0.5922, Val Acc: 0.5674, Test Acc: 0.5106\n",
                        "Seed: 456, Epoch: 130, Loss: 1.4395, Train Acc: 0.5966, Val Acc: 0.5716, Test Acc: 0.5120\n",
                        "Seed: 456, Epoch: 140, Loss: 1.4225, Train Acc: 0.5985, Val Acc: 0.5726, Test Acc: 0.5107\n",
                        "Seed: 456, Epoch: 150, Loss: 1.4127, Train Acc: 0.6017, Val Acc: 0.5769, Test Acc: 0.5135\n",
                        "Seed: 456, Epoch: 160, Loss: 1.4031, Train Acc: 0.6033, Val Acc: 0.5718, Test Acc: 0.5064\n",
                        "Seed: 456, Epoch: 170, Loss: 1.3962, Train Acc: 0.6055, Val Acc: 0.5750, Test Acc: 0.5084\n",
                        "Seed: 456, Epoch: 180, Loss: 1.3942, Train Acc: 0.6045, Val Acc: 0.5714, Test Acc: 0.5105\n",
                        "Seed: 456, Epoch: 190, Loss: 1.3880, Train Acc: 0.6060, Val Acc: 0.5751, Test Acc: 0.5100\n",
                        "Seed: 456, Epoch: 200, Loss: 1.3805, Train Acc: 0.6061, Val Acc: 0.5736, Test Acc: 0.5094\n",
                        "Seed: 456, Epoch: 210, Loss: 1.3762, Train Acc: 0.6050, Val Acc: 0.5661, Test Acc: 0.5009\n",
                        "Seed: 456, Epoch: 220, Loss: 1.3716, Train Acc: 0.6091, Val Acc: 0.5795, Test Acc: 0.5197\n",
                        "Seed: 456, Epoch: 230, Loss: 1.3624, Train Acc: 0.6125, Val Acc: 0.5842, Test Acc: 0.5194\n",
                        "Seed: 456, Epoch: 240, Loss: 1.3587, Train Acc: 0.6122, Val Acc: 0.5831, Test Acc: 0.5167\n",
                        "Seed: 456, Epoch: 250, Loss: 1.3555, Train Acc: 0.6126, Val Acc: 0.5809, Test Acc: 0.5212\n",
                        "Seed: 456, Epoch: 260, Loss: 1.3498, Train Acc: 0.6143, Val Acc: 0.5859, Test Acc: 0.5243\n",
                        "Seed: 456, Epoch: 270, Loss: 1.3488, Train Acc: 0.6137, Val Acc: 0.5804, Test Acc: 0.5220\n",
                        "Seed: 456, Epoch: 280, Loss: 1.3406, Train Acc: 0.6169, Val Acc: 0.5862, Test Acc: 0.5228\n",
                        "Seed: 456, Epoch: 290, Loss: 1.3366, Train Acc: 0.6180, Val Acc: 0.5858, Test Acc: 0.5212\n",
                        "Seed: 456, Epoch: 300, Loss: 1.3592, Train Acc: 0.6180, Val Acc: 0.5913, Test Acc: 0.5315\n",
                        "Seed: 456, Epoch: 310, Loss: 1.3526, Train Acc: 0.6132, Val Acc: 0.5892, Test Acc: 0.5261\n",
                        "Seed: 456, Epoch: 320, Loss: 1.3406, Train Acc: 0.6143, Val Acc: 0.5830, Test Acc: 0.5258\n",
                        "Seed: 456, Epoch: 330, Loss: 1.3279, Train Acc: 0.6189, Val Acc: 0.5852, Test Acc: 0.5240\n",
                        "Seed: 456, Epoch: 340, Loss: 1.3236, Train Acc: 0.6209, Val Acc: 0.5889, Test Acc: 0.5264\n",
                        "Seed: 456, Epoch: 350, Loss: 1.3296, Train Acc: 0.6183, Val Acc: 0.5905, Test Acc: 0.5383\n",
                        "Seed: 456, Epoch: 360, Loss: 1.3165, Train Acc: 0.6217, Val Acc: 0.5896, Test Acc: 0.5329\n",
                        "Seed: 456, Epoch: 370, Loss: 1.3149, Train Acc: 0.6213, Val Acc: 0.5846, Test Acc: 0.5259\n",
                        "Seed: 456, Epoch: 380, Loss: 1.3168, Train Acc: 0.6217, Val Acc: 0.5836, Test Acc: 0.5205\n",
                        "Seed: 456, Epoch: 390, Loss: 1.3111, Train Acc: 0.6226, Val Acc: 0.5926, Test Acc: 0.5331\n",
                        "Seed: 456, Epoch: 400, Loss: 1.3074, Train Acc: 0.6247, Val Acc: 0.5876, Test Acc: 0.5246\n",
                        "Seed: 456, Epoch: 410, Loss: 1.3434, Train Acc: 0.6212, Val Acc: 0.5826, Test Acc: 0.5177\n",
                        "Seed: 456, Epoch: 420, Loss: 1.3090, Train Acc: 0.6237, Val Acc: 0.5949, Test Acc: 0.5379\n",
                        "Seed: 456, Epoch: 430, Loss: 1.3055, Train Acc: 0.6261, Val Acc: 0.5902, Test Acc: 0.5281\n",
                        "Seed: 456, Epoch: 440, Loss: 1.2979, Train Acc: 0.6258, Val Acc: 0.5906, Test Acc: 0.5290\n",
                        "Seed: 456, Epoch: 450, Loss: 1.2974, Train Acc: 0.6249, Val Acc: 0.5882, Test Acc: 0.5333\n",
                        "Seed: 456, Epoch: 460, Loss: 1.2934, Train Acc: 0.6271, Val Acc: 0.5922, Test Acc: 0.5325\n",
                        "Seed: 456, Epoch: 470, Loss: 1.2959, Train Acc: 0.6249, Val Acc: 0.5885, Test Acc: 0.5330\n",
                        "Seed: 456, Epoch: 480, Loss: 1.2912, Train Acc: 0.6282, Val Acc: 0.5913, Test Acc: 0.5313\n",
                        "Seed: 456, Epoch: 490, Loss: 1.2872, Train Acc: 0.6291, Val Acc: 0.5872, Test Acc: 0.5247\n",
                        "Seed: 456, Epoch: 500, Loss: 1.2881, Train Acc: 0.6279, Val Acc: 0.5914, Test Acc: 0.5357\n",
                        "Seed: 42, Best Val Acc: 0.5923, Corresponding Test Acc: 0.5276, Time: 120.84s, Memory: 1.73MB, GPU Memory: 7431.61MB\n",
                        "Seed: 123, Best Val Acc: 0.5924, Corresponding Test Acc: 0.5378, Time: 122.21s, Memory: 1.70MB, GPU Memory: 6508.72MB\n",
                        "Seed: 456, Best Val Acc: 0.5971, Corresponding Test Acc: 0.5426, Time: 139.84s, Memory: 1.70MB, GPU Memory: 6526.74MB\n",
                        "\n",
                        "Average Best Validation Accuracy: 0.5939\n",
                        "Average Corresponding Test Accuracy: 0.5360\n",
                        "Average Time: 127.63s\n",
                        "Average Memory Usage: 1.71MB\n",
                        "Average GPU Memory Usage: 6822.36MB\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndataset = dataset_sparse\ngraph = dataset[0]\nnum_classes = dataset.num_classes\nin_channels = dataset.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.5, 0.5]\nclass HierarchicalGCN_CO(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_CO, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.sum_res = sum_res\n        self.act = act\n        channels = self.hidden_channels\n        self.pools = torch.nn.ModuleList()\n        self.down_convs = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(CGIPool(channels, ratio=0.9))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.0, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = F.relu(x)  \n        xs = [x]\n        edge_indices = [edge_index]\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = F.relu(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            up = res\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = F.relu(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x.log_softmax(dim=-1)\ndef train(model, data, train_idx, optimizer):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index)[train_idx]\n    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n@torch.no_grad()\ndef test(model, data, split_idx, evaluator):\n    model.eval()\n    out = model(data.x, data.edge_index)\n    y_pred = out.argmax(dim=-1, keepdim=True)\n    train_acc = evaluator.eval({\n        'y_true': data.y[split_idx['train']],\n        'y_pred': y_pred[split_idx['train']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': data.y[split_idx['valid']],\n        'y_pred': y_pred[split_idx['valid']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': data.y[split_idx['test']],\n        'y_pred': y_pred[split_idx['test']],\n    })['acc']\n    return train_acc, valid_acc, test_acc\nseeds = [42, 123, 456]\nresults = []\nval_accuracies_list = []\ntimes = []\nmemories = []\ngpu_memories = []\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfor seed in seeds:\n    graph = graph.to(device)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    val_accuracies = []\n    test_accuracies = []\n    start_time = time.time()\n    tracemalloc.start()\n    model = HierarchicalGCN_CO(\n        in_channels=graph.num_features,\n        hidden_channels=256,\n        out_channels=dataset.num_classes,\n        depth=2,\n        act=F.relu,\n        sum_res=False\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx['train'].to(device)\n    for epoch in range(1, 501):\n        loss = train(model, graph, split_idx['train'], optimizer)\n        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n        val_accuracies.append(valid_acc)\n        test_accuracies.append(test_acc)\n        if epoch % 10 == 0:\n            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    elapsed_time = end_time - start_time\n    memory_usage = current / 10**6  \n    peak_memory_usage = peak / 10**6  \n    if torch.cuda.is_available():\n        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  \n        torch.cuda.reset_max_memory_allocated(device)\n    else:\n        gpu_memory_usage = 0\n    val_accuracies_list.append(val_accuracies)\n    times.append(elapsed_time)\n    memories.append(memory_usage)\n    gpu_memories.append(gpu_memory_usage)\n    best_val_acc = max(val_accuracies)\n    best_val_index = val_accuracies.index(best_val_acc)\n    corresponding_test_acc = test_accuracies[best_val_index]\n    results.append({\n        'seed': seed,\n        'best_val_acc': best_val_acc,\n        'corresponding_test_acc': corresponding_test_acc,\n        'elapsed_time': elapsed_time,\n        'memory_usage': memory_usage,\n        'gpu_memory_usage': gpu_memory_usage\n    })\nfor result in results:\n    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\navg_val_acc = np.mean([result['best_val_acc'] for result in results])\navg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\navg_time = np.mean(times)\navg_memory = np.mean(memories)\navg_gpu_memory = np.mean(gpu_memories)\nprint(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\nprint(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\nprint(f\"Average Time: {avg_time:.2f}s\")\nprint(f\"Average Memory Usage: {avg_memory:.2f}MB\")\nprint(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### KMIS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.utils.num_nodes import maybe_num_nodes\nfrom torch_sparse import spspmm\nfrom torch_sparse import coalesce\nfrom torch_sparse import eye\nfrom torch.nn import Parameter\nfrom torch_scatter import scatter_add\nfrom torch_scatter import scatter_max\nfrom torch_scatter import scatter_add, scatter\nfrom torch_geometric.nn.inits import uniform\nfrom torch_geometric.nn.resolver import activation_resolver\nfrom torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom dataclasses import dataclass\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nfrom torch_geometric.nn.conv import MessagePassing\nfrom torch_geometric.nn.conv.gcn_conv import gcn_norm\nfrom torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\nfrom typing import Callable, Optional, Union\nfrom torch_sparse import coalesce, transpose\nfrom torch_scatter import scatter\nfrom torch import Tensor\nfrom typing import Callable, Optional, Tuple, Union\nimport torch\nfrom torch.nn import Module\nfrom torch_scatter import scatter, scatter_add, scatter_min\nfrom torch_sparse import SparseTensor, remove_diag\nfrom torch_geometric.nn.aggr import Aggregation\nfrom torch_geometric.nn.dense import Linear\nfrom torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\nScorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\nimport torch\nfrom torch_scatter import scatter_max, scatter_min\nfrom torch_geometric.typing import Adj, OptTensor, SparseTensor, Tensor\nfrom typing import Callable, Optional, Tuple, Union\nfrom torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\nScorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\nfrom torch_sparse import SparseTensor, remove_diag\nfrom torch_geometric.nn.aggr import Aggregation\nfrom torch_geometric.nn.dense import Linear\nfrom torch.nn import Module\nfrom torch_scatter import scatter_max, scatter_min\ndef maximal_independent_set(edge_index: Adj, k: int = 1,\n                            perm: OptTensor = None) -> Tensor:\n    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n    returned set.\n    The algorithm greedily selects the nodes in their canonical order. If a\n    permutation :obj:`perm` is provided, the nodes are extracted following\n    that permutation instead.\n    This method follows `Blelloch's Alogirithm\n    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n    higher values of :math:`k`.\n    Args:\n        edge_index (Tensor or SparseTensor): The graph connectivity.\n        k (int): The :math:`k` value (defaults to 1).\n        perm (LongTensor, optional): Permutation vector. Must be of size\n            :obj:`(n,)` (defaults to :obj:`None`).\n    :rtype: :class:`ByteTensor`\n    \"\"\"\n    if isinstance(edge_index, SparseTensor):\n        row, col, _ = edge_index.coo()\n        device = edge_index.device()\n        n = edge_index.size(0)\n    else:\n        row, col = edge_index[0], edge_index[1]\n        device = row.device\n        n = edge_index.max().item() + 1\n    if perm is None:\n        rank = torch.arange(n, dtype=torch.long, device=device)\n    else:\n        rank = torch.zeros_like(perm)\n        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n    mis = torch.zeros(n, dtype=torch.bool, device=device)\n    mask = mis.clone()\n    min_rank = rank.clone()\n    while not mask.all():\n        for _ in range(k):\n            min_neigh = torch.full_like(min_rank, fill_value=n)\n            scatter_min(min_rank[row], col, out=min_neigh)\n            torch.minimum(min_neigh, min_rank, out=min_rank)  \n        mis = mis | torch.eq(rank, min_rank)\n        mask = mis.clone().byte()\n        for _ in range(k):\n            max_neigh = torch.full_like(mask, fill_value=0)\n            scatter_max(mask[row], col, out=max_neigh)\n            torch.maximum(max_neigh, mask, out=mask)  \n        mask = mask.to(dtype=torch.bool)\n        min_rank = rank.clone()\n        min_rank[mask] = n\n    return mis\ndef maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n                                    perm: OptTensor = None) -> PairTensor:\n    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n    clustering of a graph, as defined in `\"Generalizing Downsampling from\n    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n    The algorithm greedily selects the nodes in their canonical order. If a\n    permutation :obj:`perm` is provided, the nodes are extracted following\n    that permutation instead.\n    This method returns both the :math:`k`-MIS and the clustering, where the\n    :math:`c`-th cluster refers to the :math:`c`-th element of the\n    :math:`k`-MIS.\n    Args:\n        edge_index (Tensor or SparseTensor): The graph connectivity.\n        k (int): The :math:`k` value (defaults to 1).\n        perm (LongTensor, optional): Permutation vector. Must be of size\n            :obj:`(n,)` (defaults to :obj:`None`).\n    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n    \"\"\"\n    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n    n, device = mis.size(0), mis.device\n    if isinstance(edge_index, SparseTensor):\n        row, col, _ = edge_index.coo()\n    else:\n        row, col = edge_index[0], edge_index[1]\n    if perm is None:\n        rank = torch.arange(n, dtype=torch.long, device=device)\n    else:\n        rank = torch.zeros_like(perm)\n        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n    rank_mis = rank[mis]\n    min_rank[mis] = rank_mis\n    for _ in range(k):\n        min_neigh = torch.full_like(min_rank, fill_value=n)\n        scatter_min(min_rank[row], col, out=min_neigh)\n        torch.minimum(min_neigh, min_rank, out=min_rank)\n    _, clusters = torch.unique(min_rank, return_inverse=True)\n    perm = torch.argsort(rank_mis)\n    return mis, perm[clusters]\nclass KMISPooling(Module):\n    _heuristics = {None, 'greedy', 'w-greedy'}\n    _passthroughs = {None, 'before', 'after'}\n    _scorers = {\n        'linear',\n        'random',\n        'constant',\n        'canonical',\n        'first',\n        'last',\n    }\n    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n                 scorer: Union[Scorer, str] = 'linear',\n                 score_heuristic: Optional[str] = 'greedy',\n                 score_passthrough: Optional[str] = 'before',\n                 aggr_x: Optional[Union[str, Aggregation]] = None,\n                 aggr_edge: str = 'sum',\n                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n                 remove_self_loops: bool = True) -> None:\n        super(KMISPooling, self).__init__()\n        assert score_heuristic in self._heuristics, \\\n            \"Unrecognized `score_heuristic` value.\"\n        assert score_passthrough in self._passthroughs, \\\n            \"Unrecognized `score_passthrough` value.\"\n        if not callable(scorer):\n            assert scorer in self._scorers, \\\n                \"Unrecognized `scorer` value.\"\n        self.k = k\n        self.scorer = scorer\n        self.score_heuristic = score_heuristic\n        self.score_passthrough = score_passthrough\n        self.aggr_x = aggr_x\n        self.aggr_edge = aggr_edge\n        self.aggr_score = aggr_score\n        self.remove_self_loops = remove_self_loops\n        if scorer == 'linear':\n            assert self.score_passthrough is not None, \\\n                \"`'score_passthrough'` must not be `None`\" \\\n                \" when using `'linear'` scorer\"\n            self.lin = torch.nn.Linear(in_channels, 1)\n    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n        if self.score_heuristic is None:\n            return x\n        row, col, _ = adj.coo()\n        x = x.view(-1)\n        if self.score_heuristic == 'greedy':\n            k_sums = torch.ones_like(x)\n        else:\n            k_sums = x.clone()\n        for _ in range(self.k):\n            scatter_add(k_sums[row], col, out=k_sums)\n        return x / k_sums\n    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n                batch: OptTensor = None) -> Tensor:\n        if self.scorer == 'linear':\n            return self.lin(x).sigmoid()\n        if self.scorer == 'random':\n            return torch.rand((x.size(0), 1), device=x.device)\n        if self.scorer == 'constant':\n            return torch.ones((x.size(0), 1), device=x.device)\n        if self.scorer == 'canonical':\n            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n        if self.scorer == 'first':\n            return x[..., [0]]\n        if self.scorer == 'last':\n            return x[..., [-1]]\n        return self.scorer(x, edge_index, edge_attr, batch)\n    def forward(self, x: Tensor, edge_index: Adj,\n                edge_attr: OptTensor = None,\n                batch: OptTensor = None) \\\n            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n        \"\"\"\"\"\"\n        edge_index = edge_index.long()\n        adj, n = edge_index, x.size(0)\n        if not isinstance(edge_index, SparseTensor):\n            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n        score = self._scorer(x, edge_index, edge_attr, batch)\n        updated_score = self._apply_heuristic(score, adj)\n        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n        row, col, val = adj.coo()\n        c = mis.sum()\n        if val is None:\n            val = torch.ones_like(row, dtype=torch.float)\n        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n                           is_sorted=False,\n                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n        if self.remove_self_loops:\n            adj = remove_diag(adj)\n        if self.score_passthrough == 'before':\n            x = self.aggr_score(x, score)\n        if self.aggr_x is None:\n            x = x[mis]\n        elif isinstance(self.aggr_x, str):\n            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n                        reduce=self.aggr_x)\n        else:\n            x = self.aggr_x(x, cluster, dim_size=c)\n        if self.score_passthrough == 'after':\n            x = self.aggr_score(x, score[mis])\n        if isinstance(edge_index, SparseTensor):\n            edge_index, edge_attr = adj, None\n        else:\n            row, col, edge_attr = adj.coo()\n            edge_index = torch.stack([row, col])\n        if batch is not None:\n            batch = batch[mis]\n        perm = perm[mis]\n        return x, edge_index, edge_attr, batch, mis, cluster, perm\n    def __repr__(self):\n        if self.scorer == 'linear':\n            channels = f\"in_channels={self.lin.in_channels}, \"\n        else:\n            channels = \"\"\n        return f'{self.__class__.__name__}({channels}k={self.k})'"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 042, Epoch: 010, Loss: 3.0120, Train Acc: 0.2774, Val Acc: 0.3013, Test Acc: 0.2686\n",
                        "Seed: 042, Epoch: 020, Loss: 2.5648, Train Acc: 0.3541, Val Acc: 0.3868, Test Acc: 0.3409\n",
                        "Seed: 042, Epoch: 030, Loss: 2.1364, Train Acc: 0.4323, Val Acc: 0.4336, Test Acc: 0.3797\n",
                        "Seed: 042, Epoch: 040, Loss: 1.9311, Train Acc: 0.4804, Val Acc: 0.4747, Test Acc: 0.4109\n",
                        "Seed: 042, Epoch: 050, Loss: 1.7729, Train Acc: 0.5200, Val Acc: 0.5127, Test Acc: 0.4550\n",
                        "Seed: 042, Epoch: 060, Loss: 1.6788, Train Acc: 0.5392, Val Acc: 0.5205, Test Acc: 0.4574\n",
                        "Seed: 042, Epoch: 070, Loss: 1.6091, Train Acc: 0.5524, Val Acc: 0.5294, Test Acc: 0.4669\n",
                        "Seed: 042, Epoch: 080, Loss: 1.5656, Train Acc: 0.5651, Val Acc: 0.5417, Test Acc: 0.4758\n",
                        "Seed: 042, Epoch: 090, Loss: 1.5336, Train Acc: 0.5705, Val Acc: 0.5475, Test Acc: 0.4822\n",
                        "Seed: 042, Epoch: 100, Loss: 1.5167, Train Acc: 0.5768, Val Acc: 0.5544, Test Acc: 0.4880\n",
                        "Seed: 042, Epoch: 110, Loss: 1.4931, Train Acc: 0.5791, Val Acc: 0.5543, Test Acc: 0.4854\n",
                        "Seed: 042, Epoch: 120, Loss: 1.4770, Train Acc: 0.5844, Val Acc: 0.5601, Test Acc: 0.4951\n",
                        "Seed: 042, Epoch: 130, Loss: 1.4667, Train Acc: 0.5859, Val Acc: 0.5617, Test Acc: 0.4970\n",
                        "Seed: 042, Epoch: 140, Loss: 1.4601, Train Acc: 0.5865, Val Acc: 0.5621, Test Acc: 0.4978\n",
                        "Seed: 042, Epoch: 150, Loss: 1.4446, Train Acc: 0.5910, Val Acc: 0.5687, Test Acc: 0.5057\n",
                        "Seed: 042, Epoch: 160, Loss: 1.4325, Train Acc: 0.5939, Val Acc: 0.5711, Test Acc: 0.5063\n",
                        "Seed: 042, Epoch: 170, Loss: 1.4347, Train Acc: 0.5912, Val Acc: 0.5695, Test Acc: 0.5128\n",
                        "Seed: 042, Epoch: 180, Loss: 1.4209, Train Acc: 0.5950, Val Acc: 0.5716, Test Acc: 0.5114\n",
                        "Seed: 042, Epoch: 190, Loss: 1.4105, Train Acc: 0.5977, Val Acc: 0.5702, Test Acc: 0.5019\n",
                        "Seed: 042, Epoch: 200, Loss: 1.4257, Train Acc: 0.5973, Val Acc: 0.5743, Test Acc: 0.5071\n",
                        "Seed: 042, Epoch: 210, Loss: 1.4040, Train Acc: 0.6011, Val Acc: 0.5777, Test Acc: 0.5136\n",
                        "Seed: 042, Epoch: 220, Loss: 1.3929, Train Acc: 0.6030, Val Acc: 0.5778, Test Acc: 0.5115\n",
                        "Seed: 042, Epoch: 230, Loss: 1.3901, Train Acc: 0.6032, Val Acc: 0.5748, Test Acc: 0.5054\n",
                        "Seed: 042, Epoch: 240, Loss: 1.3858, Train Acc: 0.6062, Val Acc: 0.5794, Test Acc: 0.5124\n",
                        "Seed: 042, Epoch: 250, Loss: 1.3803, Train Acc: 0.6064, Val Acc: 0.5809, Test Acc: 0.5149\n",
                        "Seed: 042, Epoch: 260, Loss: 1.3727, Train Acc: 0.6075, Val Acc: 0.5778, Test Acc: 0.5121\n",
                        "Seed: 042, Epoch: 270, Loss: 1.3924, Train Acc: 0.5988, Val Acc: 0.5646, Test Acc: 0.4987\n",
                        "Seed: 042, Epoch: 280, Loss: 1.3750, Train Acc: 0.6075, Val Acc: 0.5840, Test Acc: 0.5230\n",
                        "Seed: 042, Epoch: 290, Loss: 1.3678, Train Acc: 0.6099, Val Acc: 0.5790, Test Acc: 0.5149\n",
                        "Seed: 042, Epoch: 300, Loss: 1.3615, Train Acc: 0.6099, Val Acc: 0.5809, Test Acc: 0.5181\n",
                        "Seed: 042, Epoch: 310, Loss: 1.3605, Train Acc: 0.6107, Val Acc: 0.5802, Test Acc: 0.5131\n",
                        "Seed: 042, Epoch: 320, Loss: 1.3531, Train Acc: 0.6125, Val Acc: 0.5839, Test Acc: 0.5214\n",
                        "Seed: 042, Epoch: 330, Loss: 1.3489, Train Acc: 0.6140, Val Acc: 0.5854, Test Acc: 0.5212\n",
                        "Seed: 042, Epoch: 340, Loss: 1.3705, Train Acc: 0.6084, Val Acc: 0.5793, Test Acc: 0.5250\n",
                        "Seed: 042, Epoch: 350, Loss: 1.3453, Train Acc: 0.6142, Val Acc: 0.5833, Test Acc: 0.5183\n",
                        "Seed: 042, Epoch: 360, Loss: 1.3416, Train Acc: 0.6154, Val Acc: 0.5840, Test Acc: 0.5187\n",
                        "Seed: 042, Epoch: 370, Loss: 1.3376, Train Acc: 0.6162, Val Acc: 0.5861, Test Acc: 0.5208\n",
                        "Seed: 042, Epoch: 380, Loss: 1.3409, Train Acc: 0.6157, Val Acc: 0.5854, Test Acc: 0.5217\n",
                        "Seed: 042, Epoch: 390, Loss: 1.3433, Train Acc: 0.6150, Val Acc: 0.5857, Test Acc: 0.5232\n",
                        "Seed: 042, Epoch: 400, Loss: 1.3327, Train Acc: 0.6174, Val Acc: 0.5866, Test Acc: 0.5264\n",
                        "Seed: 042, Epoch: 410, Loss: 1.3292, Train Acc: 0.6177, Val Acc: 0.5869, Test Acc: 0.5257\n",
                        "Seed: 042, Epoch: 420, Loss: 1.3299, Train Acc: 0.6172, Val Acc: 0.5815, Test Acc: 0.5125\n",
                        "Seed: 042, Epoch: 430, Loss: 1.3288, Train Acc: 0.6175, Val Acc: 0.5873, Test Acc: 0.5206\n",
                        "Seed: 042, Epoch: 440, Loss: 1.3286, Train Acc: 0.6189, Val Acc: 0.5887, Test Acc: 0.5269\n",
                        "Seed: 042, Epoch: 450, Loss: 1.3235, Train Acc: 0.6188, Val Acc: 0.5851, Test Acc: 0.5190\n",
                        "Seed: 042, Epoch: 460, Loss: 1.3253, Train Acc: 0.6203, Val Acc: 0.5889, Test Acc: 0.5260\n",
                        "Seed: 042, Epoch: 470, Loss: 1.3174, Train Acc: 0.6214, Val Acc: 0.5904, Test Acc: 0.5287\n",
                        "Seed: 042, Epoch: 480, Loss: 1.3168, Train Acc: 0.6216, Val Acc: 0.5880, Test Acc: 0.5237\n",
                        "Seed: 042, Epoch: 490, Loss: 1.3132, Train Acc: 0.6218, Val Acc: 0.5891, Test Acc: 0.5277\n",
                        "Seed: 042, Epoch: 500, Loss: 1.3146, Train Acc: 0.6230, Val Acc: 0.5853, Test Acc: 0.5217\n",
                        "Seed: 123, Epoch: 010, Loss: 2.8030, Train Acc: 0.2816, Val Acc: 0.2939, Test Acc: 0.2562\n",
                        "Seed: 123, Epoch: 020, Loss: 2.3807, Train Acc: 0.3914, Val Acc: 0.4103, Test Acc: 0.3531\n",
                        "Seed: 123, Epoch: 030, Loss: 2.0018, Train Acc: 0.4622, Val Acc: 0.4653, Test Acc: 0.4047\n",
                        "Seed: 123, Epoch: 040, Loss: 1.8049, Train Acc: 0.5173, Val Acc: 0.5085, Test Acc: 0.4501\n",
                        "Seed: 123, Epoch: 050, Loss: 1.6746, Train Acc: 0.5411, Val Acc: 0.5319, Test Acc: 0.4658\n",
                        "Seed: 123, Epoch: 060, Loss: 1.5989, Train Acc: 0.5560, Val Acc: 0.5432, Test Acc: 0.4790\n",
                        "Seed: 123, Epoch: 070, Loss: 1.5579, Train Acc: 0.5647, Val Acc: 0.5447, Test Acc: 0.4736\n",
                        "Seed: 123, Epoch: 080, Loss: 1.5251, Train Acc: 0.5742, Val Acc: 0.5539, Test Acc: 0.4872\n",
                        "Seed: 123, Epoch: 090, Loss: 1.5003, Train Acc: 0.5802, Val Acc: 0.5593, Test Acc: 0.4935\n",
                        "Seed: 123, Epoch: 100, Loss: 1.4791, Train Acc: 0.5841, Val Acc: 0.5557, Test Acc: 0.4901\n",
                        "Seed: 123, Epoch: 110, Loss: 1.4663, Train Acc: 0.5839, Val Acc: 0.5519, Test Acc: 0.4885\n",
                        "Seed: 123, Epoch: 120, Loss: 1.4502, Train Acc: 0.5893, Val Acc: 0.5616, Test Acc: 0.4960\n",
                        "Seed: 123, Epoch: 130, Loss: 1.4342, Train Acc: 0.5942, Val Acc: 0.5702, Test Acc: 0.5032\n",
                        "Seed: 123, Epoch: 140, Loss: 1.4226, Train Acc: 0.5969, Val Acc: 0.5711, Test Acc: 0.5057\n",
                        "Seed: 123, Epoch: 150, Loss: 1.4115, Train Acc: 0.6007, Val Acc: 0.5759, Test Acc: 0.5093\n",
                        "Seed: 123, Epoch: 160, Loss: 1.4075, Train Acc: 0.5986, Val Acc: 0.5687, Test Acc: 0.5057\n",
                        "Seed: 123, Epoch: 170, Loss: 1.3957, Train Acc: 0.6034, Val Acc: 0.5808, Test Acc: 0.5164\n",
                        "Seed: 123, Epoch: 180, Loss: 1.3896, Train Acc: 0.6045, Val Acc: 0.5720, Test Acc: 0.5060\n",
                        "Seed: 123, Epoch: 190, Loss: 1.3831, Train Acc: 0.6065, Val Acc: 0.5764, Test Acc: 0.5113\n",
                        "Seed: 123, Epoch: 200, Loss: 1.3808, Train Acc: 0.6050, Val Acc: 0.5778, Test Acc: 0.5125\n",
                        "Seed: 123, Epoch: 210, Loss: 1.3758, Train Acc: 0.6086, Val Acc: 0.5791, Test Acc: 0.5162\n",
                        "Seed: 123, Epoch: 220, Loss: 1.3682, Train Acc: 0.6109, Val Acc: 0.5842, Test Acc: 0.5186\n",
                        "Seed: 123, Epoch: 230, Loss: 1.3674, Train Acc: 0.6110, Val Acc: 0.5790, Test Acc: 0.5111\n",
                        "Seed: 123, Epoch: 240, Loss: 1.3613, Train Acc: 0.6119, Val Acc: 0.5826, Test Acc: 0.5163\n",
                        "Seed: 123, Epoch: 250, Loss: 1.3551, Train Acc: 0.6145, Val Acc: 0.5815, Test Acc: 0.5149\n",
                        "Seed: 123, Epoch: 260, Loss: 1.3492, Train Acc: 0.6153, Val Acc: 0.5818, Test Acc: 0.5168\n",
                        "Seed: 123, Epoch: 270, Loss: 1.3463, Train Acc: 0.6156, Val Acc: 0.5844, Test Acc: 0.5186\n",
                        "Seed: 123, Epoch: 280, Loss: 1.3410, Train Acc: 0.6163, Val Acc: 0.5838, Test Acc: 0.5192\n",
                        "Seed: 123, Epoch: 290, Loss: 1.3412, Train Acc: 0.6166, Val Acc: 0.5812, Test Acc: 0.5149\n",
                        "Seed: 123, Epoch: 300, Loss: 1.3339, Train Acc: 0.6179, Val Acc: 0.5881, Test Acc: 0.5259\n",
                        "Seed: 123, Epoch: 310, Loss: 1.3394, Train Acc: 0.6190, Val Acc: 0.5834, Test Acc: 0.5190\n",
                        "Seed: 123, Epoch: 320, Loss: 1.3345, Train Acc: 0.6158, Val Acc: 0.5849, Test Acc: 0.5202\n",
                        "Seed: 123, Epoch: 330, Loss: 1.3264, Train Acc: 0.6204, Val Acc: 0.5831, Test Acc: 0.5186\n",
                        "Seed: 123, Epoch: 340, Loss: 1.3335, Train Acc: 0.6153, Val Acc: 0.5861, Test Acc: 0.5345\n",
                        "Seed: 123, Epoch: 350, Loss: 1.3286, Train Acc: 0.6191, Val Acc: 0.5859, Test Acc: 0.5277\n",
                        "Seed: 123, Epoch: 360, Loss: 1.3198, Train Acc: 0.6220, Val Acc: 0.5888, Test Acc: 0.5251\n",
                        "Seed: 123, Epoch: 370, Loss: 1.3136, Train Acc: 0.6225, Val Acc: 0.5897, Test Acc: 0.5253\n",
                        "Seed: 123, Epoch: 380, Loss: 1.3106, Train Acc: 0.6235, Val Acc: 0.5909, Test Acc: 0.5306\n",
                        "Seed: 123, Epoch: 390, Loss: 1.3073, Train Acc: 0.6228, Val Acc: 0.5878, Test Acc: 0.5278\n",
                        "Seed: 123, Epoch: 400, Loss: 1.3056, Train Acc: 0.6258, Val Acc: 0.5913, Test Acc: 0.5296\n",
                        "Seed: 123, Epoch: 410, Loss: 1.3040, Train Acc: 0.6252, Val Acc: 0.5882, Test Acc: 0.5245\n",
                        "Seed: 123, Epoch: 420, Loss: 1.3064, Train Acc: 0.6218, Val Acc: 0.5888, Test Acc: 0.5279\n",
                        "Seed: 123, Epoch: 430, Loss: 1.3592, Train Acc: 0.6121, Val Acc: 0.5837, Test Acc: 0.5234\n",
                        "Seed: 123, Epoch: 440, Loss: 1.3330, Train Acc: 0.6173, Val Acc: 0.5838, Test Acc: 0.5218\n",
                        "Seed: 123, Epoch: 450, Loss: 1.3248, Train Acc: 0.6249, Val Acc: 0.5852, Test Acc: 0.5227\n",
                        "Seed: 123, Epoch: 460, Loss: 1.3026, Train Acc: 0.6262, Val Acc: 0.5905, Test Acc: 0.5276\n",
                        "Seed: 123, Epoch: 470, Loss: 1.2922, Train Acc: 0.6282, Val Acc: 0.5880, Test Acc: 0.5259\n",
                        "Seed: 123, Epoch: 480, Loss: 1.2877, Train Acc: 0.6293, Val Acc: 0.5894, Test Acc: 0.5256\n",
                        "Seed: 123, Epoch: 490, Loss: 1.3148, Train Acc: 0.6263, Val Acc: 0.5840, Test Acc: 0.5190\n",
                        "Seed: 123, Epoch: 500, Loss: 1.2838, Train Acc: 0.6281, Val Acc: 0.5925, Test Acc: 0.5321\n",
                        "Seed: 456, Epoch: 010, Loss: 3.3511, Train Acc: 0.0913, Val Acc: 0.1759, Test Acc: 0.2447\n",
                        "Seed: 456, Epoch: 020, Loss: 2.6230, Train Acc: 0.3587, Val Acc: 0.3968, Test Acc: 0.3630\n",
                        "Seed: 456, Epoch: 030, Loss: 2.0834, Train Acc: 0.4490, Val Acc: 0.4577, Test Acc: 0.4008\n",
                        "Seed: 456, Epoch: 040, Loss: 1.8649, Train Acc: 0.5026, Val Acc: 0.4929, Test Acc: 0.4301\n",
                        "Seed: 456, Epoch: 050, Loss: 1.7295, Train Acc: 0.5313, Val Acc: 0.5171, Test Acc: 0.4553\n",
                        "Seed: 456, Epoch: 060, Loss: 1.6265, Train Acc: 0.5521, Val Acc: 0.5296, Test Acc: 0.4654\n",
                        "Seed: 456, Epoch: 070, Loss: 1.5731, Train Acc: 0.5640, Val Acc: 0.5417, Test Acc: 0.4763\n",
                        "Seed: 456, Epoch: 080, Loss: 1.5432, Train Acc: 0.5701, Val Acc: 0.5510, Test Acc: 0.4877\n",
                        "Seed: 456, Epoch: 090, Loss: 1.5224, Train Acc: 0.5751, Val Acc: 0.5566, Test Acc: 0.4928\n",
                        "Seed: 456, Epoch: 100, Loss: 1.5001, Train Acc: 0.5804, Val Acc: 0.5600, Test Acc: 0.4981\n",
                        "Seed: 456, Epoch: 110, Loss: 1.5024, Train Acc: 0.5784, Val Acc: 0.5494, Test Acc: 0.4782\n",
                        "Seed: 456, Epoch: 120, Loss: 1.4693, Train Acc: 0.5873, Val Acc: 0.5601, Test Acc: 0.4978\n",
                        "Seed: 456, Epoch: 130, Loss: 1.4542, Train Acc: 0.5905, Val Acc: 0.5628, Test Acc: 0.4997\n",
                        "Seed: 456, Epoch: 140, Loss: 1.4454, Train Acc: 0.5933, Val Acc: 0.5695, Test Acc: 0.5112\n",
                        "Seed: 456, Epoch: 150, Loss: 1.4336, Train Acc: 0.5954, Val Acc: 0.5695, Test Acc: 0.5041\n",
                        "Seed: 456, Epoch: 160, Loss: 1.4253, Train Acc: 0.5976, Val Acc: 0.5704, Test Acc: 0.5075\n",
                        "Seed: 456, Epoch: 170, Loss: 1.4233, Train Acc: 0.6005, Val Acc: 0.5742, Test Acc: 0.5119\n",
                        "Seed: 456, Epoch: 180, Loss: 1.4119, Train Acc: 0.6004, Val Acc: 0.5684, Test Acc: 0.5010\n",
                        "Seed: 456, Epoch: 190, Loss: 1.4037, Train Acc: 0.6020, Val Acc: 0.5750, Test Acc: 0.5112\n",
                        "Seed: 456, Epoch: 200, Loss: 1.4019, Train Acc: 0.6011, Val Acc: 0.5709, Test Acc: 0.5105\n",
                        "Seed: 456, Epoch: 210, Loss: 1.3917, Train Acc: 0.6046, Val Acc: 0.5750, Test Acc: 0.5121\n",
                        "Seed: 456, Epoch: 220, Loss: 1.3859, Train Acc: 0.6061, Val Acc: 0.5789, Test Acc: 0.5164\n",
                        "Seed: 456, Epoch: 230, Loss: 1.3828, Train Acc: 0.6072, Val Acc: 0.5773, Test Acc: 0.5130\n",
                        "Seed: 456, Epoch: 240, Loss: 1.3742, Train Acc: 0.6082, Val Acc: 0.5763, Test Acc: 0.5115\n",
                        "Seed: 456, Epoch: 250, Loss: 1.3850, Train Acc: 0.6046, Val Acc: 0.5713, Test Acc: 0.5039\n",
                        "Seed: 456, Epoch: 260, Loss: 1.3690, Train Acc: 0.6101, Val Acc: 0.5850, Test Acc: 0.5200\n",
                        "Seed: 456, Epoch: 270, Loss: 1.3707, Train Acc: 0.6072, Val Acc: 0.5846, Test Acc: 0.5297\n",
                        "Seed: 456, Epoch: 280, Loss: 1.3584, Train Acc: 0.6119, Val Acc: 0.5839, Test Acc: 0.5204\n",
                        "Seed: 456, Epoch: 290, Loss: 1.3555, Train Acc: 0.6129, Val Acc: 0.5837, Test Acc: 0.5168\n",
                        "Seed: 456, Epoch: 300, Loss: 1.3538, Train Acc: 0.6122, Val Acc: 0.5781, Test Acc: 0.5150\n",
                        "Seed: 456, Epoch: 310, Loss: 1.3506, Train Acc: 0.6144, Val Acc: 0.5852, Test Acc: 0.5227\n",
                        "Seed: 456, Epoch: 320, Loss: 1.3457, Train Acc: 0.6157, Val Acc: 0.5847, Test Acc: 0.5205\n",
                        "Seed: 456, Epoch: 330, Loss: 1.3455, Train Acc: 0.6138, Val Acc: 0.5853, Test Acc: 0.5253\n",
                        "Seed: 456, Epoch: 340, Loss: 1.3398, Train Acc: 0.6161, Val Acc: 0.5870, Test Acc: 0.5240\n",
                        "Seed: 456, Epoch: 350, Loss: 1.3388, Train Acc: 0.6166, Val Acc: 0.5832, Test Acc: 0.5187\n",
                        "Seed: 456, Epoch: 360, Loss: 1.3389, Train Acc: 0.6168, Val Acc: 0.5890, Test Acc: 0.5306\n",
                        "Seed: 456, Epoch: 370, Loss: 1.3345, Train Acc: 0.6173, Val Acc: 0.5849, Test Acc: 0.5201\n",
                        "Seed: 456, Epoch: 380, Loss: 1.3298, Train Acc: 0.6184, Val Acc: 0.5903, Test Acc: 0.5329\n",
                        "Seed: 456, Epoch: 390, Loss: 1.3321, Train Acc: 0.6157, Val Acc: 0.5823, Test Acc: 0.5246\n",
                        "Seed: 456, Epoch: 400, Loss: 1.3293, Train Acc: 0.6180, Val Acc: 0.5803, Test Acc: 0.5155\n",
                        "Seed: 456, Epoch: 410, Loss: 1.3221, Train Acc: 0.6198, Val Acc: 0.5839, Test Acc: 0.5216\n",
                        "Seed: 456, Epoch: 420, Loss: 1.3322, Train Acc: 0.6155, Val Acc: 0.5840, Test Acc: 0.5245\n",
                        "Seed: 456, Epoch: 430, Loss: 1.3207, Train Acc: 0.6200, Val Acc: 0.5869, Test Acc: 0.5247\n",
                        "Seed: 456, Epoch: 440, Loss: 1.3161, Train Acc: 0.6215, Val Acc: 0.5888, Test Acc: 0.5268\n",
                        "Seed: 456, Epoch: 450, Loss: 1.3141, Train Acc: 0.6221, Val Acc: 0.5865, Test Acc: 0.5216\n",
                        "Seed: 456, Epoch: 460, Loss: 1.3214, Train Acc: 0.6205, Val Acc: 0.5869, Test Acc: 0.5328\n",
                        "Seed: 456, Epoch: 470, Loss: 1.3117, Train Acc: 0.6223, Val Acc: 0.5842, Test Acc: 0.5197\n",
                        "Seed: 456, Epoch: 480, Loss: 1.3160, Train Acc: 0.6199, Val Acc: 0.5833, Test Acc: 0.5178\n",
                        "Seed: 456, Epoch: 490, Loss: 1.3110, Train Acc: 0.6218, Val Acc: 0.5906, Test Acc: 0.5318\n",
                        "Seed: 456, Epoch: 500, Loss: 1.3056, Train Acc: 0.6231, Val Acc: 0.5883, Test Acc: 0.5274\n",
                        "Seed: 42, Best Val Acc: 0.5909, Corresponding Test Acc: 0.5334, Time: 72.57s, Memory: 9.12MB, GPU Memory: 4520.89MB\n",
                        "Seed: 123, Best Val Acc: 0.5941, Corresponding Test Acc: 0.5334, Time: 72.55s, Memory: 1.52MB, GPU Memory: 4520.40MB\n",
                        "Seed: 456, Best Val Acc: 0.5928, Corresponding Test Acc: 0.5347, Time: 72.25s, Memory: 1.52MB, GPU Memory: 4520.09MB\n",
                        "\n",
                        "Average Best Validation Accuracy: 0.5926\n",
                        "Average Corresponding Test Accuracy: 0.5338\n",
                        "Average Time: 72.46s\n",
                        "Average Memory Usage: 4.05MB\n",
                        "Average GPU Memory Usage: 4520.46MB\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndataset = dataset_sparse\ngraph = dataset[0]\nnum_classes = dataset.num_classes\nin_channels = dataset.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.5, 0.5]\nclass HierarchicalGCN_KMIS(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_KMIS, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.sum_res = sum_res\n        self.act = act\n        channels = self.hidden_channels\n        self.pools = torch.nn.ModuleList()\n        self.down_convs = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(KMISPooling(256, k=5, aggr_x='sum'))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.0, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = F.relu(x)  \n        xs = [x]\n        edge_indices = [edge_index]\n        for i in range(1, self.depth + 1):\n            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n            x = self.down_convs[i](x, edge_index)\n            x = F.relu(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            up = res\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = F.relu(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x.log_softmax(dim=-1)\ndef train(model, data, train_idx, optimizer):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index)[train_idx]\n    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n@torch.no_grad()\ndef test(model, data, split_idx, evaluator):\n    model.eval()\n    out = model(data.x, data.edge_index)\n    y_pred = out.argmax(dim=-1, keepdim=True)\n    train_acc = evaluator.eval({\n        'y_true': data.y[split_idx['train']],\n        'y_pred': y_pred[split_idx['train']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': data.y[split_idx['valid']],\n        'y_pred': y_pred[split_idx['valid']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': data.y[split_idx['test']],\n        'y_pred': y_pred[split_idx['test']],\n    })['acc']\n    return train_acc, valid_acc, test_acc\nseeds = [42, 123, 456]\nresults = []\nval_accuracies_list = []\ntimes = []\nmemories = []\ngpu_memories = []\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfor seed in seeds:\n    graph = graph.to(device)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    val_accuracies = []\n    test_accuracies = []\n    start_time = time.time()\n    tracemalloc.start()\n    model = HierarchicalGCN_KMIS(\n        in_channels=graph.num_features,\n        hidden_channels=256,\n        out_channels=dataset.num_classes,\n        depth=2,\n        act=F.relu,\n        sum_res=False\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx['train'].to(device)\n    for epoch in range(1, 501):\n        loss = train(model, graph, split_idx['train'], optimizer)\n        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n        val_accuracies.append(valid_acc)\n        test_accuracies.append(test_acc)\n        if epoch % 10 == 0:\n            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    elapsed_time = end_time - start_time\n    memory_usage = current / 10**6  \n    peak_memory_usage = peak / 10**6  \n    if torch.cuda.is_available():\n        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  \n        torch.cuda.reset_max_memory_allocated(device)\n    else:\n        gpu_memory_usage = 0\n    val_accuracies_list.append(val_accuracies)\n    times.append(elapsed_time)\n    memories.append(memory_usage)\n    gpu_memories.append(gpu_memory_usage)\n    best_val_acc = max(val_accuracies)\n    best_val_index = val_accuracies.index(best_val_acc)\n    corresponding_test_acc = test_accuracies[best_val_index]\n    results.append({\n        'seed': seed,\n        'best_val_acc': best_val_acc,\n        'corresponding_test_acc': corresponding_test_acc,\n        'elapsed_time': elapsed_time,\n        'memory_usage': memory_usage,\n        'gpu_memory_usage': gpu_memory_usage\n    })\nfor result in results:\n    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\navg_val_acc = np.mean([result['best_val_acc'] for result in results])\navg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\navg_time = np.mean(times)\navg_memory = np.mean(memories)\navg_gpu_memory = np.mean(gpu_memories)\nprint(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\nprint(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\nprint(f\"Average Time: {avg_time:.2f}s\")\nprint(f\"Average Memory Usage: {avg_memory:.2f}MB\")\nprint(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### GSAPool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [],
            "source": "import math\nfrom typing import Union, Optional, Callable\nfrom torch_scatter import scatter_add, scatter_max\nfrom torch_geometric.utils import softmax\nfrom torch_geometric.nn import GCNConv, SAGEConv, GATConv, ChebConv, GraphConv\ndef uniform(size, tensor):\n    if tensor is not None:\n        bound = 1.0 / math.sqrt(size)\n        tensor.data.uniform_(-bound, bound)\ndef maybe_num_nodes(edge_index, num_nodes=None):\n    if num_nodes is not None:\n        return num_nodes\n    elif isinstance(edge_index, Tensor):\n        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n    else:\n        return max(edge_index.size(0), edge_index.size(1))\ndef topk(x, ratio, batch, min_score=None, tol=1e-7):\n    if min_score is not None:\n        scores_max = scatter_max(x, batch)[0][batch] - tol\n        scores_min = scores_max.clamp(max=min_score)\n        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n    else:\n        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n        cum_num_nodes = torch.cat(\n            [num_nodes.new_zeros(1),\n             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n        dense_x = x.new_full((batch_size * max_num_nodes, ),\n                             torch.finfo(x.dtype).min)\n        dense_x[index] = x\n        dense_x = dense_x.view(batch_size, max_num_nodes)\n        _, perm = dense_x.sort(dim=-1, descending=True)\n        perm = perm + cum_num_nodes.view(-1, 1)\n        perm = perm.view(-1)\n        if isinstance(ratio, int):\n            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n            k = torch.min(k, num_nodes)\n        else:\n            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n        mask = [\n            torch.arange(k[i], dtype=torch.long, device=x.device) +\n            i * max_num_nodes for i in range(batch_size)\n        ]\n        mask = torch.cat(mask, dim=0)\n        perm = perm[mask]\n    return perm\ndef filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n    mask = perm.new_full((num_nodes, ), -1)\n    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n    mask[perm] = i\n    row, col = edge_index\n    row, col = mask[row], mask[col]\n    mask = (row >= 0) & (col >= 0)\n    row, col = row[mask], col[mask]\n    if edge_attr is not None:\n        edge_attr = edge_attr[mask]\n    return torch.stack([row, col], dim=0), edge_attr\nclass GSAPool(torch.nn.Module):\n    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n                        min_score=None, multiplier=1,\n                        non_linearity=torch.tanh,\n                        cus_drop_ratio =0):\n        super(GSAPool,self).__init__()\n        self.in_channels = in_channels\n        self.ratio = pooling_ratio\n        self.alpha = alpha\n        self.sbtl_layer = GCNConv(in_channels,1)\n        self.fbtl_layer = nn.Linear(in_channels, 1)\n        self.fusion = GCNConv(in_channels,in_channels)\n        self.min_score = min_score\n        self.multiplier = multiplier\n        self.fusion_flag = 0\n        self.non_linearity = non_linearity\n        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n    def conv_selection(self, conv, in_channels, conv_type=0):\n        if(conv_type == 0):\n            out_channels = 1\n        elif(conv_type == 1):\n            out_channels = in_channels\n        if(conv == \"GCNConv\"):\n            return GCNConv(in_channels,out_channels)\n        elif(conv == \"ChebConv\"):\n            return ChebConv(in_channels,out_channels,1)\n        elif(conv == \"SAGEConv\"):\n            return SAGEConv(in_channels,out_channels)\n        elif(conv == \"GATConv\"):\n            return GATConv(in_channels,out_channels, heads=1, concat=True)\n        elif(conv == \"GraphConv\"):\n            return GraphConv(in_channels,out_channels)\n        else:\n            raise ValueError\n    def forward(self, x, edge_index, edge_attr=None, batch=None):\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        x = x.unsqueeze(-1) if x.dim() == 1 else x\n        score_s = self.sbtl_layer(x,edge_index).squeeze()\n        score_f = self.fbtl_layer(x).squeeze()\n        score = score_s*self.alpha + score_f*(1-self.alpha)\n        score = score.unsqueeze(-1) if score.dim()==0 else score\n        if self.min_score is None:\n            score = self.non_linearity(score)\n        else:\n            score = softmax(score, batch)\n        sc = self.dropout(score)\n        perm = topk(sc, self.ratio, batch)\n        if(self.fusion_flag == 1):\n            x = self.fusion(x, edge_index)\n        x_ae = x[perm]\n        x = x[perm] * score[perm].view(-1, 1)\n        x = self.multiplier * x if self.multiplier != 1 else x\n        batch = batch[perm]\n        edge_index, edge_attr = filter_adj(\n            edge_index, edge_attr, perm, num_nodes=score.size(0))\n        return x, edge_index, edge_attr, batch, perm, x_ae"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 042, Epoch: 010, Loss: 3.1351, Train Acc: 0.2226, Val Acc: 0.2992, Test Acc: 0.2945\n",
                        "Seed: 042, Epoch: 020, Loss: 2.5531, Train Acc: 0.3552, Val Acc: 0.3848, Test Acc: 0.3429\n",
                        "Seed: 042, Epoch: 030, Loss: 2.1509, Train Acc: 0.4393, Val Acc: 0.4487, Test Acc: 0.3909\n",
                        "Seed: 042, Epoch: 040, Loss: 1.9204, Train Acc: 0.4870, Val Acc: 0.4847, Test Acc: 0.4309\n",
                        "Seed: 042, Epoch: 050, Loss: 1.7517, Train Acc: 0.5223, Val Acc: 0.5113, Test Acc: 0.4510\n",
                        "Seed: 042, Epoch: 060, Loss: 1.6709, Train Acc: 0.5415, Val Acc: 0.5294, Test Acc: 0.4660\n",
                        "Seed: 042, Epoch: 070, Loss: 1.6126, Train Acc: 0.5539, Val Acc: 0.5405, Test Acc: 0.4756\n",
                        "Seed: 042, Epoch: 080, Loss: 1.5653, Train Acc: 0.5641, Val Acc: 0.5488, Test Acc: 0.4843\n",
                        "Seed: 042, Epoch: 090, Loss: 1.5510, Train Acc: 0.5673, Val Acc: 0.5489, Test Acc: 0.4833\n",
                        "Seed: 042, Epoch: 100, Loss: 1.5212, Train Acc: 0.5751, Val Acc: 0.5548, Test Acc: 0.4893\n",
                        "Seed: 042, Epoch: 110, Loss: 1.4951, Train Acc: 0.5793, Val Acc: 0.5569, Test Acc: 0.4932\n",
                        "Seed: 042, Epoch: 120, Loss: 1.4772, Train Acc: 0.5836, Val Acc: 0.5593, Test Acc: 0.4951\n",
                        "Seed: 042, Epoch: 130, Loss: 1.4642, Train Acc: 0.5863, Val Acc: 0.5598, Test Acc: 0.4927\n",
                        "Seed: 042, Epoch: 140, Loss: 1.4496, Train Acc: 0.5908, Val Acc: 0.5701, Test Acc: 0.5092\n",
                        "Seed: 042, Epoch: 150, Loss: 1.4347, Train Acc: 0.5938, Val Acc: 0.5697, Test Acc: 0.5053\n",
                        "Seed: 042, Epoch: 160, Loss: 1.4310, Train Acc: 0.5925, Val Acc: 0.5690, Test Acc: 0.5016\n",
                        "Seed: 042, Epoch: 170, Loss: 1.4229, Train Acc: 0.5975, Val Acc: 0.5695, Test Acc: 0.5030\n",
                        "Seed: 042, Epoch: 180, Loss: 1.4119, Train Acc: 0.5993, Val Acc: 0.5745, Test Acc: 0.5061\n",
                        "Seed: 042, Epoch: 190, Loss: 1.4036, Train Acc: 0.6000, Val Acc: 0.5719, Test Acc: 0.5028\n",
                        "Seed: 042, Epoch: 200, Loss: 1.3981, Train Acc: 0.6008, Val Acc: 0.5748, Test Acc: 0.5137\n",
                        "Seed: 042, Epoch: 210, Loss: 1.3928, Train Acc: 0.6037, Val Acc: 0.5779, Test Acc: 0.5096\n",
                        "Seed: 042, Epoch: 220, Loss: 1.3839, Train Acc: 0.6050, Val Acc: 0.5833, Test Acc: 0.5199\n",
                        "Seed: 042, Epoch: 230, Loss: 1.3850, Train Acc: 0.6061, Val Acc: 0.5827, Test Acc: 0.5183\n",
                        "Seed: 042, Epoch: 240, Loss: 1.3760, Train Acc: 0.6064, Val Acc: 0.5750, Test Acc: 0.5105\n",
                        "Seed: 042, Epoch: 250, Loss: 1.4040, Train Acc: 0.6051, Val Acc: 0.5867, Test Acc: 0.5285\n",
                        "Seed: 042, Epoch: 260, Loss: 1.3713, Train Acc: 0.6073, Val Acc: 0.5736, Test Acc: 0.5074\n",
                        "Seed: 042, Epoch: 270, Loss: 1.3687, Train Acc: 0.6079, Val Acc: 0.5836, Test Acc: 0.5222\n",
                        "Seed: 042, Epoch: 280, Loss: 1.3631, Train Acc: 0.6098, Val Acc: 0.5813, Test Acc: 0.5224\n",
                        "Seed: 042, Epoch: 290, Loss: 1.3556, Train Acc: 0.6103, Val Acc: 0.5794, Test Acc: 0.5129\n",
                        "Seed: 042, Epoch: 300, Loss: 1.3503, Train Acc: 0.6123, Val Acc: 0.5820, Test Acc: 0.5203\n",
                        "Seed: 042, Epoch: 310, Loss: 1.3514, Train Acc: 0.6121, Val Acc: 0.5848, Test Acc: 0.5271\n",
                        "Seed: 042, Epoch: 320, Loss: 1.3554, Train Acc: 0.6112, Val Acc: 0.5834, Test Acc: 0.5246\n",
                        "Seed: 042, Epoch: 330, Loss: 1.3426, Train Acc: 0.6154, Val Acc: 0.5833, Test Acc: 0.5179\n",
                        "Seed: 042, Epoch: 340, Loss: 1.3369, Train Acc: 0.6157, Val Acc: 0.5814, Test Acc: 0.5153\n",
                        "Seed: 042, Epoch: 350, Loss: 1.3363, Train Acc: 0.6165, Val Acc: 0.5865, Test Acc: 0.5230\n",
                        "Seed: 042, Epoch: 360, Loss: 1.3351, Train Acc: 0.6158, Val Acc: 0.5870, Test Acc: 0.5270\n",
                        "Seed: 042, Epoch: 370, Loss: 1.3319, Train Acc: 0.6167, Val Acc: 0.5833, Test Acc: 0.5176\n",
                        "Seed: 042, Epoch: 380, Loss: 1.3265, Train Acc: 0.6177, Val Acc: 0.5877, Test Acc: 0.5262\n",
                        "Seed: 042, Epoch: 390, Loss: 1.3236, Train Acc: 0.6170, Val Acc: 0.5873, Test Acc: 0.5263\n",
                        "Seed: 042, Epoch: 400, Loss: 1.3252, Train Acc: 0.6189, Val Acc: 0.5851, Test Acc: 0.5187\n",
                        "Seed: 042, Epoch: 410, Loss: 1.3196, Train Acc: 0.6197, Val Acc: 0.5841, Test Acc: 0.5198\n",
                        "Seed: 042, Epoch: 420, Loss: 1.3150, Train Acc: 0.6210, Val Acc: 0.5889, Test Acc: 0.5247\n",
                        "Seed: 042, Epoch: 430, Loss: 1.3281, Train Acc: 0.6139, Val Acc: 0.5742, Test Acc: 0.5057\n",
                        "Seed: 042, Epoch: 440, Loss: 1.3214, Train Acc: 0.6204, Val Acc: 0.5850, Test Acc: 0.5195\n",
                        "Seed: 042, Epoch: 450, Loss: 1.3144, Train Acc: 0.6207, Val Acc: 0.5842, Test Acc: 0.5210\n",
                        "Seed: 042, Epoch: 460, Loss: 1.3133, Train Acc: 0.6213, Val Acc: 0.5883, Test Acc: 0.5250\n",
                        "Seed: 042, Epoch: 470, Loss: 1.3111, Train Acc: 0.6225, Val Acc: 0.5845, Test Acc: 0.5195\n",
                        "Seed: 042, Epoch: 480, Loss: 1.3061, Train Acc: 0.6218, Val Acc: 0.5862, Test Acc: 0.5233\n",
                        "Seed: 042, Epoch: 490, Loss: 1.3133, Train Acc: 0.6221, Val Acc: 0.5857, Test Acc: 0.5220\n",
                        "Seed: 042, Epoch: 500, Loss: 1.3021, Train Acc: 0.6232, Val Acc: 0.5897, Test Acc: 0.5305\n",
                        "Seed: 123, Epoch: 010, Loss: 3.1682, Train Acc: 0.2616, Val Acc: 0.2511, Test Acc: 0.2445\n",
                        "Seed: 123, Epoch: 020, Loss: 2.6134, Train Acc: 0.3227, Val Acc: 0.3519, Test Acc: 0.3369\n",
                        "Seed: 123, Epoch: 030, Loss: 2.1776, Train Acc: 0.4197, Val Acc: 0.4319, Test Acc: 0.3763\n",
                        "Seed: 123, Epoch: 040, Loss: 1.9283, Train Acc: 0.4835, Val Acc: 0.4839, Test Acc: 0.4256\n",
                        "Seed: 123, Epoch: 050, Loss: 1.7570, Train Acc: 0.5218, Val Acc: 0.5064, Test Acc: 0.4450\n",
                        "Seed: 123, Epoch: 060, Loss: 1.6782, Train Acc: 0.5392, Val Acc: 0.5252, Test Acc: 0.4688\n",
                        "Seed: 123, Epoch: 070, Loss: 1.6188, Train Acc: 0.5531, Val Acc: 0.5344, Test Acc: 0.4723\n",
                        "Seed: 123, Epoch: 080, Loss: 1.5745, Train Acc: 0.5633, Val Acc: 0.5441, Test Acc: 0.4836\n",
                        "Seed: 123, Epoch: 090, Loss: 1.5460, Train Acc: 0.5702, Val Acc: 0.5508, Test Acc: 0.4858\n",
                        "Seed: 123, Epoch: 100, Loss: 1.5201, Train Acc: 0.5742, Val Acc: 0.5499, Test Acc: 0.4828\n",
                        "Seed: 123, Epoch: 110, Loss: 1.4999, Train Acc: 0.5812, Val Acc: 0.5604, Test Acc: 0.4967\n",
                        "Seed: 123, Epoch: 120, Loss: 1.4800, Train Acc: 0.5848, Val Acc: 0.5625, Test Acc: 0.4988\n",
                        "Seed: 123, Epoch: 130, Loss: 1.4683, Train Acc: 0.5844, Val Acc: 0.5639, Test Acc: 0.4986\n",
                        "Seed: 123, Epoch: 140, Loss: 1.4586, Train Acc: 0.5907, Val Acc: 0.5656, Test Acc: 0.4961\n",
                        "Seed: 123, Epoch: 150, Loss: 1.4418, Train Acc: 0.5937, Val Acc: 0.5676, Test Acc: 0.5011\n",
                        "Seed: 123, Epoch: 160, Loss: 1.4440, Train Acc: 0.5928, Val Acc: 0.5644, Test Acc: 0.4977\n",
                        "Seed: 123, Epoch: 170, Loss: 1.4263, Train Acc: 0.5964, Val Acc: 0.5653, Test Acc: 0.4991\n",
                        "Seed: 123, Epoch: 180, Loss: 1.4188, Train Acc: 0.5985, Val Acc: 0.5679, Test Acc: 0.5027\n",
                        "Seed: 123, Epoch: 190, Loss: 1.4100, Train Acc: 0.6007, Val Acc: 0.5751, Test Acc: 0.5123\n",
                        "Seed: 123, Epoch: 200, Loss: 1.4030, Train Acc: 0.6027, Val Acc: 0.5745, Test Acc: 0.5087\n",
                        "Seed: 123, Epoch: 210, Loss: 1.4097, Train Acc: 0.5996, Val Acc: 0.5672, Test Acc: 0.5022\n",
                        "Seed: 123, Epoch: 220, Loss: 1.3968, Train Acc: 0.6030, Val Acc: 0.5784, Test Acc: 0.5145\n",
                        "Seed: 123, Epoch: 230, Loss: 1.3859, Train Acc: 0.6056, Val Acc: 0.5728, Test Acc: 0.5074\n",
                        "Seed: 123, Epoch: 240, Loss: 1.3822, Train Acc: 0.6068, Val Acc: 0.5791, Test Acc: 0.5152\n",
                        "Seed: 123, Epoch: 250, Loss: 1.3803, Train Acc: 0.6070, Val Acc: 0.5804, Test Acc: 0.5189\n",
                        "Seed: 123, Epoch: 260, Loss: 1.3783, Train Acc: 0.6085, Val Acc: 0.5777, Test Acc: 0.5120\n",
                        "Seed: 123, Epoch: 270, Loss: 1.3706, Train Acc: 0.6102, Val Acc: 0.5794, Test Acc: 0.5140\n",
                        "Seed: 123, Epoch: 280, Loss: 1.3668, Train Acc: 0.6108, Val Acc: 0.5773, Test Acc: 0.5130\n",
                        "Seed: 123, Epoch: 290, Loss: 1.3704, Train Acc: 0.6110, Val Acc: 0.5809, Test Acc: 0.5163\n",
                        "Seed: 123, Epoch: 300, Loss: 1.3615, Train Acc: 0.6127, Val Acc: 0.5822, Test Acc: 0.5182\n",
                        "Seed: 123, Epoch: 310, Loss: 1.3553, Train Acc: 0.6134, Val Acc: 0.5798, Test Acc: 0.5165\n",
                        "Seed: 123, Epoch: 320, Loss: 1.3509, Train Acc: 0.6141, Val Acc: 0.5871, Test Acc: 0.5253\n",
                        "Seed: 123, Epoch: 330, Loss: 1.3475, Train Acc: 0.6144, Val Acc: 0.5869, Test Acc: 0.5282\n",
                        "Seed: 123, Epoch: 340, Loss: 1.3449, Train Acc: 0.6164, Val Acc: 0.5866, Test Acc: 0.5262\n",
                        "Seed: 123, Epoch: 350, Loss: 1.3410, Train Acc: 0.6157, Val Acc: 0.5844, Test Acc: 0.5219\n",
                        "Seed: 123, Epoch: 360, Loss: 1.3520, Train Acc: 0.6154, Val Acc: 0.5852, Test Acc: 0.5246\n",
                        "Seed: 123, Epoch: 370, Loss: 1.3398, Train Acc: 0.6166, Val Acc: 0.5808, Test Acc: 0.5179\n",
                        "Seed: 123, Epoch: 380, Loss: 1.3367, Train Acc: 0.6167, Val Acc: 0.5851, Test Acc: 0.5236\n",
                        "Seed: 123, Epoch: 390, Loss: 1.3374, Train Acc: 0.6186, Val Acc: 0.5875, Test Acc: 0.5267\n",
                        "Seed: 123, Epoch: 400, Loss: 1.3320, Train Acc: 0.6198, Val Acc: 0.5880, Test Acc: 0.5275\n",
                        "Seed: 123, Epoch: 410, Loss: 1.3318, Train Acc: 0.6196, Val Acc: 0.5907, Test Acc: 0.5338\n",
                        "Seed: 123, Epoch: 420, Loss: 1.3242, Train Acc: 0.6204, Val Acc: 0.5876, Test Acc: 0.5252\n",
                        "Seed: 123, Epoch: 430, Loss: 1.3245, Train Acc: 0.6212, Val Acc: 0.5871, Test Acc: 0.5263\n",
                        "Seed: 123, Epoch: 440, Loss: 1.3269, Train Acc: 0.6207, Val Acc: 0.5810, Test Acc: 0.5175\n",
                        "Seed: 123, Epoch: 450, Loss: 1.3227, Train Acc: 0.6207, Val Acc: 0.5893, Test Acc: 0.5367\n",
                        "Seed: 123, Epoch: 460, Loss: 1.3185, Train Acc: 0.6217, Val Acc: 0.5890, Test Acc: 0.5317\n",
                        "Seed: 123, Epoch: 470, Loss: 1.3133, Train Acc: 0.6235, Val Acc: 0.5898, Test Acc: 0.5272\n",
                        "Seed: 123, Epoch: 480, Loss: 1.3520, Train Acc: 0.6151, Val Acc: 0.5799, Test Acc: 0.5210\n",
                        "Seed: 123, Epoch: 490, Loss: 1.3162, Train Acc: 0.6198, Val Acc: 0.5877, Test Acc: 0.5298\n",
                        "Seed: 123, Epoch: 500, Loss: 1.3127, Train Acc: 0.6237, Val Acc: 0.5896, Test Acc: 0.5299\n",
                        "Seed: 456, Epoch: 010, Loss: 3.1123, Train Acc: 0.1911, Val Acc: 0.3130, Test Acc: 0.3394\n",
                        "Seed: 456, Epoch: 020, Loss: 2.5424, Train Acc: 0.3635, Val Acc: 0.4057, Test Acc: 0.3661\n",
                        "Seed: 456, Epoch: 030, Loss: 2.1058, Train Acc: 0.4435, Val Acc: 0.4479, Test Acc: 0.3879\n",
                        "Seed: 456, Epoch: 040, Loss: 1.8704, Train Acc: 0.5006, Val Acc: 0.4909, Test Acc: 0.4382\n",
                        "Seed: 456, Epoch: 050, Loss: 1.7392, Train Acc: 0.5296, Val Acc: 0.5074, Test Acc: 0.4413\n",
                        "Seed: 456, Epoch: 060, Loss: 1.6495, Train Acc: 0.5466, Val Acc: 0.5324, Test Acc: 0.4725\n",
                        "Seed: 456, Epoch: 070, Loss: 1.5924, Train Acc: 0.5569, Val Acc: 0.5377, Test Acc: 0.4704\n",
                        "Seed: 456, Epoch: 080, Loss: 1.5624, Train Acc: 0.5651, Val Acc: 0.5465, Test Acc: 0.4842\n",
                        "Seed: 456, Epoch: 090, Loss: 1.5283, Train Acc: 0.5722, Val Acc: 0.5494, Test Acc: 0.4846\n",
                        "Seed: 456, Epoch: 100, Loss: 1.5025, Train Acc: 0.5764, Val Acc: 0.5498, Test Acc: 0.4826\n",
                        "Seed: 456, Epoch: 110, Loss: 1.4863, Train Acc: 0.5821, Val Acc: 0.5572, Test Acc: 0.4949\n",
                        "Seed: 456, Epoch: 120, Loss: 1.4633, Train Acc: 0.5861, Val Acc: 0.5606, Test Acc: 0.4921\n",
                        "Seed: 456, Epoch: 130, Loss: 1.4553, Train Acc: 0.5887, Val Acc: 0.5615, Test Acc: 0.4944\n",
                        "Seed: 456, Epoch: 140, Loss: 1.4395, Train Acc: 0.5922, Val Acc: 0.5630, Test Acc: 0.4959\n",
                        "Seed: 456, Epoch: 150, Loss: 1.4301, Train Acc: 0.5950, Val Acc: 0.5718, Test Acc: 0.5081\n",
                        "Seed: 456, Epoch: 160, Loss: 1.4247, Train Acc: 0.5969, Val Acc: 0.5696, Test Acc: 0.5080\n",
                        "Seed: 456, Epoch: 170, Loss: 1.4151, Train Acc: 0.5974, Val Acc: 0.5662, Test Acc: 0.4994\n",
                        "Seed: 456, Epoch: 180, Loss: 1.4044, Train Acc: 0.6006, Val Acc: 0.5752, Test Acc: 0.5133\n",
                        "Seed: 456, Epoch: 190, Loss: 1.3963, Train Acc: 0.6037, Val Acc: 0.5764, Test Acc: 0.5134\n",
                        "Seed: 456, Epoch: 200, Loss: 1.4047, Train Acc: 0.6032, Val Acc: 0.5785, Test Acc: 0.5197\n",
                        "Seed: 456, Epoch: 210, Loss: 1.3842, Train Acc: 0.6062, Val Acc: 0.5773, Test Acc: 0.5111\n",
                        "Seed: 456, Epoch: 220, Loss: 1.3902, Train Acc: 0.6030, Val Acc: 0.5709, Test Acc: 0.5050\n",
                        "Seed: 456, Epoch: 230, Loss: 1.3807, Train Acc: 0.6076, Val Acc: 0.5781, Test Acc: 0.5134\n",
                        "Seed: 456, Epoch: 240, Loss: 1.3718, Train Acc: 0.6095, Val Acc: 0.5821, Test Acc: 0.5212\n",
                        "Seed: 456, Epoch: 250, Loss: 1.3673, Train Acc: 0.6102, Val Acc: 0.5811, Test Acc: 0.5178\n",
                        "Seed: 456, Epoch: 260, Loss: 1.3636, Train Acc: 0.6082, Val Acc: 0.5777, Test Acc: 0.5145\n",
                        "Seed: 456, Epoch: 270, Loss: 1.3580, Train Acc: 0.6117, Val Acc: 0.5818, Test Acc: 0.5179\n",
                        "Seed: 456, Epoch: 280, Loss: 1.3522, Train Acc: 0.6140, Val Acc: 0.5837, Test Acc: 0.5208\n",
                        "Seed: 456, Epoch: 290, Loss: 1.3853, Train Acc: 0.6113, Val Acc: 0.5837, Test Acc: 0.5207\n",
                        "Seed: 456, Epoch: 300, Loss: 1.3559, Train Acc: 0.6134, Val Acc: 0.5869, Test Acc: 0.5250\n",
                        "Seed: 456, Epoch: 310, Loss: 1.3463, Train Acc: 0.6156, Val Acc: 0.5830, Test Acc: 0.5167\n",
                        "Seed: 456, Epoch: 320, Loss: 1.3416, Train Acc: 0.6160, Val Acc: 0.5815, Test Acc: 0.5160\n",
                        "Seed: 456, Epoch: 330, Loss: 1.3368, Train Acc: 0.6165, Val Acc: 0.5865, Test Acc: 0.5254\n",
                        "Seed: 456, Epoch: 340, Loss: 1.3351, Train Acc: 0.6182, Val Acc: 0.5857, Test Acc: 0.5216\n",
                        "Seed: 456, Epoch: 350, Loss: 1.3301, Train Acc: 0.6188, Val Acc: 0.5842, Test Acc: 0.5207\n",
                        "Seed: 456, Epoch: 360, Loss: 1.3349, Train Acc: 0.6170, Val Acc: 0.5789, Test Acc: 0.5138\n",
                        "Seed: 456, Epoch: 370, Loss: 1.3257, Train Acc: 0.6199, Val Acc: 0.5867, Test Acc: 0.5282\n",
                        "Seed: 456, Epoch: 380, Loss: 1.3260, Train Acc: 0.6193, Val Acc: 0.5890, Test Acc: 0.5293\n",
                        "Seed: 456, Epoch: 390, Loss: 1.3201, Train Acc: 0.6210, Val Acc: 0.5872, Test Acc: 0.5248\n",
                        "Seed: 456, Epoch: 400, Loss: 1.3204, Train Acc: 0.6200, Val Acc: 0.5886, Test Acc: 0.5313\n",
                        "Seed: 456, Epoch: 410, Loss: 1.3328, Train Acc: 0.6140, Val Acc: 0.5808, Test Acc: 0.5144\n",
                        "Seed: 456, Epoch: 420, Loss: 1.3321, Train Acc: 0.6172, Val Acc: 0.5838, Test Acc: 0.5256\n",
                        "Seed: 456, Epoch: 430, Loss: 1.3421, Train Acc: 0.6158, Val Acc: 0.5904, Test Acc: 0.5372\n",
                        "Seed: 456, Epoch: 440, Loss: 1.3207, Train Acc: 0.6220, Val Acc: 0.5853, Test Acc: 0.5229\n",
                        "Seed: 456, Epoch: 450, Loss: 1.3133, Train Acc: 0.6223, Val Acc: 0.5870, Test Acc: 0.5250\n",
                        "Seed: 456, Epoch: 460, Loss: 1.3067, Train Acc: 0.6240, Val Acc: 0.5897, Test Acc: 0.5281\n",
                        "Seed: 456, Epoch: 470, Loss: 1.3043, Train Acc: 0.6244, Val Acc: 0.5913, Test Acc: 0.5306\n",
                        "Seed: 456, Epoch: 480, Loss: 1.3015, Train Acc: 0.6247, Val Acc: 0.5886, Test Acc: 0.5261\n",
                        "Seed: 456, Epoch: 490, Loss: 1.3001, Train Acc: 0.6248, Val Acc: 0.5916, Test Acc: 0.5333\n",
                        "Seed: 456, Epoch: 500, Loss: 1.3145, Train Acc: 0.6249, Val Acc: 0.5886, Test Acc: 0.5269\n",
                        "Seed: 42, Best Val Acc: 0.5916, Corresponding Test Acc: 0.5343, Time: 70.46s, Memory: 1.61MB, GPU Memory: 5035.75MB\n",
                        "Seed: 123, Best Val Acc: 0.5938, Corresponding Test Acc: 0.5420, Time: 65.41s, Memory: 1.58MB, GPU Memory: 5033.00MB\n",
                        "Seed: 456, Best Val Acc: 0.5923, Corresponding Test Acc: 0.5364, Time: 73.04s, Memory: 1.58MB, GPU Memory: 5036.45MB\n",
                        "\n",
                        "Average Best Validation Accuracy: 0.5926\n",
                        "Average Corresponding Test Accuracy: 0.5376\n",
                        "Average Time: 69.64s\n",
                        "Average Memory Usage: 1.59MB\n",
                        "Average GPU Memory Usage: 5035.07MB\n"
                    ]
                }
            ],
            "source": "from torch_geometric.nn import ASAPooling\ndataset = dataset_sparse\ngraph = dataset[0]\nnum_classes = dataset.num_classes\nin_channels = dataset.num_features\nhidden_channels = 64\nout_channels = num_classes\ndepth = 2\npool_ratios = [0.5, 0.5]\nclass HierarchicalGCN_GSA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n        super(HierarchicalGCN_GSA, self).__init__()\n        assert depth >= 1\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.depth = depth\n        self.pool_ratios = pool_ratios\n        self.sum_res = sum_res\n        self.act = act\n        channels = self.hidden_channels\n        self.pools = torch.nn.ModuleList()\n        self.down_convs = torch.nn.ModuleList()\n        self.down_convs.append(GCNConv(self.in_channels, channels))\n        for i in range(self.depth):\n            self.pools.append(GSAPool(256, pooling_ratio=0.9, alpha = 0.6, cus_drop_ratio = 0))\n            self.down_convs.append(GCNConv(channels, channels))\n        in_channels = channels if sum_res else 2 * channels\n        self.up_convs = torch.nn.ModuleList()\n        for i in range(self.depth):\n            self.up_convs.append(GCNConv(in_channels, channels))\n        self.up_convs.append(GCNConv(channels, self.out_channels))\n    def forward(self, x, edge_index, batch=None):\n        x, edge_index = x.to(device), edge_index.to(device)\n        if batch is None:\n            batch = edge_index.new_zeros(x.size(0))\n        if batch is not None:\n            batch = batch.to(device)\n        x = F.dropout(x, p=0.0, training=self.training)\n        x = self.down_convs[0](x, edge_index)\n        x = F.relu(x)  \n        xs = [x]\n        edge_indices = [edge_index]\n        for i in range(1, self.depth + 1):\n            x, edge_index, edge_attr, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n            x = self.down_convs[i](x, edge_index)\n            x = F.relu(x)\n            if i < self.depth:\n                xs.append(x)\n                edge_indices.append(edge_index)\n        for i in range(self.depth):\n            j = self.depth - 1 - i\n            res = xs[j]\n            edge_index = edge_indices[j]\n            up = res\n            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n            x = self.up_convs[i](x, edge_index)\n            x = F.relu(x)\n        x = self.up_convs[-1](x, edge_index)\n        return x.log_softmax(dim=-1)\ndef train(model, data, train_idx, optimizer):\n    model.train()\n    optimizer.zero_grad()\n    out = model(data.x, data.edge_index)[train_idx]\n    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n    loss.backward()\n    optimizer.step()\n    return loss.item()\n@torch.no_grad()\ndef test(model, data, split_idx, evaluator):\n    model.eval()\n    out = model(data.x, data.edge_index)\n    y_pred = out.argmax(dim=-1, keepdim=True)\n    train_acc = evaluator.eval({\n        'y_true': data.y[split_idx['train']],\n        'y_pred': y_pred[split_idx['train']],\n    })['acc']\n    valid_acc = evaluator.eval({\n        'y_true': data.y[split_idx['valid']],\n        'y_pred': y_pred[split_idx['valid']],\n    })['acc']\n    test_acc = evaluator.eval({\n        'y_true': data.y[split_idx['test']],\n        'y_pred': y_pred[split_idx['test']],\n    })['acc']\n    return train_acc, valid_acc, test_acc\nseeds = [42, 123, 456]\nresults = []\nval_accuracies_list = []\ntimes = []\nmemories = []\ngpu_memories = []\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfor seed in seeds:\n    graph = graph.to(device)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    val_accuracies = []\n    test_accuracies = []\n    start_time = time.time()\n    tracemalloc.start()\n    model = HierarchicalGCN_GSA(\n        in_channels=graph.num_features,\n        hidden_channels=256,\n        out_channels=dataset.num_classes,\n        depth=2,\n        act=F.relu,\n        sum_res=False\n    ).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n    split_idx = dataset.get_idx_split()\n    train_idx = split_idx['train'].to(device)\n    for epoch in range(1, 501):\n        loss = train(model, graph, split_idx['train'], optimizer)\n        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n        val_accuracies.append(valid_acc)\n        test_accuracies.append(test_acc)\n        if epoch % 10 == 0:\n            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    elapsed_time = end_time - start_time\n    memory_usage = current / 10**6  \n    peak_memory_usage = peak / 10**6  \n    if torch.cuda.is_available():\n        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  \n        torch.cuda.reset_max_memory_allocated(device)\n    else:\n        gpu_memory_usage = 0\n    val_accuracies_list.append(val_accuracies)\n    times.append(elapsed_time)\n    memories.append(memory_usage)\n    gpu_memories.append(gpu_memory_usage)\n    best_val_acc = max(val_accuracies)\n    best_val_index = val_accuracies.index(best_val_acc)\n    corresponding_test_acc = test_accuracies[best_val_index]\n    results.append({\n        'seed': seed,\n        'best_val_acc': best_val_acc,\n        'corresponding_test_acc': corresponding_test_acc,\n        'elapsed_time': elapsed_time,\n        'memory_usage': memory_usage,\n        'gpu_memory_usage': gpu_memory_usage\n    })\nfor result in results:\n    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\navg_val_acc = np.mean([result['best_val_acc'] for result in results])\navg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\navg_time = np.mean(times)\navg_memory = np.mean(memories)\navg_gpu_memory = np.mean(gpu_memories)\nprint(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\nprint(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\nprint(f\"Average Time: {avg_time:.2f}s\")\nprint(f\"Average Memory Usage: {avg_memory:.2f}MB\")\nprint(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "CG-ODE",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}