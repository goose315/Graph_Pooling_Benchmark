{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch_geometric.datasets import GitHub\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "max_nodes = 150\n",
    "dataset_sparse = PygNodePropPredDataset(root='/data/Zeyu/Pooling', name=\"ogbn-arxiv\")\n",
    "evaluator = Evaluator(\"ogbn-arxiv\")\n",
    "eval_metric = dataset_sparse.eval_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 042, Epoch: 010, Loss: 3.2028, Train Acc: 0.1639, Val Acc: 0.2573, Test Acc: 0.2359\n",
      "Seed: 042, Epoch: 020, Loss: 2.5912, Train Acc: 0.3391, Val Acc: 0.3953, Test Acc: 0.3635\n",
      "Seed: 042, Epoch: 030, Loss: 2.1476, Train Acc: 0.4297, Val Acc: 0.4340, Test Acc: 0.3805\n",
      "Seed: 042, Epoch: 040, Loss: 1.9460, Train Acc: 0.4819, Val Acc: 0.4755, Test Acc: 0.4163\n",
      "Seed: 042, Epoch: 050, Loss: 1.7602, Train Acc: 0.5206, Val Acc: 0.5038, Test Acc: 0.4446\n",
      "Seed: 042, Epoch: 060, Loss: 1.6680, Train Acc: 0.5400, Val Acc: 0.5195, Test Acc: 0.4594\n",
      "Seed: 042, Epoch: 070, Loss: 1.6075, Train Acc: 0.5552, Val Acc: 0.5365, Test Acc: 0.4731\n",
      "Seed: 042, Epoch: 080, Loss: 1.5767, Train Acc: 0.5618, Val Acc: 0.5450, Test Acc: 0.4829\n",
      "Seed: 042, Epoch: 090, Loss: 1.5513, Train Acc: 0.5672, Val Acc: 0.5433, Test Acc: 0.4763\n",
      "Seed: 042, Epoch: 100, Loss: 1.5279, Train Acc: 0.5714, Val Acc: 0.5533, Test Acc: 0.4947\n",
      "Seed: 042, Epoch: 110, Loss: 1.5049, Train Acc: 0.5774, Val Acc: 0.5526, Test Acc: 0.4898\n",
      "Seed: 042, Epoch: 120, Loss: 1.4900, Train Acc: 0.5814, Val Acc: 0.5594, Test Acc: 0.4935\n",
      "Seed: 042, Epoch: 130, Loss: 1.4785, Train Acc: 0.5852, Val Acc: 0.5669, Test Acc: 0.5010\n",
      "Seed: 042, Epoch: 140, Loss: 1.4579, Train Acc: 0.5891, Val Acc: 0.5653, Test Acc: 0.4964\n",
      "Seed: 042, Epoch: 150, Loss: 1.4453, Train Acc: 0.5898, Val Acc: 0.5645, Test Acc: 0.5026\n",
      "Seed: 042, Epoch: 160, Loss: 1.4350, Train Acc: 0.5921, Val Acc: 0.5648, Test Acc: 0.4962\n",
      "Seed: 042, Epoch: 170, Loss: 1.4240, Train Acc: 0.5954, Val Acc: 0.5666, Test Acc: 0.4999\n",
      "Seed: 042, Epoch: 180, Loss: 1.4203, Train Acc: 0.5984, Val Acc: 0.5729, Test Acc: 0.5038\n",
      "Seed: 042, Epoch: 190, Loss: 1.4106, Train Acc: 0.6006, Val Acc: 0.5774, Test Acc: 0.5100\n",
      "Seed: 042, Epoch: 200, Loss: 1.4176, Train Acc: 0.5988, Val Acc: 0.5719, Test Acc: 0.5024\n",
      "Seed: 042, Epoch: 210, Loss: 1.3991, Train Acc: 0.6026, Val Acc: 0.5779, Test Acc: 0.5123\n",
      "Seed: 042, Epoch: 220, Loss: 1.3939, Train Acc: 0.6039, Val Acc: 0.5744, Test Acc: 0.5045\n",
      "Seed: 042, Epoch: 230, Loss: 1.3887, Train Acc: 0.6045, Val Acc: 0.5799, Test Acc: 0.5156\n",
      "Seed: 042, Epoch: 240, Loss: 1.3836, Train Acc: 0.6065, Val Acc: 0.5767, Test Acc: 0.5084\n",
      "Seed: 042, Epoch: 250, Loss: 1.3846, Train Acc: 0.6047, Val Acc: 0.5790, Test Acc: 0.5153\n",
      "Seed: 042, Epoch: 260, Loss: 1.3774, Train Acc: 0.6076, Val Acc: 0.5844, Test Acc: 0.5224\n",
      "Seed: 042, Epoch: 270, Loss: 1.3709, Train Acc: 0.6092, Val Acc: 0.5798, Test Acc: 0.5107\n",
      "Seed: 042, Epoch: 280, Loss: 1.3705, Train Acc: 0.6091, Val Acc: 0.5822, Test Acc: 0.5147\n",
      "Seed: 042, Epoch: 290, Loss: 1.3733, Train Acc: 0.6079, Val Acc: 0.5848, Test Acc: 0.5252\n",
      "Seed: 042, Epoch: 300, Loss: 1.3624, Train Acc: 0.6118, Val Acc: 0.5828, Test Acc: 0.5163\n",
      "Seed: 042, Epoch: 310, Loss: 1.3571, Train Acc: 0.6130, Val Acc: 0.5819, Test Acc: 0.5160\n",
      "Seed: 042, Epoch: 320, Loss: 1.3554, Train Acc: 0.6110, Val Acc: 0.5840, Test Acc: 0.5163\n",
      "Seed: 042, Epoch: 330, Loss: 1.3524, Train Acc: 0.6140, Val Acc: 0.5853, Test Acc: 0.5180\n",
      "Seed: 042, Epoch: 340, Loss: 1.3559, Train Acc: 0.6140, Val Acc: 0.5844, Test Acc: 0.5155\n",
      "Seed: 042, Epoch: 350, Loss: 1.3436, Train Acc: 0.6158, Val Acc: 0.5850, Test Acc: 0.5171\n",
      "Seed: 042, Epoch: 360, Loss: 1.3416, Train Acc: 0.6162, Val Acc: 0.5888, Test Acc: 0.5209\n",
      "Seed: 042, Epoch: 370, Loss: 1.3395, Train Acc: 0.6168, Val Acc: 0.5862, Test Acc: 0.5175\n",
      "Seed: 042, Epoch: 380, Loss: 1.3504, Train Acc: 0.6156, Val Acc: 0.5873, Test Acc: 0.5233\n",
      "Seed: 042, Epoch: 390, Loss: 1.3353, Train Acc: 0.6176, Val Acc: 0.5895, Test Acc: 0.5218\n",
      "Seed: 042, Epoch: 400, Loss: 1.3316, Train Acc: 0.6180, Val Acc: 0.5856, Test Acc: 0.5227\n",
      "Seed: 042, Epoch: 410, Loss: 1.3432, Train Acc: 0.6142, Val Acc: 0.5818, Test Acc: 0.5206\n",
      "Seed: 042, Epoch: 420, Loss: 1.3305, Train Acc: 0.6185, Val Acc: 0.5857, Test Acc: 0.5176\n",
      "Seed: 042, Epoch: 430, Loss: 1.3288, Train Acc: 0.6204, Val Acc: 0.5884, Test Acc: 0.5210\n",
      "Seed: 042, Epoch: 440, Loss: 1.3223, Train Acc: 0.6212, Val Acc: 0.5889, Test Acc: 0.5237\n",
      "Seed: 042, Epoch: 450, Loss: 1.3214, Train Acc: 0.6210, Val Acc: 0.5878, Test Acc: 0.5205\n",
      "Seed: 042, Epoch: 460, Loss: 1.3230, Train Acc: 0.6195, Val Acc: 0.5891, Test Acc: 0.5259\n",
      "Seed: 042, Epoch: 470, Loss: 1.3198, Train Acc: 0.6219, Val Acc: 0.5903, Test Acc: 0.5278\n",
      "Seed: 042, Epoch: 480, Loss: 1.3187, Train Acc: 0.6224, Val Acc: 0.5884, Test Acc: 0.5226\n",
      "Seed: 042, Epoch: 490, Loss: 1.3134, Train Acc: 0.6221, Val Acc: 0.5919, Test Acc: 0.5287\n",
      "Seed: 042, Epoch: 500, Loss: 1.3120, Train Acc: 0.6232, Val Acc: 0.5912, Test Acc: 0.5280\n",
      "Seed: 123, Epoch: 010, Loss: 3.0325, Train Acc: 0.2982, Val Acc: 0.3055, Test Acc: 0.2936\n",
      "Seed: 123, Epoch: 020, Loss: 2.4385, Train Acc: 0.3836, Val Acc: 0.4035, Test Acc: 0.3511\n",
      "Seed: 123, Epoch: 030, Loss: 2.1034, Train Acc: 0.4492, Val Acc: 0.4451, Test Acc: 0.3879\n",
      "Seed: 123, Epoch: 040, Loss: 1.8595, Train Acc: 0.5020, Val Acc: 0.4927, Test Acc: 0.4415\n",
      "Seed: 123, Epoch: 050, Loss: 1.7259, Train Acc: 0.5289, Val Acc: 0.5164, Test Acc: 0.4560\n",
      "Seed: 123, Epoch: 060, Loss: 1.6397, Train Acc: 0.5486, Val Acc: 0.5329, Test Acc: 0.4737\n",
      "Seed: 123, Epoch: 070, Loss: 1.5848, Train Acc: 0.5596, Val Acc: 0.5356, Test Acc: 0.4699\n",
      "Seed: 123, Epoch: 080, Loss: 1.5481, Train Acc: 0.5691, Val Acc: 0.5473, Test Acc: 0.4851\n",
      "Seed: 123, Epoch: 090, Loss: 1.5315, Train Acc: 0.5718, Val Acc: 0.5415, Test Acc: 0.4739\n",
      "Seed: 123, Epoch: 100, Loss: 1.4995, Train Acc: 0.5793, Val Acc: 0.5510, Test Acc: 0.4852\n",
      "Seed: 123, Epoch: 110, Loss: 1.4809, Train Acc: 0.5833, Val Acc: 0.5565, Test Acc: 0.4931\n",
      "Seed: 123, Epoch: 120, Loss: 1.4668, Train Acc: 0.5869, Val Acc: 0.5594, Test Acc: 0.4969\n",
      "Seed: 123, Epoch: 130, Loss: 1.4504, Train Acc: 0.5911, Val Acc: 0.5653, Test Acc: 0.5009\n",
      "Seed: 123, Epoch: 140, Loss: 1.4447, Train Acc: 0.5921, Val Acc: 0.5660, Test Acc: 0.5015\n",
      "Seed: 123, Epoch: 150, Loss: 1.4320, Train Acc: 0.5953, Val Acc: 0.5693, Test Acc: 0.5048\n",
      "Seed: 123, Epoch: 160, Loss: 1.4229, Train Acc: 0.5979, Val Acc: 0.5703, Test Acc: 0.5086\n",
      "Seed: 123, Epoch: 170, Loss: 1.4273, Train Acc: 0.5945, Val Acc: 0.5658, Test Acc: 0.5037\n",
      "Seed: 123, Epoch: 180, Loss: 1.4115, Train Acc: 0.5996, Val Acc: 0.5705, Test Acc: 0.5089\n",
      "Seed: 123, Epoch: 190, Loss: 1.4000, Train Acc: 0.6024, Val Acc: 0.5731, Test Acc: 0.5083\n",
      "Seed: 123, Epoch: 200, Loss: 1.3977, Train Acc: 0.6021, Val Acc: 0.5780, Test Acc: 0.5215\n",
      "Seed: 123, Epoch: 210, Loss: 1.3892, Train Acc: 0.6046, Val Acc: 0.5712, Test Acc: 0.5077\n",
      "Seed: 123, Epoch: 220, Loss: 1.3827, Train Acc: 0.6061, Val Acc: 0.5784, Test Acc: 0.5166\n",
      "Seed: 123, Epoch: 230, Loss: 1.3808, Train Acc: 0.6061, Val Acc: 0.5804, Test Acc: 0.5159\n",
      "Seed: 123, Epoch: 240, Loss: 1.3822, Train Acc: 0.6091, Val Acc: 0.5806, Test Acc: 0.5158\n",
      "Seed: 123, Epoch: 250, Loss: 1.3790, Train Acc: 0.6069, Val Acc: 0.5793, Test Acc: 0.5193\n",
      "Seed: 123, Epoch: 260, Loss: 1.3738, Train Acc: 0.6091, Val Acc: 0.5828, Test Acc: 0.5201\n",
      "Seed: 123, Epoch: 270, Loss: 1.3623, Train Acc: 0.6113, Val Acc: 0.5814, Test Acc: 0.5181\n",
      "Seed: 123, Epoch: 280, Loss: 1.3610, Train Acc: 0.6119, Val Acc: 0.5789, Test Acc: 0.5131\n",
      "Seed: 123, Epoch: 290, Loss: 1.3546, Train Acc: 0.6137, Val Acc: 0.5856, Test Acc: 0.5237\n",
      "Seed: 123, Epoch: 300, Loss: 1.3516, Train Acc: 0.6151, Val Acc: 0.5804, Test Acc: 0.5166\n",
      "Seed: 123, Epoch: 310, Loss: 1.3678, Train Acc: 0.6102, Val Acc: 0.5746, Test Acc: 0.5083\n",
      "Seed: 123, Epoch: 320, Loss: 1.3492, Train Acc: 0.6152, Val Acc: 0.5844, Test Acc: 0.5210\n",
      "Seed: 123, Epoch: 330, Loss: 1.3431, Train Acc: 0.6154, Val Acc: 0.5814, Test Acc: 0.5176\n",
      "Seed: 123, Epoch: 340, Loss: 1.3379, Train Acc: 0.6171, Val Acc: 0.5839, Test Acc: 0.5224\n",
      "Seed: 123, Epoch: 350, Loss: 1.3389, Train Acc: 0.6167, Val Acc: 0.5851, Test Acc: 0.5231\n",
      "Seed: 123, Epoch: 360, Loss: 1.3418, Train Acc: 0.6163, Val Acc: 0.5874, Test Acc: 0.5266\n",
      "Seed: 123, Epoch: 370, Loss: 1.3304, Train Acc: 0.6196, Val Acc: 0.5870, Test Acc: 0.5247\n",
      "Seed: 123, Epoch: 380, Loss: 1.3413, Train Acc: 0.6157, Val Acc: 0.5845, Test Acc: 0.5218\n",
      "Seed: 123, Epoch: 390, Loss: 1.3263, Train Acc: 0.6200, Val Acc: 0.5871, Test Acc: 0.5239\n",
      "Seed: 123, Epoch: 400, Loss: 1.3264, Train Acc: 0.6198, Val Acc: 0.5915, Test Acc: 0.5323\n",
      "Seed: 123, Epoch: 410, Loss: 1.3210, Train Acc: 0.6212, Val Acc: 0.5901, Test Acc: 0.5266\n",
      "Seed: 123, Epoch: 420, Loss: 1.3225, Train Acc: 0.6208, Val Acc: 0.5853, Test Acc: 0.5194\n",
      "Seed: 123, Epoch: 430, Loss: 1.3226, Train Acc: 0.6204, Val Acc: 0.5877, Test Acc: 0.5269\n",
      "Seed: 123, Epoch: 440, Loss: 1.3256, Train Acc: 0.6214, Val Acc: 0.5882, Test Acc: 0.5293\n",
      "Seed: 123, Epoch: 450, Loss: 1.3129, Train Acc: 0.6229, Val Acc: 0.5900, Test Acc: 0.5272\n",
      "Seed: 123, Epoch: 460, Loss: 1.3395, Train Acc: 0.6132, Val Acc: 0.5821, Test Acc: 0.5217\n",
      "Seed: 123, Epoch: 470, Loss: 1.3102, Train Acc: 0.6213, Val Acc: 0.5842, Test Acc: 0.5251\n",
      "Seed: 123, Epoch: 480, Loss: 1.3062, Train Acc: 0.6245, Val Acc: 0.5917, Test Acc: 0.5291\n",
      "Seed: 123, Epoch: 490, Loss: 1.3057, Train Acc: 0.6246, Val Acc: 0.5862, Test Acc: 0.5203\n",
      "Seed: 123, Epoch: 500, Loss: 1.3076, Train Acc: 0.6234, Val Acc: 0.5908, Test Acc: 0.5355\n",
      "Seed: 456, Epoch: 010, Loss: 3.2165, Train Acc: 0.2190, Val Acc: 0.1738, Test Acc: 0.1989\n",
      "Seed: 456, Epoch: 020, Loss: 2.6125, Train Acc: 0.3716, Val Acc: 0.3993, Test Acc: 0.3536\n",
      "Seed: 456, Epoch: 030, Loss: 2.1645, Train Acc: 0.4291, Val Acc: 0.4311, Test Acc: 0.3738\n",
      "Seed: 456, Epoch: 040, Loss: 1.9536, Train Acc: 0.4678, Val Acc: 0.4512, Test Acc: 0.4022\n",
      "Seed: 456, Epoch: 050, Loss: 1.7592, Train Acc: 0.5202, Val Acc: 0.4987, Test Acc: 0.4325\n",
      "Seed: 456, Epoch: 060, Loss: 1.6682, Train Acc: 0.5412, Val Acc: 0.5302, Test Acc: 0.4711\n",
      "Seed: 456, Epoch: 070, Loss: 1.6159, Train Acc: 0.5519, Val Acc: 0.5339, Test Acc: 0.4676\n",
      "Seed: 456, Epoch: 080, Loss: 1.5773, Train Acc: 0.5607, Val Acc: 0.5393, Test Acc: 0.4803\n",
      "Seed: 456, Epoch: 090, Loss: 1.5489, Train Acc: 0.5685, Val Acc: 0.5479, Test Acc: 0.4819\n",
      "Seed: 456, Epoch: 100, Loss: 1.5292, Train Acc: 0.5723, Val Acc: 0.5523, Test Acc: 0.4929\n",
      "Seed: 456, Epoch: 110, Loss: 1.5082, Train Acc: 0.5771, Val Acc: 0.5551, Test Acc: 0.4919\n",
      "Seed: 456, Epoch: 120, Loss: 1.4978, Train Acc: 0.5778, Val Acc: 0.5589, Test Acc: 0.4965\n",
      "Seed: 456, Epoch: 130, Loss: 1.4834, Train Acc: 0.5855, Val Acc: 0.5624, Test Acc: 0.4997\n",
      "Seed: 456, Epoch: 140, Loss: 1.4647, Train Acc: 0.5880, Val Acc: 0.5654, Test Acc: 0.5016\n",
      "Seed: 456, Epoch: 150, Loss: 1.4530, Train Acc: 0.5911, Val Acc: 0.5633, Test Acc: 0.4987\n",
      "Seed: 456, Epoch: 160, Loss: 1.4465, Train Acc: 0.5927, Val Acc: 0.5693, Test Acc: 0.5092\n",
      "Seed: 456, Epoch: 170, Loss: 1.4375, Train Acc: 0.5937, Val Acc: 0.5676, Test Acc: 0.5073\n",
      "Seed: 456, Epoch: 180, Loss: 1.4240, Train Acc: 0.5976, Val Acc: 0.5683, Test Acc: 0.5052\n",
      "Seed: 456, Epoch: 190, Loss: 1.4187, Train Acc: 0.5961, Val Acc: 0.5657, Test Acc: 0.5019\n",
      "Seed: 456, Epoch: 200, Loss: 1.4111, Train Acc: 0.5992, Val Acc: 0.5728, Test Acc: 0.5106\n",
      "Seed: 456, Epoch: 210, Loss: 1.4059, Train Acc: 0.6019, Val Acc: 0.5719, Test Acc: 0.5064\n",
      "Seed: 456, Epoch: 220, Loss: 1.3989, Train Acc: 0.6033, Val Acc: 0.5739, Test Acc: 0.5104\n",
      "Seed: 456, Epoch: 230, Loss: 1.4132, Train Acc: 0.6001, Val Acc: 0.5645, Test Acc: 0.4957\n",
      "Seed: 456, Epoch: 240, Loss: 1.3891, Train Acc: 0.6050, Val Acc: 0.5778, Test Acc: 0.5162\n",
      "Seed: 456, Epoch: 250, Loss: 1.3997, Train Acc: 0.6018, Val Acc: 0.5706, Test Acc: 0.5109\n",
      "Seed: 456, Epoch: 260, Loss: 1.3842, Train Acc: 0.6052, Val Acc: 0.5756, Test Acc: 0.5147\n",
      "Seed: 456, Epoch: 270, Loss: 1.3979, Train Acc: 0.6036, Val Acc: 0.5795, Test Acc: 0.5243\n",
      "Seed: 456, Epoch: 280, Loss: 1.3751, Train Acc: 0.6075, Val Acc: 0.5801, Test Acc: 0.5158\n",
      "Seed: 456, Epoch: 290, Loss: 1.3675, Train Acc: 0.6090, Val Acc: 0.5797, Test Acc: 0.5144\n",
      "Seed: 456, Epoch: 300, Loss: 1.3633, Train Acc: 0.6106, Val Acc: 0.5810, Test Acc: 0.5160\n",
      "Seed: 456, Epoch: 310, Loss: 1.3685, Train Acc: 0.6095, Val Acc: 0.5791, Test Acc: 0.5124\n",
      "Seed: 456, Epoch: 320, Loss: 1.3605, Train Acc: 0.6107, Val Acc: 0.5802, Test Acc: 0.5131\n",
      "Seed: 456, Epoch: 330, Loss: 1.3559, Train Acc: 0.6121, Val Acc: 0.5819, Test Acc: 0.5172\n",
      "Seed: 456, Epoch: 340, Loss: 1.3508, Train Acc: 0.6134, Val Acc: 0.5825, Test Acc: 0.5169\n",
      "Seed: 456, Epoch: 350, Loss: 1.4329, Train Acc: 0.6042, Val Acc: 0.5803, Test Acc: 0.5154\n",
      "Seed: 456, Epoch: 360, Loss: 1.3597, Train Acc: 0.6089, Val Acc: 0.5818, Test Acc: 0.5217\n",
      "Seed: 456, Epoch: 370, Loss: 1.3494, Train Acc: 0.6139, Val Acc: 0.5828, Test Acc: 0.5190\n",
      "Seed: 456, Epoch: 380, Loss: 1.3430, Train Acc: 0.6160, Val Acc: 0.5839, Test Acc: 0.5196\n",
      "Seed: 456, Epoch: 390, Loss: 1.3402, Train Acc: 0.6161, Val Acc: 0.5832, Test Acc: 0.5185\n",
      "Seed: 456, Epoch: 400, Loss: 1.3378, Train Acc: 0.6167, Val Acc: 0.5830, Test Acc: 0.5172\n",
      "Seed: 456, Epoch: 410, Loss: 1.3342, Train Acc: 0.6178, Val Acc: 0.5842, Test Acc: 0.5195\n",
      "Seed: 456, Epoch: 420, Loss: 1.3365, Train Acc: 0.6153, Val Acc: 0.5832, Test Acc: 0.5255\n",
      "Seed: 456, Epoch: 430, Loss: 1.3305, Train Acc: 0.6190, Val Acc: 0.5851, Test Acc: 0.5212\n",
      "Seed: 456, Epoch: 440, Loss: 1.3287, Train Acc: 0.6190, Val Acc: 0.5817, Test Acc: 0.5144\n",
      "Seed: 456, Epoch: 450, Loss: 1.3380, Train Acc: 0.6152, Val Acc: 0.5826, Test Acc: 0.5220\n",
      "Seed: 456, Epoch: 460, Loss: 1.3264, Train Acc: 0.6187, Val Acc: 0.5851, Test Acc: 0.5251\n",
      "Seed: 456, Epoch: 470, Loss: 1.3222, Train Acc: 0.6199, Val Acc: 0.5860, Test Acc: 0.5228\n",
      "Seed: 456, Epoch: 480, Loss: 1.3225, Train Acc: 0.6197, Val Acc: 0.5893, Test Acc: 0.5309\n",
      "Seed: 456, Epoch: 490, Loss: 1.3221, Train Acc: 0.6202, Val Acc: 0.5837, Test Acc: 0.5203\n",
      "Seed: 456, Epoch: 500, Loss: 1.3177, Train Acc: 0.6214, Val Acc: 0.5900, Test Acc: 0.5305\n",
      "Seed: 42, Best Val Acc: 0.5913, Corresponding Test Acc: 0.5376, Time: 121.57s, Memory: 2.31MB, GPU Memory: 6650.66MB\n",
      "Seed: 42, Best Val Acc: 0.5924, Corresponding Test Acc: 0.5315, Time: 61.82s, Memory: 28.71MB, GPU Memory: 5734.25MB\n",
      "Seed: 123, Best Val Acc: 0.5942, Corresponding Test Acc: 0.5348, Time: 73.23s, Memory: 1.52MB, GPU Memory: 4877.93MB\n",
      "Seed: 456, Best Val Acc: 0.5900, Corresponding Test Acc: 0.5305, Time: 72.16s, Memory: 1.52MB, GPU Memory: 4878.14MB\n",
      "\n",
      "Average Best Validation Accuracy: 0.5920\n",
      "Average Corresponding Test Accuracy: 0.5336\n",
      "Average Time: 82.19s\n",
      "Average Memory Usage: 8.52MB\n",
      "Average GPU Memory Usage: 5535.25MB\n",
      "\n",
      "Validation Accuracy Variance: 0.0000\n",
      "Test Accuracy Variance: 0.0000\n",
      "Time Variance: 536.5768\n",
      "Memory Usage Variance: 135.9798MB\n",
      "GPU Memory Usage Variance: 536897.2186MB\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import GitHub\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_TopK(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_TopK, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.sum_res = sum_res\n",
    "        self.act = act\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(TopKPooling(channels, ratio=0.9))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.0, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = F.relu(x)  # 这里的self.act应该是一个函数\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "\n",
    "            up = res\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "val_acc_variances = []\n",
    "test_acc_variances = []\n",
    "time_variances = []\n",
    "memory_variances = []\n",
    "gpu_memory_variances = []\n",
    "\n",
    "# Run the experiments\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    test_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    model = HierarchicalGCN_TopK(\n",
    "        in_channels=graph.num_features,\n",
    "        hidden_channels=256,\n",
    "        out_channels=dataset.num_classes,\n",
    "        depth=2,\n",
    "        act=F.relu,\n",
    "        sum_res=False\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    for epoch in range(1, 501):\n",
    "        loss = train(model, graph, split_idx['train'], optimizer)\n",
    "        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n",
    "        val_accuracies.append(valid_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    memory_usage = current / 10**6  # Convert to MB\n",
    "    peak_memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  # Convert to MB\n",
    "        torch.cuda.reset_max_memory_allocated(device)\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    val_accuracies_list.append(val_accuracies)\n",
    "    times.append(elapsed_time)\n",
    "    memories.append(memory_usage)\n",
    "    gpu_memories.append(gpu_memory_usage)\n",
    "\n",
    "    best_val_acc = max(val_accuracies)\n",
    "    best_val_index = val_accuracies.index(best_val_acc)\n",
    "    corresponding_test_acc = test_accuracies[best_val_index]\n",
    "\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'corresponding_test_acc': corresponding_test_acc,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'memory_usage': memory_usage,\n",
    "        'gpu_memory_usage': gpu_memory_usage\n",
    "    })\n",
    "\n",
    "# Calculate average and variance results\n",
    "avg_val_acc = np.mean([result['best_val_acc'] for result in results])\n",
    "avg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\n",
    "avg_time = np.mean(times)\n",
    "avg_memory = np.mean(memories)\n",
    "avg_gpu_memory = np.mean(gpu_memories)\n",
    "\n",
    "val_acc_variance = np.var([result['best_val_acc'] for result in results])\n",
    "test_acc_variance = np.var([result['corresponding_test_acc'] for result in results])\n",
    "time_variance = np.var(times)\n",
    "memory_variance = np.var(memories)\n",
    "gpu_memory_variance = np.var(gpu_memories)\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n",
    "          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n",
    "          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n",
    "          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\n",
    "\n",
    "print(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "print(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\n",
    "print(f\"Average Time: {avg_time:.2f}s\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f}MB\")\n",
    "print(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")\n",
    "\n",
    "# Display variances\n",
    "print(f\"\\nValidation Accuracy Variance: {val_acc_variance:.4f}\")\n",
    "print(f\"Test Accuracy Variance: {test_acc_variance:.4f}\")\n",
    "print(f\"Time Variance: {time_variance:.4f}\")\n",
    "print(f\"Memory Usage Variance: {memory_variance:.4f}MB\")\n",
    "print(f\"GPU Memory Usage Variance: {gpu_memory_variance:.4f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 042, Epoch: 010, Loss: 2.9403, Train Acc: 0.2903, Val Acc: 0.3134, Test Acc: 0.2990\n",
      "Seed: 042, Epoch: 020, Loss: 2.4534, Train Acc: 0.3768, Val Acc: 0.4007, Test Acc: 0.3550\n",
      "Seed: 042, Epoch: 030, Loss: 2.0780, Train Acc: 0.4409, Val Acc: 0.4381, Test Acc: 0.3763\n",
      "Seed: 042, Epoch: 040, Loss: 1.8896, Train Acc: 0.4966, Val Acc: 0.4855, Test Acc: 0.4264\n",
      "Seed: 042, Epoch: 050, Loss: 1.7429, Train Acc: 0.5291, Val Acc: 0.5180, Test Acc: 0.4522\n",
      "Seed: 042, Epoch: 060, Loss: 1.6436, Train Acc: 0.5456, Val Acc: 0.5330, Test Acc: 0.4698\n",
      "Seed: 042, Epoch: 070, Loss: 1.5837, Train Acc: 0.5602, Val Acc: 0.5428, Test Acc: 0.4767\n",
      "Seed: 042, Epoch: 080, Loss: 1.5461, Train Acc: 0.5676, Val Acc: 0.5466, Test Acc: 0.4823\n",
      "Seed: 042, Epoch: 090, Loss: 1.5217, Train Acc: 0.5741, Val Acc: 0.5520, Test Acc: 0.4874\n",
      "Seed: 042, Epoch: 100, Loss: 1.5000, Train Acc: 0.5785, Val Acc: 0.5563, Test Acc: 0.4945\n",
      "Seed: 042, Epoch: 110, Loss: 1.4831, Train Acc: 0.5831, Val Acc: 0.5609, Test Acc: 0.4980\n",
      "Seed: 042, Epoch: 120, Loss: 1.4696, Train Acc: 0.5839, Val Acc: 0.5591, Test Acc: 0.4938\n",
      "Seed: 042, Epoch: 130, Loss: 1.4607, Train Acc: 0.5892, Val Acc: 0.5654, Test Acc: 0.5014\n",
      "Seed: 042, Epoch: 140, Loss: 1.4439, Train Acc: 0.5925, Val Acc: 0.5689, Test Acc: 0.5054\n",
      "Seed: 042, Epoch: 150, Loss: 1.4335, Train Acc: 0.5946, Val Acc: 0.5726, Test Acc: 0.5064\n",
      "Seed: 042, Epoch: 160, Loss: 1.4228, Train Acc: 0.5973, Val Acc: 0.5678, Test Acc: 0.5038\n",
      "Seed: 042, Epoch: 170, Loss: 1.4172, Train Acc: 0.6001, Val Acc: 0.5719, Test Acc: 0.5079\n",
      "Seed: 042, Epoch: 180, Loss: 1.4078, Train Acc: 0.6006, Val Acc: 0.5760, Test Acc: 0.5146\n",
      "Seed: 042, Epoch: 190, Loss: 1.4066, Train Acc: 0.6024, Val Acc: 0.5803, Test Acc: 0.5161\n",
      "Seed: 042, Epoch: 200, Loss: 1.3946, Train Acc: 0.6023, Val Acc: 0.5771, Test Acc: 0.5141\n",
      "Seed: 042, Epoch: 210, Loss: 1.3863, Train Acc: 0.6053, Val Acc: 0.5787, Test Acc: 0.5117\n",
      "Seed: 042, Epoch: 220, Loss: 1.3853, Train Acc: 0.6065, Val Acc: 0.5790, Test Acc: 0.5130\n",
      "Seed: 042, Epoch: 230, Loss: 1.3841, Train Acc: 0.6072, Val Acc: 0.5767, Test Acc: 0.5104\n",
      "Seed: 042, Epoch: 240, Loss: 1.3700, Train Acc: 0.6088, Val Acc: 0.5798, Test Acc: 0.5114\n",
      "Seed: 042, Epoch: 250, Loss: 1.3695, Train Acc: 0.6089, Val Acc: 0.5812, Test Acc: 0.5147\n",
      "Seed: 042, Epoch: 260, Loss: 1.3682, Train Acc: 0.6111, Val Acc: 0.5818, Test Acc: 0.5166\n",
      "Seed: 042, Epoch: 270, Loss: 1.3635, Train Acc: 0.6096, Val Acc: 0.5794, Test Acc: 0.5148\n",
      "Seed: 042, Epoch: 280, Loss: 1.3553, Train Acc: 0.6114, Val Acc: 0.5786, Test Acc: 0.5124\n",
      "Seed: 042, Epoch: 290, Loss: 1.3525, Train Acc: 0.6124, Val Acc: 0.5832, Test Acc: 0.5194\n",
      "Seed: 042, Epoch: 300, Loss: 1.3510, Train Acc: 0.6137, Val Acc: 0.5852, Test Acc: 0.5191\n",
      "Seed: 042, Epoch: 310, Loss: 1.3466, Train Acc: 0.6146, Val Acc: 0.5844, Test Acc: 0.5205\n",
      "Seed: 042, Epoch: 320, Loss: 1.3424, Train Acc: 0.6162, Val Acc: 0.5853, Test Acc: 0.5182\n",
      "Seed: 042, Epoch: 330, Loss: 1.3483, Train Acc: 0.6158, Val Acc: 0.5837, Test Acc: 0.5196\n",
      "Seed: 042, Epoch: 340, Loss: 1.3381, Train Acc: 0.6165, Val Acc: 0.5875, Test Acc: 0.5272\n",
      "Seed: 042, Epoch: 350, Loss: 1.3394, Train Acc: 0.6178, Val Acc: 0.5883, Test Acc: 0.5275\n",
      "Seed: 042, Epoch: 360, Loss: 1.3389, Train Acc: 0.6157, Val Acc: 0.5796, Test Acc: 0.5230\n",
      "Seed: 042, Epoch: 370, Loss: 1.3266, Train Acc: 0.6183, Val Acc: 0.5864, Test Acc: 0.5227\n",
      "Seed: 042, Epoch: 380, Loss: 1.3327, Train Acc: 0.6183, Val Acc: 0.5871, Test Acc: 0.5231\n",
      "Seed: 042, Epoch: 390, Loss: 1.3253, Train Acc: 0.6202, Val Acc: 0.5886, Test Acc: 0.5289\n",
      "Seed: 042, Epoch: 400, Loss: 1.3252, Train Acc: 0.6195, Val Acc: 0.5894, Test Acc: 0.5361\n",
      "Seed: 042, Epoch: 410, Loss: 1.3184, Train Acc: 0.6216, Val Acc: 0.5877, Test Acc: 0.5245\n",
      "Seed: 042, Epoch: 420, Loss: 1.3470, Train Acc: 0.6094, Val Acc: 0.5764, Test Acc: 0.5119\n",
      "Seed: 042, Epoch: 430, Loss: 1.3166, Train Acc: 0.6187, Val Acc: 0.5867, Test Acc: 0.5360\n",
      "Seed: 042, Epoch: 440, Loss: 1.3158, Train Acc: 0.6216, Val Acc: 0.5881, Test Acc: 0.5273\n",
      "Seed: 042, Epoch: 450, Loss: 1.3120, Train Acc: 0.6218, Val Acc: 0.5840, Test Acc: 0.5247\n",
      "Seed: 042, Epoch: 460, Loss: 1.3191, Train Acc: 0.6229, Val Acc: 0.5862, Test Acc: 0.5248\n",
      "Seed: 042, Epoch: 470, Loss: 1.3098, Train Acc: 0.6223, Val Acc: 0.5869, Test Acc: 0.5264\n",
      "Seed: 042, Epoch: 480, Loss: 1.3053, Train Acc: 0.6243, Val Acc: 0.5908, Test Acc: 0.5299\n",
      "Seed: 042, Epoch: 490, Loss: 1.3047, Train Acc: 0.6245, Val Acc: 0.5865, Test Acc: 0.5234\n",
      "Seed: 042, Epoch: 500, Loss: 1.2998, Train Acc: 0.6252, Val Acc: 0.5926, Test Acc: 0.5330\n",
      "Seed: 123, Epoch: 010, Loss: 2.8968, Train Acc: 0.2945, Val Acc: 0.3117, Test Acc: 0.3064\n",
      "Seed: 123, Epoch: 020, Loss: 2.4204, Train Acc: 0.3916, Val Acc: 0.4144, Test Acc: 0.3670\n",
      "Seed: 123, Epoch: 030, Loss: 2.0850, Train Acc: 0.4486, Val Acc: 0.4463, Test Acc: 0.3858\n",
      "Seed: 123, Epoch: 040, Loss: 1.8441, Train Acc: 0.5078, Val Acc: 0.5019, Test Acc: 0.4400\n",
      "Seed: 123, Epoch: 050, Loss: 1.6811, Train Acc: 0.5394, Val Acc: 0.5214, Test Acc: 0.4604\n",
      "Seed: 123, Epoch: 060, Loss: 1.6118, Train Acc: 0.5535, Val Acc: 0.5311, Test Acc: 0.4665\n",
      "Seed: 123, Epoch: 070, Loss: 1.5649, Train Acc: 0.5655, Val Acc: 0.5433, Test Acc: 0.4803\n",
      "Seed: 123, Epoch: 080, Loss: 1.5338, Train Acc: 0.5721, Val Acc: 0.5530, Test Acc: 0.4915\n",
      "Seed: 123, Epoch: 090, Loss: 1.5117, Train Acc: 0.5775, Val Acc: 0.5545, Test Acc: 0.4890\n",
      "Seed: 123, Epoch: 100, Loss: 1.5059, Train Acc: 0.5775, Val Acc: 0.5570, Test Acc: 0.4984\n",
      "Seed: 123, Epoch: 110, Loss: 1.4733, Train Acc: 0.5860, Val Acc: 0.5624, Test Acc: 0.4997\n",
      "Seed: 123, Epoch: 120, Loss: 1.4585, Train Acc: 0.5890, Val Acc: 0.5619, Test Acc: 0.5011\n",
      "Seed: 123, Epoch: 130, Loss: 1.4646, Train Acc: 0.5854, Val Acc: 0.5632, Test Acc: 0.5126\n",
      "Seed: 123, Epoch: 140, Loss: 1.4347, Train Acc: 0.5932, Val Acc: 0.5657, Test Acc: 0.5013\n",
      "Seed: 123, Epoch: 150, Loss: 1.4250, Train Acc: 0.5963, Val Acc: 0.5721, Test Acc: 0.5132\n",
      "Seed: 123, Epoch: 160, Loss: 1.4231, Train Acc: 0.5990, Val Acc: 0.5716, Test Acc: 0.5101\n",
      "Seed: 123, Epoch: 170, Loss: 1.4114, Train Acc: 0.6004, Val Acc: 0.5716, Test Acc: 0.5113\n",
      "Seed: 123, Epoch: 180, Loss: 1.4209, Train Acc: 0.6000, Val Acc: 0.5687, Test Acc: 0.5076\n",
      "Seed: 123, Epoch: 190, Loss: 1.3951, Train Acc: 0.6029, Val Acc: 0.5772, Test Acc: 0.5119\n",
      "Seed: 123, Epoch: 200, Loss: 1.3933, Train Acc: 0.6043, Val Acc: 0.5762, Test Acc: 0.5121\n",
      "Seed: 123, Epoch: 210, Loss: 1.3929, Train Acc: 0.6047, Val Acc: 0.5714, Test Acc: 0.5055\n",
      "Seed: 123, Epoch: 220, Loss: 1.3814, Train Acc: 0.6060, Val Acc: 0.5830, Test Acc: 0.5234\n",
      "Seed: 123, Epoch: 230, Loss: 1.3762, Train Acc: 0.6068, Val Acc: 0.5786, Test Acc: 0.5132\n",
      "Seed: 123, Epoch: 240, Loss: 1.3803, Train Acc: 0.6048, Val Acc: 0.5810, Test Acc: 0.5267\n",
      "Seed: 123, Epoch: 250, Loss: 1.3670, Train Acc: 0.6087, Val Acc: 0.5815, Test Acc: 0.5206\n",
      "Seed: 123, Epoch: 260, Loss: 1.3649, Train Acc: 0.6089, Val Acc: 0.5795, Test Acc: 0.5162\n",
      "Seed: 123, Epoch: 270, Loss: 1.3599, Train Acc: 0.6124, Val Acc: 0.5832, Test Acc: 0.5205\n",
      "Seed: 123, Epoch: 280, Loss: 1.3539, Train Acc: 0.6130, Val Acc: 0.5818, Test Acc: 0.5181\n",
      "Seed: 123, Epoch: 290, Loss: 1.3553, Train Acc: 0.6117, Val Acc: 0.5818, Test Acc: 0.5225\n",
      "Seed: 123, Epoch: 300, Loss: 1.3530, Train Acc: 0.6133, Val Acc: 0.5821, Test Acc: 0.5152\n",
      "Seed: 123, Epoch: 310, Loss: 1.3471, Train Acc: 0.6142, Val Acc: 0.5828, Test Acc: 0.5199\n",
      "Seed: 123, Epoch: 320, Loss: 1.3418, Train Acc: 0.6156, Val Acc: 0.5826, Test Acc: 0.5189\n",
      "Seed: 123, Epoch: 330, Loss: 1.3430, Train Acc: 0.6138, Val Acc: 0.5881, Test Acc: 0.5278\n",
      "Seed: 123, Epoch: 340, Loss: 1.3363, Train Acc: 0.6178, Val Acc: 0.5849, Test Acc: 0.5207\n",
      "Seed: 123, Epoch: 350, Loss: 1.3345, Train Acc: 0.6187, Val Acc: 0.5857, Test Acc: 0.5230\n",
      "Seed: 123, Epoch: 360, Loss: 1.3301, Train Acc: 0.6182, Val Acc: 0.5862, Test Acc: 0.5214\n",
      "Seed: 123, Epoch: 370, Loss: 1.3306, Train Acc: 0.6186, Val Acc: 0.5835, Test Acc: 0.5187\n",
      "Seed: 123, Epoch: 380, Loss: 1.3253, Train Acc: 0.6202, Val Acc: 0.5849, Test Acc: 0.5203\n",
      "Seed: 123, Epoch: 390, Loss: 1.3388, Train Acc: 0.6161, Val Acc: 0.5816, Test Acc: 0.5220\n",
      "Seed: 123, Epoch: 400, Loss: 1.3262, Train Acc: 0.6202, Val Acc: 0.5835, Test Acc: 0.5164\n",
      "Seed: 123, Epoch: 410, Loss: 1.3159, Train Acc: 0.6215, Val Acc: 0.5908, Test Acc: 0.5282\n",
      "Seed: 123, Epoch: 420, Loss: 1.3306, Train Acc: 0.6219, Val Acc: 0.5898, Test Acc: 0.5300\n",
      "Seed: 123, Epoch: 430, Loss: 1.3166, Train Acc: 0.6220, Val Acc: 0.5852, Test Acc: 0.5188\n",
      "Seed: 123, Epoch: 440, Loss: 1.3115, Train Acc: 0.6228, Val Acc: 0.5919, Test Acc: 0.5292\n",
      "Seed: 123, Epoch: 450, Loss: 1.3126, Train Acc: 0.6216, Val Acc: 0.5818, Test Acc: 0.5175\n",
      "Seed: 123, Epoch: 460, Loss: 1.3128, Train Acc: 0.6243, Val Acc: 0.5876, Test Acc: 0.5225\n",
      "Seed: 123, Epoch: 470, Loss: 1.3023, Train Acc: 0.6242, Val Acc: 0.5900, Test Acc: 0.5311\n",
      "Seed: 123, Epoch: 480, Loss: 1.3063, Train Acc: 0.6232, Val Acc: 0.5876, Test Acc: 0.5264\n",
      "Seed: 123, Epoch: 490, Loss: 1.3020, Train Acc: 0.6229, Val Acc: 0.5880, Test Acc: 0.5240\n",
      "Seed: 123, Epoch: 500, Loss: 1.3121, Train Acc: 0.6205, Val Acc: 0.5859, Test Acc: 0.5280\n",
      "Seed: 456, Epoch: 010, Loss: 3.0682, Train Acc: 0.2463, Val Acc: 0.2139, Test Acc: 0.1799\n",
      "Seed: 456, Epoch: 020, Loss: 2.5427, Train Acc: 0.3507, Val Acc: 0.3606, Test Acc: 0.3320\n",
      "Seed: 456, Epoch: 030, Loss: 2.1026, Train Acc: 0.4379, Val Acc: 0.4484, Test Acc: 0.3919\n",
      "Seed: 456, Epoch: 040, Loss: 1.8896, Train Acc: 0.4904, Val Acc: 0.4931, Test Acc: 0.4313\n",
      "Seed: 456, Epoch: 050, Loss: 1.7226, Train Acc: 0.5291, Val Acc: 0.5122, Test Acc: 0.4506\n",
      "Seed: 456, Epoch: 060, Loss: 1.6422, Train Acc: 0.5457, Val Acc: 0.5250, Test Acc: 0.4610\n",
      "Seed: 456, Epoch: 070, Loss: 1.5833, Train Acc: 0.5605, Val Acc: 0.5392, Test Acc: 0.4751\n",
      "Seed: 456, Epoch: 080, Loss: 1.5657, Train Acc: 0.5652, Val Acc: 0.5408, Test Acc: 0.4750\n",
      "Seed: 456, Epoch: 090, Loss: 1.5200, Train Acc: 0.5741, Val Acc: 0.5541, Test Acc: 0.4911\n",
      "Seed: 456, Epoch: 100, Loss: 1.5008, Train Acc: 0.5753, Val Acc: 0.5495, Test Acc: 0.4808\n",
      "Seed: 456, Epoch: 110, Loss: 1.4790, Train Acc: 0.5828, Val Acc: 0.5567, Test Acc: 0.4917\n",
      "Seed: 456, Epoch: 120, Loss: 1.4631, Train Acc: 0.5870, Val Acc: 0.5596, Test Acc: 0.4936\n",
      "Seed: 456, Epoch: 130, Loss: 1.4533, Train Acc: 0.5890, Val Acc: 0.5661, Test Acc: 0.5047\n",
      "Seed: 456, Epoch: 140, Loss: 1.4389, Train Acc: 0.5925, Val Acc: 0.5669, Test Acc: 0.5052\n",
      "Seed: 456, Epoch: 150, Loss: 1.4390, Train Acc: 0.5931, Val Acc: 0.5634, Test Acc: 0.4982\n",
      "Seed: 456, Epoch: 160, Loss: 1.4243, Train Acc: 0.5960, Val Acc: 0.5660, Test Acc: 0.5026\n",
      "Seed: 456, Epoch: 170, Loss: 1.4292, Train Acc: 0.5910, Val Acc: 0.5660, Test Acc: 0.5104\n",
      "Seed: 456, Epoch: 180, Loss: 1.4114, Train Acc: 0.5989, Val Acc: 0.5686, Test Acc: 0.5066\n",
      "Seed: 456, Epoch: 190, Loss: 1.4030, Train Acc: 0.6004, Val Acc: 0.5709, Test Acc: 0.5082\n",
      "Seed: 456, Epoch: 200, Loss: 1.3974, Train Acc: 0.6023, Val Acc: 0.5736, Test Acc: 0.5112\n",
      "Seed: 456, Epoch: 210, Loss: 1.3931, Train Acc: 0.6031, Val Acc: 0.5750, Test Acc: 0.5129\n",
      "Seed: 456, Epoch: 220, Loss: 1.3877, Train Acc: 0.6044, Val Acc: 0.5726, Test Acc: 0.5073\n",
      "Seed: 456, Epoch: 230, Loss: 1.3843, Train Acc: 0.6050, Val Acc: 0.5761, Test Acc: 0.5153\n",
      "Seed: 456, Epoch: 240, Loss: 1.3846, Train Acc: 0.6059, Val Acc: 0.5808, Test Acc: 0.5216\n",
      "Seed: 456, Epoch: 250, Loss: 1.3720, Train Acc: 0.6080, Val Acc: 0.5748, Test Acc: 0.5092\n",
      "Seed: 456, Epoch: 260, Loss: 1.3697, Train Acc: 0.6097, Val Acc: 0.5778, Test Acc: 0.5149\n",
      "Seed: 456, Epoch: 270, Loss: 1.3789, Train Acc: 0.6060, Val Acc: 0.5747, Test Acc: 0.5218\n",
      "Seed: 456, Epoch: 280, Loss: 1.3672, Train Acc: 0.6073, Val Acc: 0.5747, Test Acc: 0.5159\n",
      "Seed: 456, Epoch: 290, Loss: 1.3636, Train Acc: 0.6086, Val Acc: 0.5765, Test Acc: 0.5153\n",
      "Seed: 456, Epoch: 300, Loss: 1.3569, Train Acc: 0.6112, Val Acc: 0.5832, Test Acc: 0.5236\n",
      "Seed: 456, Epoch: 310, Loss: 1.3559, Train Acc: 0.6135, Val Acc: 0.5843, Test Acc: 0.5248\n",
      "Seed: 456, Epoch: 320, Loss: 1.3473, Train Acc: 0.6145, Val Acc: 0.5821, Test Acc: 0.5193\n",
      "Seed: 456, Epoch: 330, Loss: 1.3434, Train Acc: 0.6154, Val Acc: 0.5826, Test Acc: 0.5216\n",
      "Seed: 456, Epoch: 340, Loss: 1.3448, Train Acc: 0.6140, Val Acc: 0.5783, Test Acc: 0.5170\n",
      "Seed: 456, Epoch: 350, Loss: 1.3400, Train Acc: 0.6175, Val Acc: 0.5846, Test Acc: 0.5232\n",
      "Seed: 456, Epoch: 360, Loss: 1.3362, Train Acc: 0.6181, Val Acc: 0.5882, Test Acc: 0.5274\n",
      "Seed: 456, Epoch: 370, Loss: 1.3348, Train Acc: 0.6166, Val Acc: 0.5815, Test Acc: 0.5152\n",
      "Seed: 456, Epoch: 380, Loss: 1.3426, Train Acc: 0.6167, Val Acc: 0.5802, Test Acc: 0.5151\n",
      "Seed: 456, Epoch: 390, Loss: 1.3318, Train Acc: 0.6175, Val Acc: 0.5864, Test Acc: 0.5297\n",
      "Seed: 456, Epoch: 400, Loss: 1.3273, Train Acc: 0.6195, Val Acc: 0.5867, Test Acc: 0.5241\n",
      "Seed: 456, Epoch: 410, Loss: 1.3203, Train Acc: 0.6212, Val Acc: 0.5874, Test Acc: 0.5255\n",
      "Seed: 456, Epoch: 420, Loss: 1.3170, Train Acc: 0.6219, Val Acc: 0.5874, Test Acc: 0.5241\n",
      "Seed: 456, Epoch: 430, Loss: 1.3470, Train Acc: 0.6108, Val Acc: 0.5836, Test Acc: 0.5260\n",
      "Seed: 456, Epoch: 440, Loss: 1.3387, Train Acc: 0.6138, Val Acc: 0.5816, Test Acc: 0.5240\n",
      "Seed: 456, Epoch: 450, Loss: 1.3253, Train Acc: 0.6185, Val Acc: 0.5858, Test Acc: 0.5243\n",
      "Seed: 456, Epoch: 460, Loss: 1.3153, Train Acc: 0.6229, Val Acc: 0.5907, Test Acc: 0.5293\n",
      "Seed: 456, Epoch: 470, Loss: 1.3097, Train Acc: 0.6231, Val Acc: 0.5895, Test Acc: 0.5300\n",
      "Seed: 456, Epoch: 480, Loss: 1.3064, Train Acc: 0.6246, Val Acc: 0.5895, Test Acc: 0.5288\n",
      "Seed: 456, Epoch: 490, Loss: 1.3100, Train Acc: 0.6239, Val Acc: 0.5872, Test Acc: 0.5264\n",
      "Seed: 456, Epoch: 500, Loss: 1.3025, Train Acc: 0.6250, Val Acc: 0.5896, Test Acc: 0.5326\n",
      "Seed: 42, Best Val Acc: 0.5932, Corresponding Test Acc: 0.5318, Time: 60.65s, Memory: 1.71MB, GPU Memory: 4254.78MB\n",
      "Seed: 123, Best Val Acc: 0.5938, Corresponding Test Acc: 0.5353, Time: 73.81s, Memory: 1.56MB, GPU Memory: 4277.71MB\n",
      "Seed: 456, Best Val Acc: 0.5924, Corresponding Test Acc: 0.5345, Time: 60.38s, Memory: 1.56MB, GPU Memory: 4277.06MB\n",
      "\n",
      "Average Best Validation Accuracy: 0.5931\n",
      "Average Corresponding Test Accuracy: 0.5339\n",
      "Average Time: 64.95s\n",
      "Average Memory Usage: 1.61MB\n",
      "Average GPU Memory Usage: 4269.85MB\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGPooling\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_SAG(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_SAG, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.sum_res = sum_res\n",
    "        self.act = act\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(SAGPooling(channels, ratio=0.7))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.0, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = F.relu(x)  # 这里的self.act应该是一个函数\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "\n",
    "            up = res\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    test_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    model = HierarchicalGCN_SAG(\n",
    "        in_channels=graph.num_features,\n",
    "        hidden_channels=256,\n",
    "        out_channels=dataset.num_classes,\n",
    "        depth=2,\n",
    "        act=F.relu,\n",
    "        sum_res=False\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    for epoch in range(1, 501):\n",
    "        loss = train(model, graph, split_idx['train'], optimizer)\n",
    "        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n",
    "        val_accuracies.append(valid_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    memory_usage = current / 10**6  # Convert to MB\n",
    "    peak_memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  # Convert to MB\n",
    "        torch.cuda.reset_max_memory_allocated(device)\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    val_accuracies_list.append(val_accuracies)\n",
    "    times.append(elapsed_time)\n",
    "    memories.append(memory_usage)\n",
    "    gpu_memories.append(gpu_memory_usage)\n",
    "\n",
    "    best_val_acc = max(val_accuracies)\n",
    "    best_val_index = val_accuracies.index(best_val_acc)\n",
    "    corresponding_test_acc = test_accuracies[best_val_index]\n",
    "\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'corresponding_test_acc': corresponding_test_acc,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'memory_usage': memory_usage,\n",
    "        'gpu_memory_usage': gpu_memory_usage\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n",
    "          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n",
    "          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n",
    "          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\n",
    "\n",
    "# Calculate average results\n",
    "avg_val_acc = np.mean([result['best_val_acc'] for result in results])\n",
    "avg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\n",
    "avg_time = np.mean(times)\n",
    "avg_memory = np.mean(memories)\n",
    "avg_gpu_memory = np.mean(gpu_memories)\n",
    "\n",
    "print(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "print(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\n",
    "print(f\"Average Time: {avg_time:.2f}s\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f}MB\")\n",
    "print(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "class GPR_prop(MessagePassing):\n",
    "    '''\n",
    "    propagation class for GPR_GNN\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, alpha, Init, Gamma=None, bias=True, **kwargs):\n",
    "        super(GPR_prop, self).__init__(aggr='add', **kwargs)\n",
    "        self.K = K\n",
    "        self.Init = Init\n",
    "        self.alpha = alpha\n",
    "\n",
    "        assert Init in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']\n",
    "        if Init == 'SGC':\n",
    "            # SGC-like\n",
    "            TEMP = 0.0*np.ones(K+1)\n",
    "            TEMP[alpha] = 1.0\n",
    "        elif Init == 'PPR':\n",
    "            # PPR-like\n",
    "            TEMP = alpha*(1-alpha)**np.arange(K+1)\n",
    "            TEMP[-1] = (1-alpha)**K\n",
    "        elif Init == 'NPPR':\n",
    "            # Negative PPR\n",
    "            TEMP = (alpha)**np.arange(K+1)\n",
    "            TEMP = TEMP/np.sum(np.abs(TEMP))\n",
    "        elif Init == 'Random':\n",
    "            # Random\n",
    "            bound = np.sqrt(3/(K+1))\n",
    "            TEMP = np.random.uniform(-bound, bound, K+1)\n",
    "            TEMP = TEMP/np.sum(np.abs(TEMP))\n",
    "        elif Init == 'WS':\n",
    "            # Specify Gamma\n",
    "            TEMP = Gamma\n",
    "\n",
    "        self.temp = Parameter(torch.tensor(TEMP))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.zeros_(self.temp)\n",
    "        for k in range(self.K+1):\n",
    "            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n",
    "        self.temp.data[-1] = (1-self.alpha)**self.K\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        edge_index, norm = gcn_norm(\n",
    "            edge_index, edge_weight, num_nodes=x.size(0), dtype=x.dtype)\n",
    "\n",
    "        hidden = x*(self.temp[0])\n",
    "        for k in range(self.K):\n",
    "            x = self.propagate(edge_index, x=x, norm=norm)\n",
    "            gamma = self.temp[k+1]\n",
    "            hidden = hidden + gamma*x\n",
    "        return hidden\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n",
    "                                           self.temp)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) # in case all the edges are removed\n",
    "\n",
    "        edge_index = edge_index.type(torch.long)\n",
    "        row, col = edge_index\n",
    "        # print(row, col)\n",
    "        # print(edge_weight.shape, row.shape, num_nodes)\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        # row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "class graph_attention(torch.nn.Module):\n",
    "    # reference: https://github.com/gordicaleksa/pytorch-GAT/blob/39c8f0ee634477033e8b1a6e9a6da3c7ed71bbd1/models/definitions/GAT.py#L324\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    nodes_dim = 0      # node dimension/axis\n",
    "    head_dim = 1       # attention head dimension/axis\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, dropout_prob=0.6, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Saving these as we'll need them in forward propagation in children layers (imp1/2/3)\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        # You can treat this one matrix as num_of_heads independent W matrices\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the additive scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
    "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
    "        Feel free to experiment - there may be better initializations depending on your problem.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features = x  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = scores_source_lifted + scores_target_lifted\n",
    "\n",
    "        return torch.sigmoid(scores_per_edge)\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "\n",
    "\n",
    "class CoPooling(torch.nn.Module):\n",
    "    # reference for GAT code: https://github.com/PetarV-/GAT\n",
    "    # reference for generalized pagerank code: https://github.com/jianhao2016/GPRGNN\n",
    "    def __init__(self, ratio=0.5, K=0.05, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=None):\n",
    "        super(CoPooling, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "        self.edge_ratio = edge_ratio\n",
    "\n",
    "        self.prop1 = GPR_prop(K, alpha, Init, Gamma)\n",
    "\n",
    "        score_dim = 32\n",
    "        self.G_att = graph_attention(num_in_features=nhid, num_out_features=score_dim, num_of_heads=1)\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(2*nhid, nhid))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        self.bias = Parameter(torch.Tensor(nhid))\n",
    "        nn.init.zeros_(self.bias.data)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        nn.init.zeros_(self.bias.data)\n",
    "        self.prop1.reset_parameters()\n",
    "        self.G_att.init_params()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None, nodes_index=None, node_attr=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        ori_batch = batch.clone()\n",
    "        device = x.device\n",
    "        num_nodes = x.shape[0]\n",
    "\n",
    "        # cut edges based on scores\n",
    "        x_cut = self.prop1(x, edge_index) # run generalized pagerank to update features\n",
    "\n",
    "        attention = self.G_att(x_cut, edge_index) # get the attention weights after sigmoid\n",
    "        attention = attention.sum(dim=1) #sum the weights on head dim\n",
    "        edge_index, attention = add_self_loops(edge_index, attention, 1.0, num_nodes) # add self loops in case no edges\n",
    "\n",
    "        # to get a systemitic adj matrix\n",
    "        edge_index_t, attention_t = transpose(edge_index, attention, num_nodes, num_nodes)\n",
    "        edge_tmp = torch.cat((edge_index, edge_index_t), 1)\n",
    "        att_tmp = torch.cat((attention, attention_t),0)\n",
    "        edge_index, attention = coalesce(edge_tmp, att_tmp, num_nodes, num_nodes, 'mean')\n",
    "\n",
    "        attention_np = attention.cpu().data.numpy()\n",
    "        cut_val = np.percentile(attention_np, int(100*(1-self.edge_ratio))) # this is for keep the top edge_ratio edges\n",
    "        attention = attention * (attention >= cut_val) # keep the edge_ratio higher weights of edges\n",
    "\n",
    "        kep_idx = attention > 0.0\n",
    "        cut_edge_index, cut_edge_attr = edge_index[:, kep_idx], attention[kep_idx]\n",
    "\n",
    "        # Graph Pooling based on nodes\n",
    "        x_information_score = self.calc_information_score(x, cut_edge_index, cut_edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x_topk = x[perm]\n",
    "        batch = batch[perm]\n",
    "        if nodes_index is not None:\n",
    "            nodes_index = nodes_index[perm]\n",
    "\n",
    "        if node_attr is not None:\n",
    "            node_attr = node_attr[perm]\n",
    "        if cut_edge_index is not None or cut_edge_index.nelement() != 0:\n",
    "            induced_edge_index, induced_edge_attr = filter_adj(cut_edge_index, cut_edge_attr, perm, num_nodes=num_nodes)\n",
    "        else:\n",
    "            print('All edges are cut!')\n",
    "            induced_edge_index, induced_edge_attr = cut_edge_index, cut_edge_attr\n",
    "\n",
    "        # update node features\n",
    "        attention_dense = (to_dense_adj(cut_edge_index, edge_attr=cut_edge_attr, max_num_nodes=num_nodes)).squeeze()\n",
    "        x = F.relu(torch.matmul(torch.cat((x_topk, torch.matmul(attention_dense[perm],x)), 1), self.weight) + self.bias)\n",
    "\n",
    "        return x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 106.83 GiB. GPU 0 has a total capacty of 79.21 GiB of which 62.00 GiB is free. Process 686389 has 2.02 GiB memory in use. Process 273867 has 10.64 GiB memory in use. Including non-PyTorch memory, this process has 4.54 GiB memory in use. Of the allocated memory 2.86 GiB is allocated by PyTorch, and 333.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 144\u001b[0m\n\u001b[1;32m    141\u001b[0m train_idx \u001b[38;5;241m=\u001b[39m split_idx[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m501\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     train_acc, valid_acc, test_acc \u001b[38;5;241m=\u001b[39m test(model, graph, split_idx, evaluator)\n\u001b[1;32m    146\u001b[0m     val_accuracies\u001b[38;5;241m.\u001b[39mappend(valid_acc)\n",
      "Cell \u001b[0;32mIn[8], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, data, train_idx, optimizer)\u001b[0m\n\u001b[1;32m     77\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     79\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 80\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m[train_idx]\n\u001b[1;32m     81\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(out, data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)[train_idx])\n\u001b[1;32m     82\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 53\u001b[0m, in \u001b[0;36mHierarchicalGCN_CO.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     50\u001b[0m edge_indices \u001b[38;5;241m=\u001b[39m [edge_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 53\u001b[0m     x, edge_index, perm, _, batch, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpools\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_convs[i](x, edge_index)\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 396\u001b[0m, in \u001b[0;36mCoPooling.forward\u001b[0;34m(self, x, edge_index, edge_attr, batch, nodes_index, node_attr)\u001b[0m\n\u001b[1;32m    393\u001b[0m     induced_edge_index, induced_edge_attr \u001b[38;5;241m=\u001b[39m cut_edge_index, cut_edge_attr\n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# update node features\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m attention_dense \u001b[38;5;241m=\u001b[39m (\u001b[43mto_dense_adj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcut_edge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcut_edge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    397\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(torch\u001b[38;5;241m.\u001b[39mmatmul(torch\u001b[38;5;241m.\u001b[39mcat((x_topk, torch\u001b[38;5;241m.\u001b[39mmatmul(attention_dense[perm],x)), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch_geometric/utils/_to_dense_adj.py:97\u001b[0m, in \u001b[0;36mto_dense_adj\u001b[0;34m(edge_index, batch, edge_attr, max_num_nodes, batch_size)\u001b[0m\n\u001b[1;32m     94\u001b[0m flattened_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m max_num_nodes \u001b[38;5;241m*\u001b[39m max_num_nodes\n\u001b[1;32m     96\u001b[0m idx \u001b[38;5;241m=\u001b[39m idx0 \u001b[38;5;241m*\u001b[39m max_num_nodes \u001b[38;5;241m*\u001b[39m max_num_nodes \u001b[38;5;241m+\u001b[39m idx1 \u001b[38;5;241m*\u001b[39m max_num_nodes \u001b[38;5;241m+\u001b[39m idx2\n\u001b[0;32m---> 97\u001b[0m adj \u001b[38;5;241m=\u001b[39m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflattened_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m adj \u001b[38;5;241m=\u001b[39m adj\u001b[38;5;241m.\u001b[39mview(size)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adj\n",
      "File \u001b[0;32m~/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch_geometric/utils/_scatter.py:75\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     74\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 106.83 GiB. GPU 0 has a total capacty of 79.21 GiB of which 62.00 GiB is free. Process 686389 has 2.02 GiB memory in use. Process 273867 has 10.64 GiB memory in use. Including non-PyTorch memory, this process has 4.54 GiB memory in use. Of the allocated memory 2.86 GiB is allocated by PyTorch, and 333.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import ASAPooling\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_CO(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_CO, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.sum_res = sum_res\n",
    "        self.act = act\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(CoPooling(ratio=0.7, K=2, edge_ratio=0.6, nhid=256, alpha=0.1, Init='Random', Gamma=1.0))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.0, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = F.relu(x)  # 这里的self.act应该是一个函数\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, perm, _, batch, _, _, _ = self.pools[i - 1](x, edge_index, edge_attr=None, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "\n",
    "            up = res\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    test_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    model = HierarchicalGCN_CO(\n",
    "        in_channels=graph.num_features,\n",
    "        hidden_channels=256,\n",
    "        out_channels=dataset.num_classes,\n",
    "        depth=2,\n",
    "        act=F.relu,\n",
    "        sum_res=False\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    for epoch in range(1, 501):\n",
    "        loss = train(model, graph, split_idx['train'], optimizer)\n",
    "        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n",
    "        val_accuracies.append(valid_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    memory_usage = current / 10**6  # Convert to MB\n",
    "    peak_memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  # Convert to MB\n",
    "        torch.cuda.reset_max_memory_allocated(device)\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    val_accuracies_list.append(val_accuracies)\n",
    "    times.append(elapsed_time)\n",
    "    memories.append(memory_usage)\n",
    "    gpu_memories.append(gpu_memory_usage)\n",
    "\n",
    "    best_val_acc = max(val_accuracies)\n",
    "    best_val_index = val_accuracies.index(best_val_acc)\n",
    "    corresponding_test_acc = test_accuracies[best_val_index]\n",
    "\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'corresponding_test_acc': corresponding_test_acc,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'memory_usage': memory_usage,\n",
    "        'gpu_memory_usage': gpu_memory_usage\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n",
    "          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n",
    "          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n",
    "          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\n",
    "\n",
    "# Calculate average results\n",
    "avg_val_acc = np.mean([result['best_val_acc'] for result in results])\n",
    "avg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\n",
    "avg_time = np.mean(times)\n",
    "avg_memory = np.mean(memories)\n",
    "avg_gpu_memory = np.mean(gpu_memories)\n",
    "\n",
    "print(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "print(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\n",
    "print(f\"Average Time: {avg_time:.2f}s\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f}MB\")\n",
    "print(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGIPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch_geometric.datasets import GitHub\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "max_nodes = 150\n",
    "dataset_sparse = PygNodePropPredDataset(root='/data/Zeyu/Pooling', name=\"ogbn-arxiv\")\n",
    "evaluator = Evaluator(\"ogbn-arxiv\")\n",
    "eval_metric = dataset_sparse.eval_metric\n",
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from typing import Callable, Optional, Union\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "@dataclass(init=False)\n",
    "class SelectOutput:\n",
    "    r\"\"\"The output of the :class:`Select` method, which holds an assignment\n",
    "    from selected nodes to their respective cluster(s).\n",
    "\n",
    "    Args:\n",
    "        node_index (torch.Tensor): The indices of the selected nodes.\n",
    "        num_nodes (int): The number of nodes.\n",
    "        cluster_index (torch.Tensor): The indices of the clusters each node in\n",
    "            :obj:`node_index` is assigned to.\n",
    "        num_clusters (int): The number of clusters.\n",
    "        weight (torch.Tensor, optional): A weight vector, denoting the strength\n",
    "            of the assignment of a node to its cluster. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    node_index: Tensor\n",
    "    num_nodes: int\n",
    "    cluster_index: Tensor\n",
    "    num_clusters: int\n",
    "    weight: Optional[Tensor] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_index: Tensor,\n",
    "        num_nodes: int,\n",
    "        cluster_index: Tensor,\n",
    "        num_clusters: int,\n",
    "        weight: Optional[Tensor] = None,\n",
    "    ):\n",
    "        if node_index.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'node_index' to be one-dimensional \"\n",
    "                             f\"(got {node_index.dim()} dimensions)\")\n",
    "\n",
    "        if cluster_index.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'cluster_index' to be one-dimensional \"\n",
    "                             f\"(got {cluster_index.dim()} dimensions)\")\n",
    "\n",
    "        if node_index.numel() != cluster_index.numel():\n",
    "            raise ValueError(f\"Expected 'node_index' and 'cluster_index' to \"\n",
    "                             f\"hold the same number of values (got \"\n",
    "                             f\"{node_index.numel()} and \"\n",
    "                             f\"{cluster_index.numel()} values)\")\n",
    "\n",
    "        if weight is not None and weight.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'weight' vector to be one-dimensional \"\n",
    "                             f\"(got {weight.dim()} dimensions)\")\n",
    "\n",
    "        if weight is not None and weight.numel() != node_index.numel():\n",
    "            raise ValueError(f\"Expected 'weight' to hold {node_index.numel()} \"\n",
    "                             f\"values (got {weight.numel()} values)\")\n",
    "\n",
    "        self.node_index = node_index\n",
    "        self.num_nodes = num_nodes\n",
    "        self.cluster_index = cluster_index\n",
    "        self.num_clusters = num_clusters\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Select(torch.nn.Module):\n",
    "    r\"\"\"An abstract base class for implementing custom node selections as\n",
    "    described in the `\"Understanding Pooling in Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1905.05178>`_ paper, which maps the nodes of an\n",
    "    input graph to supernodes in the coarsened graph.\n",
    "\n",
    "    Specifically, :class:`Select` returns a :class:`SelectOutput` output, which\n",
    "    holds a (sparse) mapping :math:`\\mathbf{C} \\in {[0, 1]}^{N \\times C}` that\n",
    "    assigns selected nodes to one or more of :math:`C` super nodes.\n",
    "    \"\"\"\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> SelectOutput:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'\n",
    "\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels * 2, in_channels)\n",
    "        self.fc2 = nn.Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CGIPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.5, non_lin=torch.tanh):\n",
    "        super(CGIPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.non_lin = non_lin\n",
    "        self.hidden_dim = in_channels\n",
    "        self.transform = GraphConv(in_channels, self.hidden_dim)\n",
    "        self.pp_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n",
    "        self.np_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        self.positive_pooling = GraphConv(self.hidden_dim, 1)\n",
    "        self.negative_pooling = GraphConv(self.hidden_dim, 1)\n",
    "\n",
    "        self.discriminator = Discriminator(self.hidden_dim)\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        device = x.device  # 获取输入张量的设备信息\n",
    "\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_transform = F.leaky_relu(self.transform(x, edge_index), 0.2)\n",
    "        x_tp = F.leaky_relu(self.pp_conv(x, edge_index), 0.2)\n",
    "        x_tn = F.leaky_relu(self.np_conv(x, edge_index), 0.2)\n",
    "        s_pp = self.positive_pooling(x_tp, edge_index).squeeze()\n",
    "        s_np = self.negative_pooling(x_tn, edge_index).squeeze()\n",
    "\n",
    "        perm_positive = topk(s_pp, 1, batch)\n",
    "        perm_negative = topk(s_np, 1, batch)\n",
    "        x_pp = x_transform[perm_positive] * self.non_lin(s_pp[perm_positive]).view(-1, 1)\n",
    "        x_np = x_transform[perm_negative] * self.non_lin(s_np[perm_negative]).view(-1, 1)\n",
    "\n",
    "        x_pp_readout = gap(x_pp, batch[perm_positive])\n",
    "        x_np_readout = gap(x_np, batch[perm_negative])\n",
    "        x_readout = gap(x_transform, batch)\n",
    "\n",
    "        positive_pair = torch.cat([x_pp_readout, x_readout], dim=1)\n",
    "        negative_pair = torch.cat([x_np_readout, x_readout], dim=1)\n",
    "\n",
    "        real = torch.ones(positive_pair.shape[0], device=device)  # 将张量移动到相应设备\n",
    "        fake = torch.zeros(negative_pair.shape[0], device=device)  # 将张量移动到相应设备\n",
    "        #real_loss = self.loss_fn(self.discriminator(positive_pair), real)\n",
    "        #fake_loss = self.loss_fn(self.discriminator(negative_pair), fake)\n",
    "        #discrimination_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        score = (s_pp - s_np)\n",
    "\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x_transform[perm] * self.non_lin(score[perm]).view(-1, 1)\n",
    "        batch = batch[perm]\n",
    "\n",
    "        filter_edge_index, filter_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "        return x, filter_edge_index, filter_edge_attr, batch, perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 042, Epoch: 010, Loss: 2.8205, Train Acc: 0.2963, Val Acc: 0.3084, Test Acc: 0.2757\n",
      "Seed: 042, Epoch: 020, Loss: 2.3297, Train Acc: 0.4035, Val Acc: 0.4122, Test Acc: 0.3680\n",
      "Seed: 042, Epoch: 030, Loss: 2.0551, Train Acc: 0.4566, Val Acc: 0.4540, Test Acc: 0.4031\n",
      "Seed: 042, Epoch: 040, Loss: 1.8393, Train Acc: 0.5041, Val Acc: 0.4865, Test Acc: 0.4186\n",
      "Seed: 042, Epoch: 050, Loss: 1.6957, Train Acc: 0.5352, Val Acc: 0.5162, Test Acc: 0.4551\n",
      "Seed: 042, Epoch: 060, Loss: 1.6249, Train Acc: 0.5521, Val Acc: 0.5399, Test Acc: 0.4818\n",
      "Seed: 042, Epoch: 070, Loss: 1.5724, Train Acc: 0.5626, Val Acc: 0.5419, Test Acc: 0.4788\n",
      "Seed: 042, Epoch: 080, Loss: 1.5360, Train Acc: 0.5711, Val Acc: 0.5487, Test Acc: 0.4865\n",
      "Seed: 042, Epoch: 090, Loss: 1.5195, Train Acc: 0.5749, Val Acc: 0.5599, Test Acc: 0.4941\n",
      "Seed: 042, Epoch: 100, Loss: 1.4943, Train Acc: 0.5811, Val Acc: 0.5610, Test Acc: 0.4968\n",
      "Seed: 042, Epoch: 110, Loss: 1.4771, Train Acc: 0.5832, Val Acc: 0.5623, Test Acc: 0.5025\n",
      "Seed: 042, Epoch: 120, Loss: 1.4655, Train Acc: 0.5874, Val Acc: 0.5685, Test Acc: 0.5115\n",
      "Seed: 042, Epoch: 130, Loss: 1.4554, Train Acc: 0.5899, Val Acc: 0.5628, Test Acc: 0.4942\n",
      "Seed: 042, Epoch: 140, Loss: 1.4415, Train Acc: 0.5932, Val Acc: 0.5702, Test Acc: 0.5112\n",
      "Seed: 042, Epoch: 150, Loss: 1.4311, Train Acc: 0.5948, Val Acc: 0.5675, Test Acc: 0.4999\n",
      "Seed: 042, Epoch: 160, Loss: 1.4220, Train Acc: 0.5975, Val Acc: 0.5717, Test Acc: 0.5074\n",
      "Seed: 042, Epoch: 170, Loss: 1.4192, Train Acc: 0.5977, Val Acc: 0.5752, Test Acc: 0.5106\n",
      "Seed: 042, Epoch: 180, Loss: 1.4095, Train Acc: 0.5975, Val Acc: 0.5691, Test Acc: 0.5060\n",
      "Seed: 042, Epoch: 190, Loss: 1.4033, Train Acc: 0.6017, Val Acc: 0.5769, Test Acc: 0.5110\n",
      "Seed: 042, Epoch: 200, Loss: 1.3973, Train Acc: 0.6026, Val Acc: 0.5733, Test Acc: 0.5108\n",
      "Seed: 042, Epoch: 210, Loss: 1.3893, Train Acc: 0.6051, Val Acc: 0.5790, Test Acc: 0.5155\n",
      "Seed: 042, Epoch: 220, Loss: 1.3953, Train Acc: 0.6028, Val Acc: 0.5728, Test Acc: 0.5110\n",
      "Seed: 042, Epoch: 230, Loss: 1.3829, Train Acc: 0.6069, Val Acc: 0.5821, Test Acc: 0.5212\n",
      "Seed: 042, Epoch: 240, Loss: 1.3768, Train Acc: 0.6073, Val Acc: 0.5815, Test Acc: 0.5205\n",
      "Seed: 042, Epoch: 250, Loss: 1.3711, Train Acc: 0.6093, Val Acc: 0.5784, Test Acc: 0.5152\n",
      "Seed: 042, Epoch: 260, Loss: 1.3792, Train Acc: 0.6052, Val Acc: 0.5768, Test Acc: 0.5169\n",
      "Seed: 042, Epoch: 270, Loss: 1.3644, Train Acc: 0.6104, Val Acc: 0.5796, Test Acc: 0.5154\n",
      "Seed: 042, Epoch: 280, Loss: 1.3653, Train Acc: 0.6085, Val Acc: 0.5724, Test Acc: 0.5060\n",
      "Seed: 042, Epoch: 290, Loss: 1.3554, Train Acc: 0.6118, Val Acc: 0.5792, Test Acc: 0.5130\n",
      "Seed: 042, Epoch: 300, Loss: 1.3540, Train Acc: 0.6122, Val Acc: 0.5835, Test Acc: 0.5207\n",
      "Seed: 042, Epoch: 310, Loss: 1.3667, Train Acc: 0.6095, Val Acc: 0.5769, Test Acc: 0.5178\n",
      "Seed: 042, Epoch: 320, Loss: 1.3502, Train Acc: 0.6137, Val Acc: 0.5879, Test Acc: 0.5271\n",
      "Seed: 042, Epoch: 330, Loss: 1.3423, Train Acc: 0.6137, Val Acc: 0.5793, Test Acc: 0.5147\n",
      "Seed: 042, Epoch: 340, Loss: 1.3554, Train Acc: 0.6111, Val Acc: 0.5814, Test Acc: 0.5199\n",
      "Seed: 042, Epoch: 350, Loss: 1.3391, Train Acc: 0.6143, Val Acc: 0.5823, Test Acc: 0.5187\n",
      "Seed: 042, Epoch: 360, Loss: 1.3410, Train Acc: 0.6167, Val Acc: 0.5861, Test Acc: 0.5229\n",
      "Seed: 042, Epoch: 370, Loss: 1.3338, Train Acc: 0.6175, Val Acc: 0.5881, Test Acc: 0.5297\n",
      "Seed: 042, Epoch: 380, Loss: 1.3267, Train Acc: 0.6192, Val Acc: 0.5857, Test Acc: 0.5221\n",
      "Seed: 042, Epoch: 390, Loss: 1.3276, Train Acc: 0.6186, Val Acc: 0.5893, Test Acc: 0.5313\n",
      "Seed: 042, Epoch: 400, Loss: 1.3444, Train Acc: 0.6151, Val Acc: 0.5825, Test Acc: 0.5163\n",
      "Seed: 042, Epoch: 410, Loss: 1.3211, Train Acc: 0.6202, Val Acc: 0.5872, Test Acc: 0.5243\n",
      "Seed: 042, Epoch: 420, Loss: 1.3172, Train Acc: 0.6207, Val Acc: 0.5852, Test Acc: 0.5220\n",
      "Seed: 042, Epoch: 430, Loss: 1.3353, Train Acc: 0.6187, Val Acc: 0.5889, Test Acc: 0.5343\n",
      "Seed: 042, Epoch: 440, Loss: 1.3134, Train Acc: 0.6217, Val Acc: 0.5891, Test Acc: 0.5263\n",
      "Seed: 042, Epoch: 450, Loss: 1.3114, Train Acc: 0.6225, Val Acc: 0.5921, Test Acc: 0.5327\n",
      "Seed: 042, Epoch: 460, Loss: 1.3092, Train Acc: 0.6225, Val Acc: 0.5832, Test Acc: 0.5197\n",
      "Seed: 042, Epoch: 470, Loss: 1.3057, Train Acc: 0.6234, Val Acc: 0.5839, Test Acc: 0.5215\n",
      "Seed: 042, Epoch: 480, Loss: 1.3044, Train Acc: 0.6244, Val Acc: 0.5918, Test Acc: 0.5293\n",
      "Seed: 042, Epoch: 490, Loss: 1.3137, Train Acc: 0.6222, Val Acc: 0.5873, Test Acc: 0.5230\n",
      "Seed: 042, Epoch: 500, Loss: 1.3031, Train Acc: 0.6248, Val Acc: 0.5876, Test Acc: 0.5246\n",
      "Seed: 123, Epoch: 010, Loss: 2.9168, Train Acc: 0.3030, Val Acc: 0.3430, Test Acc: 0.3112\n",
      "Seed: 123, Epoch: 020, Loss: 2.3309, Train Acc: 0.4099, Val Acc: 0.4387, Test Acc: 0.3883\n",
      "Seed: 123, Epoch: 030, Loss: 1.9882, Train Acc: 0.4675, Val Acc: 0.4706, Test Acc: 0.4273\n",
      "Seed: 123, Epoch: 040, Loss: 1.7906, Train Acc: 0.5208, Val Acc: 0.5099, Test Acc: 0.4518\n",
      "Seed: 123, Epoch: 050, Loss: 1.6828, Train Acc: 0.5382, Val Acc: 0.5238, Test Acc: 0.4589\n",
      "Seed: 123, Epoch: 060, Loss: 1.6110, Train Acc: 0.5522, Val Acc: 0.5318, Test Acc: 0.4648\n",
      "Seed: 123, Epoch: 070, Loss: 1.5696, Train Acc: 0.5648, Val Acc: 0.5455, Test Acc: 0.4748\n",
      "Seed: 123, Epoch: 080, Loss: 1.5321, Train Acc: 0.5719, Val Acc: 0.5512, Test Acc: 0.4846\n",
      "Seed: 123, Epoch: 090, Loss: 1.5055, Train Acc: 0.5781, Val Acc: 0.5561, Test Acc: 0.4896\n",
      "Seed: 123, Epoch: 100, Loss: 1.5145, Train Acc: 0.5753, Val Acc: 0.5462, Test Acc: 0.4796\n",
      "Seed: 123, Epoch: 110, Loss: 1.4787, Train Acc: 0.5865, Val Acc: 0.5618, Test Acc: 0.4983\n",
      "Seed: 123, Epoch: 120, Loss: 1.4565, Train Acc: 0.5913, Val Acc: 0.5699, Test Acc: 0.5052\n",
      "Seed: 123, Epoch: 130, Loss: 1.4391, Train Acc: 0.5936, Val Acc: 0.5694, Test Acc: 0.5060\n",
      "Seed: 123, Epoch: 140, Loss: 1.4277, Train Acc: 0.5969, Val Acc: 0.5704, Test Acc: 0.5057\n",
      "Seed: 123, Epoch: 150, Loss: 1.4201, Train Acc: 0.5965, Val Acc: 0.5670, Test Acc: 0.5021\n",
      "Seed: 123, Epoch: 160, Loss: 1.4164, Train Acc: 0.5987, Val Acc: 0.5698, Test Acc: 0.5058\n",
      "Seed: 123, Epoch: 170, Loss: 1.4070, Train Acc: 0.6009, Val Acc: 0.5750, Test Acc: 0.5151\n",
      "Seed: 123, Epoch: 180, Loss: 1.4001, Train Acc: 0.6034, Val Acc: 0.5738, Test Acc: 0.5095\n",
      "Seed: 123, Epoch: 190, Loss: 1.3936, Train Acc: 0.6040, Val Acc: 0.5751, Test Acc: 0.5140\n",
      "Seed: 123, Epoch: 200, Loss: 1.3868, Train Acc: 0.6048, Val Acc: 0.5762, Test Acc: 0.5107\n",
      "Seed: 123, Epoch: 210, Loss: 1.3797, Train Acc: 0.6072, Val Acc: 0.5802, Test Acc: 0.5192\n",
      "Seed: 123, Epoch: 220, Loss: 1.3834, Train Acc: 0.6084, Val Acc: 0.5774, Test Acc: 0.5130\n",
      "Seed: 123, Epoch: 230, Loss: 1.3734, Train Acc: 0.6091, Val Acc: 0.5774, Test Acc: 0.5126\n",
      "Seed: 123, Epoch: 240, Loss: 1.3649, Train Acc: 0.6105, Val Acc: 0.5792, Test Acc: 0.5157\n",
      "Seed: 123, Epoch: 250, Loss: 1.3621, Train Acc: 0.6106, Val Acc: 0.5812, Test Acc: 0.5215\n",
      "Seed: 123, Epoch: 260, Loss: 1.3708, Train Acc: 0.6102, Val Acc: 0.5810, Test Acc: 0.5175\n",
      "Seed: 123, Epoch: 270, Loss: 1.3565, Train Acc: 0.6122, Val Acc: 0.5816, Test Acc: 0.5168\n",
      "Seed: 123, Epoch: 280, Loss: 1.3526, Train Acc: 0.6127, Val Acc: 0.5836, Test Acc: 0.5213\n",
      "Seed: 123, Epoch: 290, Loss: 1.3471, Train Acc: 0.6137, Val Acc: 0.5852, Test Acc: 0.5219\n",
      "Seed: 123, Epoch: 300, Loss: 1.3414, Train Acc: 0.6165, Val Acc: 0.5879, Test Acc: 0.5270\n",
      "Seed: 123, Epoch: 310, Loss: 1.3413, Train Acc: 0.6160, Val Acc: 0.5829, Test Acc: 0.5170\n",
      "Seed: 123, Epoch: 320, Loss: 1.3418, Train Acc: 0.6145, Val Acc: 0.5830, Test Acc: 0.5156\n",
      "Seed: 123, Epoch: 330, Loss: 1.3345, Train Acc: 0.6176, Val Acc: 0.5869, Test Acc: 0.5233\n",
      "Seed: 123, Epoch: 340, Loss: 1.3287, Train Acc: 0.6188, Val Acc: 0.5868, Test Acc: 0.5256\n",
      "Seed: 123, Epoch: 350, Loss: 1.3651, Train Acc: 0.6167, Val Acc: 0.5836, Test Acc: 0.5268\n",
      "Seed: 123, Epoch: 360, Loss: 1.3515, Train Acc: 0.6005, Val Acc: 0.5512, Test Acc: 0.4888\n",
      "Seed: 123, Epoch: 370, Loss: 1.3482, Train Acc: 0.6105, Val Acc: 0.5900, Test Acc: 0.5453\n",
      "Seed: 123, Epoch: 380, Loss: 1.3302, Train Acc: 0.6174, Val Acc: 0.5899, Test Acc: 0.5342\n",
      "Seed: 123, Epoch: 390, Loss: 1.3203, Train Acc: 0.6197, Val Acc: 0.5911, Test Acc: 0.5293\n",
      "Seed: 123, Epoch: 400, Loss: 1.3171, Train Acc: 0.6206, Val Acc: 0.5878, Test Acc: 0.5230\n",
      "Seed: 123, Epoch: 410, Loss: 1.3197, Train Acc: 0.6210, Val Acc: 0.5844, Test Acc: 0.5211\n",
      "Seed: 123, Epoch: 420, Loss: 1.3109, Train Acc: 0.6231, Val Acc: 0.5883, Test Acc: 0.5252\n",
      "Seed: 123, Epoch: 430, Loss: 1.3069, Train Acc: 0.6231, Val Acc: 0.5890, Test Acc: 0.5278\n",
      "Seed: 123, Epoch: 440, Loss: 1.3160, Train Acc: 0.6190, Val Acc: 0.5871, Test Acc: 0.5259\n",
      "Seed: 123, Epoch: 450, Loss: 1.3050, Train Acc: 0.6228, Val Acc: 0.5867, Test Acc: 0.5239\n",
      "Seed: 123, Epoch: 460, Loss: 1.3031, Train Acc: 0.6234, Val Acc: 0.5902, Test Acc: 0.5308\n",
      "Seed: 123, Epoch: 470, Loss: 1.3044, Train Acc: 0.6249, Val Acc: 0.5898, Test Acc: 0.5298\n",
      "Seed: 123, Epoch: 480, Loss: 1.2993, Train Acc: 0.6248, Val Acc: 0.5905, Test Acc: 0.5311\n",
      "Seed: 123, Epoch: 490, Loss: 1.2950, Train Acc: 0.6250, Val Acc: 0.5888, Test Acc: 0.5279\n",
      "Seed: 123, Epoch: 500, Loss: 1.3294, Train Acc: 0.6179, Val Acc: 0.5756, Test Acc: 0.5238\n",
      "Seed: 456, Epoch: 010, Loss: 2.8576, Train Acc: 0.2941, Val Acc: 0.3253, Test Acc: 0.3023\n",
      "Seed: 456, Epoch: 020, Loss: 2.2614, Train Acc: 0.4116, Val Acc: 0.4415, Test Acc: 0.3923\n",
      "Seed: 456, Epoch: 030, Loss: 2.0118, Train Acc: 0.4859, Val Acc: 0.4891, Test Acc: 0.4362\n",
      "Seed: 456, Epoch: 040, Loss: 1.7594, Train Acc: 0.5187, Val Acc: 0.5088, Test Acc: 0.4534\n",
      "Seed: 456, Epoch: 050, Loss: 1.6582, Train Acc: 0.5415, Val Acc: 0.5191, Test Acc: 0.4545\n",
      "Seed: 456, Epoch: 060, Loss: 1.5920, Train Acc: 0.5594, Val Acc: 0.5415, Test Acc: 0.4790\n",
      "Seed: 456, Epoch: 070, Loss: 1.5579, Train Acc: 0.5660, Val Acc: 0.5467, Test Acc: 0.4791\n",
      "Seed: 456, Epoch: 080, Loss: 1.5294, Train Acc: 0.5728, Val Acc: 0.5581, Test Acc: 0.4976\n",
      "Seed: 456, Epoch: 090, Loss: 1.5020, Train Acc: 0.5797, Val Acc: 0.5557, Test Acc: 0.4879\n",
      "Seed: 456, Epoch: 100, Loss: 1.4789, Train Acc: 0.5856, Val Acc: 0.5601, Test Acc: 0.4942\n",
      "Seed: 456, Epoch: 110, Loss: 1.4618, Train Acc: 0.5895, Val Acc: 0.5651, Test Acc: 0.5007\n",
      "Seed: 456, Epoch: 120, Loss: 1.4535, Train Acc: 0.5922, Val Acc: 0.5674, Test Acc: 0.5106\n",
      "Seed: 456, Epoch: 130, Loss: 1.4395, Train Acc: 0.5966, Val Acc: 0.5716, Test Acc: 0.5120\n",
      "Seed: 456, Epoch: 140, Loss: 1.4225, Train Acc: 0.5985, Val Acc: 0.5726, Test Acc: 0.5107\n",
      "Seed: 456, Epoch: 150, Loss: 1.4127, Train Acc: 0.6017, Val Acc: 0.5769, Test Acc: 0.5135\n",
      "Seed: 456, Epoch: 160, Loss: 1.4031, Train Acc: 0.6033, Val Acc: 0.5718, Test Acc: 0.5064\n",
      "Seed: 456, Epoch: 170, Loss: 1.3962, Train Acc: 0.6055, Val Acc: 0.5750, Test Acc: 0.5084\n",
      "Seed: 456, Epoch: 180, Loss: 1.3942, Train Acc: 0.6045, Val Acc: 0.5714, Test Acc: 0.5105\n",
      "Seed: 456, Epoch: 190, Loss: 1.3880, Train Acc: 0.6060, Val Acc: 0.5751, Test Acc: 0.5100\n",
      "Seed: 456, Epoch: 200, Loss: 1.3805, Train Acc: 0.6061, Val Acc: 0.5736, Test Acc: 0.5094\n",
      "Seed: 456, Epoch: 210, Loss: 1.3762, Train Acc: 0.6050, Val Acc: 0.5661, Test Acc: 0.5009\n",
      "Seed: 456, Epoch: 220, Loss: 1.3716, Train Acc: 0.6091, Val Acc: 0.5795, Test Acc: 0.5197\n",
      "Seed: 456, Epoch: 230, Loss: 1.3624, Train Acc: 0.6125, Val Acc: 0.5842, Test Acc: 0.5194\n",
      "Seed: 456, Epoch: 240, Loss: 1.3587, Train Acc: 0.6122, Val Acc: 0.5831, Test Acc: 0.5167\n",
      "Seed: 456, Epoch: 250, Loss: 1.3555, Train Acc: 0.6126, Val Acc: 0.5809, Test Acc: 0.5212\n",
      "Seed: 456, Epoch: 260, Loss: 1.3498, Train Acc: 0.6143, Val Acc: 0.5859, Test Acc: 0.5243\n",
      "Seed: 456, Epoch: 270, Loss: 1.3488, Train Acc: 0.6137, Val Acc: 0.5804, Test Acc: 0.5220\n",
      "Seed: 456, Epoch: 280, Loss: 1.3406, Train Acc: 0.6169, Val Acc: 0.5862, Test Acc: 0.5228\n",
      "Seed: 456, Epoch: 290, Loss: 1.3366, Train Acc: 0.6180, Val Acc: 0.5858, Test Acc: 0.5212\n",
      "Seed: 456, Epoch: 300, Loss: 1.3592, Train Acc: 0.6180, Val Acc: 0.5913, Test Acc: 0.5315\n",
      "Seed: 456, Epoch: 310, Loss: 1.3526, Train Acc: 0.6132, Val Acc: 0.5892, Test Acc: 0.5261\n",
      "Seed: 456, Epoch: 320, Loss: 1.3406, Train Acc: 0.6143, Val Acc: 0.5830, Test Acc: 0.5258\n",
      "Seed: 456, Epoch: 330, Loss: 1.3279, Train Acc: 0.6189, Val Acc: 0.5852, Test Acc: 0.5240\n",
      "Seed: 456, Epoch: 340, Loss: 1.3236, Train Acc: 0.6209, Val Acc: 0.5889, Test Acc: 0.5264\n",
      "Seed: 456, Epoch: 350, Loss: 1.3296, Train Acc: 0.6183, Val Acc: 0.5905, Test Acc: 0.5383\n",
      "Seed: 456, Epoch: 360, Loss: 1.3165, Train Acc: 0.6217, Val Acc: 0.5896, Test Acc: 0.5329\n",
      "Seed: 456, Epoch: 370, Loss: 1.3149, Train Acc: 0.6213, Val Acc: 0.5846, Test Acc: 0.5259\n",
      "Seed: 456, Epoch: 380, Loss: 1.3168, Train Acc: 0.6217, Val Acc: 0.5836, Test Acc: 0.5205\n",
      "Seed: 456, Epoch: 390, Loss: 1.3111, Train Acc: 0.6226, Val Acc: 0.5926, Test Acc: 0.5331\n",
      "Seed: 456, Epoch: 400, Loss: 1.3074, Train Acc: 0.6247, Val Acc: 0.5876, Test Acc: 0.5246\n",
      "Seed: 456, Epoch: 410, Loss: 1.3434, Train Acc: 0.6212, Val Acc: 0.5826, Test Acc: 0.5177\n",
      "Seed: 456, Epoch: 420, Loss: 1.3090, Train Acc: 0.6237, Val Acc: 0.5949, Test Acc: 0.5379\n",
      "Seed: 456, Epoch: 430, Loss: 1.3055, Train Acc: 0.6261, Val Acc: 0.5902, Test Acc: 0.5281\n",
      "Seed: 456, Epoch: 440, Loss: 1.2979, Train Acc: 0.6258, Val Acc: 0.5906, Test Acc: 0.5290\n",
      "Seed: 456, Epoch: 450, Loss: 1.2974, Train Acc: 0.6249, Val Acc: 0.5882, Test Acc: 0.5333\n",
      "Seed: 456, Epoch: 460, Loss: 1.2934, Train Acc: 0.6271, Val Acc: 0.5922, Test Acc: 0.5325\n",
      "Seed: 456, Epoch: 470, Loss: 1.2959, Train Acc: 0.6249, Val Acc: 0.5885, Test Acc: 0.5330\n",
      "Seed: 456, Epoch: 480, Loss: 1.2912, Train Acc: 0.6282, Val Acc: 0.5913, Test Acc: 0.5313\n",
      "Seed: 456, Epoch: 490, Loss: 1.2872, Train Acc: 0.6291, Val Acc: 0.5872, Test Acc: 0.5247\n",
      "Seed: 456, Epoch: 500, Loss: 1.2881, Train Acc: 0.6279, Val Acc: 0.5914, Test Acc: 0.5357\n",
      "Seed: 42, Best Val Acc: 0.5923, Corresponding Test Acc: 0.5276, Time: 120.84s, Memory: 1.73MB, GPU Memory: 7431.61MB\n",
      "Seed: 123, Best Val Acc: 0.5924, Corresponding Test Acc: 0.5378, Time: 122.21s, Memory: 1.70MB, GPU Memory: 6508.72MB\n",
      "Seed: 456, Best Val Acc: 0.5971, Corresponding Test Acc: 0.5426, Time: 139.84s, Memory: 1.70MB, GPU Memory: 6526.74MB\n",
      "\n",
      "Average Best Validation Accuracy: 0.5939\n",
      "Average Corresponding Test Accuracy: 0.5360\n",
      "Average Time: 127.63s\n",
      "Average Memory Usage: 1.71MB\n",
      "Average GPU Memory Usage: 6822.36MB\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import ASAPooling\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_CO(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_CO, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.sum_res = sum_res\n",
    "        self.act = act\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(CGIPool(channels, ratio=0.9))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.0, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = F.relu(x)  # 这里的self.act应该是一个函数\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "\n",
    "            up = res\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    test_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    model = HierarchicalGCN_CO(\n",
    "        in_channels=graph.num_features,\n",
    "        hidden_channels=256,\n",
    "        out_channels=dataset.num_classes,\n",
    "        depth=2,\n",
    "        act=F.relu,\n",
    "        sum_res=False\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    for epoch in range(1, 501):\n",
    "        loss = train(model, graph, split_idx['train'], optimizer)\n",
    "        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n",
    "        val_accuracies.append(valid_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    memory_usage = current / 10**6  # Convert to MB\n",
    "    peak_memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  # Convert to MB\n",
    "        torch.cuda.reset_max_memory_allocated(device)\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    val_accuracies_list.append(val_accuracies)\n",
    "    times.append(elapsed_time)\n",
    "    memories.append(memory_usage)\n",
    "    gpu_memories.append(gpu_memory_usage)\n",
    "\n",
    "    best_val_acc = max(val_accuracies)\n",
    "    best_val_index = val_accuracies.index(best_val_acc)\n",
    "    corresponding_test_acc = test_accuracies[best_val_index]\n",
    "\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'corresponding_test_acc': corresponding_test_acc,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'memory_usage': memory_usage,\n",
    "        'gpu_memory_usage': gpu_memory_usage\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n",
    "          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n",
    "          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n",
    "          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\n",
    "\n",
    "# Calculate average results\n",
    "avg_val_acc = np.mean([result['best_val_acc'] for result in results])\n",
    "avg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\n",
    "avg_time = np.mean(times)\n",
    "avg_memory = np.mean(memories)\n",
    "avg_gpu_memory = np.mean(gpu_memories)\n",
    "\n",
    "print(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "print(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\n",
    "print(f\"Average Time: {avg_time:.2f}s\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f}MB\")\n",
    "print(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch_scatter import scatter, scatter_add, scatter_min\n",
    "from torch_sparse import SparseTensor, remove_diag\n",
    "\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.nn.dense import Linear\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\n",
    "\n",
    "Scorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch_scatter import scatter_max, scatter_min\n",
    "\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor, Tensor\n",
    "\n",
    "\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\n",
    "Scorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\n",
    "from torch_sparse import SparseTensor, remove_diag\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.nn.dense import Linear\n",
    "from torch.nn import Module\n",
    "from torch_scatter import scatter_max, scatter_min\n",
    "\n",
    "def maximal_independent_set(edge_index: Adj, k: int = 1,\n",
    "                            perm: OptTensor = None) -> Tensor:\n",
    "    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n",
    "    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n",
    "    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n",
    "    returned set.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method follows `Blelloch's Alogirithm\n",
    "    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n",
    "    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n",
    "    higher values of :math:`k`.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: :class:`ByteTensor`\n",
    "    \"\"\"\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "        device = edge_index.device()\n",
    "        n = edge_index.size(0)\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        device = row.device\n",
    "        n = edge_index.max().item() + 1\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    mis = torch.zeros(n, dtype=torch.bool, device=device)\n",
    "    mask = mis.clone()\n",
    "    min_rank = rank.clone()\n",
    "\n",
    "    while not mask.all():\n",
    "        for _ in range(k):\n",
    "            min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "            scatter_min(min_rank[row], col, out=min_neigh)\n",
    "            torch.minimum(min_neigh, min_rank, out=min_rank)  # self-loops\n",
    "\n",
    "        mis = mis | torch.eq(rank, min_rank)\n",
    "        mask = mis.clone().byte()\n",
    "\n",
    "        for _ in range(k):\n",
    "            max_neigh = torch.full_like(mask, fill_value=0)\n",
    "            scatter_max(mask[row], col, out=max_neigh)\n",
    "            torch.maximum(max_neigh, mask, out=mask)  # self-loops\n",
    "\n",
    "        mask = mask.to(dtype=torch.bool)\n",
    "        min_rank = rank.clone()\n",
    "        min_rank[mask] = n\n",
    "\n",
    "    return mis\n",
    "\n",
    "def maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n",
    "                                    perm: OptTensor = None) -> PairTensor:\n",
    "    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n",
    "    clustering of a graph, as defined in `\"Generalizing Downsampling from\n",
    "    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method returns both the :math:`k`-MIS and the clustering, where the\n",
    "    :math:`c`-th cluster refers to the :math:`c`-th element of the\n",
    "    :math:`k`-MIS.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n",
    "    \"\"\"\n",
    "    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n",
    "    n, device = mis.size(0), mis.device\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n",
    "    rank_mis = rank[mis]\n",
    "    min_rank[mis] = rank_mis\n",
    "\n",
    "    for _ in range(k):\n",
    "        min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "        scatter_min(min_rank[row], col, out=min_neigh)\n",
    "        torch.minimum(min_neigh, min_rank, out=min_rank)\n",
    "\n",
    "    _, clusters = torch.unique(min_rank, return_inverse=True)\n",
    "    perm = torch.argsort(rank_mis)\n",
    "    return mis, perm[clusters]\n",
    "\n",
    "\n",
    "class KMISPooling(Module):\n",
    "\n",
    "    _heuristics = {None, 'greedy', 'w-greedy'}\n",
    "    _passthroughs = {None, 'before', 'after'}\n",
    "    _scorers = {\n",
    "        'linear',\n",
    "        'random',\n",
    "        'constant',\n",
    "        'canonical',\n",
    "        'first',\n",
    "        'last',\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n",
    "                 scorer: Union[Scorer, str] = 'linear',\n",
    "                 score_heuristic: Optional[str] = 'greedy',\n",
    "                 score_passthrough: Optional[str] = 'before',\n",
    "                 aggr_x: Optional[Union[str, Aggregation]] = None,\n",
    "                 aggr_edge: str = 'sum',\n",
    "                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n",
    "                 remove_self_loops: bool = True) -> None:\n",
    "        super(KMISPooling, self).__init__()\n",
    "        assert score_heuristic in self._heuristics, \\\n",
    "            \"Unrecognized `score_heuristic` value.\"\n",
    "        assert score_passthrough in self._passthroughs, \\\n",
    "            \"Unrecognized `score_passthrough` value.\"\n",
    "\n",
    "        if not callable(scorer):\n",
    "            assert scorer in self._scorers, \\\n",
    "                \"Unrecognized `scorer` value.\"\n",
    "\n",
    "        self.k = k\n",
    "        self.scorer = scorer\n",
    "        self.score_heuristic = score_heuristic\n",
    "        self.score_passthrough = score_passthrough\n",
    "\n",
    "        self.aggr_x = aggr_x\n",
    "        self.aggr_edge = aggr_edge\n",
    "        self.aggr_score = aggr_score\n",
    "        self.remove_self_loops = remove_self_loops\n",
    "\n",
    "        if scorer == 'linear':\n",
    "            assert self.score_passthrough is not None, \\\n",
    "                \"`'score_passthrough'` must not be `None`\" \\\n",
    "                \" when using `'linear'` scorer\"\n",
    "\n",
    "            self.lin = torch.nn.Linear(in_channels, 1)\n",
    "\n",
    "    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n",
    "        if self.score_heuristic is None:\n",
    "            return x\n",
    "\n",
    "        row, col, _ = adj.coo()\n",
    "        x = x.view(-1)\n",
    "\n",
    "        if self.score_heuristic == 'greedy':\n",
    "            k_sums = torch.ones_like(x)\n",
    "        else:\n",
    "            k_sums = x.clone()\n",
    "\n",
    "        for _ in range(self.k):\n",
    "            scatter_add(k_sums[row], col, out=k_sums)\n",
    "\n",
    "        return x / k_sums\n",
    "\n",
    "    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) -> Tensor:\n",
    "        if self.scorer == 'linear':\n",
    "            return self.lin(x).sigmoid()\n",
    "\n",
    "        if self.scorer == 'random':\n",
    "            return torch.rand((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'constant':\n",
    "            return torch.ones((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'canonical':\n",
    "            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n",
    "\n",
    "        if self.scorer == 'first':\n",
    "            return x[..., [0]]\n",
    "\n",
    "        if self.scorer == 'last':\n",
    "            return x[..., [-1]]\n",
    "\n",
    "        return self.scorer(x, edge_index, edge_attr, batch)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) \\\n",
    "            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n",
    "        \"\"\"\"\"\"\n",
    "        edge_index = edge_index.long()\n",
    "        adj, n = edge_index, x.size(0)\n",
    "\n",
    "        if not isinstance(edge_index, SparseTensor):\n",
    "            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n",
    "\n",
    "        score = self._scorer(x, edge_index, edge_attr, batch)\n",
    "        updated_score = self._apply_heuristic(score, adj)\n",
    "        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n",
    "\n",
    "        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n",
    "\n",
    "        row, col, val = adj.coo()\n",
    "        c = mis.sum()\n",
    "\n",
    "        if val is None:\n",
    "            val = torch.ones_like(row, dtype=torch.float)\n",
    "\n",
    "        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n",
    "                           is_sorted=False,\n",
    "                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n",
    "\n",
    "        if self.remove_self_loops:\n",
    "            adj = remove_diag(adj)\n",
    "\n",
    "        if self.score_passthrough == 'before':\n",
    "            x = self.aggr_score(x, score)\n",
    "\n",
    "        if self.aggr_x is None:\n",
    "            x = x[mis]\n",
    "        elif isinstance(self.aggr_x, str):\n",
    "            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n",
    "                        reduce=self.aggr_x)\n",
    "        else:\n",
    "            x = self.aggr_x(x, cluster, dim_size=c)\n",
    "\n",
    "        if self.score_passthrough == 'after':\n",
    "            x = self.aggr_score(x, score[mis])\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_index, edge_attr = adj, None\n",
    "\n",
    "        else:\n",
    "            row, col, edge_attr = adj.coo()\n",
    "            edge_index = torch.stack([row, col])\n",
    "\n",
    "        if batch is not None:\n",
    "            batch = batch[mis]\n",
    "\n",
    "        #attn = x\n",
    "        #select_out = topk(attn, batch)\n",
    "        perm = perm[mis]\n",
    "\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, mis, cluster, perm\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.scorer == 'linear':\n",
    "            channels = f\"in_channels={self.lin.in_channels}, \"\n",
    "        else:\n",
    "            channels = \"\"\n",
    "\n",
    "        return f'{self.__class__.__name__}({channels}k={self.k})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 042, Epoch: 010, Loss: 3.0120, Train Acc: 0.2774, Val Acc: 0.3013, Test Acc: 0.2686\n",
      "Seed: 042, Epoch: 020, Loss: 2.5648, Train Acc: 0.3541, Val Acc: 0.3868, Test Acc: 0.3409\n",
      "Seed: 042, Epoch: 030, Loss: 2.1364, Train Acc: 0.4323, Val Acc: 0.4336, Test Acc: 0.3797\n",
      "Seed: 042, Epoch: 040, Loss: 1.9311, Train Acc: 0.4804, Val Acc: 0.4747, Test Acc: 0.4109\n",
      "Seed: 042, Epoch: 050, Loss: 1.7729, Train Acc: 0.5200, Val Acc: 0.5127, Test Acc: 0.4550\n",
      "Seed: 042, Epoch: 060, Loss: 1.6788, Train Acc: 0.5392, Val Acc: 0.5205, Test Acc: 0.4574\n",
      "Seed: 042, Epoch: 070, Loss: 1.6091, Train Acc: 0.5524, Val Acc: 0.5294, Test Acc: 0.4669\n",
      "Seed: 042, Epoch: 080, Loss: 1.5656, Train Acc: 0.5651, Val Acc: 0.5417, Test Acc: 0.4758\n",
      "Seed: 042, Epoch: 090, Loss: 1.5336, Train Acc: 0.5705, Val Acc: 0.5475, Test Acc: 0.4822\n",
      "Seed: 042, Epoch: 100, Loss: 1.5167, Train Acc: 0.5768, Val Acc: 0.5544, Test Acc: 0.4880\n",
      "Seed: 042, Epoch: 110, Loss: 1.4931, Train Acc: 0.5791, Val Acc: 0.5543, Test Acc: 0.4854\n",
      "Seed: 042, Epoch: 120, Loss: 1.4770, Train Acc: 0.5844, Val Acc: 0.5601, Test Acc: 0.4951\n",
      "Seed: 042, Epoch: 130, Loss: 1.4667, Train Acc: 0.5859, Val Acc: 0.5617, Test Acc: 0.4970\n",
      "Seed: 042, Epoch: 140, Loss: 1.4601, Train Acc: 0.5865, Val Acc: 0.5621, Test Acc: 0.4978\n",
      "Seed: 042, Epoch: 150, Loss: 1.4446, Train Acc: 0.5910, Val Acc: 0.5687, Test Acc: 0.5057\n",
      "Seed: 042, Epoch: 160, Loss: 1.4325, Train Acc: 0.5939, Val Acc: 0.5711, Test Acc: 0.5063\n",
      "Seed: 042, Epoch: 170, Loss: 1.4347, Train Acc: 0.5912, Val Acc: 0.5695, Test Acc: 0.5128\n",
      "Seed: 042, Epoch: 180, Loss: 1.4209, Train Acc: 0.5950, Val Acc: 0.5716, Test Acc: 0.5114\n",
      "Seed: 042, Epoch: 190, Loss: 1.4105, Train Acc: 0.5977, Val Acc: 0.5702, Test Acc: 0.5019\n",
      "Seed: 042, Epoch: 200, Loss: 1.4257, Train Acc: 0.5973, Val Acc: 0.5743, Test Acc: 0.5071\n",
      "Seed: 042, Epoch: 210, Loss: 1.4040, Train Acc: 0.6011, Val Acc: 0.5777, Test Acc: 0.5136\n",
      "Seed: 042, Epoch: 220, Loss: 1.3929, Train Acc: 0.6030, Val Acc: 0.5778, Test Acc: 0.5115\n",
      "Seed: 042, Epoch: 230, Loss: 1.3901, Train Acc: 0.6032, Val Acc: 0.5748, Test Acc: 0.5054\n",
      "Seed: 042, Epoch: 240, Loss: 1.3858, Train Acc: 0.6062, Val Acc: 0.5794, Test Acc: 0.5124\n",
      "Seed: 042, Epoch: 250, Loss: 1.3803, Train Acc: 0.6064, Val Acc: 0.5809, Test Acc: 0.5149\n",
      "Seed: 042, Epoch: 260, Loss: 1.3727, Train Acc: 0.6075, Val Acc: 0.5778, Test Acc: 0.5121\n",
      "Seed: 042, Epoch: 270, Loss: 1.3924, Train Acc: 0.5988, Val Acc: 0.5646, Test Acc: 0.4987\n",
      "Seed: 042, Epoch: 280, Loss: 1.3750, Train Acc: 0.6075, Val Acc: 0.5840, Test Acc: 0.5230\n",
      "Seed: 042, Epoch: 290, Loss: 1.3678, Train Acc: 0.6099, Val Acc: 0.5790, Test Acc: 0.5149\n",
      "Seed: 042, Epoch: 300, Loss: 1.3615, Train Acc: 0.6099, Val Acc: 0.5809, Test Acc: 0.5181\n",
      "Seed: 042, Epoch: 310, Loss: 1.3605, Train Acc: 0.6107, Val Acc: 0.5802, Test Acc: 0.5131\n",
      "Seed: 042, Epoch: 320, Loss: 1.3531, Train Acc: 0.6125, Val Acc: 0.5839, Test Acc: 0.5214\n",
      "Seed: 042, Epoch: 330, Loss: 1.3489, Train Acc: 0.6140, Val Acc: 0.5854, Test Acc: 0.5212\n",
      "Seed: 042, Epoch: 340, Loss: 1.3705, Train Acc: 0.6084, Val Acc: 0.5793, Test Acc: 0.5250\n",
      "Seed: 042, Epoch: 350, Loss: 1.3453, Train Acc: 0.6142, Val Acc: 0.5833, Test Acc: 0.5183\n",
      "Seed: 042, Epoch: 360, Loss: 1.3416, Train Acc: 0.6154, Val Acc: 0.5840, Test Acc: 0.5187\n",
      "Seed: 042, Epoch: 370, Loss: 1.3376, Train Acc: 0.6162, Val Acc: 0.5861, Test Acc: 0.5208\n",
      "Seed: 042, Epoch: 380, Loss: 1.3409, Train Acc: 0.6157, Val Acc: 0.5854, Test Acc: 0.5217\n",
      "Seed: 042, Epoch: 390, Loss: 1.3433, Train Acc: 0.6150, Val Acc: 0.5857, Test Acc: 0.5232\n",
      "Seed: 042, Epoch: 400, Loss: 1.3327, Train Acc: 0.6174, Val Acc: 0.5866, Test Acc: 0.5264\n",
      "Seed: 042, Epoch: 410, Loss: 1.3292, Train Acc: 0.6177, Val Acc: 0.5869, Test Acc: 0.5257\n",
      "Seed: 042, Epoch: 420, Loss: 1.3299, Train Acc: 0.6172, Val Acc: 0.5815, Test Acc: 0.5125\n",
      "Seed: 042, Epoch: 430, Loss: 1.3288, Train Acc: 0.6175, Val Acc: 0.5873, Test Acc: 0.5206\n",
      "Seed: 042, Epoch: 440, Loss: 1.3286, Train Acc: 0.6189, Val Acc: 0.5887, Test Acc: 0.5269\n",
      "Seed: 042, Epoch: 450, Loss: 1.3235, Train Acc: 0.6188, Val Acc: 0.5851, Test Acc: 0.5190\n",
      "Seed: 042, Epoch: 460, Loss: 1.3253, Train Acc: 0.6203, Val Acc: 0.5889, Test Acc: 0.5260\n",
      "Seed: 042, Epoch: 470, Loss: 1.3174, Train Acc: 0.6214, Val Acc: 0.5904, Test Acc: 0.5287\n",
      "Seed: 042, Epoch: 480, Loss: 1.3168, Train Acc: 0.6216, Val Acc: 0.5880, Test Acc: 0.5237\n",
      "Seed: 042, Epoch: 490, Loss: 1.3132, Train Acc: 0.6218, Val Acc: 0.5891, Test Acc: 0.5277\n",
      "Seed: 042, Epoch: 500, Loss: 1.3146, Train Acc: 0.6230, Val Acc: 0.5853, Test Acc: 0.5217\n",
      "Seed: 123, Epoch: 010, Loss: 2.8030, Train Acc: 0.2816, Val Acc: 0.2939, Test Acc: 0.2562\n",
      "Seed: 123, Epoch: 020, Loss: 2.3807, Train Acc: 0.3914, Val Acc: 0.4103, Test Acc: 0.3531\n",
      "Seed: 123, Epoch: 030, Loss: 2.0018, Train Acc: 0.4622, Val Acc: 0.4653, Test Acc: 0.4047\n",
      "Seed: 123, Epoch: 040, Loss: 1.8049, Train Acc: 0.5173, Val Acc: 0.5085, Test Acc: 0.4501\n",
      "Seed: 123, Epoch: 050, Loss: 1.6746, Train Acc: 0.5411, Val Acc: 0.5319, Test Acc: 0.4658\n",
      "Seed: 123, Epoch: 060, Loss: 1.5989, Train Acc: 0.5560, Val Acc: 0.5432, Test Acc: 0.4790\n",
      "Seed: 123, Epoch: 070, Loss: 1.5579, Train Acc: 0.5647, Val Acc: 0.5447, Test Acc: 0.4736\n",
      "Seed: 123, Epoch: 080, Loss: 1.5251, Train Acc: 0.5742, Val Acc: 0.5539, Test Acc: 0.4872\n",
      "Seed: 123, Epoch: 090, Loss: 1.5003, Train Acc: 0.5802, Val Acc: 0.5593, Test Acc: 0.4935\n",
      "Seed: 123, Epoch: 100, Loss: 1.4791, Train Acc: 0.5841, Val Acc: 0.5557, Test Acc: 0.4901\n",
      "Seed: 123, Epoch: 110, Loss: 1.4663, Train Acc: 0.5839, Val Acc: 0.5519, Test Acc: 0.4885\n",
      "Seed: 123, Epoch: 120, Loss: 1.4502, Train Acc: 0.5893, Val Acc: 0.5616, Test Acc: 0.4960\n",
      "Seed: 123, Epoch: 130, Loss: 1.4342, Train Acc: 0.5942, Val Acc: 0.5702, Test Acc: 0.5032\n",
      "Seed: 123, Epoch: 140, Loss: 1.4226, Train Acc: 0.5969, Val Acc: 0.5711, Test Acc: 0.5057\n",
      "Seed: 123, Epoch: 150, Loss: 1.4115, Train Acc: 0.6007, Val Acc: 0.5759, Test Acc: 0.5093\n",
      "Seed: 123, Epoch: 160, Loss: 1.4075, Train Acc: 0.5986, Val Acc: 0.5687, Test Acc: 0.5057\n",
      "Seed: 123, Epoch: 170, Loss: 1.3957, Train Acc: 0.6034, Val Acc: 0.5808, Test Acc: 0.5164\n",
      "Seed: 123, Epoch: 180, Loss: 1.3896, Train Acc: 0.6045, Val Acc: 0.5720, Test Acc: 0.5060\n",
      "Seed: 123, Epoch: 190, Loss: 1.3831, Train Acc: 0.6065, Val Acc: 0.5764, Test Acc: 0.5113\n",
      "Seed: 123, Epoch: 200, Loss: 1.3808, Train Acc: 0.6050, Val Acc: 0.5778, Test Acc: 0.5125\n",
      "Seed: 123, Epoch: 210, Loss: 1.3758, Train Acc: 0.6086, Val Acc: 0.5791, Test Acc: 0.5162\n",
      "Seed: 123, Epoch: 220, Loss: 1.3682, Train Acc: 0.6109, Val Acc: 0.5842, Test Acc: 0.5186\n",
      "Seed: 123, Epoch: 230, Loss: 1.3674, Train Acc: 0.6110, Val Acc: 0.5790, Test Acc: 0.5111\n",
      "Seed: 123, Epoch: 240, Loss: 1.3613, Train Acc: 0.6119, Val Acc: 0.5826, Test Acc: 0.5163\n",
      "Seed: 123, Epoch: 250, Loss: 1.3551, Train Acc: 0.6145, Val Acc: 0.5815, Test Acc: 0.5149\n",
      "Seed: 123, Epoch: 260, Loss: 1.3492, Train Acc: 0.6153, Val Acc: 0.5818, Test Acc: 0.5168\n",
      "Seed: 123, Epoch: 270, Loss: 1.3463, Train Acc: 0.6156, Val Acc: 0.5844, Test Acc: 0.5186\n",
      "Seed: 123, Epoch: 280, Loss: 1.3410, Train Acc: 0.6163, Val Acc: 0.5838, Test Acc: 0.5192\n",
      "Seed: 123, Epoch: 290, Loss: 1.3412, Train Acc: 0.6166, Val Acc: 0.5812, Test Acc: 0.5149\n",
      "Seed: 123, Epoch: 300, Loss: 1.3339, Train Acc: 0.6179, Val Acc: 0.5881, Test Acc: 0.5259\n",
      "Seed: 123, Epoch: 310, Loss: 1.3394, Train Acc: 0.6190, Val Acc: 0.5834, Test Acc: 0.5190\n",
      "Seed: 123, Epoch: 320, Loss: 1.3345, Train Acc: 0.6158, Val Acc: 0.5849, Test Acc: 0.5202\n",
      "Seed: 123, Epoch: 330, Loss: 1.3264, Train Acc: 0.6204, Val Acc: 0.5831, Test Acc: 0.5186\n",
      "Seed: 123, Epoch: 340, Loss: 1.3335, Train Acc: 0.6153, Val Acc: 0.5861, Test Acc: 0.5345\n",
      "Seed: 123, Epoch: 350, Loss: 1.3286, Train Acc: 0.6191, Val Acc: 0.5859, Test Acc: 0.5277\n",
      "Seed: 123, Epoch: 360, Loss: 1.3198, Train Acc: 0.6220, Val Acc: 0.5888, Test Acc: 0.5251\n",
      "Seed: 123, Epoch: 370, Loss: 1.3136, Train Acc: 0.6225, Val Acc: 0.5897, Test Acc: 0.5253\n",
      "Seed: 123, Epoch: 380, Loss: 1.3106, Train Acc: 0.6235, Val Acc: 0.5909, Test Acc: 0.5306\n",
      "Seed: 123, Epoch: 390, Loss: 1.3073, Train Acc: 0.6228, Val Acc: 0.5878, Test Acc: 0.5278\n",
      "Seed: 123, Epoch: 400, Loss: 1.3056, Train Acc: 0.6258, Val Acc: 0.5913, Test Acc: 0.5296\n",
      "Seed: 123, Epoch: 410, Loss: 1.3040, Train Acc: 0.6252, Val Acc: 0.5882, Test Acc: 0.5245\n",
      "Seed: 123, Epoch: 420, Loss: 1.3064, Train Acc: 0.6218, Val Acc: 0.5888, Test Acc: 0.5279\n",
      "Seed: 123, Epoch: 430, Loss: 1.3592, Train Acc: 0.6121, Val Acc: 0.5837, Test Acc: 0.5234\n",
      "Seed: 123, Epoch: 440, Loss: 1.3330, Train Acc: 0.6173, Val Acc: 0.5838, Test Acc: 0.5218\n",
      "Seed: 123, Epoch: 450, Loss: 1.3248, Train Acc: 0.6249, Val Acc: 0.5852, Test Acc: 0.5227\n",
      "Seed: 123, Epoch: 460, Loss: 1.3026, Train Acc: 0.6262, Val Acc: 0.5905, Test Acc: 0.5276\n",
      "Seed: 123, Epoch: 470, Loss: 1.2922, Train Acc: 0.6282, Val Acc: 0.5880, Test Acc: 0.5259\n",
      "Seed: 123, Epoch: 480, Loss: 1.2877, Train Acc: 0.6293, Val Acc: 0.5894, Test Acc: 0.5256\n",
      "Seed: 123, Epoch: 490, Loss: 1.3148, Train Acc: 0.6263, Val Acc: 0.5840, Test Acc: 0.5190\n",
      "Seed: 123, Epoch: 500, Loss: 1.2838, Train Acc: 0.6281, Val Acc: 0.5925, Test Acc: 0.5321\n",
      "Seed: 456, Epoch: 010, Loss: 3.3511, Train Acc: 0.0913, Val Acc: 0.1759, Test Acc: 0.2447\n",
      "Seed: 456, Epoch: 020, Loss: 2.6230, Train Acc: 0.3587, Val Acc: 0.3968, Test Acc: 0.3630\n",
      "Seed: 456, Epoch: 030, Loss: 2.0834, Train Acc: 0.4490, Val Acc: 0.4577, Test Acc: 0.4008\n",
      "Seed: 456, Epoch: 040, Loss: 1.8649, Train Acc: 0.5026, Val Acc: 0.4929, Test Acc: 0.4301\n",
      "Seed: 456, Epoch: 050, Loss: 1.7295, Train Acc: 0.5313, Val Acc: 0.5171, Test Acc: 0.4553\n",
      "Seed: 456, Epoch: 060, Loss: 1.6265, Train Acc: 0.5521, Val Acc: 0.5296, Test Acc: 0.4654\n",
      "Seed: 456, Epoch: 070, Loss: 1.5731, Train Acc: 0.5640, Val Acc: 0.5417, Test Acc: 0.4763\n",
      "Seed: 456, Epoch: 080, Loss: 1.5432, Train Acc: 0.5701, Val Acc: 0.5510, Test Acc: 0.4877\n",
      "Seed: 456, Epoch: 090, Loss: 1.5224, Train Acc: 0.5751, Val Acc: 0.5566, Test Acc: 0.4928\n",
      "Seed: 456, Epoch: 100, Loss: 1.5001, Train Acc: 0.5804, Val Acc: 0.5600, Test Acc: 0.4981\n",
      "Seed: 456, Epoch: 110, Loss: 1.5024, Train Acc: 0.5784, Val Acc: 0.5494, Test Acc: 0.4782\n",
      "Seed: 456, Epoch: 120, Loss: 1.4693, Train Acc: 0.5873, Val Acc: 0.5601, Test Acc: 0.4978\n",
      "Seed: 456, Epoch: 130, Loss: 1.4542, Train Acc: 0.5905, Val Acc: 0.5628, Test Acc: 0.4997\n",
      "Seed: 456, Epoch: 140, Loss: 1.4454, Train Acc: 0.5933, Val Acc: 0.5695, Test Acc: 0.5112\n",
      "Seed: 456, Epoch: 150, Loss: 1.4336, Train Acc: 0.5954, Val Acc: 0.5695, Test Acc: 0.5041\n",
      "Seed: 456, Epoch: 160, Loss: 1.4253, Train Acc: 0.5976, Val Acc: 0.5704, Test Acc: 0.5075\n",
      "Seed: 456, Epoch: 170, Loss: 1.4233, Train Acc: 0.6005, Val Acc: 0.5742, Test Acc: 0.5119\n",
      "Seed: 456, Epoch: 180, Loss: 1.4119, Train Acc: 0.6004, Val Acc: 0.5684, Test Acc: 0.5010\n",
      "Seed: 456, Epoch: 190, Loss: 1.4037, Train Acc: 0.6020, Val Acc: 0.5750, Test Acc: 0.5112\n",
      "Seed: 456, Epoch: 200, Loss: 1.4019, Train Acc: 0.6011, Val Acc: 0.5709, Test Acc: 0.5105\n",
      "Seed: 456, Epoch: 210, Loss: 1.3917, Train Acc: 0.6046, Val Acc: 0.5750, Test Acc: 0.5121\n",
      "Seed: 456, Epoch: 220, Loss: 1.3859, Train Acc: 0.6061, Val Acc: 0.5789, Test Acc: 0.5164\n",
      "Seed: 456, Epoch: 230, Loss: 1.3828, Train Acc: 0.6072, Val Acc: 0.5773, Test Acc: 0.5130\n",
      "Seed: 456, Epoch: 240, Loss: 1.3742, Train Acc: 0.6082, Val Acc: 0.5763, Test Acc: 0.5115\n",
      "Seed: 456, Epoch: 250, Loss: 1.3850, Train Acc: 0.6046, Val Acc: 0.5713, Test Acc: 0.5039\n",
      "Seed: 456, Epoch: 260, Loss: 1.3690, Train Acc: 0.6101, Val Acc: 0.5850, Test Acc: 0.5200\n",
      "Seed: 456, Epoch: 270, Loss: 1.3707, Train Acc: 0.6072, Val Acc: 0.5846, Test Acc: 0.5297\n",
      "Seed: 456, Epoch: 280, Loss: 1.3584, Train Acc: 0.6119, Val Acc: 0.5839, Test Acc: 0.5204\n",
      "Seed: 456, Epoch: 290, Loss: 1.3555, Train Acc: 0.6129, Val Acc: 0.5837, Test Acc: 0.5168\n",
      "Seed: 456, Epoch: 300, Loss: 1.3538, Train Acc: 0.6122, Val Acc: 0.5781, Test Acc: 0.5150\n",
      "Seed: 456, Epoch: 310, Loss: 1.3506, Train Acc: 0.6144, Val Acc: 0.5852, Test Acc: 0.5227\n",
      "Seed: 456, Epoch: 320, Loss: 1.3457, Train Acc: 0.6157, Val Acc: 0.5847, Test Acc: 0.5205\n",
      "Seed: 456, Epoch: 330, Loss: 1.3455, Train Acc: 0.6138, Val Acc: 0.5853, Test Acc: 0.5253\n",
      "Seed: 456, Epoch: 340, Loss: 1.3398, Train Acc: 0.6161, Val Acc: 0.5870, Test Acc: 0.5240\n",
      "Seed: 456, Epoch: 350, Loss: 1.3388, Train Acc: 0.6166, Val Acc: 0.5832, Test Acc: 0.5187\n",
      "Seed: 456, Epoch: 360, Loss: 1.3389, Train Acc: 0.6168, Val Acc: 0.5890, Test Acc: 0.5306\n",
      "Seed: 456, Epoch: 370, Loss: 1.3345, Train Acc: 0.6173, Val Acc: 0.5849, Test Acc: 0.5201\n",
      "Seed: 456, Epoch: 380, Loss: 1.3298, Train Acc: 0.6184, Val Acc: 0.5903, Test Acc: 0.5329\n",
      "Seed: 456, Epoch: 390, Loss: 1.3321, Train Acc: 0.6157, Val Acc: 0.5823, Test Acc: 0.5246\n",
      "Seed: 456, Epoch: 400, Loss: 1.3293, Train Acc: 0.6180, Val Acc: 0.5803, Test Acc: 0.5155\n",
      "Seed: 456, Epoch: 410, Loss: 1.3221, Train Acc: 0.6198, Val Acc: 0.5839, Test Acc: 0.5216\n",
      "Seed: 456, Epoch: 420, Loss: 1.3322, Train Acc: 0.6155, Val Acc: 0.5840, Test Acc: 0.5245\n",
      "Seed: 456, Epoch: 430, Loss: 1.3207, Train Acc: 0.6200, Val Acc: 0.5869, Test Acc: 0.5247\n",
      "Seed: 456, Epoch: 440, Loss: 1.3161, Train Acc: 0.6215, Val Acc: 0.5888, Test Acc: 0.5268\n",
      "Seed: 456, Epoch: 450, Loss: 1.3141, Train Acc: 0.6221, Val Acc: 0.5865, Test Acc: 0.5216\n",
      "Seed: 456, Epoch: 460, Loss: 1.3214, Train Acc: 0.6205, Val Acc: 0.5869, Test Acc: 0.5328\n",
      "Seed: 456, Epoch: 470, Loss: 1.3117, Train Acc: 0.6223, Val Acc: 0.5842, Test Acc: 0.5197\n",
      "Seed: 456, Epoch: 480, Loss: 1.3160, Train Acc: 0.6199, Val Acc: 0.5833, Test Acc: 0.5178\n",
      "Seed: 456, Epoch: 490, Loss: 1.3110, Train Acc: 0.6218, Val Acc: 0.5906, Test Acc: 0.5318\n",
      "Seed: 456, Epoch: 500, Loss: 1.3056, Train Acc: 0.6231, Val Acc: 0.5883, Test Acc: 0.5274\n",
      "Seed: 42, Best Val Acc: 0.5909, Corresponding Test Acc: 0.5334, Time: 72.57s, Memory: 9.12MB, GPU Memory: 4520.89MB\n",
      "Seed: 123, Best Val Acc: 0.5941, Corresponding Test Acc: 0.5334, Time: 72.55s, Memory: 1.52MB, GPU Memory: 4520.40MB\n",
      "Seed: 456, Best Val Acc: 0.5928, Corresponding Test Acc: 0.5347, Time: 72.25s, Memory: 1.52MB, GPU Memory: 4520.09MB\n",
      "\n",
      "Average Best Validation Accuracy: 0.5926\n",
      "Average Corresponding Test Accuracy: 0.5338\n",
      "Average Time: 72.46s\n",
      "Average Memory Usage: 4.05MB\n",
      "Average GPU Memory Usage: 4520.46MB\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import ASAPooling\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.sum_res = sum_res\n",
    "        self.act = act\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(256, k=5, aggr_x='sum'))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.0, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = F.relu(x)  \n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "\n",
    "            up = res\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    test_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    model = HierarchicalGCN_KMIS(\n",
    "        in_channels=graph.num_features,\n",
    "        hidden_channels=256,\n",
    "        out_channels=dataset.num_classes,\n",
    "        depth=2,\n",
    "        act=F.relu,\n",
    "        sum_res=False\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    for epoch in range(1, 501):\n",
    "        loss = train(model, graph, split_idx['train'], optimizer)\n",
    "        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n",
    "        val_accuracies.append(valid_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    memory_usage = current / 10**6  # Convert to MB\n",
    "    peak_memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  # Convert to MB\n",
    "        torch.cuda.reset_max_memory_allocated(device)\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    val_accuracies_list.append(val_accuracies)\n",
    "    times.append(elapsed_time)\n",
    "    memories.append(memory_usage)\n",
    "    gpu_memories.append(gpu_memory_usage)\n",
    "\n",
    "    best_val_acc = max(val_accuracies)\n",
    "    best_val_index = val_accuracies.index(best_val_acc)\n",
    "    corresponding_test_acc = test_accuracies[best_val_index]\n",
    "\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'corresponding_test_acc': corresponding_test_acc,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'memory_usage': memory_usage,\n",
    "        'gpu_memory_usage': gpu_memory_usage\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n",
    "          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n",
    "          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n",
    "          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\n",
    "\n",
    "# Calculate average results\n",
    "avg_val_acc = np.mean([result['best_val_acc'] for result in results])\n",
    "avg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\n",
    "avg_time = np.mean(times)\n",
    "avg_memory = np.mean(memories)\n",
    "avg_gpu_memory = np.mean(gpu_memories)\n",
    "\n",
    "print(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "print(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\n",
    "print(f\"Average Time: {avg_time:.2f}s\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f}MB\")\n",
    "print(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSAPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Union, Optional, Callable\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, ChebConv, GraphConv\n",
    "\n",
    "def uniform(size, tensor):\n",
    "    if tensor is not None:\n",
    "        bound = 1.0 / math.sqrt(size)\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "\n",
    "def topk(x, ratio, batch, min_score=None, tol=1e-7):\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n",
    "    else:\n",
    "        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "        cum_num_nodes = torch.cat(\n",
    "            [num_nodes.new_zeros(1),\n",
    "             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "        dense_x = x.new_full((batch_size * max_num_nodes, ),\n",
    "                             torch.finfo(x.dtype).min)\n",
    "        dense_x[index] = x\n",
    "        dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "        _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "        perm = perm + cum_num_nodes.view(-1, 1)\n",
    "        perm = perm.view(-1)\n",
    "\n",
    "        if isinstance(ratio, int):\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n",
    "            k = torch.min(k, num_nodes)\n",
    "        else:\n",
    "            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "\n",
    "        mask = [\n",
    "            torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "            i * max_num_nodes for i in range(batch_size)\n",
    "        ]\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "\n",
    "        perm = perm[mask]\n",
    "\n",
    "    return perm\n",
    "\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "\n",
    "class GSAPool(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n",
    "                        min_score=None, multiplier=1,\n",
    "                        non_linearity=torch.tanh,\n",
    "                        cus_drop_ratio =0):\n",
    "        super(GSAPool,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.ratio = pooling_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.sbtl_layer = GCNConv(in_channels,1)\n",
    "        self.fbtl_layer = nn.Linear(in_channels, 1)\n",
    "        self.fusion = GCNConv(in_channels,in_channels)\n",
    "\n",
    "        self.min_score = min_score\n",
    "        self.multiplier = multiplier\n",
    "        self.fusion_flag = 0\n",
    "        self.non_linearity = non_linearity\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n",
    "\n",
    "    def conv_selection(self, conv, in_channels, conv_type=0):\n",
    "        if(conv_type == 0):\n",
    "            out_channels = 1\n",
    "        elif(conv_type == 1):\n",
    "            out_channels = in_channels\n",
    "        if(conv == \"GCNConv\"):\n",
    "            return GCNConv(in_channels,out_channels)\n",
    "        elif(conv == \"ChebConv\"):\n",
    "            return ChebConv(in_channels,out_channels,1)\n",
    "        elif(conv == \"SAGEConv\"):\n",
    "            return SAGEConv(in_channels,out_channels)\n",
    "        elif(conv == \"GATConv\"):\n",
    "            return GATConv(in_channels,out_channels, heads=1, concat=True)\n",
    "        elif(conv == \"GraphConv\"):\n",
    "            return GraphConv(in_channels,out_channels)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "\n",
    "        #SBTL\n",
    "        score_s = self.sbtl_layer(x,edge_index).squeeze()\n",
    "        #FBTL\n",
    "        score_f = self.fbtl_layer(x).squeeze()\n",
    "        #hyperparametr alpha\n",
    "        score = score_s*self.alpha + score_f*(1-self.alpha)\n",
    "\n",
    "        score = score.unsqueeze(-1) if score.dim()==0 else score\n",
    "\n",
    "        if self.min_score is None:\n",
    "            score = self.non_linearity(score)\n",
    "        else:\n",
    "            score = softmax(score, batch)\n",
    "\n",
    "        sc = self.dropout(score)\n",
    "        perm = topk(sc, self.ratio, batch)\n",
    "\n",
    "        #fusion\n",
    "        if(self.fusion_flag == 1):\n",
    "            x = self.fusion(x, edge_index)\n",
    "        x_ae = x[perm]\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        x = self.multiplier * x if self.multiplier != 1 else x\n",
    "\n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_attr = filter_adj(\n",
    "            edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, perm, x_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 042, Epoch: 010, Loss: 3.1351, Train Acc: 0.2226, Val Acc: 0.2992, Test Acc: 0.2945\n",
      "Seed: 042, Epoch: 020, Loss: 2.5531, Train Acc: 0.3552, Val Acc: 0.3848, Test Acc: 0.3429\n",
      "Seed: 042, Epoch: 030, Loss: 2.1509, Train Acc: 0.4393, Val Acc: 0.4487, Test Acc: 0.3909\n",
      "Seed: 042, Epoch: 040, Loss: 1.9204, Train Acc: 0.4870, Val Acc: 0.4847, Test Acc: 0.4309\n",
      "Seed: 042, Epoch: 050, Loss: 1.7517, Train Acc: 0.5223, Val Acc: 0.5113, Test Acc: 0.4510\n",
      "Seed: 042, Epoch: 060, Loss: 1.6709, Train Acc: 0.5415, Val Acc: 0.5294, Test Acc: 0.4660\n",
      "Seed: 042, Epoch: 070, Loss: 1.6126, Train Acc: 0.5539, Val Acc: 0.5405, Test Acc: 0.4756\n",
      "Seed: 042, Epoch: 080, Loss: 1.5653, Train Acc: 0.5641, Val Acc: 0.5488, Test Acc: 0.4843\n",
      "Seed: 042, Epoch: 090, Loss: 1.5510, Train Acc: 0.5673, Val Acc: 0.5489, Test Acc: 0.4833\n",
      "Seed: 042, Epoch: 100, Loss: 1.5212, Train Acc: 0.5751, Val Acc: 0.5548, Test Acc: 0.4893\n",
      "Seed: 042, Epoch: 110, Loss: 1.4951, Train Acc: 0.5793, Val Acc: 0.5569, Test Acc: 0.4932\n",
      "Seed: 042, Epoch: 120, Loss: 1.4772, Train Acc: 0.5836, Val Acc: 0.5593, Test Acc: 0.4951\n",
      "Seed: 042, Epoch: 130, Loss: 1.4642, Train Acc: 0.5863, Val Acc: 0.5598, Test Acc: 0.4927\n",
      "Seed: 042, Epoch: 140, Loss: 1.4496, Train Acc: 0.5908, Val Acc: 0.5701, Test Acc: 0.5092\n",
      "Seed: 042, Epoch: 150, Loss: 1.4347, Train Acc: 0.5938, Val Acc: 0.5697, Test Acc: 0.5053\n",
      "Seed: 042, Epoch: 160, Loss: 1.4310, Train Acc: 0.5925, Val Acc: 0.5690, Test Acc: 0.5016\n",
      "Seed: 042, Epoch: 170, Loss: 1.4229, Train Acc: 0.5975, Val Acc: 0.5695, Test Acc: 0.5030\n",
      "Seed: 042, Epoch: 180, Loss: 1.4119, Train Acc: 0.5993, Val Acc: 0.5745, Test Acc: 0.5061\n",
      "Seed: 042, Epoch: 190, Loss: 1.4036, Train Acc: 0.6000, Val Acc: 0.5719, Test Acc: 0.5028\n",
      "Seed: 042, Epoch: 200, Loss: 1.3981, Train Acc: 0.6008, Val Acc: 0.5748, Test Acc: 0.5137\n",
      "Seed: 042, Epoch: 210, Loss: 1.3928, Train Acc: 0.6037, Val Acc: 0.5779, Test Acc: 0.5096\n",
      "Seed: 042, Epoch: 220, Loss: 1.3839, Train Acc: 0.6050, Val Acc: 0.5833, Test Acc: 0.5199\n",
      "Seed: 042, Epoch: 230, Loss: 1.3850, Train Acc: 0.6061, Val Acc: 0.5827, Test Acc: 0.5183\n",
      "Seed: 042, Epoch: 240, Loss: 1.3760, Train Acc: 0.6064, Val Acc: 0.5750, Test Acc: 0.5105\n",
      "Seed: 042, Epoch: 250, Loss: 1.4040, Train Acc: 0.6051, Val Acc: 0.5867, Test Acc: 0.5285\n",
      "Seed: 042, Epoch: 260, Loss: 1.3713, Train Acc: 0.6073, Val Acc: 0.5736, Test Acc: 0.5074\n",
      "Seed: 042, Epoch: 270, Loss: 1.3687, Train Acc: 0.6079, Val Acc: 0.5836, Test Acc: 0.5222\n",
      "Seed: 042, Epoch: 280, Loss: 1.3631, Train Acc: 0.6098, Val Acc: 0.5813, Test Acc: 0.5224\n",
      "Seed: 042, Epoch: 290, Loss: 1.3556, Train Acc: 0.6103, Val Acc: 0.5794, Test Acc: 0.5129\n",
      "Seed: 042, Epoch: 300, Loss: 1.3503, Train Acc: 0.6123, Val Acc: 0.5820, Test Acc: 0.5203\n",
      "Seed: 042, Epoch: 310, Loss: 1.3514, Train Acc: 0.6121, Val Acc: 0.5848, Test Acc: 0.5271\n",
      "Seed: 042, Epoch: 320, Loss: 1.3554, Train Acc: 0.6112, Val Acc: 0.5834, Test Acc: 0.5246\n",
      "Seed: 042, Epoch: 330, Loss: 1.3426, Train Acc: 0.6154, Val Acc: 0.5833, Test Acc: 0.5179\n",
      "Seed: 042, Epoch: 340, Loss: 1.3369, Train Acc: 0.6157, Val Acc: 0.5814, Test Acc: 0.5153\n",
      "Seed: 042, Epoch: 350, Loss: 1.3363, Train Acc: 0.6165, Val Acc: 0.5865, Test Acc: 0.5230\n",
      "Seed: 042, Epoch: 360, Loss: 1.3351, Train Acc: 0.6158, Val Acc: 0.5870, Test Acc: 0.5270\n",
      "Seed: 042, Epoch: 370, Loss: 1.3319, Train Acc: 0.6167, Val Acc: 0.5833, Test Acc: 0.5176\n",
      "Seed: 042, Epoch: 380, Loss: 1.3265, Train Acc: 0.6177, Val Acc: 0.5877, Test Acc: 0.5262\n",
      "Seed: 042, Epoch: 390, Loss: 1.3236, Train Acc: 0.6170, Val Acc: 0.5873, Test Acc: 0.5263\n",
      "Seed: 042, Epoch: 400, Loss: 1.3252, Train Acc: 0.6189, Val Acc: 0.5851, Test Acc: 0.5187\n",
      "Seed: 042, Epoch: 410, Loss: 1.3196, Train Acc: 0.6197, Val Acc: 0.5841, Test Acc: 0.5198\n",
      "Seed: 042, Epoch: 420, Loss: 1.3150, Train Acc: 0.6210, Val Acc: 0.5889, Test Acc: 0.5247\n",
      "Seed: 042, Epoch: 430, Loss: 1.3281, Train Acc: 0.6139, Val Acc: 0.5742, Test Acc: 0.5057\n",
      "Seed: 042, Epoch: 440, Loss: 1.3214, Train Acc: 0.6204, Val Acc: 0.5850, Test Acc: 0.5195\n",
      "Seed: 042, Epoch: 450, Loss: 1.3144, Train Acc: 0.6207, Val Acc: 0.5842, Test Acc: 0.5210\n",
      "Seed: 042, Epoch: 460, Loss: 1.3133, Train Acc: 0.6213, Val Acc: 0.5883, Test Acc: 0.5250\n",
      "Seed: 042, Epoch: 470, Loss: 1.3111, Train Acc: 0.6225, Val Acc: 0.5845, Test Acc: 0.5195\n",
      "Seed: 042, Epoch: 480, Loss: 1.3061, Train Acc: 0.6218, Val Acc: 0.5862, Test Acc: 0.5233\n",
      "Seed: 042, Epoch: 490, Loss: 1.3133, Train Acc: 0.6221, Val Acc: 0.5857, Test Acc: 0.5220\n",
      "Seed: 042, Epoch: 500, Loss: 1.3021, Train Acc: 0.6232, Val Acc: 0.5897, Test Acc: 0.5305\n",
      "Seed: 123, Epoch: 010, Loss: 3.1682, Train Acc: 0.2616, Val Acc: 0.2511, Test Acc: 0.2445\n",
      "Seed: 123, Epoch: 020, Loss: 2.6134, Train Acc: 0.3227, Val Acc: 0.3519, Test Acc: 0.3369\n",
      "Seed: 123, Epoch: 030, Loss: 2.1776, Train Acc: 0.4197, Val Acc: 0.4319, Test Acc: 0.3763\n",
      "Seed: 123, Epoch: 040, Loss: 1.9283, Train Acc: 0.4835, Val Acc: 0.4839, Test Acc: 0.4256\n",
      "Seed: 123, Epoch: 050, Loss: 1.7570, Train Acc: 0.5218, Val Acc: 0.5064, Test Acc: 0.4450\n",
      "Seed: 123, Epoch: 060, Loss: 1.6782, Train Acc: 0.5392, Val Acc: 0.5252, Test Acc: 0.4688\n",
      "Seed: 123, Epoch: 070, Loss: 1.6188, Train Acc: 0.5531, Val Acc: 0.5344, Test Acc: 0.4723\n",
      "Seed: 123, Epoch: 080, Loss: 1.5745, Train Acc: 0.5633, Val Acc: 0.5441, Test Acc: 0.4836\n",
      "Seed: 123, Epoch: 090, Loss: 1.5460, Train Acc: 0.5702, Val Acc: 0.5508, Test Acc: 0.4858\n",
      "Seed: 123, Epoch: 100, Loss: 1.5201, Train Acc: 0.5742, Val Acc: 0.5499, Test Acc: 0.4828\n",
      "Seed: 123, Epoch: 110, Loss: 1.4999, Train Acc: 0.5812, Val Acc: 0.5604, Test Acc: 0.4967\n",
      "Seed: 123, Epoch: 120, Loss: 1.4800, Train Acc: 0.5848, Val Acc: 0.5625, Test Acc: 0.4988\n",
      "Seed: 123, Epoch: 130, Loss: 1.4683, Train Acc: 0.5844, Val Acc: 0.5639, Test Acc: 0.4986\n",
      "Seed: 123, Epoch: 140, Loss: 1.4586, Train Acc: 0.5907, Val Acc: 0.5656, Test Acc: 0.4961\n",
      "Seed: 123, Epoch: 150, Loss: 1.4418, Train Acc: 0.5937, Val Acc: 0.5676, Test Acc: 0.5011\n",
      "Seed: 123, Epoch: 160, Loss: 1.4440, Train Acc: 0.5928, Val Acc: 0.5644, Test Acc: 0.4977\n",
      "Seed: 123, Epoch: 170, Loss: 1.4263, Train Acc: 0.5964, Val Acc: 0.5653, Test Acc: 0.4991\n",
      "Seed: 123, Epoch: 180, Loss: 1.4188, Train Acc: 0.5985, Val Acc: 0.5679, Test Acc: 0.5027\n",
      "Seed: 123, Epoch: 190, Loss: 1.4100, Train Acc: 0.6007, Val Acc: 0.5751, Test Acc: 0.5123\n",
      "Seed: 123, Epoch: 200, Loss: 1.4030, Train Acc: 0.6027, Val Acc: 0.5745, Test Acc: 0.5087\n",
      "Seed: 123, Epoch: 210, Loss: 1.4097, Train Acc: 0.5996, Val Acc: 0.5672, Test Acc: 0.5022\n",
      "Seed: 123, Epoch: 220, Loss: 1.3968, Train Acc: 0.6030, Val Acc: 0.5784, Test Acc: 0.5145\n",
      "Seed: 123, Epoch: 230, Loss: 1.3859, Train Acc: 0.6056, Val Acc: 0.5728, Test Acc: 0.5074\n",
      "Seed: 123, Epoch: 240, Loss: 1.3822, Train Acc: 0.6068, Val Acc: 0.5791, Test Acc: 0.5152\n",
      "Seed: 123, Epoch: 250, Loss: 1.3803, Train Acc: 0.6070, Val Acc: 0.5804, Test Acc: 0.5189\n",
      "Seed: 123, Epoch: 260, Loss: 1.3783, Train Acc: 0.6085, Val Acc: 0.5777, Test Acc: 0.5120\n",
      "Seed: 123, Epoch: 270, Loss: 1.3706, Train Acc: 0.6102, Val Acc: 0.5794, Test Acc: 0.5140\n",
      "Seed: 123, Epoch: 280, Loss: 1.3668, Train Acc: 0.6108, Val Acc: 0.5773, Test Acc: 0.5130\n",
      "Seed: 123, Epoch: 290, Loss: 1.3704, Train Acc: 0.6110, Val Acc: 0.5809, Test Acc: 0.5163\n",
      "Seed: 123, Epoch: 300, Loss: 1.3615, Train Acc: 0.6127, Val Acc: 0.5822, Test Acc: 0.5182\n",
      "Seed: 123, Epoch: 310, Loss: 1.3553, Train Acc: 0.6134, Val Acc: 0.5798, Test Acc: 0.5165\n",
      "Seed: 123, Epoch: 320, Loss: 1.3509, Train Acc: 0.6141, Val Acc: 0.5871, Test Acc: 0.5253\n",
      "Seed: 123, Epoch: 330, Loss: 1.3475, Train Acc: 0.6144, Val Acc: 0.5869, Test Acc: 0.5282\n",
      "Seed: 123, Epoch: 340, Loss: 1.3449, Train Acc: 0.6164, Val Acc: 0.5866, Test Acc: 0.5262\n",
      "Seed: 123, Epoch: 350, Loss: 1.3410, Train Acc: 0.6157, Val Acc: 0.5844, Test Acc: 0.5219\n",
      "Seed: 123, Epoch: 360, Loss: 1.3520, Train Acc: 0.6154, Val Acc: 0.5852, Test Acc: 0.5246\n",
      "Seed: 123, Epoch: 370, Loss: 1.3398, Train Acc: 0.6166, Val Acc: 0.5808, Test Acc: 0.5179\n",
      "Seed: 123, Epoch: 380, Loss: 1.3367, Train Acc: 0.6167, Val Acc: 0.5851, Test Acc: 0.5236\n",
      "Seed: 123, Epoch: 390, Loss: 1.3374, Train Acc: 0.6186, Val Acc: 0.5875, Test Acc: 0.5267\n",
      "Seed: 123, Epoch: 400, Loss: 1.3320, Train Acc: 0.6198, Val Acc: 0.5880, Test Acc: 0.5275\n",
      "Seed: 123, Epoch: 410, Loss: 1.3318, Train Acc: 0.6196, Val Acc: 0.5907, Test Acc: 0.5338\n",
      "Seed: 123, Epoch: 420, Loss: 1.3242, Train Acc: 0.6204, Val Acc: 0.5876, Test Acc: 0.5252\n",
      "Seed: 123, Epoch: 430, Loss: 1.3245, Train Acc: 0.6212, Val Acc: 0.5871, Test Acc: 0.5263\n",
      "Seed: 123, Epoch: 440, Loss: 1.3269, Train Acc: 0.6207, Val Acc: 0.5810, Test Acc: 0.5175\n",
      "Seed: 123, Epoch: 450, Loss: 1.3227, Train Acc: 0.6207, Val Acc: 0.5893, Test Acc: 0.5367\n",
      "Seed: 123, Epoch: 460, Loss: 1.3185, Train Acc: 0.6217, Val Acc: 0.5890, Test Acc: 0.5317\n",
      "Seed: 123, Epoch: 470, Loss: 1.3133, Train Acc: 0.6235, Val Acc: 0.5898, Test Acc: 0.5272\n",
      "Seed: 123, Epoch: 480, Loss: 1.3520, Train Acc: 0.6151, Val Acc: 0.5799, Test Acc: 0.5210\n",
      "Seed: 123, Epoch: 490, Loss: 1.3162, Train Acc: 0.6198, Val Acc: 0.5877, Test Acc: 0.5298\n",
      "Seed: 123, Epoch: 500, Loss: 1.3127, Train Acc: 0.6237, Val Acc: 0.5896, Test Acc: 0.5299\n",
      "Seed: 456, Epoch: 010, Loss: 3.1123, Train Acc: 0.1911, Val Acc: 0.3130, Test Acc: 0.3394\n",
      "Seed: 456, Epoch: 020, Loss: 2.5424, Train Acc: 0.3635, Val Acc: 0.4057, Test Acc: 0.3661\n",
      "Seed: 456, Epoch: 030, Loss: 2.1058, Train Acc: 0.4435, Val Acc: 0.4479, Test Acc: 0.3879\n",
      "Seed: 456, Epoch: 040, Loss: 1.8704, Train Acc: 0.5006, Val Acc: 0.4909, Test Acc: 0.4382\n",
      "Seed: 456, Epoch: 050, Loss: 1.7392, Train Acc: 0.5296, Val Acc: 0.5074, Test Acc: 0.4413\n",
      "Seed: 456, Epoch: 060, Loss: 1.6495, Train Acc: 0.5466, Val Acc: 0.5324, Test Acc: 0.4725\n",
      "Seed: 456, Epoch: 070, Loss: 1.5924, Train Acc: 0.5569, Val Acc: 0.5377, Test Acc: 0.4704\n",
      "Seed: 456, Epoch: 080, Loss: 1.5624, Train Acc: 0.5651, Val Acc: 0.5465, Test Acc: 0.4842\n",
      "Seed: 456, Epoch: 090, Loss: 1.5283, Train Acc: 0.5722, Val Acc: 0.5494, Test Acc: 0.4846\n",
      "Seed: 456, Epoch: 100, Loss: 1.5025, Train Acc: 0.5764, Val Acc: 0.5498, Test Acc: 0.4826\n",
      "Seed: 456, Epoch: 110, Loss: 1.4863, Train Acc: 0.5821, Val Acc: 0.5572, Test Acc: 0.4949\n",
      "Seed: 456, Epoch: 120, Loss: 1.4633, Train Acc: 0.5861, Val Acc: 0.5606, Test Acc: 0.4921\n",
      "Seed: 456, Epoch: 130, Loss: 1.4553, Train Acc: 0.5887, Val Acc: 0.5615, Test Acc: 0.4944\n",
      "Seed: 456, Epoch: 140, Loss: 1.4395, Train Acc: 0.5922, Val Acc: 0.5630, Test Acc: 0.4959\n",
      "Seed: 456, Epoch: 150, Loss: 1.4301, Train Acc: 0.5950, Val Acc: 0.5718, Test Acc: 0.5081\n",
      "Seed: 456, Epoch: 160, Loss: 1.4247, Train Acc: 0.5969, Val Acc: 0.5696, Test Acc: 0.5080\n",
      "Seed: 456, Epoch: 170, Loss: 1.4151, Train Acc: 0.5974, Val Acc: 0.5662, Test Acc: 0.4994\n",
      "Seed: 456, Epoch: 180, Loss: 1.4044, Train Acc: 0.6006, Val Acc: 0.5752, Test Acc: 0.5133\n",
      "Seed: 456, Epoch: 190, Loss: 1.3963, Train Acc: 0.6037, Val Acc: 0.5764, Test Acc: 0.5134\n",
      "Seed: 456, Epoch: 200, Loss: 1.4047, Train Acc: 0.6032, Val Acc: 0.5785, Test Acc: 0.5197\n",
      "Seed: 456, Epoch: 210, Loss: 1.3842, Train Acc: 0.6062, Val Acc: 0.5773, Test Acc: 0.5111\n",
      "Seed: 456, Epoch: 220, Loss: 1.3902, Train Acc: 0.6030, Val Acc: 0.5709, Test Acc: 0.5050\n",
      "Seed: 456, Epoch: 230, Loss: 1.3807, Train Acc: 0.6076, Val Acc: 0.5781, Test Acc: 0.5134\n",
      "Seed: 456, Epoch: 240, Loss: 1.3718, Train Acc: 0.6095, Val Acc: 0.5821, Test Acc: 0.5212\n",
      "Seed: 456, Epoch: 250, Loss: 1.3673, Train Acc: 0.6102, Val Acc: 0.5811, Test Acc: 0.5178\n",
      "Seed: 456, Epoch: 260, Loss: 1.3636, Train Acc: 0.6082, Val Acc: 0.5777, Test Acc: 0.5145\n",
      "Seed: 456, Epoch: 270, Loss: 1.3580, Train Acc: 0.6117, Val Acc: 0.5818, Test Acc: 0.5179\n",
      "Seed: 456, Epoch: 280, Loss: 1.3522, Train Acc: 0.6140, Val Acc: 0.5837, Test Acc: 0.5208\n",
      "Seed: 456, Epoch: 290, Loss: 1.3853, Train Acc: 0.6113, Val Acc: 0.5837, Test Acc: 0.5207\n",
      "Seed: 456, Epoch: 300, Loss: 1.3559, Train Acc: 0.6134, Val Acc: 0.5869, Test Acc: 0.5250\n",
      "Seed: 456, Epoch: 310, Loss: 1.3463, Train Acc: 0.6156, Val Acc: 0.5830, Test Acc: 0.5167\n",
      "Seed: 456, Epoch: 320, Loss: 1.3416, Train Acc: 0.6160, Val Acc: 0.5815, Test Acc: 0.5160\n",
      "Seed: 456, Epoch: 330, Loss: 1.3368, Train Acc: 0.6165, Val Acc: 0.5865, Test Acc: 0.5254\n",
      "Seed: 456, Epoch: 340, Loss: 1.3351, Train Acc: 0.6182, Val Acc: 0.5857, Test Acc: 0.5216\n",
      "Seed: 456, Epoch: 350, Loss: 1.3301, Train Acc: 0.6188, Val Acc: 0.5842, Test Acc: 0.5207\n",
      "Seed: 456, Epoch: 360, Loss: 1.3349, Train Acc: 0.6170, Val Acc: 0.5789, Test Acc: 0.5138\n",
      "Seed: 456, Epoch: 370, Loss: 1.3257, Train Acc: 0.6199, Val Acc: 0.5867, Test Acc: 0.5282\n",
      "Seed: 456, Epoch: 380, Loss: 1.3260, Train Acc: 0.6193, Val Acc: 0.5890, Test Acc: 0.5293\n",
      "Seed: 456, Epoch: 390, Loss: 1.3201, Train Acc: 0.6210, Val Acc: 0.5872, Test Acc: 0.5248\n",
      "Seed: 456, Epoch: 400, Loss: 1.3204, Train Acc: 0.6200, Val Acc: 0.5886, Test Acc: 0.5313\n",
      "Seed: 456, Epoch: 410, Loss: 1.3328, Train Acc: 0.6140, Val Acc: 0.5808, Test Acc: 0.5144\n",
      "Seed: 456, Epoch: 420, Loss: 1.3321, Train Acc: 0.6172, Val Acc: 0.5838, Test Acc: 0.5256\n",
      "Seed: 456, Epoch: 430, Loss: 1.3421, Train Acc: 0.6158, Val Acc: 0.5904, Test Acc: 0.5372\n",
      "Seed: 456, Epoch: 440, Loss: 1.3207, Train Acc: 0.6220, Val Acc: 0.5853, Test Acc: 0.5229\n",
      "Seed: 456, Epoch: 450, Loss: 1.3133, Train Acc: 0.6223, Val Acc: 0.5870, Test Acc: 0.5250\n",
      "Seed: 456, Epoch: 460, Loss: 1.3067, Train Acc: 0.6240, Val Acc: 0.5897, Test Acc: 0.5281\n",
      "Seed: 456, Epoch: 470, Loss: 1.3043, Train Acc: 0.6244, Val Acc: 0.5913, Test Acc: 0.5306\n",
      "Seed: 456, Epoch: 480, Loss: 1.3015, Train Acc: 0.6247, Val Acc: 0.5886, Test Acc: 0.5261\n",
      "Seed: 456, Epoch: 490, Loss: 1.3001, Train Acc: 0.6248, Val Acc: 0.5916, Test Acc: 0.5333\n",
      "Seed: 456, Epoch: 500, Loss: 1.3145, Train Acc: 0.6249, Val Acc: 0.5886, Test Acc: 0.5269\n",
      "Seed: 42, Best Val Acc: 0.5916, Corresponding Test Acc: 0.5343, Time: 70.46s, Memory: 1.61MB, GPU Memory: 5035.75MB\n",
      "Seed: 123, Best Val Acc: 0.5938, Corresponding Test Acc: 0.5420, Time: 65.41s, Memory: 1.58MB, GPU Memory: 5033.00MB\n",
      "Seed: 456, Best Val Acc: 0.5923, Corresponding Test Acc: 0.5364, Time: 73.04s, Memory: 1.58MB, GPU Memory: 5036.45MB\n",
      "\n",
      "Average Best Validation Accuracy: 0.5926\n",
      "Average Corresponding Test Accuracy: 0.5376\n",
      "Average Time: 69.64s\n",
      "Average Memory Usage: 1.59MB\n",
      "Average GPU Memory Usage: 5035.07MB\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import ASAPooling\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.sum_res = sum_res\n",
    "        self.act = act\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.down_convs.append(GCNConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(GSAPool(256, pooling_ratio=0.9, alpha = 0.6, cus_drop_ratio = 0))\n",
    "            self.down_convs.append(GCNConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(GCNConv(in_channels, channels))\n",
    "        self.up_convs.append(GCNConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.0, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = F.relu(x)  \n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, edge_attr, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "\n",
    "            up = res\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x.log_softmax(dim=-1)\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)[train_idx]\n",
    "    loss = F.nll_loss(out, data.y.squeeze(1)[train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    out = model(data.x, data.edge_index)\n",
    "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "    train_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['acc']\n",
    "    valid_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['acc']\n",
    "    test_acc = evaluator.eval({\n",
    "        'y_true': data.y[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['acc']\n",
    "\n",
    "    return train_acc, valid_acc, test_acc\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    test_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    model = HierarchicalGCN_GSA(\n",
    "        in_channels=graph.num_features,\n",
    "        hidden_channels=256,\n",
    "        out_channels=dataset.num_classes,\n",
    "        depth=2,\n",
    "        act=F.relu,\n",
    "        sum_res=False\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    split_idx = dataset.get_idx_split()\n",
    "    train_idx = split_idx['train'].to(device)\n",
    "\n",
    "    for epoch in range(1, 501):\n",
    "        loss = train(model, graph, split_idx['train'], optimizer)\n",
    "        train_acc, valid_acc, test_acc = test(model, graph, split_idx, evaluator)\n",
    "        val_accuracies.append(valid_acc)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Seed: {seed:03d}, Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "                  f'Train Acc: {train_acc:.4f}, Val Acc: {valid_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    memory_usage = current / 10**6  # Convert to MB\n",
    "    peak_memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.max_memory_allocated(device) / 1024**2  # Convert to MB\n",
    "        torch.cuda.reset_max_memory_allocated(device)\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    val_accuracies_list.append(val_accuracies)\n",
    "    times.append(elapsed_time)\n",
    "    memories.append(memory_usage)\n",
    "    gpu_memories.append(gpu_memory_usage)\n",
    "\n",
    "    best_val_acc = max(val_accuracies)\n",
    "    best_val_index = val_accuracies.index(best_val_acc)\n",
    "    corresponding_test_acc = test_accuracies[best_val_index]\n",
    "\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'corresponding_test_acc': corresponding_test_acc,\n",
    "        'elapsed_time': elapsed_time,\n",
    "        'memory_usage': memory_usage,\n",
    "        'gpu_memory_usage': gpu_memory_usage\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for result in results:\n",
    "    print(f\"Seed: {result['seed']}, Best Val Acc: {result['best_val_acc']:.4f}, \"\n",
    "          f\"Corresponding Test Acc: {result['corresponding_test_acc']:.4f}, \"\n",
    "          f\"Time: {result['elapsed_time']:.2f}s, Memory: {result['memory_usage']:.2f}MB, \"\n",
    "          f\"GPU Memory: {result['gpu_memory_usage']:.2f}MB\")\n",
    "\n",
    "# Calculate average results\n",
    "avg_val_acc = np.mean([result['best_val_acc'] for result in results])\n",
    "avg_test_acc = np.mean([result['corresponding_test_acc'] for result in results])\n",
    "avg_time = np.mean(times)\n",
    "avg_memory = np.mean(memories)\n",
    "avg_gpu_memory = np.mean(gpu_memories)\n",
    "\n",
    "print(f\"\\nAverage Best Validation Accuracy: {avg_val_acc:.4f}\")\n",
    "print(f\"Average Corresponding Test Accuracy: {avg_test_acc:.4f}\")\n",
    "print(f\"Average Time: {avg_time:.2f}s\")\n",
    "print(f\"Average Memory Usage: {avg_memory:.2f}MB\")\n",
    "print(f\"Average GPU Memory Usage: {avg_gpu_memory:.2f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CG-ODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
