{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "\n",
    "@dataclass(init=False)\n",
    "class SelectOutput:\n",
    "    r\"\"\"The output of the :class:`Select` method, which holds an assignment\n",
    "    from selected nodes to their respective cluster(s).\n",
    "\n",
    "    Args:\n",
    "        node_index (torch.Tensor): The indices of the selected nodes.\n",
    "        num_nodes (int): The number of nodes.\n",
    "        cluster_index (torch.Tensor): The indices of the clusters each node in\n",
    "            :obj:`node_index` is assigned to.\n",
    "        num_clusters (int): The number of clusters.\n",
    "        weight (torch.Tensor, optional): A weight vector, denoting the strength\n",
    "            of the assignment of a node to its cluster. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    node_index: Tensor\n",
    "    num_nodes: int\n",
    "    cluster_index: Tensor\n",
    "    num_clusters: int\n",
    "    weight: Optional[Tensor] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_index: Tensor,\n",
    "        num_nodes: int,\n",
    "        cluster_index: Tensor,\n",
    "        num_clusters: int,\n",
    "        weight: Optional[Tensor] = None,\n",
    "    ):\n",
    "        if node_index.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'node_index' to be one-dimensional \"\n",
    "                             f\"(got {node_index.dim()} dimensions)\")\n",
    "\n",
    "        if cluster_index.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'cluster_index' to be one-dimensional \"\n",
    "                             f\"(got {cluster_index.dim()} dimensions)\")\n",
    "\n",
    "        if node_index.numel() != cluster_index.numel():\n",
    "            raise ValueError(f\"Expected 'node_index' and 'cluster_index' to \"\n",
    "                             f\"hold the same number of values (got \"\n",
    "                             f\"{node_index.numel()} and \"\n",
    "                             f\"{cluster_index.numel()} values)\")\n",
    "\n",
    "        if weight is not None and weight.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'weight' vector to be one-dimensional \"\n",
    "                             f\"(got {weight.dim()} dimensions)\")\n",
    "\n",
    "        if weight is not None and weight.numel() != node_index.numel():\n",
    "            raise ValueError(f\"Expected 'weight' to hold {node_index.numel()} \"\n",
    "                             f\"values (got {weight.numel()} values)\")\n",
    "\n",
    "        self.node_index = node_index\n",
    "        self.num_nodes = num_nodes\n",
    "        self.cluster_index = cluster_index\n",
    "        self.num_clusters = num_clusters\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Select(torch.nn.Module):\n",
    "    r\"\"\"An abstract base class for implementing custom node selections as\n",
    "    described in the `\"Understanding Pooling in Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1905.05178>`_ paper, which maps the nodes of an\n",
    "    input graph to supernodes in the coarsened graph.\n",
    "\n",
    "    Specifically, :class:`Select` returns a :class:`SelectOutput` output, which\n",
    "    holds a (sparse) mapping :math:`\\mathbf{C} \\in {[0, 1]}^{N \\times C}` that\n",
    "    assigns selected nodes to one or more of :math:`C` super nodes.\n",
    "    \"\"\"\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> SelectOutput:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'\n",
    "\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels * 2, in_channels)\n",
    "        self.fc2 = nn.Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CGIPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.5, non_lin=torch.tanh):\n",
    "        super(CGIPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.non_lin = non_lin\n",
    "        self.hidden_dim = in_channels\n",
    "        self.transform = GraphConv(in_channels, self.hidden_dim)\n",
    "        self.pp_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n",
    "        self.np_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        self.positive_pooling = GraphConv(self.hidden_dim, 1)\n",
    "        self.negative_pooling = GraphConv(self.hidden_dim, 1)\n",
    "\n",
    "        self.discriminator = Discriminator(self.hidden_dim)\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        device = x.device  # 获取输入张量的设备信息\n",
    "\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_transform = F.leaky_relu(self.transform(x, edge_index), 0.2)\n",
    "        x_tp = F.leaky_relu(self.pp_conv(x, edge_index), 0.2)\n",
    "        x_tn = F.leaky_relu(self.np_conv(x, edge_index), 0.2)\n",
    "        s_pp = self.positive_pooling(x_tp, edge_index).squeeze()\n",
    "        s_np = self.negative_pooling(x_tn, edge_index).squeeze()\n",
    "\n",
    "        perm_positive = topk(s_pp, 1, batch)\n",
    "        perm_negative = topk(s_np, 1, batch)\n",
    "        x_pp = x_transform[perm_positive] * self.non_lin(s_pp[perm_positive]).view(-1, 1)\n",
    "        x_np = x_transform[perm_negative] * self.non_lin(s_np[perm_negative]).view(-1, 1)\n",
    "\n",
    "        x_pp_readout = gap(x_pp, batch[perm_positive])\n",
    "        x_np_readout = gap(x_np, batch[perm_negative])\n",
    "        x_readout = gap(x_transform, batch)\n",
    "\n",
    "        positive_pair = torch.cat([x_pp_readout, x_readout], dim=1)\n",
    "        negative_pair = torch.cat([x_np_readout, x_readout], dim=1)\n",
    "\n",
    "        real = torch.ones(positive_pair.shape[0], device=device)  # 将张量移动到相应设备\n",
    "        fake = torch.zeros(negative_pair.shape[0], device=device)  # 将张量移动到相应设备\n",
    "        #real_loss = self.loss_fn(self.discriminator(positive_pair), real)\n",
    "        #fake_loss = self.loss_fn(self.discriminator(negative_pair), fake)\n",
    "        #discrimination_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        score = (s_pp - s_np)\n",
    "\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x_transform[perm] * self.non_lin(score[perm]).view(-1, 1)\n",
    "        batch = batch[perm]\n",
    "\n",
    "        filter_edge_index, filter_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "        return x, filter_edge_index, filter_edge_attr, batch, perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time: 12.02 seconds\n",
      "Var Time: 0.01 seconds\n",
      "Average Memory: 48.67 MB\n",
      "Average Best Val Acc: 0.8690\n",
      "Std Best Test Acc: 0.0488\n",
      "Average Test Acc: 0.8621\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 150\n",
    "data_path = \"/data/Zeyu/Pooling/1\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"MUTAG\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_CGI(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_CGI, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = CGIPool(hidden_channels, ratio=0.9)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = CGIPool(hidden_channels, ratio=0.9)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, perm = self.pool1(x, edge_index, None, batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, perm = self.pool2(x, edge_index, None, batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time: 31.07 seconds\n",
      "Var Time: 0.17 seconds\n",
      "Average Memory: 1636.00 MB\n",
      "Average Best Val Acc: 0.7508\n",
      "Std Best Test Acc: 0.0147\n",
      "Average Test Acc: 0.7207\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"DD\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_CGI(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_CGI, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = CGIPool(hidden_channels, ratio=0.9)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = CGIPool(hidden_channels, ratio=0.9)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, perm = self.pool1(x, edge_index, None, batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, perm = self.pool2(x, edge_index, None, batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 0.6974, Val Acc: 0.4800, Test Acc: 0.5000\n",
      "Seed: 42, Epoch: 002, Loss: 0.6918, Val Acc: 0.5533, Test Acc: 0.5667\n",
      "Seed: 42, Epoch: 003, Loss: 0.6868, Val Acc: 0.6333, Test Acc: 0.6467\n",
      "Seed: 42, Epoch: 004, Loss: 0.6797, Val Acc: 0.7067, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 005, Loss: 0.6765, Val Acc: 0.7000, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 006, Loss: 0.6685, Val Acc: 0.6933, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 007, Loss: 0.6585, Val Acc: 0.7067, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 008, Loss: 0.6525, Val Acc: 0.7067, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 009, Loss: 0.6398, Val Acc: 0.7067, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 010, Loss: 0.6223, Val Acc: 0.7000, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 011, Loss: 0.6214, Val Acc: 0.7067, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 012, Loss: 0.6033, Val Acc: 0.7000, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 013, Loss: 0.5968, Val Acc: 0.7067, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 014, Loss: 0.5936, Val Acc: 0.7467, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 015, Loss: 0.5829, Val Acc: 0.8067, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 016, Loss: 0.5805, Val Acc: 0.7667, Test Acc: 0.8267\n",
      "Seed: 42, Epoch: 017, Loss: 0.5705, Val Acc: 0.7800, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 018, Loss: 0.5533, Val Acc: 0.7933, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 019, Loss: 0.5529, Val Acc: 0.7733, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 020, Loss: 0.5472, Val Acc: 0.7733, Test Acc: 0.8200\n",
      "Seed: 42, Epoch: 021, Loss: 0.5364, Val Acc: 0.7867, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 022, Loss: 0.5210, Val Acc: 0.8000, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 023, Loss: 0.5283, Val Acc: 0.8000, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 024, Loss: 0.5138, Val Acc: 0.7600, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 025, Loss: 0.5098, Val Acc: 0.7867, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 026, Loss: 0.5043, Val Acc: 0.7933, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 027, Loss: 0.5030, Val Acc: 0.8333, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 028, Loss: 0.4996, Val Acc: 0.7933, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 029, Loss: 0.5147, Val Acc: 0.7733, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 030, Loss: 0.5058, Val Acc: 0.7800, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 031, Loss: 0.5190, Val Acc: 0.8000, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 032, Loss: 0.5127, Val Acc: 0.8200, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 033, Loss: 0.4996, Val Acc: 0.8000, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 034, Loss: 0.5122, Val Acc: 0.7933, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 035, Loss: 0.5019, Val Acc: 0.7733, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 036, Loss: 0.5039, Val Acc: 0.7933, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 037, Loss: 0.4942, Val Acc: 0.7867, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 038, Loss: 0.5051, Val Acc: 0.7733, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 039, Loss: 0.4988, Val Acc: 0.7733, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 040, Loss: 0.4962, Val Acc: 0.7467, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 041, Loss: 0.4822, Val Acc: 0.7533, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 042, Loss: 0.4786, Val Acc: 0.7733, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 043, Loss: 0.4859, Val Acc: 0.8200, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 044, Loss: 0.4903, Val Acc: 0.8133, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 045, Loss: 0.4821, Val Acc: 0.7800, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 046, Loss: 0.4888, Val Acc: 0.7867, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 047, Loss: 0.4865, Val Acc: 0.8200, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 048, Loss: 0.4778, Val Acc: 0.8133, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 049, Loss: 0.4792, Val Acc: 0.8000, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 050, Loss: 0.4677, Val Acc: 0.7733, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 051, Loss: 0.4845, Val Acc: 0.7733, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 052, Loss: 0.4778, Val Acc: 0.8000, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 053, Loss: 0.4741, Val Acc: 0.7933, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 054, Loss: 0.4628, Val Acc: 0.8000, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 055, Loss: 0.4736, Val Acc: 0.8200, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 056, Loss: 0.4605, Val Acc: 0.7733, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 057, Loss: 0.5103, Val Acc: 0.7733, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 058, Loss: 0.4766, Val Acc: 0.8067, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 059, Loss: 0.4524, Val Acc: 0.7933, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 060, Loss: 0.4666, Val Acc: 0.8000, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 061, Loss: 0.4596, Val Acc: 0.7867, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 062, Loss: 0.4542, Val Acc: 0.7800, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 063, Loss: 0.4775, Val Acc: 0.7867, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 064, Loss: 0.4658, Val Acc: 0.7867, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 065, Loss: 0.4622, Val Acc: 0.7867, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 066, Loss: 0.4360, Val Acc: 0.8200, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 067, Loss: 0.4650, Val Acc: 0.7933, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 068, Loss: 0.4446, Val Acc: 0.7667, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 069, Loss: 0.4414, Val Acc: 0.7667, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 070, Loss: 0.4437, Val Acc: 0.7800, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 071, Loss: 0.4444, Val Acc: 0.7867, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 072, Loss: 0.4650, Val Acc: 0.7933, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 073, Loss: 0.4698, Val Acc: 0.7933, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 074, Loss: 0.4446, Val Acc: 0.7933, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 075, Loss: 0.4426, Val Acc: 0.7867, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 076, Loss: 0.4378, Val Acc: 0.7600, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 077, Loss: 0.4453, Val Acc: 0.7867, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 078, Loss: 0.4252, Val Acc: 0.7933, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 079, Loss: 0.4203, Val Acc: 0.7867, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 080, Loss: 0.4372, Val Acc: 0.8000, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 081, Loss: 0.4203, Val Acc: 0.7800, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 082, Loss: 0.4393, Val Acc: 0.7667, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 083, Loss: 0.4248, Val Acc: 0.7667, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 084, Loss: 0.4234, Val Acc: 0.7667, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 085, Loss: 0.4311, Val Acc: 0.7733, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 086, Loss: 0.4110, Val Acc: 0.7667, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 087, Loss: 0.4284, Val Acc: 0.7400, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 088, Loss: 0.4224, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 089, Loss: 0.4223, Val Acc: 0.8000, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 090, Loss: 0.4109, Val Acc: 0.7600, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 091, Loss: 0.4169, Val Acc: 0.7333, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 092, Loss: 0.4146, Val Acc: 0.7200, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 093, Loss: 0.4116, Val Acc: 0.7200, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 094, Loss: 0.4104, Val Acc: 0.7533, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 095, Loss: 0.4112, Val Acc: 0.7333, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 096, Loss: 0.4037, Val Acc: 0.7467, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 097, Loss: 0.4118, Val Acc: 0.7600, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 098, Loss: 0.3836, Val Acc: 0.8000, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 099, Loss: 0.3988, Val Acc: 0.8067, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 100, Loss: 0.4029, Val Acc: 0.8067, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 101, Loss: 0.3932, Val Acc: 0.7600, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 102, Loss: 0.4018, Val Acc: 0.7467, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 103, Loss: 0.3979, Val Acc: 0.7533, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 104, Loss: 0.3917, Val Acc: 0.7733, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 105, Loss: 0.3923, Val Acc: 0.7533, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 106, Loss: 0.3985, Val Acc: 0.7333, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 107, Loss: 0.3862, Val Acc: 0.7667, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 108, Loss: 0.3644, Val Acc: 0.7533, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 109, Loss: 0.3930, Val Acc: 0.7467, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 110, Loss: 0.3749, Val Acc: 0.7267, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 111, Loss: 0.3757, Val Acc: 0.7600, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 112, Loss: 0.3844, Val Acc: 0.7267, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 113, Loss: 0.3790, Val Acc: 0.7467, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 114, Loss: 0.3772, Val Acc: 0.7533, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 115, Loss: 0.3772, Val Acc: 0.7600, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 116, Loss: 0.3823, Val Acc: 0.7600, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 117, Loss: 0.3906, Val Acc: 0.7600, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 118, Loss: 0.3604, Val Acc: 0.7600, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 119, Loss: 0.4161, Val Acc: 0.7600, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 120, Loss: 0.3621, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 121, Loss: 0.4028, Val Acc: 0.7467, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 122, Loss: 0.3880, Val Acc: 0.7000, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 123, Loss: 0.4509, Val Acc: 0.7533, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 124, Loss: 0.3588, Val Acc: 0.7467, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 125, Loss: 0.4238, Val Acc: 0.7533, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 126, Loss: 0.3855, Val Acc: 0.7533, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 127, Loss: 0.3727, Val Acc: 0.7333, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 128, Loss: 0.3975, Val Acc: 0.7200, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 129, Loss: 0.3747, Val Acc: 0.7733, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 130, Loss: 0.3679, Val Acc: 0.7933, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 131, Loss: 0.3906, Val Acc: 0.7933, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 132, Loss: 0.3852, Val Acc: 0.7533, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 133, Loss: 0.3631, Val Acc: 0.7467, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 134, Loss: 0.3733, Val Acc: 0.7533, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 135, Loss: 0.3768, Val Acc: 0.7400, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 136, Loss: 0.3671, Val Acc: 0.7200, Test Acc: 0.6600\n",
      "Seed: 42, Epoch: 137, Loss: 0.3778, Val Acc: 0.7467, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 138, Loss: 0.3671, Val Acc: 0.7667, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 139, Loss: 0.3508, Val Acc: 0.7267, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 140, Loss: 0.3652, Val Acc: 0.7133, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 141, Loss: 0.3654, Val Acc: 0.7533, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 142, Loss: 0.3768, Val Acc: 0.7800, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 143, Loss: 0.3601, Val Acc: 0.7667, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 144, Loss: 0.3752, Val Acc: 0.7467, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 145, Loss: 0.3564, Val Acc: 0.7533, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 146, Loss: 0.3993, Val Acc: 0.7200, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 147, Loss: 0.4026, Val Acc: 0.7067, Test Acc: 0.6800\n",
      "Seed: 42, Epoch: 148, Loss: 0.3846, Val Acc: 0.7467, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 149, Loss: 0.3646, Val Acc: 0.7733, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 150, Loss: 0.3758, Val Acc: 0.7600, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 151, Loss: 0.3648, Val Acc: 0.7533, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 152, Loss: 0.3668, Val Acc: 0.7400, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 153, Loss: 0.3817, Val Acc: 0.7600, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 154, Loss: 0.3727, Val Acc: 0.7867, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 155, Loss: 0.3685, Val Acc: 0.7467, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 156, Loss: 0.3719, Val Acc: 0.7067, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 157, Loss: 0.3562, Val Acc: 0.7333, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 158, Loss: 0.3939, Val Acc: 0.7333, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 159, Loss: 0.3776, Val Acc: 0.7533, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 160, Loss: 0.3509, Val Acc: 0.7467, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 161, Loss: 0.3824, Val Acc: 0.7667, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 162, Loss: 0.3560, Val Acc: 0.7467, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 163, Loss: 0.3539, Val Acc: 0.7467, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 164, Loss: 0.3566, Val Acc: 0.7467, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 165, Loss: 0.3419, Val Acc: 0.7533, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 166, Loss: 0.3401, Val Acc: 0.7467, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 167, Loss: 0.3464, Val Acc: 0.7333, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 168, Loss: 0.3358, Val Acc: 0.7267, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 169, Loss: 0.3308, Val Acc: 0.7000, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 170, Loss: 0.3365, Val Acc: 0.7133, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 171, Loss: 0.3376, Val Acc: 0.7333, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 172, Loss: 0.3402, Val Acc: 0.7467, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 173, Loss: 0.3551, Val Acc: 0.7467, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 174, Loss: 0.3595, Val Acc: 0.7467, Test Acc: 0.6933\n",
      "Seed: 42, Epoch: 175, Loss: 0.3465, Val Acc: 0.7400, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 176, Loss: 0.3571, Val Acc: 0.7200, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 177, Loss: 0.3432, Val Acc: 0.7400, Test Acc: 0.7200\n",
      "Early stopping at epoch 177 for seed 42\n",
      "Seed: 43, Epoch: 001, Loss: 0.6968, Val Acc: 0.6200, Test Acc: 0.5867\n",
      "Seed: 43, Epoch: 002, Loss: 0.6855, Val Acc: 0.6267, Test Acc: 0.5933\n",
      "Seed: 43, Epoch: 003, Loss: 0.6738, Val Acc: 0.6267, Test Acc: 0.5933\n",
      "Seed: 43, Epoch: 004, Loss: 0.6667, Val Acc: 0.6133, Test Acc: 0.6267\n",
      "Seed: 43, Epoch: 005, Loss: 0.6567, Val Acc: 0.5800, Test Acc: 0.5933\n",
      "Seed: 43, Epoch: 006, Loss: 0.6470, Val Acc: 0.5867, Test Acc: 0.5400\n",
      "Seed: 43, Epoch: 007, Loss: 0.7043, Val Acc: 0.5867, Test Acc: 0.5667\n",
      "Seed: 43, Epoch: 008, Loss: 0.6827, Val Acc: 0.5933, Test Acc: 0.6000\n",
      "Seed: 43, Epoch: 009, Loss: 0.6612, Val Acc: 0.5933, Test Acc: 0.6000\n",
      "Seed: 43, Epoch: 010, Loss: 0.6473, Val Acc: 0.6133, Test Acc: 0.6267\n",
      "Seed: 43, Epoch: 011, Loss: 0.6356, Val Acc: 0.6600, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 012, Loss: 0.6325, Val Acc: 0.6600, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 013, Loss: 0.6228, Val Acc: 0.6867, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 014, Loss: 0.6191, Val Acc: 0.6867, Test Acc: 0.8000\n",
      "Seed: 43, Epoch: 015, Loss: 0.6219, Val Acc: 0.6533, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 016, Loss: 0.5928, Val Acc: 0.6933, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 017, Loss: 0.5927, Val Acc: 0.6867, Test Acc: 0.7933\n",
      "Seed: 43, Epoch: 018, Loss: 0.5663, Val Acc: 0.6667, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 019, Loss: 0.5806, Val Acc: 0.6667, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 020, Loss: 0.5532, Val Acc: 0.7000, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 021, Loss: 0.5537, Val Acc: 0.6867, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 022, Loss: 0.5400, Val Acc: 0.6267, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 023, Loss: 0.5328, Val Acc: 0.6733, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 024, Loss: 0.5134, Val Acc: 0.6867, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 025, Loss: 0.5213, Val Acc: 0.6800, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 026, Loss: 0.5037, Val Acc: 0.6667, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 027, Loss: 0.5058, Val Acc: 0.6733, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 028, Loss: 0.4844, Val Acc: 0.6933, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 029, Loss: 0.4968, Val Acc: 0.6867, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 030, Loss: 0.4733, Val Acc: 0.6600, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 031, Loss: 0.5005, Val Acc: 0.6733, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 032, Loss: 0.4842, Val Acc: 0.7067, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 033, Loss: 0.5033, Val Acc: 0.7200, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 034, Loss: 0.5009, Val Acc: 0.6800, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 035, Loss: 0.4653, Val Acc: 0.6600, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 036, Loss: 0.4891, Val Acc: 0.6600, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 037, Loss: 0.4808, Val Acc: 0.6667, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 038, Loss: 0.4683, Val Acc: 0.6867, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 039, Loss: 0.4706, Val Acc: 0.7200, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 040, Loss: 0.4607, Val Acc: 0.6933, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 041, Loss: 0.4628, Val Acc: 0.6600, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 042, Loss: 0.4578, Val Acc: 0.6867, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 043, Loss: 0.4542, Val Acc: 0.7267, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 044, Loss: 0.4530, Val Acc: 0.7200, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 045, Loss: 0.4556, Val Acc: 0.6933, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 046, Loss: 0.4477, Val Acc: 0.6867, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 047, Loss: 0.4536, Val Acc: 0.6733, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 048, Loss: 0.4472, Val Acc: 0.6800, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 049, Loss: 0.4425, Val Acc: 0.7067, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 050, Loss: 0.4390, Val Acc: 0.7067, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 051, Loss: 0.4311, Val Acc: 0.6867, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 052, Loss: 0.4484, Val Acc: 0.6933, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 053, Loss: 0.4238, Val Acc: 0.6800, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 054, Loss: 0.4302, Val Acc: 0.6933, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 055, Loss: 0.4312, Val Acc: 0.6933, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 056, Loss: 0.4195, Val Acc: 0.6733, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 057, Loss: 0.4437, Val Acc: 0.6933, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 058, Loss: 0.4314, Val Acc: 0.7333, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 059, Loss: 0.4507, Val Acc: 0.7000, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 060, Loss: 0.4361, Val Acc: 0.6933, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 061, Loss: 0.4325, Val Acc: 0.7067, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 062, Loss: 0.4234, Val Acc: 0.7000, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 063, Loss: 0.4345, Val Acc: 0.6933, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 064, Loss: 0.4282, Val Acc: 0.7000, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 065, Loss: 0.4211, Val Acc: 0.7067, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 066, Loss: 0.4175, Val Acc: 0.6933, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 067, Loss: 0.4290, Val Acc: 0.6933, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 068, Loss: 0.4275, Val Acc: 0.6800, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 069, Loss: 0.4401, Val Acc: 0.7400, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 070, Loss: 0.4356, Val Acc: 0.7000, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 071, Loss: 0.4313, Val Acc: 0.6867, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 072, Loss: 0.4329, Val Acc: 0.6867, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 073, Loss: 0.4104, Val Acc: 0.7133, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 074, Loss: 0.4047, Val Acc: 0.7133, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 075, Loss: 0.4167, Val Acc: 0.6867, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 076, Loss: 0.4124, Val Acc: 0.7067, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 077, Loss: 0.4106, Val Acc: 0.7067, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 078, Loss: 0.4029, Val Acc: 0.7067, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 079, Loss: 0.4043, Val Acc: 0.6733, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 080, Loss: 0.3948, Val Acc: 0.6733, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 081, Loss: 0.3918, Val Acc: 0.6733, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 082, Loss: 0.4080, Val Acc: 0.6667, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 083, Loss: 0.4039, Val Acc: 0.6533, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 084, Loss: 0.4028, Val Acc: 0.6867, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 085, Loss: 0.4210, Val Acc: 0.6467, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 086, Loss: 0.4107, Val Acc: 0.6933, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 087, Loss: 0.4040, Val Acc: 0.6800, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 088, Loss: 0.4119, Val Acc: 0.7000, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 089, Loss: 0.4024, Val Acc: 0.6867, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 090, Loss: 0.3907, Val Acc: 0.6867, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 091, Loss: 0.3953, Val Acc: 0.6800, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 092, Loss: 0.3889, Val Acc: 0.6867, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 093, Loss: 0.3790, Val Acc: 0.6867, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 094, Loss: 0.3921, Val Acc: 0.7200, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 095, Loss: 0.3811, Val Acc: 0.7133, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 096, Loss: 0.3778, Val Acc: 0.7067, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 097, Loss: 0.3703, Val Acc: 0.7000, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 098, Loss: 0.3842, Val Acc: 0.6867, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 099, Loss: 0.3816, Val Acc: 0.6867, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 100, Loss: 0.3827, Val Acc: 0.6600, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 101, Loss: 0.3929, Val Acc: 0.6867, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 102, Loss: 0.3807, Val Acc: 0.6867, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 103, Loss: 0.3741, Val Acc: 0.6933, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 104, Loss: 0.3995, Val Acc: 0.6933, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 105, Loss: 0.4958, Val Acc: 0.7067, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 106, Loss: 0.3885, Val Acc: 0.6800, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 107, Loss: 0.4446, Val Acc: 0.6800, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 108, Loss: 0.4273, Val Acc: 0.6867, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 109, Loss: 0.4068, Val Acc: 0.6667, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 110, Loss: 0.4177, Val Acc: 0.6867, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 111, Loss: 0.4020, Val Acc: 0.7200, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 112, Loss: 0.3958, Val Acc: 0.7267, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 113, Loss: 0.3997, Val Acc: 0.7267, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 114, Loss: 0.3963, Val Acc: 0.7400, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 115, Loss: 0.3804, Val Acc: 0.7067, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 116, Loss: 0.3863, Val Acc: 0.7000, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 117, Loss: 0.3806, Val Acc: 0.7267, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 118, Loss: 0.3520, Val Acc: 0.7133, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 119, Loss: 0.3824, Val Acc: 0.7133, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 120, Loss: 0.3649, Val Acc: 0.6933, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 121, Loss: 0.3613, Val Acc: 0.6867, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 122, Loss: 0.3695, Val Acc: 0.6800, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 123, Loss: 0.3435, Val Acc: 0.7067, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 124, Loss: 0.3475, Val Acc: 0.6933, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 125, Loss: 0.3600, Val Acc: 0.7133, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 126, Loss: 0.3366, Val Acc: 0.7133, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 127, Loss: 0.3488, Val Acc: 0.7067, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 128, Loss: 0.3611, Val Acc: 0.7067, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 129, Loss: 0.3424, Val Acc: 0.7333, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 130, Loss: 0.3465, Val Acc: 0.7067, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 131, Loss: 0.3410, Val Acc: 0.7200, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 132, Loss: 0.3421, Val Acc: 0.7133, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 133, Loss: 0.3432, Val Acc: 0.7333, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 134, Loss: 0.3409, Val Acc: 0.7333, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 135, Loss: 0.3356, Val Acc: 0.7133, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 136, Loss: 0.3299, Val Acc: 0.6800, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 137, Loss: 0.3316, Val Acc: 0.7000, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 138, Loss: 0.3238, Val Acc: 0.7000, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 139, Loss: 0.3386, Val Acc: 0.6867, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 140, Loss: 0.3394, Val Acc: 0.6933, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 141, Loss: 0.3287, Val Acc: 0.7067, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 142, Loss: 0.3687, Val Acc: 0.7133, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 143, Loss: 0.3613, Val Acc: 0.6933, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 144, Loss: 0.3266, Val Acc: 0.6933, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 145, Loss: 0.3767, Val Acc: 0.7200, Test Acc: 0.6933\n",
      "Seed: 43, Epoch: 146, Loss: 0.3336, Val Acc: 0.7333, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 147, Loss: 0.3923, Val Acc: 0.7267, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 148, Loss: 0.3305, Val Acc: 0.6800, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 149, Loss: 0.3788, Val Acc: 0.7000, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 150, Loss: 0.3490, Val Acc: 0.7133, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 151, Loss: 0.3491, Val Acc: 0.7133, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 152, Loss: 0.3473, Val Acc: 0.7333, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 153, Loss: 0.3466, Val Acc: 0.7200, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 154, Loss: 0.3339, Val Acc: 0.7200, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 155, Loss: 0.3340, Val Acc: 0.7133, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 156, Loss: 0.3375, Val Acc: 0.7133, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 157, Loss: 0.3065, Val Acc: 0.7000, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 158, Loss: 0.3139, Val Acc: 0.7067, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 159, Loss: 0.3035, Val Acc: 0.7133, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 160, Loss: 0.3110, Val Acc: 0.7133, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 161, Loss: 0.3016, Val Acc: 0.7067, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 162, Loss: 0.3082, Val Acc: 0.7000, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 163, Loss: 0.2954, Val Acc: 0.7133, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 164, Loss: 0.3014, Val Acc: 0.6867, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 165, Loss: 0.3028, Val Acc: 0.6867, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 166, Loss: 0.2941, Val Acc: 0.7000, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 167, Loss: 0.3168, Val Acc: 0.7067, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 168, Loss: 0.2974, Val Acc: 0.7200, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 169, Loss: 0.3198, Val Acc: 0.7200, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 170, Loss: 0.2899, Val Acc: 0.6800, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 171, Loss: 0.3008, Val Acc: 0.6800, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 172, Loss: 0.3068, Val Acc: 0.6933, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 173, Loss: 0.3048, Val Acc: 0.7000, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 174, Loss: 0.3039, Val Acc: 0.6933, Test Acc: 0.6867\n",
      "Seed: 43, Epoch: 175, Loss: 2.0520, Val Acc: 0.6733, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 176, Loss: 1.3718, Val Acc: 0.6800, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 177, Loss: 0.5414, Val Acc: 0.6867, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 178, Loss: 0.4169, Val Acc: 0.6600, Test Acc: 0.6600\n",
      "Seed: 43, Epoch: 179, Loss: 0.4902, Val Acc: 0.6933, Test Acc: 0.6400\n",
      "Seed: 43, Epoch: 180, Loss: 0.4821, Val Acc: 0.6733, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 181, Loss: 0.4588, Val Acc: 0.6733, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 182, Loss: 0.4407, Val Acc: 0.7067, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 183, Loss: 0.4407, Val Acc: 0.7133, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 184, Loss: 0.4294, Val Acc: 0.7267, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 185, Loss: 0.4348, Val Acc: 0.7133, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 186, Loss: 0.4109, Val Acc: 0.7067, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 187, Loss: 0.4183, Val Acc: 0.7000, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 188, Loss: 0.3986, Val Acc: 0.6600, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 189, Loss: 0.4355, Val Acc: 0.7000, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 190, Loss: 0.4113, Val Acc: 0.7067, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 191, Loss: 0.3884, Val Acc: 0.7067, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 192, Loss: 0.3696, Val Acc: 0.7267, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 193, Loss: 0.3570, Val Acc: 0.7267, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 194, Loss: 0.3694, Val Acc: 0.7267, Test Acc: 0.6800\n",
      "Seed: 43, Epoch: 195, Loss: 0.3484, Val Acc: 0.7133, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 196, Loss: 0.3453, Val Acc: 0.7000, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 197, Loss: 0.3178, Val Acc: 0.6667, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 198, Loss: 0.3542, Val Acc: 0.7000, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 199, Loss: 0.3289, Val Acc: 0.7133, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 200, Loss: 0.3048, Val Acc: 0.7000, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 001, Loss: 0.6884, Val Acc: 0.6067, Test Acc: 0.5333\n",
      "Seed: 44, Epoch: 002, Loss: 0.6721, Val Acc: 0.6133, Test Acc: 0.5333\n",
      "Seed: 44, Epoch: 003, Loss: 0.6617, Val Acc: 0.6000, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 004, Loss: 0.6491, Val Acc: 0.5933, Test Acc: 0.5600\n",
      "Seed: 44, Epoch: 005, Loss: 0.6386, Val Acc: 0.5800, Test Acc: 0.6067\n",
      "Seed: 44, Epoch: 006, Loss: 0.6235, Val Acc: 0.6400, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 007, Loss: 0.6072, Val Acc: 0.6867, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 008, Loss: 0.5944, Val Acc: 0.7000, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 009, Loss: 0.5787, Val Acc: 0.7467, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 010, Loss: 0.5678, Val Acc: 0.7400, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 011, Loss: 0.5533, Val Acc: 0.7667, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 012, Loss: 0.5388, Val Acc: 0.7533, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 013, Loss: 0.5249, Val Acc: 0.7667, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 014, Loss: 0.5075, Val Acc: 0.7600, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 015, Loss: 0.5180, Val Acc: 0.7667, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 016, Loss: 0.4984, Val Acc: 0.7800, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 017, Loss: 0.5046, Val Acc: 0.7667, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 018, Loss: 0.5122, Val Acc: 0.7667, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 019, Loss: 0.5035, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 020, Loss: 0.4948, Val Acc: 0.7667, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 021, Loss: 0.4923, Val Acc: 0.7800, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 022, Loss: 0.4896, Val Acc: 0.7533, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 023, Loss: 0.4949, Val Acc: 0.7533, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 024, Loss: 0.4898, Val Acc: 0.7667, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 025, Loss: 0.4917, Val Acc: 0.7867, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 026, Loss: 0.4821, Val Acc: 0.7800, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 027, Loss: 0.4815, Val Acc: 0.7800, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 028, Loss: 0.4831, Val Acc: 0.7667, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 029, Loss: 0.4903, Val Acc: 0.7467, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 030, Loss: 0.4824, Val Acc: 0.7400, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 031, Loss: 0.4766, Val Acc: 0.7533, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 032, Loss: 0.4768, Val Acc: 0.7467, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 033, Loss: 0.4718, Val Acc: 0.7533, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 034, Loss: 0.4619, Val Acc: 0.7733, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 035, Loss: 0.4577, Val Acc: 0.7533, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 036, Loss: 0.4544, Val Acc: 0.7733, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 037, Loss: 0.4574, Val Acc: 0.7733, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 038, Loss: 0.4483, Val Acc: 0.7667, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 039, Loss: 0.4481, Val Acc: 0.7667, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 040, Loss: 0.4511, Val Acc: 0.7667, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 041, Loss: 0.4434, Val Acc: 0.7600, Test Acc: 0.7667\n",
      "Seed: 44, Epoch: 042, Loss: 0.4438, Val Acc: 0.7733, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 043, Loss: 0.4378, Val Acc: 0.7600, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 044, Loss: 0.4377, Val Acc: 0.7733, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 045, Loss: 0.4312, Val Acc: 0.7600, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 046, Loss: 0.4259, Val Acc: 0.7600, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 047, Loss: 0.4341, Val Acc: 0.7600, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 048, Loss: 0.4439, Val Acc: 0.7600, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 049, Loss: 0.4820, Val Acc: 0.7200, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 050, Loss: 0.4656, Val Acc: 0.7133, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 051, Loss: 0.4502, Val Acc: 0.7533, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 052, Loss: 0.4533, Val Acc: 0.7600, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 053, Loss: 0.4580, Val Acc: 0.7667, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 054, Loss: 0.4379, Val Acc: 0.7667, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 055, Loss: 0.4252, Val Acc: 0.7600, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 056, Loss: 0.4285, Val Acc: 0.7533, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 057, Loss: 0.4202, Val Acc: 0.7733, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 058, Loss: 0.4336, Val Acc: 0.7667, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 059, Loss: 0.4309, Val Acc: 0.7533, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 060, Loss: 0.4165, Val Acc: 0.7600, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 061, Loss: 0.4144, Val Acc: 0.7533, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 062, Loss: 0.4142, Val Acc: 0.7533, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 063, Loss: 0.4087, Val Acc: 0.7533, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 064, Loss: 0.4066, Val Acc: 0.7600, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 065, Loss: 0.4034, Val Acc: 0.7667, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 066, Loss: 0.4040, Val Acc: 0.7667, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 067, Loss: 0.3986, Val Acc: 0.7800, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 068, Loss: 0.4030, Val Acc: 0.7533, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 069, Loss: 0.4034, Val Acc: 0.7600, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 070, Loss: 0.3924, Val Acc: 0.7533, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 071, Loss: 0.3941, Val Acc: 0.7600, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 072, Loss: 0.3919, Val Acc: 0.7667, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 073, Loss: 0.3868, Val Acc: 0.7667, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 074, Loss: 0.3882, Val Acc: 0.7467, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 075, Loss: 0.3899, Val Acc: 0.7200, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 076, Loss: 0.3834, Val Acc: 0.7267, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 077, Loss: 0.3777, Val Acc: 0.7267, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 078, Loss: 0.3790, Val Acc: 0.7267, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 079, Loss: 0.3787, Val Acc: 0.7200, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 080, Loss: 0.3735, Val Acc: 0.7133, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 081, Loss: 0.3684, Val Acc: 0.7333, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 082, Loss: 0.3692, Val Acc: 0.7333, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 083, Loss: 0.3752, Val Acc: 0.7067, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 084, Loss: 0.3846, Val Acc: 0.7067, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 085, Loss: 0.3747, Val Acc: 0.7200, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 086, Loss: 0.3647, Val Acc: 0.7200, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 087, Loss: 0.3825, Val Acc: 0.7000, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 088, Loss: 0.3652, Val Acc: 0.7000, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 089, Loss: 0.3706, Val Acc: 0.7000, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 090, Loss: 0.3604, Val Acc: 0.7067, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 091, Loss: 0.3694, Val Acc: 0.7000, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 092, Loss: 0.3615, Val Acc: 0.7067, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 093, Loss: 0.3595, Val Acc: 0.7200, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 094, Loss: 0.3597, Val Acc: 0.7067, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 095, Loss: 0.3512, Val Acc: 0.7200, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 096, Loss: 0.3582, Val Acc: 0.7200, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 097, Loss: 0.3540, Val Acc: 0.7267, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 098, Loss: 0.3601, Val Acc: 0.7067, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 099, Loss: 0.3680, Val Acc: 0.7067, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 100, Loss: 0.3602, Val Acc: 0.7133, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 101, Loss: 0.3664, Val Acc: 0.7267, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 102, Loss: 0.3601, Val Acc: 0.7200, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 103, Loss: 0.3584, Val Acc: 0.7667, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 104, Loss: 0.3706, Val Acc: 0.7067, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 105, Loss: 0.3601, Val Acc: 0.7067, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 106, Loss: 0.3692, Val Acc: 0.7067, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 107, Loss: 0.3686, Val Acc: 0.7133, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 108, Loss: 0.3560, Val Acc: 0.7067, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 109, Loss: 0.3516, Val Acc: 0.7200, Test Acc: 0.6600\n",
      "Seed: 44, Epoch: 110, Loss: 0.3579, Val Acc: 0.6800, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 111, Loss: 0.3500, Val Acc: 0.6933, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 112, Loss: 0.3495, Val Acc: 0.7067, Test Acc: 0.5933\n",
      "Seed: 44, Epoch: 113, Loss: 0.4993, Val Acc: 0.6667, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 114, Loss: 0.4781, Val Acc: 0.6533, Test Acc: 0.6067\n",
      "Seed: 44, Epoch: 115, Loss: 0.4211, Val Acc: 0.6733, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 116, Loss: 0.4107, Val Acc: 0.7333, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 117, Loss: 0.4379, Val Acc: 0.7333, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 118, Loss: 0.6511, Val Acc: 0.7333, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 119, Loss: 0.5180, Val Acc: 0.6733, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 120, Loss: 0.4300, Val Acc: 0.6267, Test Acc: 0.6600\n",
      "Seed: 44, Epoch: 121, Loss: 0.5359, Val Acc: 0.6133, Test Acc: 0.6467\n",
      "Seed: 44, Epoch: 122, Loss: 0.5279, Val Acc: 0.6600, Test Acc: 0.6600\n",
      "Seed: 44, Epoch: 123, Loss: 0.4287, Val Acc: 0.7400, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 124, Loss: 0.3813, Val Acc: 0.7733, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 125, Loss: 0.3753, Val Acc: 0.7800, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 126, Loss: 0.3886, Val Acc: 0.7867, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 127, Loss: 0.3898, Val Acc: 0.7667, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 128, Loss: 0.3891, Val Acc: 0.7600, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 129, Loss: 0.3875, Val Acc: 0.7533, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 130, Loss: 0.3794, Val Acc: 0.7533, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 131, Loss: 0.3727, Val Acc: 0.7600, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 132, Loss: 0.3679, Val Acc: 0.7467, Test Acc: 0.6533\n",
      "Seed: 44, Epoch: 133, Loss: 0.3637, Val Acc: 0.7533, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 134, Loss: 0.3629, Val Acc: 0.7333, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 135, Loss: 0.3612, Val Acc: 0.7333, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 136, Loss: 0.3594, Val Acc: 0.7400, Test Acc: 0.6533\n",
      "Seed: 44, Epoch: 137, Loss: 0.3576, Val Acc: 0.7333, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 138, Loss: 0.3563, Val Acc: 0.7467, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 139, Loss: 0.3535, Val Acc: 0.7467, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 140, Loss: 0.3490, Val Acc: 0.7400, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 141, Loss: 0.3497, Val Acc: 0.7267, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 142, Loss: 0.3505, Val Acc: 0.7200, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 143, Loss: 0.3446, Val Acc: 0.7133, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 144, Loss: 0.3434, Val Acc: 0.7200, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 145, Loss: 0.3415, Val Acc: 0.7200, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 146, Loss: 0.3425, Val Acc: 0.7067, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 147, Loss: 0.3402, Val Acc: 0.7133, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 148, Loss: 0.3408, Val Acc: 0.7267, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 149, Loss: 0.3367, Val Acc: 0.7133, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 150, Loss: 0.3406, Val Acc: 0.7200, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 151, Loss: 0.3350, Val Acc: 0.7067, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 152, Loss: 0.3360, Val Acc: 0.7133, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 153, Loss: 0.3348, Val Acc: 0.7000, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 154, Loss: 0.3299, Val Acc: 0.7133, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 155, Loss: 0.3308, Val Acc: 0.7333, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 156, Loss: 0.3347, Val Acc: 0.7200, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 157, Loss: 0.3286, Val Acc: 0.7333, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 158, Loss: 0.3290, Val Acc: 0.7333, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 159, Loss: 0.3282, Val Acc: 0.6867, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 160, Loss: 0.3227, Val Acc: 0.6867, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 161, Loss: 0.3227, Val Acc: 0.7000, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 162, Loss: 0.3228, Val Acc: 0.6933, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 163, Loss: 0.3235, Val Acc: 0.6867, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 164, Loss: 0.3188, Val Acc: 0.7133, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 165, Loss: 0.3163, Val Acc: 0.7200, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 166, Loss: 0.3202, Val Acc: 0.7133, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 167, Loss: 0.3190, Val Acc: 0.7133, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 168, Loss: 0.3221, Val Acc: 0.7133, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 169, Loss: 0.3237, Val Acc: 0.6867, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 170, Loss: 0.3146, Val Acc: 0.7000, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 171, Loss: 0.3094, Val Acc: 0.7067, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 172, Loss: 0.3180, Val Acc: 0.7067, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 173, Loss: 0.3264, Val Acc: 0.6933, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 174, Loss: 0.3194, Val Acc: 0.6800, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 175, Loss: 0.3210, Val Acc: 0.6933, Test Acc: 0.6733\n",
      "Early stopping at epoch 175 for seed 44\n",
      "Average Time: 34.36 seconds\n",
      "Var Time: 3.63 seconds\n",
      "Average Memory: 252.00 MB\n",
      "Average Best Val Acc: 0.7867\n",
      "Std Best Test Acc: 0.0362\n",
      "Average Test Acc: 0.7422\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-BINARY\", transform=T.Compose([T.OneHotDegree(136)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_CGI(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_CGI, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = CGIPool(hidden_channels, ratio=0.7)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = CGIPool(hidden_channels, ratio=0.7)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, perm = self.pool1(x, edge_index, None, batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, perm = self.pool2(x, edge_index, None, batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 189 for seed 44\n",
      "Average Time: 49.68 seconds\n",
      "Var Time: 1.83 seconds\n",
      "Average Memory: 328.67 MB\n",
      "Average Best Val Acc: 0.5259\n",
      "Std Best Test Acc: 0.0202\n",
      "Average Test Acc: 0.4622\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-MULTI\", transform=T.Compose([T.OneHotDegree(88)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_CGI(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_CGI, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = CGIPool(hidden_channels, ratio=0.9)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = CGIPool(hidden_channels, ratio=0.9)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, perm = self.pool1(x, edge_index, None, batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, perm = self.pool2(x, edge_index, None, batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 1.0794, Val Acc: 0.3493, Test Acc: 0.3027\n",
      "Seed: 42, Epoch: 002, Loss: 0.9672, Val Acc: 0.4133, Test Acc: 0.3813\n",
      "Seed: 42, Epoch: 003, Loss: 0.9169, Val Acc: 0.5213, Test Acc: 0.4693\n",
      "Seed: 42, Epoch: 004, Loss: 0.8609, Val Acc: 0.6987, Test Acc: 0.6427\n",
      "Seed: 42, Epoch: 005, Loss: 0.8011, Val Acc: 0.7093, Test Acc: 0.6760\n",
      "Seed: 42, Epoch: 006, Loss: 0.7357, Val Acc: 0.6840, Test Acc: 0.6627\n",
      "Seed: 42, Epoch: 007, Loss: 0.6806, Val Acc: 0.6747, Test Acc: 0.6613\n",
      "Seed: 42, Epoch: 008, Loss: 0.6358, Val Acc: 0.6613, Test Acc: 0.6573\n",
      "Seed: 42, Epoch: 009, Loss: 0.6139, Val Acc: 0.6720, Test Acc: 0.6787\n",
      "Seed: 42, Epoch: 010, Loss: 0.5901, Val Acc: 0.6720, Test Acc: 0.6760\n",
      "Seed: 42, Epoch: 011, Loss: 0.5742, Val Acc: 0.6747, Test Acc: 0.6813\n",
      "Seed: 42, Epoch: 012, Loss: 0.5639, Val Acc: 0.7133, Test Acc: 0.6960\n",
      "Seed: 42, Epoch: 013, Loss: 0.5534, Val Acc: 0.7147, Test Acc: 0.6987\n",
      "Seed: 42, Epoch: 014, Loss: 0.5393, Val Acc: 0.7160, Test Acc: 0.6960\n",
      "Seed: 42, Epoch: 015, Loss: 0.5316, Val Acc: 0.7493, Test Acc: 0.7213\n",
      "Seed: 42, Epoch: 016, Loss: 0.5214, Val Acc: 0.7733, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 017, Loss: 0.5159, Val Acc: 0.7493, Test Acc: 0.7227\n",
      "Seed: 42, Epoch: 018, Loss: 0.5161, Val Acc: 0.7573, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 019, Loss: 0.5029, Val Acc: 0.7800, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 020, Loss: 0.4965, Val Acc: 0.7467, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 021, Loss: 0.4915, Val Acc: 0.7693, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 022, Loss: 0.4876, Val Acc: 0.7560, Test Acc: 0.7307\n",
      "Seed: 42, Epoch: 023, Loss: 0.4860, Val Acc: 0.7827, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 024, Loss: 0.4802, Val Acc: 0.7720, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 025, Loss: 0.4733, Val Acc: 0.7773, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 026, Loss: 0.4719, Val Acc: 0.7573, Test Acc: 0.7173\n",
      "Seed: 42, Epoch: 027, Loss: 0.4596, Val Acc: 0.7800, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 028, Loss: 0.4493, Val Acc: 0.7800, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 029, Loss: 0.4431, Val Acc: 0.7907, Test Acc: 0.7293\n",
      "Seed: 42, Epoch: 030, Loss: 0.4372, Val Acc: 0.7693, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 031, Loss: 0.4347, Val Acc: 0.7907, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 032, Loss: 0.4385, Val Acc: 0.7773, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 033, Loss: 0.4434, Val Acc: 0.8000, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 034, Loss: 0.4333, Val Acc: 0.8000, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 035, Loss: 0.4220, Val Acc: 0.8093, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 036, Loss: 0.4192, Val Acc: 0.7947, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 037, Loss: 0.4144, Val Acc: 0.8080, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 038, Loss: 0.4079, Val Acc: 0.7933, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 039, Loss: 0.4044, Val Acc: 0.7973, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 040, Loss: 0.4063, Val Acc: 0.8133, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 041, Loss: 0.4013, Val Acc: 0.8013, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 042, Loss: 0.4007, Val Acc: 0.7840, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 043, Loss: 0.4043, Val Acc: 0.8027, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 044, Loss: 0.3952, Val Acc: 0.8053, Test Acc: 0.7573\n",
      "Seed: 42, Epoch: 045, Loss: 0.3853, Val Acc: 0.7973, Test Acc: 0.7573\n",
      "Seed: 42, Epoch: 046, Loss: 0.3836, Val Acc: 0.7760, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 047, Loss: 0.3823, Val Acc: 0.8000, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 048, Loss: 0.3792, Val Acc: 0.8120, Test Acc: 0.7547\n",
      "Seed: 42, Epoch: 049, Loss: 0.3748, Val Acc: 0.8040, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 050, Loss: 0.3773, Val Acc: 0.8053, Test Acc: 0.7587\n",
      "Seed: 42, Epoch: 051, Loss: 0.3766, Val Acc: 0.8093, Test Acc: 0.7760\n",
      "Seed: 42, Epoch: 052, Loss: 0.3828, Val Acc: 0.8027, Test Acc: 0.7587\n",
      "Seed: 42, Epoch: 053, Loss: 0.3838, Val Acc: 0.8080, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 054, Loss: 0.3729, Val Acc: 0.8040, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 055, Loss: 0.3704, Val Acc: 0.8133, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 056, Loss: 0.3646, Val Acc: 0.8013, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 057, Loss: 0.3572, Val Acc: 0.8053, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 058, Loss: 0.3630, Val Acc: 0.7973, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 059, Loss: 0.3654, Val Acc: 0.8000, Test Acc: 0.7573\n",
      "Seed: 42, Epoch: 060, Loss: 0.3560, Val Acc: 0.7827, Test Acc: 0.7587\n",
      "Seed: 42, Epoch: 061, Loss: 0.3616, Val Acc: 0.8147, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 062, Loss: 0.3524, Val Acc: 0.8067, Test Acc: 0.7747\n",
      "Seed: 42, Epoch: 063, Loss: 0.3513, Val Acc: 0.8093, Test Acc: 0.7680\n",
      "Seed: 42, Epoch: 064, Loss: 0.3547, Val Acc: 0.7920, Test Acc: 0.7627\n",
      "Seed: 42, Epoch: 065, Loss: 0.3604, Val Acc: 0.8080, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 066, Loss: 0.3566, Val Acc: 0.8027, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 067, Loss: 0.3540, Val Acc: 0.7960, Test Acc: 0.7680\n",
      "Seed: 42, Epoch: 068, Loss: 0.3509, Val Acc: 0.8120, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 069, Loss: 0.3555, Val Acc: 0.7853, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 070, Loss: 0.3545, Val Acc: 0.8067, Test Acc: 0.7787\n",
      "Seed: 42, Epoch: 071, Loss: 0.3490, Val Acc: 0.8000, Test Acc: 0.7747\n",
      "Seed: 42, Epoch: 072, Loss: 0.3645, Val Acc: 0.8160, Test Acc: 0.7827\n",
      "Seed: 42, Epoch: 073, Loss: 0.3706, Val Acc: 0.8187, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 074, Loss: 0.3579, Val Acc: 0.7840, Test Acc: 0.7760\n",
      "Seed: 42, Epoch: 075, Loss: 0.3817, Val Acc: 0.8013, Test Acc: 0.7760\n",
      "Seed: 42, Epoch: 076, Loss: 0.4608, Val Acc: 0.7893, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 077, Loss: 0.3744, Val Acc: 0.8080, Test Acc: 0.7787\n",
      "Seed: 42, Epoch: 078, Loss: 0.3739, Val Acc: 0.7907, Test Acc: 0.7560\n",
      "Seed: 42, Epoch: 079, Loss: 0.3661, Val Acc: 0.8040, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 080, Loss: 0.3966, Val Acc: 0.7640, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 081, Loss: 1.5426, Val Acc: 0.7893, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 082, Loss: 0.5407, Val Acc: 0.7773, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 083, Loss: 0.4762, Val Acc: 0.7947, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 084, Loss: 0.4384, Val Acc: 0.8000, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 085, Loss: 0.4206, Val Acc: 0.7960, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 086, Loss: 0.4036, Val Acc: 0.8067, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 087, Loss: 0.3934, Val Acc: 0.7960, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 088, Loss: 0.3820, Val Acc: 0.7973, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 089, Loss: 0.7272, Val Acc: 0.7160, Test Acc: 0.6707\n",
      "Seed: 42, Epoch: 090, Loss: 0.5995, Val Acc: 0.7387, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 091, Loss: 0.5246, Val Acc: 0.6733, Test Acc: 0.6613\n",
      "Seed: 42, Epoch: 092, Loss: 0.5636, Val Acc: 0.7773, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 093, Loss: 0.4707, Val Acc: 0.7627, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 094, Loss: 0.4503, Val Acc: 0.7773, Test Acc: 0.7507\n",
      "Seed: 42, Epoch: 095, Loss: 0.4343, Val Acc: 0.7827, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 096, Loss: 0.4269, Val Acc: 0.7707, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 097, Loss: 0.4263, Val Acc: 0.7773, Test Acc: 0.7560\n",
      "Seed: 42, Epoch: 098, Loss: 0.4160, Val Acc: 0.7667, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 099, Loss: 0.4087, Val Acc: 0.7787, Test Acc: 0.7747\n",
      "Seed: 42, Epoch: 100, Loss: 0.4017, Val Acc: 0.7840, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 101, Loss: 0.4167, Val Acc: 0.7827, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 102, Loss: 0.4093, Val Acc: 0.7920, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 103, Loss: 0.3967, Val Acc: 0.7813, Test Acc: 0.7760\n",
      "Seed: 42, Epoch: 104, Loss: 0.3953, Val Acc: 0.7880, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 105, Loss: 0.3936, Val Acc: 0.7773, Test Acc: 0.7747\n",
      "Seed: 42, Epoch: 106, Loss: 0.3963, Val Acc: 0.7773, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 107, Loss: 0.4053, Val Acc: 0.7800, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 108, Loss: 0.4038, Val Acc: 0.7653, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 109, Loss: 0.4031, Val Acc: 0.7653, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 110, Loss: 0.3938, Val Acc: 0.7747, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 111, Loss: 0.3869, Val Acc: 0.7800, Test Acc: 0.7573\n",
      "Seed: 42, Epoch: 112, Loss: 0.3854, Val Acc: 0.7720, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 113, Loss: 0.3872, Val Acc: 0.7907, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 114, Loss: 0.3811, Val Acc: 0.7827, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 115, Loss: 0.3771, Val Acc: 0.7920, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 116, Loss: 0.3692, Val Acc: 0.7813, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 117, Loss: 0.3690, Val Acc: 0.7947, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 118, Loss: 0.3648, Val Acc: 0.7907, Test Acc: 0.7760\n",
      "Seed: 42, Epoch: 119, Loss: 0.3624, Val Acc: 0.7947, Test Acc: 0.7627\n",
      "Seed: 42, Epoch: 120, Loss: 0.3565, Val Acc: 0.8000, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 121, Loss: 0.3592, Val Acc: 0.7933, Test Acc: 0.7747\n",
      "Seed: 42, Epoch: 122, Loss: 0.3553, Val Acc: 0.7933, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 123, Loss: 0.3518, Val Acc: 0.7907, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 124, Loss: 0.3501, Val Acc: 0.7973, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 125, Loss: 0.3488, Val Acc: 0.7960, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 126, Loss: 0.3443, Val Acc: 0.8067, Test Acc: 0.7720\n",
      "Seed: 42, Epoch: 127, Loss: 0.3430, Val Acc: 0.7933, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 128, Loss: 0.3472, Val Acc: 0.7933, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 129, Loss: 0.3487, Val Acc: 0.8027, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 130, Loss: 0.3474, Val Acc: 0.7973, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 131, Loss: 0.3755, Val Acc: 0.8053, Test Acc: 0.7720\n",
      "Seed: 42, Epoch: 132, Loss: 0.3540, Val Acc: 0.7933, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 133, Loss: 0.3434, Val Acc: 0.7907, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 134, Loss: 0.3463, Val Acc: 0.7893, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 135, Loss: 0.3430, Val Acc: 0.8053, Test Acc: 0.7760\n",
      "Seed: 42, Epoch: 136, Loss: 0.3412, Val Acc: 0.7947, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 137, Loss: 0.3392, Val Acc: 0.7920, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 138, Loss: 0.3398, Val Acc: 0.7960, Test Acc: 0.7747\n",
      "Seed: 42, Epoch: 139, Loss: 0.3382, Val Acc: 0.7920, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 140, Loss: 0.3397, Val Acc: 0.7880, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 141, Loss: 0.3391, Val Acc: 0.8000, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 142, Loss: 0.3377, Val Acc: 0.7907, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 143, Loss: 0.3320, Val Acc: 0.8053, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 144, Loss: 0.3320, Val Acc: 0.7987, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 145, Loss: 0.3292, Val Acc: 0.8093, Test Acc: 0.7760\n",
      "Seed: 42, Epoch: 146, Loss: 0.3313, Val Acc: 0.8040, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 147, Loss: 0.3333, Val Acc: 0.7973, Test Acc: 0.7760\n",
      "Seed: 42, Epoch: 148, Loss: 0.3296, Val Acc: 0.8107, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 149, Loss: 0.3301, Val Acc: 0.8133, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 150, Loss: 0.3268, Val Acc: 0.8080, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 151, Loss: 0.3263, Val Acc: 0.8133, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 152, Loss: 0.3215, Val Acc: 0.8067, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 153, Loss: 0.3170, Val Acc: 0.8027, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 154, Loss: 0.3191, Val Acc: 0.8040, Test Acc: 0.7720\n",
      "Seed: 42, Epoch: 155, Loss: 0.3209, Val Acc: 0.8080, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 156, Loss: 0.3169, Val Acc: 0.8067, Test Acc: 0.7627\n",
      "Seed: 42, Epoch: 157, Loss: 0.3125, Val Acc: 0.8080, Test Acc: 0.7680\n",
      "Seed: 42, Epoch: 158, Loss: 0.3108, Val Acc: 0.8040, Test Acc: 0.7787\n",
      "Seed: 42, Epoch: 159, Loss: 0.3164, Val Acc: 0.7880, Test Acc: 0.7853\n",
      "Seed: 42, Epoch: 160, Loss: 0.3176, Val Acc: 0.8053, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 161, Loss: 0.3079, Val Acc: 0.7987, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 162, Loss: 0.3163, Val Acc: 0.8040, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 163, Loss: 0.3057, Val Acc: 0.8067, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 164, Loss: 0.3035, Val Acc: 0.8053, Test Acc: 0.7827\n",
      "Seed: 42, Epoch: 165, Loss: 0.3076, Val Acc: 0.8000, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 166, Loss: 0.3006, Val Acc: 0.8040, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 167, Loss: 0.3047, Val Acc: 0.8053, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 168, Loss: 0.3187, Val Acc: 0.8067, Test Acc: 0.7720\n",
      "Seed: 42, Epoch: 169, Loss: 0.3242, Val Acc: 0.7893, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 170, Loss: 0.3260, Val Acc: 0.7947, Test Acc: 0.7747\n",
      "Seed: 42, Epoch: 171, Loss: 0.3143, Val Acc: 0.7947, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 172, Loss: 0.3105, Val Acc: 0.8093, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 173, Loss: 0.3257, Val Acc: 0.7627, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 174, Loss: 0.4109, Val Acc: 0.7653, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 175, Loss: 0.5864, Val Acc: 0.7413, Test Acc: 0.6947\n",
      "Seed: 42, Epoch: 176, Loss: 0.5634, Val Acc: 0.7587, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 177, Loss: 0.4177, Val Acc: 0.7947, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 178, Loss: 0.3871, Val Acc: 0.7853, Test Acc: 0.7480\n",
      "Seed: 42, Epoch: 179, Loss: 0.3612, Val Acc: 0.7947, Test Acc: 0.7547\n",
      "Seed: 42, Epoch: 180, Loss: 0.3531, Val Acc: 0.7920, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 181, Loss: 0.3411, Val Acc: 0.8027, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 182, Loss: 0.3418, Val Acc: 0.8027, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 183, Loss: 0.3440, Val Acc: 0.7947, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 184, Loss: 0.3271, Val Acc: 0.7947, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 185, Loss: 0.3337, Val Acc: 0.7920, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 186, Loss: 0.3293, Val Acc: 0.8053, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 187, Loss: 0.3226, Val Acc: 0.8027, Test Acc: 0.7760\n",
      "Seed: 42, Epoch: 188, Loss: 0.3183, Val Acc: 0.7987, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 189, Loss: 0.3183, Val Acc: 0.8093, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 190, Loss: 0.3141, Val Acc: 0.8040, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 191, Loss: 0.3057, Val Acc: 0.8000, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 192, Loss: 0.3025, Val Acc: 0.7920, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 193, Loss: 0.3024, Val Acc: 0.8067, Test Acc: 0.7827\n",
      "Seed: 42, Epoch: 194, Loss: 0.2969, Val Acc: 0.7973, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 195, Loss: 0.2956, Val Acc: 0.7973, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 196, Loss: 0.2985, Val Acc: 0.7947, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 197, Loss: 0.2967, Val Acc: 0.7920, Test Acc: 0.7827\n",
      "Seed: 42, Epoch: 198, Loss: 0.2906, Val Acc: 0.7933, Test Acc: 0.7827\n",
      "Seed: 42, Epoch: 199, Loss: 0.2922, Val Acc: 0.8000, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 200, Loss: 0.2945, Val Acc: 0.8053, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 001, Loss: 1.1075, Val Acc: 0.6547, Test Acc: 0.6547\n",
      "Seed: 43, Epoch: 002, Loss: 0.9124, Val Acc: 0.6800, Test Acc: 0.6800\n",
      "Seed: 43, Epoch: 003, Loss: 0.8050, Val Acc: 0.6853, Test Acc: 0.6840\n",
      "Seed: 43, Epoch: 004, Loss: 0.7057, Val Acc: 0.7080, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 005, Loss: 0.6684, Val Acc: 0.7160, Test Acc: 0.7160\n",
      "Seed: 43, Epoch: 006, Loss: 0.6431, Val Acc: 0.7240, Test Acc: 0.7160\n",
      "Seed: 43, Epoch: 007, Loss: 0.6198, Val Acc: 0.7160, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 008, Loss: 0.6124, Val Acc: 0.7213, Test Acc: 0.7160\n",
      "Seed: 43, Epoch: 009, Loss: 0.6037, Val Acc: 0.7013, Test Acc: 0.7040\n",
      "Seed: 43, Epoch: 010, Loss: 0.6017, Val Acc: 0.7200, Test Acc: 0.7187\n",
      "Seed: 43, Epoch: 011, Loss: 0.5908, Val Acc: 0.7200, Test Acc: 0.7173\n",
      "Seed: 43, Epoch: 012, Loss: 0.5809, Val Acc: 0.7200, Test Acc: 0.7173\n",
      "Seed: 43, Epoch: 013, Loss: 0.5748, Val Acc: 0.7120, Test Acc: 0.7213\n",
      "Seed: 43, Epoch: 014, Loss: 0.5699, Val Acc: 0.7080, Test Acc: 0.7107\n",
      "Seed: 43, Epoch: 015, Loss: 0.5603, Val Acc: 0.7187, Test Acc: 0.7213\n",
      "Seed: 43, Epoch: 016, Loss: 0.5747, Val Acc: 0.7200, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 017, Loss: 0.5542, Val Acc: 0.7107, Test Acc: 0.7160\n",
      "Seed: 43, Epoch: 018, Loss: 0.5367, Val Acc: 0.7280, Test Acc: 0.7293\n",
      "Seed: 43, Epoch: 019, Loss: 0.5223, Val Acc: 0.7667, Test Acc: 0.7520\n",
      "Seed: 43, Epoch: 020, Loss: 0.5057, Val Acc: 0.7707, Test Acc: 0.7587\n",
      "Seed: 43, Epoch: 021, Loss: 0.4932, Val Acc: 0.7760, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 022, Loss: 0.4813, Val Acc: 0.7813, Test Acc: 0.7560\n",
      "Seed: 43, Epoch: 023, Loss: 0.4711, Val Acc: 0.7853, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 024, Loss: 0.4642, Val Acc: 0.7653, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 025, Loss: 0.4650, Val Acc: 0.7773, Test Acc: 0.7653\n",
      "Seed: 43, Epoch: 026, Loss: 0.4621, Val Acc: 0.7733, Test Acc: 0.7627\n",
      "Seed: 43, Epoch: 027, Loss: 0.4644, Val Acc: 0.7933, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 028, Loss: 0.4512, Val Acc: 0.7867, Test Acc: 0.7653\n",
      "Seed: 43, Epoch: 029, Loss: 0.4485, Val Acc: 0.7720, Test Acc: 0.7613\n",
      "Seed: 43, Epoch: 030, Loss: 0.4450, Val Acc: 0.7853, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 031, Loss: 0.4337, Val Acc: 0.7840, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 032, Loss: 0.4313, Val Acc: 0.7813, Test Acc: 0.7747\n",
      "Seed: 43, Epoch: 033, Loss: 0.4236, Val Acc: 0.7867, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 034, Loss: 0.4194, Val Acc: 0.7907, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 035, Loss: 0.4173, Val Acc: 0.7973, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 036, Loss: 0.4138, Val Acc: 0.8013, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 037, Loss: 0.4078, Val Acc: 0.7987, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 038, Loss: 0.4081, Val Acc: 0.7720, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 039, Loss: 0.4449, Val Acc: 0.7653, Test Acc: 0.7347\n",
      "Seed: 43, Epoch: 040, Loss: 0.4225, Val Acc: 0.7893, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 041, Loss: 0.7512, Val Acc: 0.7480, Test Acc: 0.7360\n",
      "Seed: 43, Epoch: 042, Loss: 0.4772, Val Acc: 0.7747, Test Acc: 0.7640\n",
      "Seed: 43, Epoch: 043, Loss: 0.4607, Val Acc: 0.7787, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 044, Loss: 0.4550, Val Acc: 0.7787, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 045, Loss: 0.4419, Val Acc: 0.7773, Test Acc: 0.7653\n",
      "Seed: 43, Epoch: 046, Loss: 0.4311, Val Acc: 0.7867, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 047, Loss: 0.4226, Val Acc: 0.7853, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 048, Loss: 0.4143, Val Acc: 0.7867, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 049, Loss: 0.4069, Val Acc: 0.7907, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 050, Loss: 0.4017, Val Acc: 0.7933, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 051, Loss: 0.3940, Val Acc: 0.8000, Test Acc: 0.7853\n",
      "Seed: 43, Epoch: 052, Loss: 0.3938, Val Acc: 0.7840, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 053, Loss: 0.3863, Val Acc: 0.8027, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 054, Loss: 0.3761, Val Acc: 0.7987, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 055, Loss: 0.3769, Val Acc: 0.8053, Test Acc: 0.7947\n",
      "Seed: 43, Epoch: 056, Loss: 0.3733, Val Acc: 0.8027, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 057, Loss: 0.3623, Val Acc: 0.8000, Test Acc: 0.7947\n",
      "Seed: 43, Epoch: 058, Loss: 0.3579, Val Acc: 0.8080, Test Acc: 0.8013\n",
      "Seed: 43, Epoch: 059, Loss: 0.3544, Val Acc: 0.8093, Test Acc: 0.7920\n",
      "Seed: 43, Epoch: 060, Loss: 0.3496, Val Acc: 0.8107, Test Acc: 0.8027\n",
      "Seed: 43, Epoch: 061, Loss: 0.3494, Val Acc: 0.8133, Test Acc: 0.7947\n",
      "Seed: 43, Epoch: 062, Loss: 0.3461, Val Acc: 0.8053, Test Acc: 0.8053\n",
      "Seed: 43, Epoch: 063, Loss: 0.3422, Val Acc: 0.8013, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 064, Loss: 0.3342, Val Acc: 0.7987, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 065, Loss: 0.3330, Val Acc: 0.8027, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 066, Loss: 0.3439, Val Acc: 0.8040, Test Acc: 0.8013\n",
      "Seed: 43, Epoch: 067, Loss: 0.3468, Val Acc: 0.8027, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 068, Loss: 0.3368, Val Acc: 0.8133, Test Acc: 0.8000\n",
      "Seed: 43, Epoch: 069, Loss: 0.3290, Val Acc: 0.8200, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 070, Loss: 0.3199, Val Acc: 0.8107, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 071, Loss: 0.3169, Val Acc: 0.8147, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 072, Loss: 0.3120, Val Acc: 0.8160, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 073, Loss: 0.3106, Val Acc: 0.7920, Test Acc: 0.7987\n",
      "Seed: 43, Epoch: 074, Loss: 0.3095, Val Acc: 0.8173, Test Acc: 0.8053\n",
      "Seed: 43, Epoch: 075, Loss: 0.3172, Val Acc: 0.8280, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 076, Loss: 0.3210, Val Acc: 0.8253, Test Acc: 0.8013\n",
      "Seed: 43, Epoch: 077, Loss: 0.3146, Val Acc: 0.8240, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 078, Loss: 0.3151, Val Acc: 0.8213, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 079, Loss: 0.3052, Val Acc: 0.8173, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 080, Loss: 0.3016, Val Acc: 0.8240, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 081, Loss: 0.2961, Val Acc: 0.8320, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 082, Loss: 0.2942, Val Acc: 0.8267, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 083, Loss: 0.2904, Val Acc: 0.8307, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 084, Loss: 0.2941, Val Acc: 0.8040, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 085, Loss: 0.3034, Val Acc: 0.8307, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 086, Loss: 0.2884, Val Acc: 0.8360, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 087, Loss: 0.2843, Val Acc: 0.8307, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 088, Loss: 0.2811, Val Acc: 0.8267, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 089, Loss: 0.2783, Val Acc: 0.8093, Test Acc: 0.8027\n",
      "Seed: 43, Epoch: 090, Loss: 0.2932, Val Acc: 0.8173, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 091, Loss: 0.2857, Val Acc: 0.8173, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 092, Loss: 0.2873, Val Acc: 0.8120, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 093, Loss: 0.2735, Val Acc: 0.8307, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 094, Loss: 0.2726, Val Acc: 0.8240, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 095, Loss: 0.2707, Val Acc: 0.8307, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 096, Loss: 0.2721, Val Acc: 0.8333, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 097, Loss: 0.2633, Val Acc: 0.8280, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 098, Loss: 0.2697, Val Acc: 0.8093, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 099, Loss: 0.2677, Val Acc: 0.8173, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 100, Loss: 0.2636, Val Acc: 0.8173, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 101, Loss: 0.2585, Val Acc: 0.8240, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 102, Loss: 0.2592, Val Acc: 0.8307, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 103, Loss: 0.2572, Val Acc: 0.8107, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 104, Loss: 0.2624, Val Acc: 0.8093, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 105, Loss: 0.2656, Val Acc: 0.8147, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 106, Loss: 0.2599, Val Acc: 0.8187, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 107, Loss: 0.2733, Val Acc: 0.8160, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 108, Loss: 0.2647, Val Acc: 0.8240, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 109, Loss: 0.2552, Val Acc: 0.8200, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 110, Loss: 0.2519, Val Acc: 0.8213, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 111, Loss: 0.2475, Val Acc: 0.8200, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 112, Loss: 0.2424, Val Acc: 0.8107, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 113, Loss: 0.2437, Val Acc: 0.8147, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 114, Loss: 0.2502, Val Acc: 0.8293, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 115, Loss: 0.2394, Val Acc: 0.8227, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 116, Loss: 0.2361, Val Acc: 0.8200, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 117, Loss: 0.2379, Val Acc: 0.8227, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 118, Loss: 0.2414, Val Acc: 0.8067, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 119, Loss: 0.2483, Val Acc: 0.8187, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 120, Loss: 0.2400, Val Acc: 0.8253, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 121, Loss: 0.2392, Val Acc: 0.8200, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 122, Loss: 0.2440, Val Acc: 0.8200, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 123, Loss: 0.2359, Val Acc: 0.8187, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 124, Loss: 0.2452, Val Acc: 0.8200, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 125, Loss: 0.2435, Val Acc: 0.8013, Test Acc: 0.8013\n",
      "Seed: 43, Epoch: 126, Loss: 0.2599, Val Acc: 0.8187, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 127, Loss: 0.2422, Val Acc: 0.8240, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 128, Loss: 0.2325, Val Acc: 0.8267, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 129, Loss: 0.2295, Val Acc: 0.8253, Test Acc: 0.8267\n",
      "Seed: 43, Epoch: 130, Loss: 0.2288, Val Acc: 0.8253, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 131, Loss: 0.2239, Val Acc: 0.8147, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 132, Loss: 0.2235, Val Acc: 0.8227, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 133, Loss: 0.2204, Val Acc: 0.8173, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 134, Loss: 0.2207, Val Acc: 0.8160, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 135, Loss: 0.2284, Val Acc: 0.8253, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 136, Loss: 0.2411, Val Acc: 0.8200, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 137, Loss: 0.2978, Val Acc: 0.8227, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 138, Loss: 0.2649, Val Acc: 0.7853, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 139, Loss: 0.2728, Val Acc: 0.8133, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 140, Loss: 0.2735, Val Acc: 0.8227, Test Acc: 0.8267\n",
      "Seed: 43, Epoch: 141, Loss: 0.2459, Val Acc: 0.8027, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 142, Loss: 0.2527, Val Acc: 0.8120, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 143, Loss: 0.2564, Val Acc: 0.8173, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 144, Loss: 0.2346, Val Acc: 0.8200, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 145, Loss: 0.2207, Val Acc: 0.8227, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 146, Loss: 0.2208, Val Acc: 0.8267, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 147, Loss: 0.2193, Val Acc: 0.8267, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 148, Loss: 0.2161, Val Acc: 0.8160, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 149, Loss: 0.2181, Val Acc: 0.8187, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 150, Loss: 0.2176, Val Acc: 0.8187, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 151, Loss: 0.2121, Val Acc: 0.8147, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 152, Loss: 0.2098, Val Acc: 0.8147, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 153, Loss: 0.2079, Val Acc: 0.8227, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 154, Loss: 0.2039, Val Acc: 0.8200, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 155, Loss: 0.2087, Val Acc: 0.8173, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 156, Loss: 0.2072, Val Acc: 0.8067, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 157, Loss: 0.2046, Val Acc: 0.8120, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 158, Loss: 0.2036, Val Acc: 0.8107, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 159, Loss: 0.2001, Val Acc: 0.8173, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 160, Loss: 0.2019, Val Acc: 0.8213, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 161, Loss: 0.2001, Val Acc: 0.8147, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 162, Loss: 0.1980, Val Acc: 0.8200, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 163, Loss: 0.1951, Val Acc: 0.8147, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 164, Loss: 0.2111, Val Acc: 0.7987, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 165, Loss: 0.2512, Val Acc: 0.7853, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 166, Loss: 0.2861, Val Acc: 0.8120, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 167, Loss: 0.2340, Val Acc: 0.8040, Test Acc: 0.7933\n",
      "Seed: 43, Epoch: 168, Loss: 0.2313, Val Acc: 0.8053, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 169, Loss: 0.2166, Val Acc: 0.8200, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 170, Loss: 0.2050, Val Acc: 0.8253, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 171, Loss: 0.1965, Val Acc: 0.8253, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 172, Loss: 0.1919, Val Acc: 0.8227, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 173, Loss: 0.1890, Val Acc: 0.8320, Test Acc: 0.8280\n",
      "Seed: 43, Epoch: 174, Loss: 0.1871, Val Acc: 0.8293, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 175, Loss: 0.1910, Val Acc: 0.8240, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 176, Loss: 0.1887, Val Acc: 0.8213, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 177, Loss: 0.1927, Val Acc: 0.8107, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 178, Loss: 0.1986, Val Acc: 0.8187, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 179, Loss: 0.1904, Val Acc: 0.8293, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 180, Loss: 0.1856, Val Acc: 0.8227, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 181, Loss: 0.1822, Val Acc: 0.8187, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 182, Loss: 0.1829, Val Acc: 0.8267, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 183, Loss: 0.1855, Val Acc: 0.8240, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 184, Loss: 0.1834, Val Acc: 0.8120, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 185, Loss: 0.1850, Val Acc: 0.8213, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 186, Loss: 0.1819, Val Acc: 0.8280, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 187, Loss: 0.1774, Val Acc: 0.8253, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 188, Loss: 0.1750, Val Acc: 0.8080, Test Acc: 0.8040\n",
      "Seed: 43, Epoch: 189, Loss: 0.1758, Val Acc: 0.8147, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 190, Loss: 0.1806, Val Acc: 0.8200, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 191, Loss: 0.1771, Val Acc: 0.8307, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 192, Loss: 0.1694, Val Acc: 0.8253, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 193, Loss: 0.1664, Val Acc: 0.8267, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 194, Loss: 0.1643, Val Acc: 0.8160, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 195, Loss: 0.1649, Val Acc: 0.8133, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 196, Loss: 0.1644, Val Acc: 0.8173, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 197, Loss: 0.1623, Val Acc: 0.8173, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 198, Loss: 0.1627, Val Acc: 0.8240, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 199, Loss: 0.1600, Val Acc: 0.8213, Test Acc: 0.8053\n",
      "Seed: 43, Epoch: 200, Loss: 0.1579, Val Acc: 0.8187, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 001, Loss: 0.9363, Val Acc: 0.6853, Test Acc: 0.6853\n",
      "Seed: 44, Epoch: 002, Loss: 0.7921, Val Acc: 0.6667, Test Acc: 0.6653\n",
      "Seed: 44, Epoch: 003, Loss: 0.7024, Val Acc: 0.7120, Test Acc: 0.7013\n",
      "Seed: 44, Epoch: 004, Loss: 0.6479, Val Acc: 0.7120, Test Acc: 0.6987\n",
      "Seed: 44, Epoch: 005, Loss: 0.6075, Val Acc: 0.7160, Test Acc: 0.7040\n",
      "Seed: 44, Epoch: 006, Loss: 0.5707, Val Acc: 0.7267, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 007, Loss: 0.5500, Val Acc: 0.7240, Test Acc: 0.7093\n",
      "Seed: 44, Epoch: 008, Loss: 0.5302, Val Acc: 0.7213, Test Acc: 0.7173\n",
      "Seed: 44, Epoch: 009, Loss: 0.5134, Val Acc: 0.7240, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 010, Loss: 0.5045, Val Acc: 0.7467, Test Acc: 0.7680\n",
      "Seed: 44, Epoch: 011, Loss: 0.4856, Val Acc: 0.7467, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 012, Loss: 0.4700, Val Acc: 0.7600, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 013, Loss: 0.4564, Val Acc: 0.7493, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 014, Loss: 0.4491, Val Acc: 0.7480, Test Acc: 0.7493\n",
      "Seed: 44, Epoch: 015, Loss: 0.4590, Val Acc: 0.7507, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 016, Loss: 0.4305, Val Acc: 0.7560, Test Acc: 0.7773\n",
      "Seed: 44, Epoch: 017, Loss: 0.4216, Val Acc: 0.7600, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 018, Loss: 0.4152, Val Acc: 0.7467, Test Acc: 0.7680\n",
      "Seed: 44, Epoch: 019, Loss: 0.4159, Val Acc: 0.7653, Test Acc: 0.7760\n",
      "Seed: 44, Epoch: 020, Loss: 0.4039, Val Acc: 0.7587, Test Acc: 0.7760\n",
      "Seed: 44, Epoch: 021, Loss: 0.3993, Val Acc: 0.7693, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 022, Loss: 0.3951, Val Acc: 0.7667, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 023, Loss: 0.3889, Val Acc: 0.7773, Test Acc: 0.7933\n",
      "Seed: 44, Epoch: 024, Loss: 0.3859, Val Acc: 0.7813, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 025, Loss: 0.3847, Val Acc: 0.7787, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 026, Loss: 0.3832, Val Acc: 0.7760, Test Acc: 0.7773\n",
      "Seed: 44, Epoch: 027, Loss: 0.3825, Val Acc: 0.7840, Test Acc: 0.7773\n",
      "Seed: 44, Epoch: 028, Loss: 0.3736, Val Acc: 0.7853, Test Acc: 0.7880\n",
      "Seed: 44, Epoch: 029, Loss: 0.3631, Val Acc: 0.7827, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 030, Loss: 0.3575, Val Acc: 0.7893, Test Acc: 0.7880\n",
      "Seed: 44, Epoch: 031, Loss: 0.3519, Val Acc: 0.7907, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 032, Loss: 0.3585, Val Acc: 0.7973, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 033, Loss: 0.3478, Val Acc: 0.7920, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 034, Loss: 0.3393, Val Acc: 0.8013, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 035, Loss: 0.3287, Val Acc: 0.7987, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 036, Loss: 0.3267, Val Acc: 0.8000, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 037, Loss: 0.3246, Val Acc: 0.7880, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 038, Loss: 0.3383, Val Acc: 0.7933, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 039, Loss: 0.3215, Val Acc: 0.8147, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 040, Loss: 0.3146, Val Acc: 0.8013, Test Acc: 0.8187\n",
      "Seed: 44, Epoch: 041, Loss: 0.3086, Val Acc: 0.8173, Test Acc: 0.8187\n",
      "Seed: 44, Epoch: 042, Loss: 0.3061, Val Acc: 0.7960, Test Acc: 0.8213\n",
      "Seed: 44, Epoch: 043, Loss: 0.3047, Val Acc: 0.7973, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 044, Loss: 0.3056, Val Acc: 0.7947, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 045, Loss: 0.3122, Val Acc: 0.7893, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 046, Loss: 0.3025, Val Acc: 0.7840, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 047, Loss: 0.3013, Val Acc: 0.7907, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 048, Loss: 0.2943, Val Acc: 0.7920, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 049, Loss: 0.2874, Val Acc: 0.7893, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 050, Loss: 0.2813, Val Acc: 0.7947, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 051, Loss: 0.2777, Val Acc: 0.7947, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 052, Loss: 0.2739, Val Acc: 0.8040, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 053, Loss: 0.2688, Val Acc: 0.7987, Test Acc: 0.8240\n",
      "Seed: 44, Epoch: 054, Loss: 0.3060, Val Acc: 0.7747, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 055, Loss: 0.3109, Val Acc: 0.7920, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 056, Loss: 0.3094, Val Acc: 0.8013, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 057, Loss: 0.2981, Val Acc: 0.8013, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 058, Loss: 0.2766, Val Acc: 0.8053, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 059, Loss: 0.2722, Val Acc: 0.8093, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 060, Loss: 0.2619, Val Acc: 0.7987, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 061, Loss: 0.2525, Val Acc: 0.8013, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 062, Loss: 0.2505, Val Acc: 0.7987, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 063, Loss: 0.2515, Val Acc: 0.8000, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 064, Loss: 0.2503, Val Acc: 0.8000, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 065, Loss: 0.2516, Val Acc: 0.7893, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 066, Loss: 0.2595, Val Acc: 0.7880, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 067, Loss: 0.2557, Val Acc: 0.8000, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 068, Loss: 0.2502, Val Acc: 0.8067, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 069, Loss: 0.2368, Val Acc: 0.7987, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 070, Loss: 0.2403, Val Acc: 0.7987, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 071, Loss: 0.2352, Val Acc: 0.8067, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 072, Loss: 0.2301, Val Acc: 0.7987, Test Acc: 0.8200\n",
      "Seed: 44, Epoch: 073, Loss: 0.2250, Val Acc: 0.8067, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 074, Loss: 0.2212, Val Acc: 0.8027, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 075, Loss: 0.2288, Val Acc: 0.7987, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 076, Loss: 0.2301, Val Acc: 0.8040, Test Acc: 0.8213\n",
      "Seed: 44, Epoch: 077, Loss: 0.2334, Val Acc: 0.7933, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 078, Loss: 0.2272, Val Acc: 0.8080, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 079, Loss: 0.2329, Val Acc: 0.7973, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 080, Loss: 0.2259, Val Acc: 0.7920, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 081, Loss: 0.2244, Val Acc: 0.7920, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 082, Loss: 0.2161, Val Acc: 0.8040, Test Acc: 0.8200\n",
      "Seed: 44, Epoch: 083, Loss: 0.2196, Val Acc: 0.8120, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 084, Loss: 0.2159, Val Acc: 0.7933, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 085, Loss: 0.2252, Val Acc: 0.8027, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 086, Loss: 0.2190, Val Acc: 0.8040, Test Acc: 0.8227\n",
      "Seed: 44, Epoch: 087, Loss: 0.2245, Val Acc: 0.8053, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 088, Loss: 0.2120, Val Acc: 0.8000, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 089, Loss: 0.2156, Val Acc: 0.8000, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 090, Loss: 0.2183, Val Acc: 0.7960, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 091, Loss: 0.2059, Val Acc: 0.8000, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 092, Loss: 0.2047, Val Acc: 0.7960, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 093, Loss: 0.2018, Val Acc: 0.8093, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 094, Loss: 0.2007, Val Acc: 0.8040, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 095, Loss: 0.1964, Val Acc: 0.8000, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 096, Loss: 0.1977, Val Acc: 0.7973, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 097, Loss: 0.1989, Val Acc: 0.7987, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 098, Loss: 0.2214, Val Acc: 0.8000, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 099, Loss: 0.2182, Val Acc: 0.8000, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 100, Loss: 0.2075, Val Acc: 0.8040, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 101, Loss: 0.2208, Val Acc: 0.8013, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 102, Loss: 0.2000, Val Acc: 0.8053, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 103, Loss: 0.1898, Val Acc: 0.8067, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 104, Loss: 0.1872, Val Acc: 0.8027, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 105, Loss: 0.1854, Val Acc: 0.8080, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 106, Loss: 0.1822, Val Acc: 0.8027, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 107, Loss: 0.1779, Val Acc: 0.8080, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 108, Loss: 0.1754, Val Acc: 0.8080, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 109, Loss: 0.1721, Val Acc: 0.8067, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 110, Loss: 0.1785, Val Acc: 0.8040, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 111, Loss: 0.1846, Val Acc: 0.8080, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 112, Loss: 0.1821, Val Acc: 0.8000, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 113, Loss: 0.1895, Val Acc: 0.8027, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 114, Loss: 0.1885, Val Acc: 0.8000, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 115, Loss: 0.1836, Val Acc: 0.7987, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 116, Loss: 0.1695, Val Acc: 0.8067, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 117, Loss: 0.1679, Val Acc: 0.8040, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 118, Loss: 0.1646, Val Acc: 0.8027, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 119, Loss: 0.1650, Val Acc: 0.8107, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 120, Loss: 0.1644, Val Acc: 0.7947, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 121, Loss: 0.1726, Val Acc: 0.7973, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 122, Loss: 0.1855, Val Acc: 0.7920, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 123, Loss: 0.1789, Val Acc: 0.8040, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 124, Loss: 0.1756, Val Acc: 0.8053, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 125, Loss: 0.1713, Val Acc: 0.7933, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 126, Loss: 0.1665, Val Acc: 0.7960, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 127, Loss: 0.1599, Val Acc: 0.7920, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 128, Loss: 0.1564, Val Acc: 0.8040, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 129, Loss: 0.1564, Val Acc: 0.8027, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 130, Loss: 0.1552, Val Acc: 0.8000, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 131, Loss: 0.1514, Val Acc: 0.7933, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 132, Loss: 0.1504, Val Acc: 0.8027, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 133, Loss: 0.1502, Val Acc: 0.8067, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 134, Loss: 0.1465, Val Acc: 0.8000, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 135, Loss: 0.1434, Val Acc: 0.7987, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 136, Loss: 0.1465, Val Acc: 0.8000, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 137, Loss: 0.1510, Val Acc: 0.8000, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 138, Loss: 0.1538, Val Acc: 0.7920, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 139, Loss: 0.1444, Val Acc: 0.7947, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 140, Loss: 0.1468, Val Acc: 0.8080, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 141, Loss: 0.1511, Val Acc: 0.7987, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 142, Loss: 0.1439, Val Acc: 0.7973, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 143, Loss: 0.1375, Val Acc: 0.7907, Test Acc: 0.7880\n",
      "Seed: 44, Epoch: 144, Loss: 0.1333, Val Acc: 0.8027, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 145, Loss: 0.1418, Val Acc: 0.8040, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 146, Loss: 0.1429, Val Acc: 0.7987, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 147, Loss: 0.1424, Val Acc: 0.7960, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 148, Loss: 0.1439, Val Acc: 0.7987, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 149, Loss: 0.1385, Val Acc: 0.8027, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 150, Loss: 0.1498, Val Acc: 0.7893, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 151, Loss: 0.2102, Val Acc: 0.7933, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 152, Loss: 0.2300, Val Acc: 0.8013, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 153, Loss: 0.1811, Val Acc: 0.8027, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 154, Loss: 0.1700, Val Acc: 0.7840, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 155, Loss: 0.1594, Val Acc: 0.7973, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 156, Loss: 0.1533, Val Acc: 0.7907, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 157, Loss: 0.1396, Val Acc: 0.8013, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 158, Loss: 0.1402, Val Acc: 0.7880, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 159, Loss: 0.1295, Val Acc: 0.8053, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 160, Loss: 0.1258, Val Acc: 0.7987, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 161, Loss: 0.1272, Val Acc: 0.7987, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 162, Loss: 0.1238, Val Acc: 0.7987, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 163, Loss: 0.1212, Val Acc: 0.7947, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 164, Loss: 0.1183, Val Acc: 0.8053, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 165, Loss: 0.1157, Val Acc: 0.8013, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 166, Loss: 0.1162, Val Acc: 0.8027, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 167, Loss: 0.1195, Val Acc: 0.8053, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 168, Loss: 0.1125, Val Acc: 0.7933, Test Acc: 0.7933\n",
      "Seed: 44, Epoch: 169, Loss: 0.1142, Val Acc: 0.8027, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 170, Loss: 0.1171, Val Acc: 0.7973, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 171, Loss: 0.1178, Val Acc: 0.7960, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 172, Loss: 0.1090, Val Acc: 0.7853, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 173, Loss: 0.1136, Val Acc: 0.8013, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 174, Loss: 0.1156, Val Acc: 0.7920, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 175, Loss: 0.1144, Val Acc: 0.8013, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 176, Loss: 0.1082, Val Acc: 0.7933, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 177, Loss: 0.1112, Val Acc: 0.8000, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 178, Loss: 0.1160, Val Acc: 0.8067, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 179, Loss: 0.1179, Val Acc: 0.7987, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 180, Loss: 0.1146, Val Acc: 0.8053, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 181, Loss: 0.1186, Val Acc: 0.7960, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 182, Loss: 0.1113, Val Acc: 0.7880, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 183, Loss: 0.1035, Val Acc: 0.8093, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 184, Loss: 0.1057, Val Acc: 0.8040, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 185, Loss: 0.1129, Val Acc: 0.7867, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 186, Loss: 0.1199, Val Acc: 0.8013, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 187, Loss: 0.1208, Val Acc: 0.7867, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 188, Loss: 0.1395, Val Acc: 0.7947, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 189, Loss: 0.1289, Val Acc: 0.7960, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 190, Loss: 0.1166, Val Acc: 0.7987, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 191, Loss: 0.1089, Val Acc: 0.7933, Test Acc: 0.7747\n",
      "Early stopping at epoch 191 for seed 44\n",
      "Average Time: 453.12 seconds\n",
      "Var Time: 37.91 seconds\n",
      "Average Memory: 11784.00 MB\n",
      "Average Best Val Acc: 0.8240\n",
      "Std Best Test Acc: 0.0171\n",
      "Average Test Acc: 0.8040\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"COLLAB\", transform=T.Compose([T.OneHotDegree(491)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_CGI(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_CGI, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = CGIPool(hidden_channels, ratio=0.7)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = CGIPool(hidden_channels, ratio=0.7)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, perm = self.pool1(x, edge_index, None, batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, perm = self.pool2(x, edge_index, None, batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_CGI(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1549.9229 MAE=1547.7614 MAE=1544.4138 MAE=1542.0225 MAE=1538.9407 MAE=1535.5701 MAE=1532.5792 MAE=1528.6232 MAE=1523.9182 Epoch: 10/500MAE=1519.6655 MAE=1513.7695 MAE=1509.2549 MAE=1503.9119 MAE=1497.4292 MAE=1491.0306 MAE=1483.8711 MAE=1477.2434 MAE=1470.3829 MAE=1460.3391 Epoch: 20/500MAE=1456.4886 MAE=1447.4862 MAE=1433.5365 MAE=1420.7318 MAE=1406.0607 MAE=1399.5156 MAE=1385.3484 MAE=1376.6484 MAE=1351.6215 MAE=1362.4369 Epoch: 30/500MAE=1347.3710 MAE=1328.2316 MAE=1316.1289 MAE=1302.1105 MAE=1291.4336 MAE=1271.5015 MAE=1246.5602 MAE=1254.4756 MAE=1222.1191 MAE=1211.5674 Epoch: 40/500MAE=1192.2863 MAE=1176.5430 MAE=1161.6340 MAE=1142.7424 MAE=1125.0427 MAE=1084.7528 MAE=1068.3932 MAE=1061.0107 MAE=1030.2260 MAE=1019.5750 Epoch: 50/500MAE=1008.9548 MAE=989.5651 MAE=952.6211 MAE=939.3000 MAE=917.9980 MAE=907.0298 MAE=873.6317 MAE=854.9902 MAE=839.5871 MAE=803.9751 Epoch: 60/500MAE=773.7224 MAE=760.2988 MAE=751.7233 MAE=713.9708 MAE=680.4556 MAE=669.6444 MAE=699.0991 MAE=628.4437 MAE=609.1218 MAE=559.0471 Epoch: 70/500MAE=573.3292 MAE=530.0209 MAE=527.4286 MAE=470.3378 MAE=499.0223 MAE=454.4097 MAE=483.9462 MAE=386.0994 MAE=417.7247 MAE=361.3004 Epoch: 80/500MAE=309.4101 MAE=293.1172 MAE=274.6380 MAE=285.3944 MAE=230.1819 MAE=242.9257 MAE=223.2443 MAE=190.3064 MAE=175.1654 MAE=190.2560 Epoch: 90/500MAE=158.1301 MAE=131.9364 MAE=137.2740 MAE=140.8317 MAE=131.9120 MAE=142.3837 MAE=145.7154 MAE=143.9236 MAE=135.3290 MAE=137.9198 Epoch: 100/500MAE=134.8852 MAE=131.3930 MAE=121.9756 MAE=123.7965 MAE=132.2818 MAE=123.8382 MAE=119.8481 MAE=119.3086 MAE=122.6380 MAE=119.8031 Epoch: 110/500MAE=125.4323 MAE=117.7793 MAE=127.7129 MAE=119.9417 MAE=117.5364 MAE=116.5718 MAE=116.9093 MAE=117.7374 MAE=117.6885 MAE=115.2168 Epoch: 120/500MAE=117.9992 MAE=116.9834 MAE=117.3297 MAE=117.6268 MAE=115.1023 MAE=115.5689 MAE=114.8608 MAE=114.9292 MAE=113.6921 MAE=115.5038 Epoch: 130/500MAE=116.5922 MAE=114.7667 MAE=115.2963 MAE=114.6069 MAE=114.6455 MAE=115.0517 MAE=114.9450 MAE=114.7435 MAE=114.3233 MAE=114.5588 Epoch: 140/500MAE=114.2557 MAE=114.3766 MAE=114.3270 MAE=114.0904 MAE=114.2188 MAE=114.3024 MAE=114.2880 MAE=114.2160 MAE=114.2865 MAE=114.2128 Epoch: 150/500MAE=114.2012 MAE=114.2302 MAE=114.2675 MAE=114.2529 MAE=114.2481 MAE=114.2334 MAE=114.2426 MAE=114.2280 MAE=114.2319 MAE=114.2312 Epoch: 160/500MAE=114.2255 MAE=114.2277 MAE=114.2321 MAE=114.2371 MAE=114.2413 MAE=114.2395 MAE=114.2399 MAE=114.2410 MAE=114.2402 MAE=114.2364 Epoch: 170/500MAE=114.2399 MAE=114.2415 MAE=114.2425 MAE=114.2446 MAE=114.2403 MAE=114.2427 MAE=114.2402 MAE=114.2330 MAE=114.2330 MAE=114.2387 Epoch: 180/500MAE=114.2471 MAE=114.2463 MAE=114.2485 MAE=114.2495 MAE=114.2491 MAE=114.2472 MAE=114.2461 MAE=114.2482 MAE=114.2435 MAE=114.2467 Epoch: 190/500MAE=114.2436 MAE=114.2463 MAE=114.2486 MAE=114.2492 MAE=114.2487 MAE=114.2505 MAE=114.2563 MAE=114.2625 MAE=114.2634 MAE=114.2683 Epoch: 200/500MAE=114.2761 MAE=114.2765 MAE=114.2690 MAE=114.2702 MAE=114.2686 MAE=114.2613 MAE=114.2654 MAE=114.2675 MAE=114.2602 MAE=114.2578 Epoch: 210/500MAE=114.2576 MAE=114.2572 MAE=114.2552 MAE=114.2524 MAE=114.2511 MAE=114.2493 MAE=114.2462 MAE=114.2454 MAE=114.2474 MAE=114.2480 Epoch: 220/500MAE=114.2463 MAE=114.2495 MAE=114.2499 MAE=114.2562 MAE=114.2584 MAE=114.2637 MAE=114.2632 MAE=114.2609 MAE=114.2593 MAE=114.2600 Epoch: 230/500MAE=114.2610 MAE=114.2612 MAE=114.2606 MAE=114.2613 MAE=114.2570 MAE=114.2519 MAE=114.2511 MAE=114.2550 MAE=114.2546 MAE=114.2541 Epoch: 240/500MAE=114.2575 MAE=114.2553 MAE=114.2573 MAE=114.2587 MAE=114.2603 MAE=114.2620 MAE=114.2607 MAE=114.2612 MAE=114.2600 MAE=114.2635 Epoch: 250/500MAE=114.2657 MAE=114.2664 MAE=114.2691 MAE=114.2668 MAE=114.2579 MAE=114.2595 MAE=114.2548 MAE=114.2553 MAE=114.2512 MAE=114.2533 Epoch: 260/500MAE=114.2574 MAE=114.2609 MAE=114.2959 MAE=114.2985 MAE=114.2988 MAE=114.1978 MAE=114.3044 MAE=114.2030 MAE=114.2285 MAE=114.2274 Epoch: 270/500MAE=114.2296 MAE=114.3135 MAE=114.3163 MAE=114.3214 MAE=114.3227 MAE=114.3236 MAE=114.3226 MAE=114.3194 MAE=114.3471 MAE=126.3372 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 126.337 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1549.9805 MAE=1547.5437 MAE=1544.1484 MAE=1541.1781 MAE=1538.8634 MAE=1535.8560 MAE=1531.2703 MAE=1526.4099 MAE=1524.9104 Epoch: 10/500MAE=1520.9630 MAE=1515.0675 MAE=1511.8098 MAE=1504.3469 MAE=1499.7686 MAE=1491.5762 MAE=1478.9609 MAE=1478.0599 MAE=1467.4025 MAE=1461.4698 Epoch: 20/500MAE=1451.4119 MAE=1443.6945 MAE=1435.8665 MAE=1418.1035 MAE=1422.0391 MAE=1395.7197 MAE=1393.9006 MAE=1380.9536 MAE=1374.3641 MAE=1357.7512 Epoch: 30/500MAE=1345.6429 MAE=1339.9528 MAE=1322.9243 MAE=1305.6777 MAE=1292.6741 MAE=1266.0676 MAE=1278.7737 MAE=1242.2805 MAE=1227.1353 MAE=1214.5596 Epoch: 40/500MAE=1194.6987 MAE=1176.5022 MAE=1167.7030 MAE=1134.1410 MAE=1118.6185 MAE=1112.3829 MAE=1088.5980 MAE=1055.5162 MAE=1086.8562 MAE=1057.3403 Epoch: 50/500MAE=1029.5460 MAE=998.9395 MAE=978.6631 MAE=934.1534 MAE=934.2908 MAE=900.5115 MAE=886.6091 MAE=866.7791 MAE=864.4441 MAE=790.5729 Epoch: 60/500MAE=793.0186 MAE=779.4171 MAE=789.5574 MAE=711.6425 MAE=705.1511 MAE=750.3726 MAE=688.3083 MAE=607.7831 MAE=650.7999 MAE=554.9722 Epoch: 70/500MAE=537.6273 MAE=508.6546 MAE=474.5043 MAE=539.9482 MAE=412.7914 MAE=406.4468 MAE=424.0275 MAE=373.4034 MAE=382.0920 MAE=283.0045 Epoch: 80/500MAE=303.0535 MAE=306.5361 MAE=224.0417 MAE=246.0560 MAE=238.1078 MAE=204.5819 MAE=176.6440 MAE=185.2677 MAE=174.1540 MAE=163.6696 Epoch: 90/500MAE=171.0229 MAE=156.4668 MAE=134.1183 MAE=140.1107 MAE=133.7933 MAE=130.8517 MAE=174.8201 MAE=129.5858 MAE=132.6506 MAE=140.3206 Epoch: 100/500MAE=131.2950 MAE=130.6291 MAE=128.5544 MAE=131.2377 MAE=127.2177 MAE=131.1779 MAE=127.0009 MAE=133.7127 MAE=127.6524 MAE=126.0024 Epoch: 110/500MAE=128.6978 MAE=129.3854 MAE=128.1242 MAE=136.6314 MAE=130.9568 MAE=132.8167 MAE=134.1093 MAE=131.7717 MAE=134.2396 MAE=134.6024 Epoch: 120/500MAE=133.9736 MAE=134.0046 MAE=133.6123 MAE=133.6220 MAE=133.6953 MAE=133.9912 MAE=133.3062 MAE=133.2050 MAE=133.4775 MAE=133.1258 Epoch: 130/500MAE=133.1493 MAE=133.1592 MAE=133.2593 MAE=133.0366 MAE=133.0987 MAE=133.1406 MAE=132.9701 MAE=133.1510 MAE=132.9541 MAE=132.9768 Epoch: 140/500MAE=132.9525 MAE=132.9480 MAE=132.9728 MAE=132.9974 MAE=132.9608 MAE=133.0103 MAE=133.0107 MAE=133.0081 MAE=133.0036 MAE=133.0059 Epoch: 150/500MAE=133.0082 MAE=133.0050 MAE=133.0014 MAE=133.0009 MAE=133.0003 MAE=132.9986 MAE=132.9953 MAE=132.9914 MAE=132.9982 MAE=132.9974 Epoch: 160/500MAE=132.9960 MAE=132.9980 MAE=132.9984 MAE=133.0009 MAE=133.0018 MAE=132.9942 MAE=132.9922 MAE=132.9916 MAE=132.9960 MAE=132.9939 Epoch: 170/500MAE=132.9944 MAE=132.9936 MAE=132.9908 MAE=132.9839 MAE=132.9900 MAE=132.9881 MAE=132.9835 MAE=132.9817 MAE=132.9780 MAE=132.9828 Epoch: 180/500MAE=132.9801 MAE=132.9772 MAE=132.9772 MAE=132.9806 MAE=132.9784 MAE=132.9750 MAE=132.9720 MAE=132.9750 MAE=132.9760 MAE=132.9756 Epoch: 190/500MAE=132.9737 MAE=132.9723 MAE=132.9642 MAE=132.9721 MAE=132.9654 MAE=132.9641 MAE=132.9623 MAE=132.9584 MAE=132.9605 MAE=132.9588 Epoch: 200/500MAE=132.9586 MAE=132.9556 MAE=132.9528 MAE=132.9568 MAE=132.9615 MAE=132.9599 MAE=132.9499 MAE=132.9429 MAE=132.9422 MAE=132.9420 Epoch: 210/500MAE=132.9465 MAE=132.9477 MAE=132.9323 MAE=132.9368 MAE=132.9446 MAE=132.9341 MAE=132.9322 MAE=132.9276 MAE=132.9292 MAE=132.9247 Epoch: 220/500MAE=132.9180 MAE=132.9115 MAE=132.9055 MAE=132.9026 MAE=132.9067 MAE=132.9079 MAE=132.9120 MAE=132.8983 MAE=132.9001 MAE=132.8997 Epoch: 230/500MAE=132.8926 MAE=132.9009 MAE=132.9038 MAE=132.9030 MAE=132.9009 MAE=132.8912 MAE=132.8717 MAE=132.8681 MAE=132.8980 MAE=132.9054 Epoch: 240/500MAE=132.9028 MAE=132.9059 MAE=132.9043 MAE=132.9014 MAE=132.8974 MAE=132.8922 MAE=132.8919 MAE=132.8875 MAE=132.8938 MAE=132.8907 Epoch: 250/500MAE=132.8884 MAE=132.8801 MAE=132.8586 MAE=132.8515 MAE=132.8475 MAE=132.8448 MAE=132.8398 MAE=132.8453 MAE=133.0950 MAE=133.0866 MAE=196.2073 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 161.272 +/- 34.935\n",
      "\n",
      "Epoch: 1/500MAE=1549.6006 MAE=1547.5569 MAE=1544.2183 MAE=1541.4484 MAE=1538.6198 MAE=1535.5757 MAE=1531.8939 MAE=1528.3446 MAE=1524.4771 Epoch: 10/500MAE=1520.1797 MAE=1515.0562 MAE=1510.8409 MAE=1506.0496 MAE=1499.9364 MAE=1493.4034 MAE=1487.0085 MAE=1480.1053 MAE=1473.4751 MAE=1466.4152 Epoch: 20/500MAE=1458.0800 MAE=1450.8414 MAE=1439.6820 MAE=1431.4524 MAE=1420.8890 MAE=1412.4012 MAE=1403.1545 MAE=1395.8740 MAE=1375.5970 MAE=1368.9418 Epoch: 30/500MAE=1354.0233 MAE=1344.6095 MAE=1329.4645 MAE=1319.8997 MAE=1307.8171 MAE=1289.9094 MAE=1276.0043 MAE=1266.5800 MAE=1247.3531 MAE=1245.8427 Epoch: 40/500MAE=1205.7479 MAE=1191.3666 MAE=1177.1042 MAE=1158.3639 MAE=1155.6309 MAE=1128.4435 MAE=1110.7301 MAE=1090.1696 MAE=1065.7843 MAE=1047.1384 Epoch: 50/500MAE=1036.0809 MAE=1010.3508 MAE=993.9777 MAE=972.9979 MAE=945.7004 MAE=929.8376 MAE=903.5187 MAE=879.6484 MAE=856.8478 MAE=839.8349 Epoch: 60/500MAE=823.9838 MAE=805.0643 MAE=775.8862 MAE=747.7121 MAE=708.6287 MAE=705.3389 MAE=673.5944 MAE=680.3884 MAE=623.9743 MAE=625.0295 Epoch: 70/500MAE=570.3549 MAE=560.8418 MAE=538.8056 MAE=532.1586 MAE=505.1526 MAE=467.6786 MAE=454.9719 MAE=435.2417 MAE=402.9849 MAE=385.9908 Epoch: 80/500MAE=365.2268 MAE=363.2414 MAE=346.4461 MAE=309.1318 MAE=302.0368 MAE=291.1866 MAE=283.5194 MAE=255.9142 MAE=250.2310 MAE=221.6655 Epoch: 90/500MAE=212.0163 MAE=211.2617 MAE=202.1909 MAE=197.5523 MAE=199.7499 MAE=194.1379 MAE=194.0832 MAE=197.9244 MAE=197.8649 MAE=201.2718 Epoch: 100/500MAE=199.8404 MAE=198.6787 MAE=198.4666 MAE=198.2022 MAE=198.5789 MAE=198.2457 MAE=198.9890 MAE=203.9371 MAE=196.8275 MAE=196.1643 Epoch: 110/500MAE=199.8570 MAE=201.1732 MAE=197.8458 MAE=197.9952 MAE=195.6051 MAE=203.1160 MAE=195.0471 MAE=197.4988 MAE=198.7778 MAE=199.9372 Epoch: 120/500MAE=200.4297 MAE=197.4455 MAE=199.4400 MAE=197.1341 MAE=198.1036 MAE=199.3123 MAE=198.2754 MAE=198.8448 MAE=199.7436 MAE=200.1642 Epoch: 130/500MAE=199.3212 MAE=198.9386 MAE=199.0663 MAE=199.1091 MAE=199.0871 MAE=198.9412 MAE=198.9924 MAE=199.1010 MAE=199.1841 MAE=199.0386 Epoch: 140/500MAE=199.1143 MAE=199.1189 MAE=199.1907 MAE=199.2775 MAE=199.3267 MAE=199.3251 MAE=199.3335 MAE=199.3263 MAE=199.3151 MAE=199.3067 Epoch: 150/500MAE=199.2997 MAE=199.3018 MAE=199.2976 MAE=199.3021 MAE=199.3064 MAE=199.3044 MAE=199.3100 MAE=199.3178 MAE=199.3348 MAE=199.3350 Epoch: 160/500MAE=199.3433 MAE=199.3477 MAE=199.3628 MAE=199.3734 MAE=199.3653 MAE=199.3665 MAE=199.3823 MAE=199.3838 MAE=199.3825 MAE=199.3768 Epoch: 170/500MAE=199.3752 MAE=199.3902 MAE=199.3986 MAE=199.4008 MAE=199.4048 MAE=199.4035 MAE=199.3960 MAE=199.4037 MAE=199.4004 MAE=199.4073 Epoch: 180/500MAE=199.4086 MAE=199.4124 MAE=199.4198 MAE=199.4168 MAE=199.4209 MAE=199.4326 MAE=199.4299 MAE=199.4333 MAE=199.4336 MAE=199.4337 Epoch: 190/500MAE=199.4374 MAE=199.4421 MAE=199.4482 MAE=199.4337 MAE=199.4319 MAE=199.4293 MAE=199.4270 MAE=199.4333 MAE=199.4388 MAE=199.4460 Epoch: 200/500MAE=199.4476 MAE=199.4509 MAE=199.4454 MAE=199.4437 MAE=199.4473 MAE=199.4470 MAE=199.4375 MAE=199.4470 MAE=199.4526 MAE=199.4500 Epoch: 210/500MAE=199.4413 MAE=199.4558 MAE=199.4636 MAE=199.4751 MAE=199.4776 MAE=199.4758 MAE=199.4913 MAE=199.4947 MAE=199.4940 MAE=199.4981 Epoch: 220/500MAE=199.4863 MAE=199.4961 MAE=199.5051 MAE=199.5009 MAE=199.5048 MAE=199.5173 MAE=199.4379 MAE=199.4362 MAE=199.5142 MAE=199.4275 Epoch: 230/500MAE=199.5048 MAE=199.5034 MAE=199.5033 MAE=199.4320 MAE=199.4470 MAE=199.4525 MAE=199.4554 MAE=199.4512 MAE=199.4523 MAE=199.4754 Epoch: 240/500MAE=199.4731 MAE=199.4691 MAE=199.4656 MAE=199.4489 MAE=199.4428 MAE=199.4542 MAE=199.4370 MAE=199.4329 MAE=199.4402 MAE=199.4459 Epoch: 250/500MAE=199.4526 MAE=199.4679 MAE=199.4704 MAE=199.4628 MAE=199.4541 MAE=199.4523 MAE=199.4594 MAE=199.4643 MAE=199.4583 MAE=199.4821 Epoch: 260/500MAE=199.4828 MAE=199.5612 MAE=193.0658 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 171.870 +/- 32.222\n",
      "\n",
      "Epoch: 1/500MAE=1549.5464 MAE=1546.8738 MAE=1543.6001 MAE=1541.1831 MAE=1538.2927 MAE=1536.5002 MAE=1532.6366 MAE=1529.4399 MAE=1523.4348 Epoch: 10/500MAE=1520.8641 MAE=1515.6288 MAE=1509.9600 MAE=1504.9861 MAE=1497.4854 MAE=1492.2356 MAE=1483.7184 MAE=1476.1687 MAE=1471.4226 MAE=1460.3645 Epoch: 20/500MAE=1456.3475 MAE=1446.5802 MAE=1433.0524 MAE=1429.2653 MAE=1411.2706 MAE=1409.5391 MAE=1388.9634 MAE=1372.2468 MAE=1372.3201 MAE=1347.6652 Epoch: 30/500MAE=1353.0924 MAE=1329.8779 MAE=1319.4344 MAE=1308.2390 MAE=1291.2993 MAE=1274.2184 MAE=1260.6907 MAE=1241.9578 MAE=1238.4268 MAE=1210.8806 Epoch: 40/500MAE=1194.9851 MAE=1173.7312 MAE=1175.3108 MAE=1151.8632 MAE=1125.9690 MAE=1116.1945 MAE=1105.3412 MAE=1075.0552 MAE=1041.3591 MAE=1038.0557 Epoch: 50/500MAE=1015.4125 MAE=989.4657 MAE=978.9663 MAE=940.4540 MAE=915.5248 MAE=913.8754 MAE=897.9128 MAE=830.8436 MAE=844.5900 MAE=827.0574 Epoch: 60/500MAE=774.7101 MAE=787.1722 MAE=739.6522 MAE=720.8080 MAE=676.5892 MAE=650.7612 MAE=652.0810 MAE=657.7390 MAE=582.7289 MAE=572.6584 Epoch: 70/500MAE=554.5576 MAE=497.9098 MAE=464.5393 MAE=426.0046 MAE=417.6404 MAE=390.9304 MAE=388.1606 MAE=428.7946 MAE=399.9960 MAE=311.7166 Epoch: 80/500MAE=318.4052 MAE=271.1995 MAE=243.3346 MAE=230.2632 MAE=211.2614 MAE=268.0381 MAE=252.1740 MAE=205.9625 MAE=251.9079 MAE=172.4337 Epoch: 90/500MAE=184.3949 MAE=143.6906 MAE=210.5467 MAE=208.0796 MAE=144.8278 MAE=199.7829 MAE=205.3406 MAE=133.0489 MAE=128.7261 MAE=192.8658 Epoch: 100/500MAE=196.4396 MAE=197.0098 MAE=195.4982 MAE=198.3318 MAE=125.6173 MAE=122.2809 MAE=197.4468 MAE=195.7562 MAE=197.7050 MAE=201.3882 Epoch: 110/500MAE=123.1290 MAE=199.3665 MAE=121.1806 MAE=118.2630 MAE=122.2758 MAE=119.0517 MAE=194.1686 MAE=119.1862 MAE=116.7208 MAE=197.6295 Epoch: 120/500MAE=197.2452 MAE=115.8854 MAE=118.7687 MAE=193.8239 MAE=116.3323 MAE=117.7996 MAE=122.1530 MAE=116.8365 MAE=114.7216 MAE=115.6024 Epoch: 130/500MAE=115.1922 MAE=116.8198 MAE=115.5353 MAE=114.7619 MAE=114.3946 MAE=113.7092 MAE=113.3071 MAE=113.8879 MAE=112.8698 MAE=113.8592 Epoch: 140/500MAE=112.5689 MAE=111.5854 MAE=113.1866 MAE=114.8926 MAE=112.2710 MAE=112.9745 MAE=113.3078 MAE=113.4171 MAE=112.8109 MAE=113.5502 Epoch: 150/500MAE=112.5935 MAE=112.9876 MAE=113.1991 MAE=113.1287 MAE=112.9673 MAE=113.2609 MAE=113.0062 MAE=113.3961 MAE=113.2833 MAE=113.3529 Epoch: 160/500MAE=113.4990 MAE=113.3589 MAE=113.2973 MAE=113.3349 MAE=113.4759 MAE=113.4570 MAE=113.4629 MAE=113.3791 MAE=113.4539 MAE=113.4485 Epoch: 170/500MAE=113.4402 MAE=113.4327 MAE=113.4404 MAE=113.4368 MAE=113.4393 MAE=113.3784 MAE=113.3843 MAE=113.3858 MAE=113.3863 MAE=113.3908 Epoch: 180/500MAE=113.3955 MAE=113.4591 MAE=113.4665 MAE=113.4668 MAE=113.4656 MAE=113.4622 MAE=113.3990 MAE=113.4017 MAE=113.4006 MAE=113.4699 Epoch: 190/500MAE=113.4690 MAE=113.4659 MAE=113.5025 MAE=113.4656 MAE=113.4620 MAE=113.4560 MAE=113.4500 MAE=113.4495 MAE=113.4504 MAE=113.4530 Epoch: 200/500MAE=113.4570 MAE=113.1325 MAE=113.1283 MAE=113.1284 MAE=113.1291 MAE=113.1336 MAE=113.1353 MAE=113.1359 MAE=113.1381 MAE=113.1406 Epoch: 210/500MAE=113.1424 MAE=113.1423 MAE=113.1480 MAE=113.1488 MAE=113.1415 MAE=113.1465 MAE=113.1448 MAE=113.1465 MAE=112.9904 MAE=112.9929 Epoch: 220/500MAE=112.9855 MAE=113.1356 MAE=112.9812 MAE=112.9838 MAE=112.9780 MAE=112.9743 MAE=112.9718 MAE=112.9761 MAE=112.9794 MAE=112.9787 Epoch: 230/500MAE=112.9739 MAE=112.9732 MAE=112.9719 MAE=112.9686 MAE=112.9731 MAE=112.9711 MAE=113.2719 MAE=113.2706 MAE=113.2745 MAE=113.2788 Epoch: 240/500MAE=113.2791 MAE=113.2763 MAE=113.2777 MAE=113.2786 MAE=113.2772 MAE=113.2816 MAE=113.2853 MAE=113.2879 MAE=113.2867 MAE=113.2834 Epoch: 250/500MAE=113.2857 MAE=113.2874 MAE=113.2882 MAE=113.2873 MAE=113.2846 MAE=113.2182 MAE=113.2889 MAE=113.2968 MAE=113.3008 MAE=113.3033 Epoch: 260/500MAE=113.3066 MAE=113.2999 MAE=113.2910 MAE=113.2892 MAE=113.2898 MAE=113.2915 MAE=113.2387 MAE=113.2332 MAE=113.2325 MAE=113.2373 Epoch: 270/500MAE=113.2355 MAE=113.2327 MAE=113.2317 MAE=113.2342 MAE=113.2315 MAE=113.2331 MAE=113.2352 MAE=113.1668 MAE=113.1689 MAE=112.9258 Epoch: 280/500MAE=112.9270 MAE=112.9313 MAE=112.9301 MAE=113.1694 MAE=113.1679 MAE=112.9239 MAE=112.9257 MAE=112.9256 MAE=112.9235 MAE=112.9250 Epoch: 290/500MAE=112.9278 MAE=112.9268 MAE=171.2516 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 171.715 +/- 27.906\n",
      "\n",
      "Epoch: 1/500MAE=1549.6974 MAE=1547.6833 MAE=1544.2108 MAE=1541.4861 MAE=1538.8916 MAE=1535.4553 MAE=1533.6226 MAE=1528.5378 MAE=1523.8022 Epoch: 10/500MAE=1519.8104 MAE=1514.3563 MAE=1510.0590 MAE=1504.3843 MAE=1497.8872 MAE=1491.7957 MAE=1483.8040 MAE=1478.6379 MAE=1470.5500 MAE=1463.0769 Epoch: 20/500MAE=1455.6025 MAE=1445.8076 MAE=1437.3787 MAE=1428.1079 MAE=1415.9473 MAE=1406.6975 MAE=1397.0901 MAE=1382.6948 MAE=1370.4569 MAE=1361.2451 Epoch: 30/500MAE=1347.0129 MAE=1337.8601 MAE=1319.7031 MAE=1309.6426 MAE=1297.8047 MAE=1281.5104 MAE=1270.7720 MAE=1260.5269 MAE=1233.7830 MAE=1229.3125 Epoch: 40/500MAE=1203.9724 MAE=1185.1172 MAE=1165.4429 MAE=1147.9185 MAE=1132.4465 MAE=1116.1586 MAE=1097.1750 MAE=1075.6921 MAE=1058.2405 MAE=1040.3672 Epoch: 50/500MAE=1015.5600 MAE=997.7407 MAE=977.8918 MAE=954.9750 MAE=935.9229 MAE=920.7014 MAE=893.7821 MAE=875.2783 MAE=861.1915 MAE=830.1203 Epoch: 60/500MAE=812.9108 MAE=793.8142 MAE=769.3675 MAE=718.7238 MAE=700.7029 MAE=671.3982 MAE=648.6311 MAE=630.5876 MAE=601.1443 MAE=570.8375 Epoch: 70/500MAE=549.7075 MAE=535.7916 MAE=507.2532 MAE=480.4918 MAE=458.7990 MAE=415.6653 MAE=414.3786 MAE=399.9369 MAE=374.5426 MAE=334.6950 Epoch: 80/500MAE=322.2657 MAE=282.6414 MAE=268.5400 MAE=249.4866 MAE=240.7931 MAE=220.2094 MAE=230.0889 MAE=209.1702 MAE=198.0736 MAE=179.7637 Epoch: 90/500MAE=179.0774 MAE=161.1241 MAE=159.7964 MAE=135.2678 MAE=168.1536 MAE=143.2108 MAE=120.6437 MAE=134.4769 MAE=122.5458 MAE=119.5574 Epoch: 100/500MAE=126.1516 MAE=121.5497 MAE=119.1319 MAE=121.3175 MAE=119.3379 MAE=118.3450 MAE=120.2627 MAE=117.8740 MAE=115.4340 MAE=118.5182 Epoch: 110/500MAE=117.7923 MAE=120.1575 MAE=115.5546 MAE=114.7429 MAE=113.1287 MAE=114.9765 MAE=114.2184 MAE=114.4458 MAE=115.8405 MAE=115.8964 Epoch: 120/500MAE=114.9875 MAE=115.3197 MAE=112.5994 MAE=115.4201 MAE=114.1909 MAE=113.2740 MAE=113.8320 MAE=112.8891 MAE=114.1108 MAE=113.6562 Epoch: 130/500MAE=114.2381 MAE=113.0898 MAE=112.8623 MAE=112.6320 MAE=112.8398 MAE=112.8724 MAE=112.7594 MAE=112.9561 MAE=112.8275 MAE=112.7020 Epoch: 140/500MAE=112.6514 MAE=112.6529 MAE=112.7509 MAE=112.7261 MAE=112.7068 MAE=112.7280 MAE=112.7528 MAE=112.7659 MAE=112.7516 MAE=112.7603 Epoch: 150/500MAE=112.7507 MAE=112.7428 MAE=112.7388 MAE=112.6375 MAE=112.6474 MAE=112.6407 MAE=112.6432 MAE=112.6430 MAE=112.6387 MAE=112.6351 Epoch: 160/500MAE=112.6367 MAE=112.6471 MAE=112.6510 MAE=112.6583 MAE=112.6593 MAE=112.6587 MAE=112.6686 MAE=112.6687 MAE=112.6723 MAE=112.6648 Epoch: 170/500MAE=112.6644 MAE=112.6605 MAE=112.6617 MAE=112.6653 MAE=112.6683 MAE=112.6679 MAE=112.6694 MAE=112.6705 MAE=112.6725 MAE=112.6705 Epoch: 180/500MAE=112.6715 MAE=112.6733 MAE=112.6833 MAE=112.6871 MAE=112.6881 MAE=112.6866 MAE=112.6887 MAE=112.6941 MAE=112.6910 MAE=112.6930 Epoch: 190/500MAE=112.6897 MAE=112.6895 MAE=112.6828 MAE=112.6808 MAE=112.6832 MAE=112.6858 MAE=112.6875 MAE=112.6879 MAE=112.6807 MAE=112.6837 Epoch: 200/500MAE=112.6871 MAE=112.6898 MAE=112.6871 MAE=112.6889 MAE=112.6855 MAE=112.6832 MAE=112.6828 MAE=112.6886 MAE=112.6899 MAE=112.6937 Epoch: 210/500MAE=112.6830 MAE=112.6890 MAE=112.6885 MAE=112.6946 MAE=112.6947 MAE=112.6956 MAE=112.6964 MAE=112.6958 MAE=112.6916 MAE=112.6952 Epoch: 220/500MAE=112.6997 MAE=112.6972 MAE=112.6981 MAE=112.6989 MAE=112.7011 MAE=112.7026 MAE=112.7064 MAE=112.7067 MAE=112.7093 MAE=112.7142 Epoch: 230/500MAE=112.7180 MAE=112.7166 MAE=112.7261 MAE=112.7238 MAE=112.7198 MAE=112.7205 MAE=112.7157 MAE=112.7124 MAE=112.7123 MAE=112.7142 Epoch: 240/500MAE=112.7172 MAE=112.7187 MAE=112.7189 MAE=112.7192 MAE=112.7232 MAE=112.7210 MAE=112.7243 MAE=112.7253 MAE=112.7229 MAE=112.7271 Epoch: 250/500MAE=112.7299 MAE=112.7257 MAE=112.7248 MAE=112.7208 MAE=112.7203 MAE=112.7193 MAE=112.7220 MAE=112.7236 MAE=112.7284 MAE=112.7318 Epoch: 260/500MAE=112.7325 MAE=112.7340 MAE=112.7308 MAE=112.7260 MAE=112.7203 MAE=112.7228 MAE=112.7248 MAE=112.7263 MAE=112.7232 MAE=112.7253 Epoch: 270/500MAE=112.7336 MAE=112.7437 MAE=112.7458 MAE=117.2173 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 160.816 +/- 33.139\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1549.7122 MAE=1547.5952 MAE=1544.6798 MAE=1541.2622 MAE=1538.5347 MAE=1535.8660 MAE=1532.1268 MAE=1528.5599 MAE=1524.2692 Epoch: 10/500MAE=1519.9838 MAE=1515.3816 MAE=1509.6450 MAE=1503.7522 MAE=1497.5190 MAE=1491.0970 MAE=1485.9731 MAE=1480.4175 MAE=1474.0642 MAE=1464.2766 Epoch: 20/500MAE=1454.6904 MAE=1448.2954 MAE=1440.6754 MAE=1429.2593 MAE=1423.3895 MAE=1410.3452 MAE=1397.7561 MAE=1387.5751 MAE=1376.7223 MAE=1365.8074 Epoch: 30/500MAE=1357.8359 MAE=1339.7410 MAE=1334.5265 MAE=1312.2601 MAE=1308.5950 MAE=1291.7340 MAE=1270.7838 MAE=1269.0706 MAE=1249.6709 MAE=1233.1096 Epoch: 40/500MAE=1212.1251 MAE=1182.8947 MAE=1189.9722 MAE=1180.3733 MAE=1158.4060 MAE=1144.9530 MAE=1119.1417 MAE=1084.4519 MAE=1069.8368 MAE=1051.6846 Epoch: 50/500MAE=1037.6469 MAE=1020.0535 MAE=980.4586 MAE=973.6395 MAE=954.6348 MAE=938.0251 MAE=906.3003 MAE=881.3721 MAE=852.1020 MAE=840.8441 Epoch: 60/500MAE=804.8141 MAE=795.2625 MAE=799.9013 MAE=745.8809 MAE=716.1737 MAE=693.9984 MAE=674.4569 MAE=656.6646 MAE=623.0587 MAE=599.8804 Epoch: 70/500MAE=567.5514 MAE=542.4354 MAE=519.8208 MAE=485.3930 MAE=480.0933 MAE=444.5351 MAE=417.8898 MAE=402.1456 MAE=370.3386 MAE=340.1774 Epoch: 80/500MAE=331.7646 MAE=313.0036 MAE=271.7299 MAE=258.9941 MAE=282.5726 MAE=213.3774 MAE=209.2259 MAE=221.2544 MAE=197.1931 MAE=185.8118 Epoch: 90/500MAE=191.6668 MAE=165.6383 MAE=186.4669 MAE=176.2088 MAE=147.9816 MAE=142.4315 MAE=132.4851 MAE=125.3246 MAE=116.0464 MAE=129.2737 Epoch: 100/500MAE=146.2911 MAE=143.9310 MAE=142.5629 MAE=138.8284 MAE=113.6522 MAE=112.9254 MAE=136.4594 MAE=136.3000 MAE=136.3353 MAE=136.4217 Epoch: 110/500MAE=112.2490 MAE=136.1275 MAE=137.0272 MAE=137.4076 MAE=134.9173 MAE=135.9299 MAE=135.2593 MAE=134.6934 MAE=136.7833 MAE=135.2842 Epoch: 120/500MAE=136.2489 MAE=134.7037 MAE=134.5139 MAE=134.1256 MAE=133.3588 MAE=133.9134 MAE=134.4746 MAE=134.7206 MAE=136.4377 MAE=133.3107 Epoch: 130/500MAE=133.3528 MAE=133.6543 MAE=133.5064 MAE=133.1081 MAE=132.7360 MAE=132.6122 MAE=133.1642 MAE=133.5166 MAE=132.3456 MAE=130.9562 Epoch: 140/500MAE=132.0091 MAE=131.7265 MAE=132.0192 MAE=132.4828 MAE=131.9060 MAE=131.3213 MAE=131.1388 MAE=131.4438 MAE=131.6833 MAE=131.3581 Epoch: 150/500MAE=130.9432 MAE=130.9115 MAE=130.9685 MAE=131.0302 MAE=130.8722 MAE=131.4251 MAE=131.1870 MAE=131.0806 MAE=130.8884 MAE=130.8995 Epoch: 160/500MAE=130.9436 MAE=131.1598 MAE=131.2308 MAE=131.1655 MAE=131.0469 MAE=130.9906 MAE=130.9645 MAE=130.9059 MAE=130.9225 MAE=130.9814 Epoch: 170/500MAE=130.8835 MAE=130.8701 MAE=130.8788 MAE=130.9516 MAE=130.9018 MAE=130.9180 MAE=130.8796 MAE=130.8956 MAE=130.8922 MAE=130.8869 Epoch: 180/500MAE=130.9156 MAE=130.9144 MAE=130.8640 MAE=130.8961 MAE=130.8854 MAE=130.9549 MAE=130.8838 MAE=130.8878 MAE=130.8656 MAE=130.8805 Epoch: 190/500MAE=130.8857 MAE=130.8790 MAE=130.8731 MAE=130.8702 MAE=130.8739 MAE=130.8633 MAE=130.9691 MAE=130.8471 MAE=130.8625 MAE=130.8716 Epoch: 200/500MAE=130.9460 MAE=130.9392 MAE=130.8656 MAE=130.8660 MAE=130.9374 MAE=130.8616 MAE=130.8517 MAE=130.9339 MAE=130.9115 MAE=130.8733 Epoch: 210/500MAE=130.8665 MAE=130.8609 MAE=130.8974 MAE=130.8553 MAE=130.9241 MAE=130.8817 MAE=130.8533 MAE=130.8573 MAE=130.8544 MAE=130.7778 Epoch: 220/500MAE=130.7813 MAE=130.7820 MAE=130.9085 MAE=130.9001 MAE=130.8591 MAE=130.9025 MAE=130.8659 MAE=130.8415 MAE=130.8483 MAE=130.8527 Epoch: 230/500MAE=130.8523 MAE=130.8433 MAE=130.8424 MAE=130.8943 MAE=130.8605 MAE=130.8624 MAE=130.9298 MAE=130.8346 MAE=130.8806 MAE=130.8454 Epoch: 240/500MAE=130.8441 MAE=130.9092 MAE=131.1414 MAE=130.8219 MAE=131.0555 MAE=131.0464 MAE=131.0341 MAE=131.0460 MAE=131.0379 MAE=131.0499 Epoch: 250/500MAE=131.0508 MAE=131.1113 MAE=131.0421 MAE=131.0440 MAE=131.0581 MAE=131.0982 MAE=131.0460 MAE=131.0422 MAE=131.0485 MAE=131.0299 Epoch: 260/500MAE=131.0434 MAE=131.0543 MAE=131.1780 MAE=131.2599 MAE=131.2273 MAE=131.2570 MAE=131.1789 MAE=131.2425 MAE=131.1756 MAE=131.1788 Epoch: 270/500MAE=131.1746 MAE=131.1687 MAE=131.1762 MAE=131.2345 MAE=131.1620 MAE=131.1793 MAE=131.2186 MAE=131.2078 MAE=131.1760 MAE=131.1772 Epoch: 280/500MAE=131.1578 MAE=131.2275 MAE=131.1673 MAE=131.1694 MAE=131.1759 MAE=131.1867 MAE=131.1465 MAE=131.2411 MAE=131.1789 MAE=131.1625 Epoch: 290/500MAE=131.1640 MAE=131.1732 MAE=131.1619 MAE=131.1657 MAE=131.1539 MAE=131.1508 MAE=131.1567 MAE=131.1863 MAE=131.1553 MAE=131.2263 Epoch: 300/500MAE=131.2079 MAE=131.1603 MAE=131.1461 MAE=131.2202 MAE=131.1313 MAE=130.8513 MAE=130.8531 MAE=130.8450 MAE=130.9006 MAE=195.9004 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 195.900 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1549.3364 MAE=1547.4761 MAE=1544.9458 MAE=1542.5555 MAE=1540.9248 MAE=1537.9041 MAE=1534.7314 MAE=1531.1713 MAE=1526.9451 Epoch: 10/500MAE=1522.3467 MAE=1517.5508 MAE=1512.3645 MAE=1508.0701 MAE=1500.0836 MAE=1496.4333 MAE=1489.8683 MAE=1481.4207 MAE=1474.5702 MAE=1469.1174 Epoch: 20/500MAE=1461.6477 MAE=1453.5332 MAE=1444.9152 MAE=1431.9115 MAE=1427.4016 MAE=1412.1572 MAE=1403.1796 MAE=1390.8132 MAE=1376.0070 MAE=1370.9054 Epoch: 30/500MAE=1358.1698 MAE=1343.7211 MAE=1331.1357 MAE=1317.8591 MAE=1302.8303 MAE=1288.7000 MAE=1275.2705 MAE=1261.3215 MAE=1246.7378 MAE=1230.3447 Epoch: 40/500MAE=1218.6147 MAE=1197.2214 MAE=1178.4344 MAE=1162.5267 MAE=1143.7090 MAE=1119.2095 MAE=1112.2129 MAE=1086.6165 MAE=1069.9652 MAE=1047.3386 Epoch: 50/500MAE=1026.1915 MAE=1015.2956 MAE=991.1376 MAE=968.2135 MAE=947.5131 MAE=930.1178 MAE=905.5781 MAE=873.9323 MAE=869.4344 MAE=825.6872 Epoch: 60/500MAE=827.6234 MAE=795.6716 MAE=759.5156 MAE=756.3892 MAE=725.9486 MAE=711.1729 MAE=668.8453 MAE=626.5969 MAE=605.3559 MAE=590.6265 Epoch: 70/500MAE=570.4643 MAE=526.8962 MAE=514.7376 MAE=465.1137 MAE=448.8515 MAE=417.8527 MAE=413.0909 MAE=380.7348 MAE=356.4482 MAE=349.2779 Epoch: 80/500MAE=293.3842 MAE=287.7692 MAE=284.3539 MAE=269.5225 MAE=249.1632 MAE=247.2120 MAE=176.9978 MAE=159.6986 MAE=214.9888 MAE=170.8902 Epoch: 90/500MAE=204.6049 MAE=204.2319 MAE=209.3486 MAE=115.1735 MAE=111.2229 MAE=196.4632 MAE=102.9990 MAE=106.8702 MAE=102.9239 MAE=105.6869 Epoch: 100/500MAE=106.4053 MAE=104.9158 MAE=99.7539 MAE=98.9325 MAE=165.8988 MAE=100.2742 MAE=102.7298 MAE=97.5419 MAE=124.2780 MAE=97.5046 Epoch: 110/500MAE=97.9441 MAE=96.2996 MAE=97.7442 MAE=98.1264 MAE=95.1239 MAE=96.4069 MAE=97.2290 MAE=93.5474 MAE=93.4902 MAE=93.5353 Epoch: 120/500MAE=95.9375 MAE=94.9763 MAE=94.6721 MAE=93.9544 MAE=97.9344 MAE=90.9516 MAE=93.1118 MAE=92.0691 MAE=90.2621 MAE=91.2691 Epoch: 130/500MAE=92.4781 MAE=92.0256 MAE=91.2845 MAE=89.7947 MAE=92.2978 MAE=90.1240 MAE=89.5680 MAE=89.8752 MAE=91.2144 MAE=91.0901 Epoch: 140/500MAE=91.3147 MAE=91.1838 MAE=90.9875 MAE=90.0826 MAE=90.2716 MAE=90.6392 MAE=89.9089 MAE=89.8590 MAE=90.4046 MAE=90.1848 Epoch: 150/500MAE=90.1508 MAE=90.4583 MAE=90.4955 MAE=90.6046 MAE=90.5237 MAE=90.4433 MAE=90.7242 MAE=90.6738 MAE=90.6562 MAE=90.6637 Epoch: 160/500MAE=90.6171 MAE=90.4025 MAE=90.5725 MAE=90.3830 MAE=90.3731 MAE=90.5215 MAE=90.3017 MAE=90.3046 MAE=90.3056 MAE=90.2948 Epoch: 170/500MAE=90.3386 MAE=90.2930 MAE=90.2646 MAE=90.4636 MAE=90.2720 MAE=90.4551 MAE=90.4488 MAE=90.3228 MAE=90.3341 MAE=90.3485 Epoch: 180/500MAE=90.5228 MAE=90.3311 MAE=90.3557 MAE=90.3122 MAE=90.3187 MAE=90.3296 MAE=90.2873 MAE=90.5091 MAE=90.3320 MAE=90.2802 Epoch: 190/500MAE=90.2742 MAE=90.4407 MAE=90.2624 MAE=90.3351 MAE=90.3549 MAE=90.4820 MAE=90.2697 MAE=90.2670 MAE=90.2622 MAE=90.4629 Epoch: 200/500MAE=90.5243 MAE=90.2895 MAE=90.2805 MAE=90.2492 MAE=90.2975 MAE=90.3482 MAE=90.5223 MAE=90.2937 MAE=90.2810 MAE=90.4655 Epoch: 210/500MAE=90.2926 MAE=90.5487 MAE=90.5527 MAE=90.3720 MAE=90.3372 MAE=90.5362 MAE=90.3498 MAE=90.5438 MAE=90.3442 MAE=90.5270 Epoch: 220/500MAE=90.5239 MAE=90.2549 MAE=90.4538 MAE=90.2633 MAE=90.4637 MAE=90.4782 MAE=90.2779 MAE=90.2528 MAE=90.2470 MAE=90.4348 Epoch: 230/500MAE=90.2338 MAE=90.2536 MAE=90.2279 MAE=90.4001 MAE=90.1870 MAE=90.2633 MAE=90.4472 MAE=90.3988 MAE=90.3620 MAE=90.3973 Epoch: 240/500MAE=90.2861 MAE=90.3050 MAE=90.4403 MAE=90.2308 MAE=90.4668 MAE=90.2785 MAE=90.4547 MAE=90.2628 MAE=90.4539 MAE=90.2794 Epoch: 250/500MAE=90.2602 MAE=90.5053 MAE=90.4495 MAE=90.4550 MAE=90.2839 MAE=90.2862 MAE=90.4433 MAE=90.2350 MAE=90.2973 MAE=90.3004 Epoch: 260/500MAE=90.2906 MAE=90.2940 MAE=90.4788 MAE=90.5018 MAE=90.2993 MAE=90.4867 MAE=90.3095 MAE=90.3181 MAE=90.3058 MAE=90.2971 Epoch: 270/500MAE=90.3540 MAE=90.3525 MAE=90.5440 MAE=90.4032 MAE=90.5939 MAE=90.4029 MAE=90.3955 MAE=90.5826 MAE=90.3952 MAE=90.3883 Epoch: 280/500MAE=90.5863 MAE=90.3904 MAE=90.3782 MAE=90.3765 MAE=90.5708 MAE=90.5540 MAE=90.5635 MAE=127.6453 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 161.773 +/- 34.128\n",
      "\n",
      "Epoch: 1/500MAE=1549.5553 MAE=1547.7876 MAE=1545.0471 MAE=1542.2606 MAE=1539.5017 MAE=1536.4288 MAE=1533.1718 MAE=1529.2275 MAE=1525.4158 Epoch: 10/500MAE=1521.1124 MAE=1516.3746 MAE=1511.3931 MAE=1505.7188 MAE=1500.5886 MAE=1494.8333 MAE=1487.8889 MAE=1480.3191 MAE=1473.7684 MAE=1466.5898 Epoch: 20/500MAE=1457.7876 MAE=1448.9529 MAE=1442.8508 MAE=1431.4208 MAE=1424.7010 MAE=1412.0400 MAE=1396.7629 MAE=1397.3591 MAE=1380.0369 MAE=1360.8167 Epoch: 30/500MAE=1346.6787 MAE=1334.4916 MAE=1324.2682 MAE=1308.1337 MAE=1293.5586 MAE=1281.2021 MAE=1266.7795 MAE=1247.6505 MAE=1237.3861 MAE=1223.3669 Epoch: 40/500MAE=1203.6199 MAE=1185.5842 MAE=1178.4587 MAE=1147.9873 MAE=1141.8531 MAE=1112.1395 MAE=1112.8081 MAE=1086.7007 MAE=1054.6008 MAE=1030.4178 Epoch: 50/500MAE=1036.0596 MAE=1007.3598 MAE=996.7535 MAE=974.9988 MAE=936.5341 MAE=916.0216 MAE=901.4198 MAE=875.5692 MAE=877.6820 MAE=835.4294 Epoch: 60/500MAE=822.9496 MAE=790.3107 MAE=765.9499 MAE=772.0306 MAE=718.3595 MAE=709.2094 MAE=678.6730 MAE=668.4448 MAE=664.6036 MAE=603.2691 Epoch: 70/500MAE=574.8134 MAE=565.8665 MAE=551.1263 MAE=499.5687 MAE=518.1553 MAE=463.2675 MAE=428.1711 MAE=402.8050 MAE=377.3669 MAE=348.4917 Epoch: 80/500MAE=332.5019 MAE=298.4100 MAE=274.5228 MAE=283.1949 MAE=254.2533 MAE=218.5924 MAE=220.8487 MAE=184.3797 MAE=174.5032 MAE=167.4671 Epoch: 90/500MAE=155.9663 MAE=180.8689 MAE=131.7959 MAE=119.9254 MAE=125.1319 MAE=120.9734 MAE=117.9707 MAE=132.4306 MAE=107.0911 MAE=113.9755 Epoch: 100/500MAE=114.8992 MAE=106.4442 MAE=100.4235 MAE=100.7220 MAE=103.4441 MAE=99.9085 MAE=97.5883 MAE=96.9101 MAE=99.9700 MAE=99.8860 Epoch: 110/500MAE=96.0781 MAE=99.7762 MAE=93.8849 MAE=96.4814 MAE=97.3445 MAE=94.5685 MAE=97.7492 MAE=97.7726 MAE=95.9653 MAE=97.0836 Epoch: 120/500MAE=95.6575 MAE=94.9105 MAE=94.8120 MAE=95.8929 MAE=96.3023 MAE=96.5493 MAE=96.0672 MAE=95.4078 MAE=95.9469 MAE=96.0495 Epoch: 130/500MAE=95.4132 MAE=95.3393 MAE=94.7321 MAE=95.0472 MAE=95.0942 MAE=94.9166 MAE=95.1234 MAE=94.9506 MAE=95.0434 MAE=94.9768 Epoch: 140/500MAE=95.0348 MAE=94.8209 MAE=94.8263 MAE=94.9517 MAE=94.8608 MAE=94.8327 MAE=94.9845 MAE=94.9544 MAE=94.9875 MAE=94.9372 Epoch: 150/500MAE=94.9987 MAE=94.8833 MAE=94.8824 MAE=94.9536 MAE=94.9977 MAE=94.9456 MAE=95.0565 MAE=94.8710 MAE=94.9394 MAE=94.9914 Epoch: 160/500MAE=94.9420 MAE=94.9325 MAE=94.9819 MAE=94.9383 MAE=94.9119 MAE=94.9446 MAE=94.9370 MAE=94.9940 MAE=94.9801 MAE=94.9751 Epoch: 170/500MAE=95.0905 MAE=95.0221 MAE=94.9912 MAE=94.9517 MAE=95.0238 MAE=94.9849 MAE=94.9769 MAE=94.9822 MAE=94.7847 MAE=95.0943 Epoch: 180/500MAE=94.9037 MAE=94.9850 MAE=95.0297 MAE=94.9121 MAE=94.9858 MAE=94.9916 MAE=95.0355 MAE=95.0310 MAE=94.9567 MAE=94.9113 Epoch: 190/500MAE=95.0244 MAE=94.9799 MAE=95.0492 MAE=94.9819 MAE=95.0253 MAE=95.0205 MAE=95.0239 MAE=94.9792 MAE=94.9786 MAE=94.9761 Epoch: 200/500MAE=95.0183 MAE=95.0122 MAE=94.9391 MAE=94.9688 MAE=95.0348 MAE=94.9612 MAE=94.9632 MAE=94.9648 MAE=94.9614 MAE=95.2527 Epoch: 210/500MAE=95.3215 MAE=94.9616 MAE=95.3360 MAE=94.9996 MAE=95.3292 MAE=95.2864 MAE=95.3230 MAE=95.3547 MAE=95.2538 MAE=95.2799 Epoch: 220/500MAE=95.2824 MAE=95.3265 MAE=95.3337 MAE=95.3302 MAE=95.2899 MAE=95.2860 MAE=95.2073 MAE=95.2836 MAE=95.2820 MAE=95.3244 Epoch: 230/500MAE=95.2517 MAE=95.2859 MAE=95.2611 MAE=95.2994 MAE=95.3274 MAE=95.2993 MAE=95.2121 MAE=95.2893 MAE=95.2858 MAE=95.2954 Epoch: 240/500MAE=95.2523 MAE=95.2500 MAE=95.3047 MAE=95.2597 MAE=95.2595 MAE=95.3037 MAE=95.3050 MAE=95.2625 MAE=95.2552 MAE=95.2961 Epoch: 250/500MAE=95.2474 MAE=95.3059 MAE=95.2582 MAE=95.2476 MAE=95.3021 MAE=95.2555 MAE=95.2916 MAE=95.2470 MAE=95.1666 MAE=95.2467 Epoch: 260/500MAE=95.3008 MAE=95.2547 MAE=95.2313 MAE=191.6543 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 171.733 +/- 31.223\n",
      "\n",
      "Epoch: 1/500MAE=1549.8230 MAE=1547.6229 MAE=1544.4110 MAE=1541.9161 MAE=1538.8832 MAE=1536.7137 MAE=1533.7318 MAE=1529.5525 MAE=1523.6698 Epoch: 10/500MAE=1520.7446 MAE=1516.6986 MAE=1511.1532 MAE=1505.3275 MAE=1501.6982 MAE=1493.2722 MAE=1487.3245 MAE=1481.0347 MAE=1473.6665 MAE=1465.7290 Epoch: 20/500MAE=1456.7617 MAE=1448.5520 MAE=1440.4792 MAE=1431.2317 MAE=1421.6404 MAE=1404.5292 MAE=1398.0117 MAE=1394.6812 MAE=1378.6149 MAE=1348.3203 Epoch: 30/500MAE=1345.9565 MAE=1334.1007 MAE=1318.0428 MAE=1303.2961 MAE=1303.0515 MAE=1276.3512 MAE=1220.8010 MAE=1278.3090 MAE=1211.0541 MAE=1213.2750 Epoch: 40/500MAE=1183.4927 MAE=1165.6444 MAE=1158.5247 MAE=1155.4941 MAE=1154.6343 MAE=1131.9257 MAE=1094.2332 MAE=1070.5227 MAE=1003.4553 MAE=999.7618 Epoch: 50/500MAE=1042.2592 MAE=963.0206 MAE=979.3456 MAE=918.1670 MAE=921.3588 MAE=953.0198 MAE=877.6396 MAE=821.6625 MAE=810.4724 MAE=822.7591 Epoch: 60/500MAE=804.4915 MAE=772.7874 MAE=748.3423 MAE=708.1022 MAE=681.3547 MAE=671.1538 MAE=641.8896 MAE=622.5031 MAE=602.4678 MAE=591.2473 Epoch: 70/500MAE=579.5598 MAE=549.6301 MAE=554.7410 MAE=526.6255 MAE=491.6214 MAE=497.0096 MAE=391.9219 MAE=420.7684 MAE=369.5825 MAE=438.2644 Epoch: 80/500MAE=394.3985 MAE=344.2579 MAE=344.2956 MAE=299.4860 MAE=313.4585 MAE=306.7130 MAE=284.8755 MAE=284.0041 MAE=256.2515 MAE=191.5336 Epoch: 90/500MAE=235.3904 MAE=152.5708 MAE=145.5374 MAE=157.0791 MAE=153.3249 MAE=109.2087 MAE=215.9843 MAE=218.8703 MAE=209.4373 MAE=202.1022 Epoch: 100/500MAE=144.0552 MAE=107.9550 MAE=101.3925 MAE=104.2329 MAE=98.3604 MAE=106.9974 MAE=101.2776 MAE=102.6058 MAE=200.0768 MAE=98.9074 Epoch: 110/500MAE=99.9238 MAE=98.1559 MAE=203.2405 MAE=204.1194 MAE=98.6451 MAE=198.5409 MAE=201.5498 MAE=199.1129 MAE=199.6994 MAE=205.4639 Epoch: 120/500MAE=212.7547 MAE=202.7080 MAE=199.5703 MAE=203.1186 MAE=201.3918 MAE=202.2284 MAE=200.8884 MAE=202.6181 MAE=201.7238 MAE=200.7757 Epoch: 130/500MAE=201.7795 MAE=201.6151 MAE=202.1691 MAE=200.9142 MAE=201.8096 MAE=203.0048 MAE=202.7395 MAE=202.6528 MAE=202.5402 MAE=202.0621 Epoch: 140/500MAE=201.9143 MAE=201.9746 MAE=201.9109 MAE=201.8078 MAE=201.7803 MAE=201.7316 MAE=201.7968 MAE=201.8168 MAE=201.9135 MAE=201.8114 Epoch: 150/500MAE=201.9454 MAE=201.8972 MAE=201.9197 MAE=201.9225 MAE=201.8699 MAE=201.8789 MAE=201.9095 MAE=201.9163 MAE=201.8877 MAE=201.8870 Epoch: 160/500MAE=201.9344 MAE=201.8805 MAE=201.8206 MAE=201.8781 MAE=201.9130 MAE=201.8857 MAE=201.8974 MAE=201.9171 MAE=201.9113 MAE=201.8806 Epoch: 170/500MAE=201.9671 MAE=201.9441 MAE=201.9001 MAE=201.9149 MAE=201.8754 MAE=201.9084 MAE=201.8732 MAE=201.8833 MAE=201.8663 MAE=202.0647 Epoch: 180/500MAE=201.9066 MAE=201.8819 MAE=201.8885 MAE=201.9737 MAE=201.9296 MAE=201.9240 MAE=201.9725 MAE=201.9911 MAE=201.9759 MAE=201.8990 Epoch: 190/500MAE=201.9939 MAE=202.0214 MAE=201.9821 MAE=202.0092 MAE=202.0254 MAE=202.0380 MAE=202.0647 MAE=202.0495 MAE=201.9799 MAE=202.0448 Epoch: 200/500MAE=202.0069 MAE=202.0121 MAE=202.0028 MAE=202.0130 MAE=202.0326 MAE=202.0443 MAE=202.0223 MAE=202.0335 MAE=201.9578 MAE=202.0108 Epoch: 210/500MAE=201.9789 MAE=202.0130 MAE=202.0371 MAE=202.0196 MAE=202.0082 MAE=201.9865 MAE=201.9664 MAE=201.8877 MAE=201.9983 MAE=202.0126 Epoch: 220/500MAE=202.0576 MAE=201.8785 MAE=201.9833 MAE=201.9485 MAE=202.0108 MAE=201.9383 MAE=202.0490 MAE=202.0079 MAE=201.9240 MAE=202.0099 Epoch: 230/500MAE=201.9811 MAE=201.9454 MAE=202.0775 MAE=202.3150 MAE=202.3719 MAE=202.3214 MAE=202.0874 MAE=202.0989 MAE=202.2606 MAE=202.3196 Epoch: 240/500MAE=202.2507 MAE=202.3120 MAE=202.2821 MAE=202.2568 MAE=202.2655 MAE=202.2626 MAE=202.3097 MAE=202.2727 MAE=202.3182 MAE=202.2629 Epoch: 250/500MAE=202.2578 MAE=202.3652 MAE=202.3044 MAE=202.2633 MAE=202.3456 MAE=202.3081 MAE=202.2026 MAE=202.3158 MAE=202.3128 MAE=202.2996 Epoch: 260/500MAE=202.3556 MAE=202.3680 MAE=202.3372 MAE=202.3700 MAE=202.3461 MAE=202.3475 MAE=202.2565 MAE=194.7948 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 177.499 +/- 28.825\n",
      "\n",
      "Epoch: 1/500MAE=1549.7629 MAE=1546.8135 MAE=1544.1060 MAE=1541.2036 MAE=1538.6179 MAE=1535.9849 MAE=1533.2510 MAE=1527.4803 MAE=1524.6171 Epoch: 10/500MAE=1519.8002 MAE=1515.6246 MAE=1511.5238 MAE=1505.2349 MAE=1498.5388 MAE=1494.7793 MAE=1486.9229 MAE=1481.0317 MAE=1476.0693 MAE=1463.2581 Epoch: 20/500MAE=1460.8833 MAE=1440.3993 MAE=1437.1427 MAE=1431.6558 MAE=1408.5789 MAE=1411.2433 MAE=1396.8723 MAE=1370.2242 MAE=1366.2535 MAE=1356.1729 Epoch: 30/500MAE=1340.1500 MAE=1332.4081 MAE=1286.6938 MAE=1286.5300 MAE=1282.0602 MAE=1293.6539 MAE=1205.5139 MAE=1225.0562 MAE=1262.4529 MAE=1204.1393 Epoch: 40/500MAE=1206.8660 MAE=1169.3833 MAE=1138.6532 MAE=1131.9338 MAE=1134.8677 MAE=1102.3853 MAE=1068.2305 MAE=1094.8608 MAE=1026.5917 MAE=999.8456 Epoch: 50/500MAE=1001.7963 MAE=1018.8542 MAE=966.5132 MAE=890.1249 MAE=953.5012 MAE=906.5895 MAE=876.9375 MAE=864.3123 MAE=792.2239 MAE=823.4687 Epoch: 60/500MAE=822.2848 MAE=789.7812 MAE=725.9150 MAE=684.1854 MAE=685.0828 MAE=624.6708 MAE=672.9305 MAE=632.4720 MAE=613.9017 MAE=613.2219 Epoch: 70/500MAE=530.6783 MAE=520.8215 MAE=506.8631 MAE=457.2997 MAE=443.4578 MAE=421.7872 MAE=321.1357 MAE=315.3751 MAE=381.8629 MAE=348.3024 Epoch: 80/500MAE=298.5092 MAE=287.9520 MAE=247.3457 MAE=167.6913 MAE=232.1360 MAE=226.0883 MAE=197.0477 MAE=186.2499 MAE=136.0733 MAE=142.5288 Epoch: 90/500MAE=138.8783 MAE=130.3723 MAE=112.3368 MAE=121.9862 MAE=97.5210 MAE=98.9296 MAE=126.0505 MAE=89.9493 MAE=91.2736 MAE=79.2588 Epoch: 100/500MAE=98.3972 MAE=101.8071 MAE=86.4203 MAE=87.4353 MAE=76.9183 MAE=84.5220 MAE=78.4027 MAE=79.0890 MAE=85.3376 MAE=78.9921 Epoch: 110/500MAE=75.4376 MAE=73.2409 MAE=74.9785 MAE=75.2220 MAE=74.4135 MAE=74.8183 MAE=75.5043 MAE=74.4326 MAE=75.4266 MAE=75.9130 Epoch: 120/500MAE=74.6102 MAE=73.9490 MAE=74.2275 MAE=74.0425 MAE=74.0841 MAE=74.0922 MAE=74.0511 MAE=73.7863 MAE=74.0652 MAE=74.0070 Epoch: 130/500MAE=73.6892 MAE=73.8478 MAE=73.8242 MAE=73.8087 MAE=73.8398 MAE=73.8792 MAE=73.8233 MAE=73.8123 MAE=73.7494 MAE=73.8009 Epoch: 140/500MAE=73.7763 MAE=73.8486 MAE=73.7512 MAE=73.7542 MAE=73.7620 MAE=74.0785 MAE=73.7703 MAE=73.7682 MAE=73.7739 MAE=73.8679 Epoch: 150/500MAE=73.7720 MAE=73.7449 MAE=73.7431 MAE=73.7465 MAE=73.7390 MAE=73.7167 MAE=73.7225 MAE=73.7227 MAE=73.7034 MAE=73.7224 Epoch: 160/500MAE=73.7197 MAE=73.7188 MAE=73.7395 MAE=73.8239 MAE=73.7021 MAE=73.6995 MAE=73.7018 MAE=73.6993 MAE=73.7007 MAE=73.6819 Epoch: 170/500MAE=73.7162 MAE=73.6876 MAE=73.7842 MAE=73.6974 MAE=73.6921 MAE=73.7111 MAE=73.7021 MAE=73.7352 MAE=73.8945 MAE=73.8926 Epoch: 180/500MAE=73.9207 MAE=73.8812 MAE=73.8718 MAE=73.9723 MAE=73.8363 MAE=73.8319 MAE=73.8237 MAE=74.1041 MAE=74.0624 MAE=74.0583 Epoch: 190/500MAE=73.7861 MAE=73.8065 MAE=73.8076 MAE=73.8197 MAE=73.7718 MAE=73.8227 MAE=73.8991 MAE=73.8265 MAE=73.7769 MAE=73.7747 Epoch: 200/500MAE=74.0860 MAE=73.8273 MAE=73.8504 MAE=73.7928 MAE=73.8301 MAE=73.8332 MAE=73.8327 MAE=73.8456 MAE=73.8151 MAE=73.8256 Epoch: 210/500MAE=73.8243 MAE=74.0924 MAE=74.0947 MAE=73.8219 MAE=73.8211 MAE=73.8366 MAE=73.8403 MAE=74.1910 MAE=73.8539 MAE=73.8298 Epoch: 220/500MAE=74.0798 MAE=73.8586 MAE=73.8571 MAE=73.8136 MAE=73.8107 MAE=73.8331 MAE=73.8100 MAE=73.8237 MAE=73.7852 MAE=74.0642 Epoch: 230/500MAE=73.9007 MAE=73.8023 MAE=73.8129 MAE=73.7973 MAE=73.7961 MAE=73.7858 MAE=73.8674 MAE=73.8674 MAE=73.7675 MAE=73.7719 Epoch: 240/500MAE=73.7820 MAE=74.0401 MAE=73.7881 MAE=73.7715 MAE=73.7702 MAE=73.8267 MAE=73.8297 MAE=73.8294 MAE=73.8358 MAE=73.8444 Epoch: 250/500MAE=74.0670 MAE=73.8899 MAE=73.7972 MAE=73.8172 MAE=73.7943 MAE=73.7931 MAE=73.7726 MAE=73.7757 MAE=74.1288 MAE=73.7913 Epoch: 260/500MAE=73.7876 MAE=73.7907 MAE=187.9095 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 179.581 +/- 26.116\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1548.9971 MAE=1546.4353 MAE=1544.3892 MAE=1542.6182 MAE=1539.9139 MAE=1536.6091 MAE=1533.8113 MAE=1529.4485 MAE=1525.6544 Epoch: 10/500MAE=1520.7747 MAE=1515.4556 MAE=1511.2944 MAE=1505.5227 MAE=1498.8152 MAE=1491.7329 MAE=1485.8594 MAE=1478.0387 MAE=1471.3176 MAE=1464.6885 Epoch: 20/500MAE=1453.4260 MAE=1446.6716 MAE=1438.2217 MAE=1427.6857 MAE=1419.8718 MAE=1409.8706 MAE=1396.6415 MAE=1383.8544 MAE=1372.3340 MAE=1365.5195 Epoch: 30/500MAE=1354.9335 MAE=1341.5083 MAE=1312.5723 MAE=1305.9563 MAE=1293.4788 MAE=1287.2139 MAE=1258.9271 MAE=1260.8623 MAE=1244.0010 MAE=1219.2217 Epoch: 40/500MAE=1199.1748 MAE=1187.4543 MAE=1156.4617 MAE=1148.5752 MAE=1150.5520 MAE=1117.0132 MAE=1092.6578 MAE=1109.7684 MAE=1049.6038 MAE=1002.9894 Epoch: 50/500MAE=1020.5479 MAE=976.9048 MAE=1056.1912 MAE=953.5044 MAE=937.8443 MAE=916.5901 MAE=908.6279 MAE=880.3344 MAE=850.0570 MAE=833.3595 Epoch: 60/500MAE=818.5078 MAE=782.5464 MAE=762.2902 MAE=743.6305 MAE=715.2523 MAE=695.7641 MAE=671.9950 MAE=605.6533 MAE=633.9702 MAE=581.3814 Epoch: 70/500MAE=549.5332 MAE=572.2885 MAE=490.8948 MAE=473.1908 MAE=442.4291 MAE=436.5044 MAE=402.7580 MAE=337.8945 MAE=349.4210 MAE=344.9716 Epoch: 80/500MAE=303.9254 MAE=302.7887 MAE=207.8590 MAE=218.5659 MAE=243.5850 MAE=255.6635 MAE=249.4278 MAE=230.3780 MAE=231.3486 MAE=178.3875 Epoch: 90/500MAE=169.2500 MAE=174.5963 MAE=160.1169 MAE=152.2802 MAE=152.7099 MAE=134.1257 MAE=132.9629 MAE=135.0163 MAE=148.8882 MAE=151.4207 Epoch: 100/500MAE=143.8954 MAE=135.1167 MAE=104.1712 MAE=109.2202 MAE=111.4201 MAE=120.6296 MAE=128.7397 MAE=101.5917 MAE=101.1292 MAE=111.2609 Epoch: 110/500MAE=101.8327 MAE=134.3011 MAE=102.2520 MAE=102.2598 MAE=101.2993 MAE=96.7279 MAE=100.0661 MAE=98.9519 MAE=100.3487 MAE=94.8312 Epoch: 120/500MAE=97.9671 MAE=97.4369 MAE=96.9247 MAE=96.7396 MAE=96.0301 MAE=97.6831 MAE=98.0167 MAE=95.1888 MAE=95.9025 MAE=95.3749 Epoch: 130/500MAE=96.1005 MAE=95.6903 MAE=95.9939 MAE=95.9656 MAE=96.3236 MAE=96.0653 MAE=95.9655 MAE=95.9888 MAE=95.7265 MAE=95.7145 Epoch: 140/500MAE=95.6007 MAE=95.4465 MAE=95.8342 MAE=95.6539 MAE=95.4059 MAE=95.5469 MAE=95.7335 MAE=95.5833 MAE=95.6151 MAE=95.5527 Epoch: 150/500MAE=95.3839 MAE=95.5006 MAE=95.3891 MAE=95.7207 MAE=95.3393 MAE=95.5861 MAE=95.4981 MAE=95.6898 MAE=95.7198 MAE=95.2181 Epoch: 160/500MAE=95.2743 MAE=95.3244 MAE=95.7261 MAE=95.5700 MAE=95.8128 MAE=95.7393 MAE=95.3736 MAE=95.5518 MAE=95.7332 MAE=95.7859 Epoch: 170/500MAE=95.7541 MAE=95.6749 MAE=95.5904 MAE=95.4997 MAE=95.5273 MAE=95.4650 MAE=95.5775 MAE=95.3120 MAE=95.6187 MAE=95.7500 Epoch: 180/500MAE=95.5950 MAE=95.3612 MAE=95.7655 MAE=95.5446 MAE=95.5642 MAE=95.5686 MAE=95.2392 MAE=95.4528 MAE=95.5626 MAE=95.1222 Epoch: 190/500MAE=95.4827 MAE=95.3954 MAE=95.4104 MAE=95.5289 MAE=95.4559 MAE=95.3203 MAE=95.2592 MAE=95.4406 MAE=95.3657 MAE=95.4786 Epoch: 200/500MAE=95.4947 MAE=95.3398 MAE=95.4889 MAE=95.3137 MAE=95.4302 MAE=95.2840 MAE=95.4261 MAE=95.5789 MAE=95.4224 MAE=95.3855 Epoch: 210/500MAE=95.4718 MAE=95.4709 MAE=95.2267 MAE=95.5530 MAE=95.5201 MAE=95.5033 MAE=95.2315 MAE=95.4179 MAE=95.5003 MAE=95.2787 Epoch: 220/500MAE=95.5472 MAE=95.5286 MAE=95.1549 MAE=95.4423 MAE=95.4848 MAE=95.4383 MAE=95.1548 MAE=95.4236 MAE=95.5555 MAE=95.6360 Epoch: 230/500MAE=95.5752 MAE=95.3798 MAE=95.6868 MAE=95.4763 MAE=95.5901 MAE=95.4549 MAE=95.4287 MAE=95.2568 MAE=95.4403 MAE=95.3536 Epoch: 240/500MAE=95.3233 MAE=95.4882 MAE=95.3325 MAE=95.3929 MAE=95.3487 MAE=95.3877 MAE=95.3460 MAE=95.4621 MAE=95.1030 MAE=95.2740 Epoch: 250/500MAE=95.1669 MAE=95.4535 MAE=95.2789 MAE=95.5157 MAE=95.3213 MAE=95.0866 MAE=95.4335 MAE=95.1768 MAE=95.3307 MAE=95.2980 Epoch: 260/500MAE=94.9317 MAE=95.4665 MAE=95.4297 MAE=95.4173 MAE=95.3339 MAE=95.3977 MAE=95.3528 MAE=95.4657 MAE=95.3025 MAE=95.4003 MAE=99.2448 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 99.245 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1549.3535 MAE=1546.7260 MAE=1544.0193 MAE=1542.3892 MAE=1539.7745 MAE=1537.4011 MAE=1532.4399 MAE=1530.2578 MAE=1523.8490 Epoch: 10/500MAE=1520.6260 MAE=1517.0247 MAE=1513.2817 MAE=1507.0782 MAE=1502.0466 MAE=1497.3524 MAE=1490.4230 MAE=1479.7225 MAE=1476.7538 MAE=1469.0913 Epoch: 20/500MAE=1461.1732 MAE=1451.9380 MAE=1444.1810 MAE=1423.7600 MAE=1419.5638 MAE=1416.2458 MAE=1397.8442 MAE=1393.1235 MAE=1379.9426 MAE=1362.4154 Epoch: 30/500MAE=1344.8694 MAE=1338.4479 MAE=1340.2812 MAE=1319.9170 MAE=1302.5399 MAE=1277.5817 MAE=1275.4784 MAE=1256.3402 MAE=1249.2982 MAE=1220.8944 Epoch: 40/500MAE=1215.6067 MAE=1204.9031 MAE=1205.4990 MAE=1167.7362 MAE=1135.9546 MAE=1111.3141 MAE=1143.6801 MAE=1078.9166 MAE=1035.2113 MAE=1035.7965 Epoch: 50/500MAE=991.1323 MAE=1037.7474 MAE=968.6436 MAE=971.5738 MAE=915.8538 MAE=886.3466 MAE=961.0324 MAE=848.2832 MAE=861.4563 MAE=820.2892 Epoch: 60/500MAE=819.3315 MAE=815.4608 MAE=801.0579 MAE=793.7471 MAE=782.3845 MAE=729.4476 MAE=700.1351 MAE=708.9056 MAE=649.0326 MAE=613.3949 Epoch: 70/500MAE=682.2498 MAE=551.1306 MAE=560.4392 MAE=512.4183 MAE=544.0648 MAE=485.6835 MAE=563.3303 MAE=526.7446 MAE=478.4291 MAE=444.6712 Epoch: 80/500MAE=391.4222 MAE=366.4193 MAE=430.3040 MAE=316.0359 MAE=312.2125 MAE=232.5158 MAE=240.3949 MAE=254.2771 MAE=240.8819 MAE=201.1561 Epoch: 90/500MAE=166.7234 MAE=195.9582 MAE=226.3634 MAE=206.0598 MAE=181.3434 MAE=181.0552 MAE=153.8718 MAE=173.7921 MAE=150.7903 MAE=152.8487 Epoch: 100/500MAE=153.1615 MAE=150.4808 MAE=143.0544 MAE=163.2242 MAE=144.2361 MAE=146.4687 MAE=145.3809 MAE=157.0670 MAE=144.2147 MAE=149.7924 Epoch: 110/500MAE=140.9735 MAE=139.0280 MAE=140.6812 MAE=140.8901 MAE=139.8422 MAE=136.8530 MAE=137.3815 MAE=138.0444 MAE=137.8701 MAE=140.3122 Epoch: 120/500MAE=137.4892 MAE=137.7570 MAE=137.5918 MAE=137.4389 MAE=136.5688 MAE=137.5996 MAE=135.2884 MAE=138.9319 MAE=136.0227 MAE=137.8591 Epoch: 130/500MAE=136.0010 MAE=135.3978 MAE=137.1848 MAE=135.3137 MAE=135.8598 MAE=136.0559 MAE=135.0032 MAE=134.5555 MAE=134.9284 MAE=136.1621 Epoch: 140/500MAE=135.8658 MAE=135.6310 MAE=136.3692 MAE=136.3859 MAE=136.4960 MAE=136.0050 MAE=135.9413 MAE=135.6475 MAE=135.2328 MAE=135.8493 Epoch: 150/500MAE=135.4366 MAE=134.9608 MAE=135.0058 MAE=134.8407 MAE=134.8360 MAE=134.8961 MAE=135.0160 MAE=135.0039 MAE=134.8841 MAE=134.9417 Epoch: 160/500MAE=134.9629 MAE=134.9902 MAE=135.0427 MAE=134.9329 MAE=134.9630 MAE=135.7705 MAE=134.9206 MAE=135.7463 MAE=134.9252 MAE=134.8820 Epoch: 170/500MAE=134.9870 MAE=135.3805 MAE=134.9214 MAE=135.3593 MAE=135.1198 MAE=135.0592 MAE=134.8746 MAE=134.9981 MAE=135.1000 MAE=134.9436 Epoch: 180/500MAE=134.9516 MAE=134.9298 MAE=134.9299 MAE=134.9707 MAE=134.9795 MAE=135.3208 MAE=135.0311 MAE=135.6812 MAE=135.0028 MAE=135.0309 Epoch: 190/500MAE=135.5134 MAE=135.0386 MAE=135.1564 MAE=135.1241 MAE=135.0614 MAE=135.1167 MAE=135.1187 MAE=135.0830 MAE=135.4452 MAE=135.0839 Epoch: 200/500MAE=134.9713 MAE=135.0361 MAE=134.9870 MAE=134.9222 MAE=135.3732 MAE=135.0012 MAE=134.9597 MAE=134.9853 MAE=135.0214 MAE=135.0685 Epoch: 210/500MAE=135.0010 MAE=134.8090 MAE=134.7477 MAE=134.7923 MAE=135.2471 MAE=134.8277 MAE=134.8231 MAE=134.7968 MAE=134.7695 MAE=135.1726 Epoch: 220/500MAE=134.7673 MAE=134.8204 MAE=134.7460 MAE=135.1230 MAE=134.7749 MAE=135.9277 MAE=134.7655 MAE=134.8550 MAE=135.6026 MAE=134.7589 Epoch: 230/500MAE=135.0670 MAE=134.7908 MAE=134.8247 MAE=134.7757 MAE=134.7427 MAE=135.1355 MAE=135.5662 MAE=134.7108 MAE=134.8427 MAE=135.0770 Epoch: 240/500MAE=134.6806 MAE=134.7869 MAE=134.8520 MAE=134.7173 MAE=135.4227 MAE=134.6908 MAE=134.8177 MAE=134.7509 MAE=135.2005 MAE=135.7174 Epoch: 250/500MAE=135.1256 MAE=134.6690 MAE=134.7753 MAE=134.7126 MAE=134.7433 MAE=135.1217 MAE=134.7141 MAE=134.7506 MAE=134.7397 MAE=134.8719 Epoch: 260/500MAE=134.8168 MAE=134.8552 MAE=135.5157 MAE=135.6401 MAE=134.6894 MAE=134.8087 MAE=134.8023 MAE=134.7236 MAE=134.8916 MAE=134.8863 Epoch: 270/500MAE=134.8392 MAE=134.8775 MAE=134.8255 MAE=134.7208 MAE=134.7043 MAE=135.5398 MAE=134.7482 MAE=134.7106 MAE=134.7553 MAE=135.1014 Epoch: 280/500MAE=134.8557 MAE=135.2064 MAE=134.8808 MAE=135.2326 MAE=134.8046 MAE=135.6255 MAE=134.8051 MAE=134.7420 MAE=135.0635 MAE=135.1247 Epoch: 290/500MAE=134.8375 MAE=134.8097 MAE=135.5730 MAE=134.7661 MAE=134.7069 MAE=134.7220 MAE=134.7342 MAE=135.0988 MAE=134.7231 MAE=134.7099 Epoch: 300/500MAE=134.7417 MAE=134.6122 MAE=135.0523 MAE=134.6240 MAE=135.0072 MAE=134.9263 MAE=135.0810 MAE=134.7045 MAE=134.7287 MAE=134.7254 Epoch: 310/500MAE=134.9452 MAE=134.7442 MAE=134.6861 MAE=136.0890 MAE=134.7986 MAE=134.7486 MAE=134.8075 MAE=135.0643 MAE=134.6630 MAE=134.6589 Epoch: 320/500MAE=134.6712 MAE=135.0759 MAE=135.2151 MAE=134.6487 MAE=134.6276 MAE=134.6099 MAE=135.3668 MAE=134.7063 MAE=134.7109 MAE=134.6687 Epoch: 330/500MAE=134.6604 MAE=134.6510 MAE=134.6307 MAE=134.5952 MAE=134.7811 MAE=134.7323 MAE=134.6784 MAE=134.6029 MAE=134.6760 MAE=134.8368 Epoch: 340/500MAE=134.7322 MAE=134.5785 MAE=134.6065 MAE=134.8212 MAE=134.6567 MAE=134.4844 MAE=134.6393 MAE=134.5760 MAE=134.6568 MAE=134.5159 Epoch: 350/500MAE=134.5831 MAE=134.5024 MAE=134.5934 MAE=134.5939 MAE=134.5101 MAE=134.5763 MAE=134.6414 MAE=134.5775 MAE=135.4870 MAE=134.6104 Epoch: 360/500MAE=135.1674 MAE=134.6364 MAE=134.6995 MAE=135.0259 MAE=134.9430 MAE=134.6337 MAE=134.6605 MAE=134.6109 MAE=135.0501 MAE=134.6441 Epoch: 370/500MAE=134.8331 MAE=134.6719 MAE=134.6267 MAE=135.1816 MAE=134.5917 MAE=134.5978 MAE=135.0297 MAE=134.7061 MAE=134.6234 MAE=135.4671 Epoch: 380/500MAE=134.5889 MAE=134.6621 MAE=134.5828 MAE=134.7431 MAE=134.7352 MAE=135.0727 MAE=134.5614 MAE=135.1958 MAE=134.5687 MAE=134.9135 Epoch: 390/500MAE=135.9762 MAE=134.6132 MAE=134.6602 MAE=134.5827 MAE=134.6238 MAE=134.5893 MAE=134.5785 MAE=134.6710 MAE=134.9568 MAE=134.7161 Epoch: 400/500MAE=135.2493 MAE=135.8217 MAE=135.9001 MAE=134.6081 MAE=134.6578 MAE=134.9818 MAE=134.9820 MAE=134.5526 MAE=134.9747 MAE=134.6019 Epoch: 410/500MAE=134.5901 MAE=134.5504 MAE=134.5380 MAE=134.4433 MAE=134.5801 MAE=134.6235 MAE=134.7772 MAE=135.5333 MAE=134.6380 MAE=134.6082 Epoch: 420/500MAE=134.5524 MAE=134.5223 MAE=134.6667 MAE=134.5515 MAE=134.6844 MAE=134.6065 MAE=134.6569 MAE=134.7127 MAE=134.5298 MAE=134.8688 Epoch: 430/500MAE=134.5645 MAE=134.8629 MAE=134.5643 MAE=134.5837 MAE=134.5412 MAE=134.5513 MAE=134.5054 MAE=135.2383 MAE=134.5059 MAE=135.0211 Epoch: 440/500MAE=134.6823 MAE=134.6983 MAE=134.7100 MAE=134.5929 MAE=134.5121 MAE=134.5943 MAE=134.6395 MAE=134.6418 MAE=135.0764 MAE=134.5586 Epoch: 450/500MAE=134.5820 MAE=134.5774 MAE=134.6801 MAE=135.5308 MAE=134.7496 MAE=134.6511 MAE=135.2694 MAE=134.5396 MAE=134.9998 MAE=134.5570 Epoch: 460/500MAE=134.4838 MAE=134.4842 MAE=134.4894 MAE=134.6053 MAE=134.5192 MAE=134.4185 MAE=134.5451 MAE=134.3658 MAE=134.4746 MAE=135.1784 Epoch: 470/500MAE=135.5231 MAE=134.6566 MAE=134.5263 MAE=134.6604 MAE=134.6264 MAE=135.2752 MAE=134.7753 MAE=134.6210 MAE=134.4772 MAE=134.8712 Epoch: 480/500MAE=134.8172 MAE=134.8445 MAE=134.7051 MAE=134.6843 MAE=134.5889 MAE=134.7315 MAE=134.7480 MAE=134.8372 MAE=134.8545 MAE=134.8958 Epoch: 490/500MAE=135.0161 MAE=135.0670 MAE=134.9972 MAE=134.7818 MAE=134.7996 MAE=134.9428 MAE=134.8799 MAE=134.9190 MAE=134.9001 MAE=134.9757 Epoch: 500/500MAE=134.8945 MAE=129.1340 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 114.189 +/- 14.945\n",
      "\n",
      "Epoch: 1/500MAE=1548.9519 MAE=1546.4432 MAE=1544.0967 MAE=1541.2427 MAE=1539.0787 MAE=1535.2733 MAE=1533.4321 MAE=1529.1469 MAE=1525.0953 Epoch: 10/500MAE=1520.7842 MAE=1514.4247 MAE=1510.3005 MAE=1506.6051 MAE=1500.4581 MAE=1491.0775 MAE=1486.1128 MAE=1478.9481 MAE=1471.2422 MAE=1462.4591 Epoch: 20/500MAE=1454.9807 MAE=1448.4871 MAE=1438.5059 MAE=1430.0433 MAE=1423.8842 MAE=1414.0751 MAE=1392.4509 MAE=1388.0132 MAE=1372.6434 MAE=1362.8073 Epoch: 30/500MAE=1353.4058 MAE=1338.0896 MAE=1319.7916 MAE=1301.3950 MAE=1291.1084 MAE=1282.2756 MAE=1273.9750 MAE=1253.5715 MAE=1239.5276 MAE=1225.5487 Epoch: 40/500MAE=1206.4445 MAE=1175.7863 MAE=1171.8082 MAE=1125.1085 MAE=1150.2948 MAE=1114.5406 MAE=1088.2566 MAE=1058.3120 MAE=1076.9725 MAE=1048.1674 Epoch: 50/500MAE=1023.7869 MAE=992.3341 MAE=979.2891 MAE=935.2083 MAE=935.8508 MAE=944.3527 MAE=892.6926 MAE=868.1983 MAE=847.3686 MAE=822.6115 Epoch: 60/500MAE=786.4777 MAE=766.5228 MAE=744.3514 MAE=750.7068 MAE=697.5894 MAE=676.8827 MAE=645.5527 MAE=601.8050 MAE=582.7238 MAE=582.2946 Epoch: 70/500MAE=580.3856 MAE=530.7472 MAE=442.7172 MAE=490.5210 MAE=485.7931 MAE=457.3898 MAE=384.2228 MAE=387.2039 MAE=372.6879 MAE=330.9129 Epoch: 80/500MAE=304.9964 MAE=217.5933 MAE=256.7834 MAE=246.2021 MAE=152.2128 MAE=182.1290 MAE=126.8957 MAE=157.4559 MAE=179.6302 MAE=139.2986 Epoch: 90/500MAE=163.9038 MAE=125.4095 MAE=115.5942 MAE=117.7562 MAE=107.7755 MAE=108.2160 MAE=104.0136 MAE=89.8175 MAE=94.0575 MAE=87.5954 Epoch: 100/500MAE=90.6983 MAE=94.3252 MAE=86.0254 MAE=86.4928 MAE=78.8638 MAE=82.9981 MAE=76.4876 MAE=75.3428 MAE=81.5486 MAE=197.5105 Epoch: 110/500MAE=204.3604 MAE=211.1971 MAE=197.2504 MAE=192.8478 MAE=195.6555 MAE=199.4373 MAE=194.6824 MAE=195.9685 MAE=196.5819 MAE=196.0333 Epoch: 120/500MAE=196.3822 MAE=194.2723 MAE=195.1747 MAE=196.9356 MAE=191.9843 MAE=194.2395 MAE=193.6355 MAE=195.1654 MAE=194.5406 MAE=194.4882 Epoch: 130/500MAE=195.5331 MAE=192.5023 MAE=194.1009 MAE=193.6816 MAE=193.8442 MAE=193.7862 MAE=194.0102 MAE=194.1981 MAE=193.0355 MAE=194.6646 Epoch: 140/500MAE=193.6072 MAE=194.5329 MAE=193.6354 MAE=194.1930 MAE=194.2756 MAE=194.6880 MAE=193.5557 MAE=193.4858 MAE=193.2312 MAE=194.3374 Epoch: 150/500MAE=192.7806 MAE=192.7179 MAE=193.0776 MAE=193.6198 MAE=192.5508 MAE=193.7608 MAE=193.5712 MAE=193.5471 MAE=193.6785 MAE=193.5666 Epoch: 160/500MAE=192.9549 MAE=192.7863 MAE=192.9350 MAE=193.2054 MAE=193.2481 MAE=193.1142 MAE=192.9969 MAE=192.9590 MAE=193.0085 MAE=193.2626 Epoch: 170/500MAE=193.2789 MAE=193.3600 MAE=193.1898 MAE=193.1986 MAE=193.3133 MAE=193.3041 MAE=193.2340 MAE=193.3052 MAE=193.2497 MAE=193.2561 Epoch: 180/500MAE=193.2315 MAE=193.2196 MAE=193.2237 MAE=193.2254 MAE=193.2357 MAE=193.2272 MAE=193.2223 MAE=193.0957 MAE=193.2442 MAE=193.2503 Epoch: 190/500MAE=193.2919 MAE=193.2695 MAE=193.2456 MAE=193.2549 MAE=193.2165 MAE=193.2139 MAE=193.2258 MAE=193.2448 MAE=193.2462 MAE=193.2430 Epoch: 200/500MAE=193.1192 MAE=193.2158 MAE=193.2679 MAE=193.2374 MAE=193.2480 MAE=193.2505 MAE=193.2554 MAE=193.2668 MAE=193.2858 MAE=193.2804 Epoch: 210/500MAE=193.2799 MAE=193.3177 MAE=193.2492 MAE=193.2410 MAE=193.2262 MAE=193.1437 MAE=193.2177 MAE=193.2545 MAE=193.2464 MAE=193.2839 Epoch: 220/500MAE=193.2431 MAE=193.2731 MAE=193.2807 MAE=193.2833 MAE=193.3082 MAE=193.2987 MAE=193.3132 MAE=193.3378 MAE=193.3871 MAE=193.3228 Epoch: 230/500MAE=193.3169 MAE=193.1405 MAE=193.2925 MAE=193.1527 MAE=193.1517 MAE=193.2462 MAE=193.2735 MAE=193.1057 MAE=193.2393 MAE=193.1530 Epoch: 240/500MAE=193.2993 MAE=193.3164 MAE=193.3097 MAE=193.3109 MAE=193.3270 MAE=193.3591 MAE=193.3744 MAE=193.3538 MAE=193.3895 MAE=193.3600 Epoch: 250/500MAE=193.3665 MAE=193.3344 MAE=193.3928 MAE=193.3990 MAE=193.4013 MAE=193.3747 MAE=193.3840 MAE=193.4160 MAE=193.4191 MAE=193.4173 Epoch: 260/500MAE=193.4960 MAE=193.3664 MAE=193.3694 MAE=193.3668 MAE=193.3911 MAE=193.3337 MAE=193.3251 MAE=193.4459 MAE=193.3754 MAE=193.3149 Epoch: 270/500MAE=193.4349 MAE=193.4737 MAE=193.2805 MAE=193.4018 MAE=193.4140 MAE=193.4347 MAE=193.4624 MAE=193.4358 MAE=193.4912 MAE=193.4199 Epoch: 280/500MAE=193.4325 MAE=193.4316 MAE=193.4091 MAE=193.4580 MAE=193.3556 MAE=193.3542 MAE=193.3347 MAE=193.3833 MAE=193.3760 MAE=193.3957 Epoch: 290/500MAE=193.3895 MAE=193.4084 MAE=193.3469 MAE=193.3591 MAE=193.3498 MAE=193.3401 MAE=193.3672 MAE=193.3207 MAE=193.2836 MAE=193.1540 Epoch: 300/500MAE=193.2734 MAE=193.2774 MAE=193.2580 MAE=193.3073 MAE=193.2609 MAE=193.2728 MAE=193.2738 MAE=193.3532 MAE=193.3118 MAE=193.2889 Epoch: 310/500MAE=193.2486 MAE=193.2501 MAE=193.2614 MAE=193.2406 MAE=193.2632 MAE=193.2543 MAE=193.1761 MAE=193.1852 MAE=193.1945 MAE=193.2067 Epoch: 320/500MAE=193.0955 MAE=193.2641 MAE=193.2801 MAE=193.2810 MAE=193.1247 MAE=193.2793 MAE=193.2853 MAE=193.2820 MAE=193.2836 MAE=193.2861 Epoch: 330/500MAE=193.2947 MAE=193.2715 MAE=193.2587 MAE=193.2719 MAE=193.2898 MAE=193.2774 MAE=193.2328 MAE=193.2507 MAE=193.2687 MAE=193.2742 Epoch: 340/500MAE=193.2463 MAE=193.2350 MAE=193.2364 MAE=193.0781 MAE=193.1842 MAE=193.2108 MAE=193.2348 MAE=193.2581 MAE=193.2352 MAE=193.2579 Epoch: 350/500MAE=193.2378 MAE=193.2431 MAE=193.2124 MAE=193.2276 MAE=193.1923 MAE=193.2068 MAE=193.1901 MAE=193.2030 MAE=193.1641 MAE=193.1454 Epoch: 360/500MAE=193.1576 MAE=193.1579 MAE=193.1921 MAE=193.2373 MAE=193.1939 MAE=193.1705 MAE=193.1401 MAE=193.1830 MAE=193.1678 MAE=193.1583 Epoch: 370/500MAE=193.1580 MAE=193.2639 MAE=193.2056 MAE=193.1881 MAE=193.1754 MAE=193.1642 MAE=193.1550 MAE=193.1342 MAE=193.1483 MAE=193.1689 Epoch: 380/500MAE=193.2288 MAE=193.3006 MAE=193.2075 MAE=193.1885 MAE=193.1966 MAE=193.2068 MAE=193.2065 MAE=193.2038 MAE=193.0701 MAE=192.9973 Epoch: 390/500MAE=193.0947 MAE=193.1784 MAE=193.1250 MAE=193.1245 MAE=193.1217 MAE=193.1311 MAE=193.1370 MAE=193.1520 MAE=193.1584 MAE=193.1560 Epoch: 400/500MAE=193.1774 MAE=193.1612 MAE=193.1422 MAE=193.1558 MAE=193.1352 MAE=193.1492 MAE=193.1667 MAE=193.1074 MAE=193.0821 MAE=193.1163 Epoch: 410/500MAE=193.1391 MAE=193.1029 MAE=193.1295 MAE=193.1191 MAE=193.1036 MAE=193.1051 MAE=193.1034 MAE=193.0775 MAE=193.1151 MAE=193.0852 Epoch: 420/500MAE=192.9657 MAE=193.0981 MAE=193.1104 MAE=192.9634 MAE=193.0955 MAE=193.1257 MAE=193.1405 MAE=193.0208 MAE=193.1023 MAE=193.1872 Epoch: 430/500MAE=193.2020 MAE=193.2142 MAE=193.2071 MAE=193.1533 MAE=193.1796 MAE=193.1682 MAE=193.1814 MAE=193.2389 MAE=193.2473 MAE=193.2463 Epoch: 440/500MAE=193.1505 MAE=193.2195 MAE=193.2713 MAE=193.2649 MAE=193.2296 MAE=193.2400 MAE=193.3954 MAE=193.2795 MAE=193.2577 MAE=193.2609 Epoch: 450/500MAE=193.2490 MAE=193.2689 MAE=193.2869 MAE=193.2677 MAE=193.2778 MAE=193.2854 MAE=193.2683 MAE=193.2561 MAE=193.2547 MAE=193.2546 Epoch: 460/500MAE=193.3064 MAE=193.2579 MAE=193.2949 MAE=193.2462 MAE=193.2521 MAE=193.2720 MAE=193.2898 MAE=193.2289 MAE=193.2154 MAE=193.1511 Epoch: 470/500MAE=193.1914 MAE=193.1708 MAE=193.1885 MAE=193.2985 MAE=193.2647 MAE=193.2493 MAE=193.2330 MAE=193.2262 MAE=193.2320 MAE=193.2172 Epoch: 480/500MAE=193.1817 MAE=193.1738 MAE=193.1528 MAE=193.1687 MAE=193.1682 MAE=193.1538 MAE=193.1549 MAE=193.1662 MAE=193.1662 MAE=193.1749 Epoch: 490/500MAE=193.1847 MAE=193.1915 MAE=193.1562 MAE=193.1420 MAE=193.1274 MAE=193.1243 MAE=193.1477 MAE=193.1474 MAE=193.0751 MAE=193.1880 Epoch: 500/500MAE=193.1178 MAE=196.1991 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 141.526 +/- 40.540\n",
      "\n",
      "Epoch: 1/500MAE=1549.2010 MAE=1546.8777 MAE=1544.3541 MAE=1542.5881 MAE=1539.5573 MAE=1536.2422 MAE=1532.0959 MAE=1528.8411 MAE=1524.6487 Epoch: 10/500MAE=1520.8871 MAE=1516.5424 MAE=1510.2318 MAE=1503.3333 MAE=1499.5781 MAE=1489.4867 MAE=1483.1047 MAE=1480.4553 MAE=1466.5416 MAE=1468.9788 Epoch: 20/500MAE=1459.8108 MAE=1449.3552 MAE=1443.7688 MAE=1434.8856 MAE=1430.0725 MAE=1420.5558 MAE=1399.7440 MAE=1384.0344 MAE=1372.5583 MAE=1352.5848 Epoch: 30/500MAE=1348.8396 MAE=1335.2347 MAE=1329.0724 MAE=1306.2881 MAE=1288.4226 MAE=1273.6082 MAE=1262.4612 MAE=1256.9154 MAE=1239.2156 MAE=1214.9729 Epoch: 40/500MAE=1186.2419 MAE=1180.3826 MAE=1155.6741 MAE=1145.5698 MAE=1127.0242 MAE=1096.7136 MAE=1075.8455 MAE=1059.1266 MAE=1048.0037 MAE=1035.9502 Epoch: 50/500MAE=1007.9245 MAE=976.2228 MAE=963.3315 MAE=949.7018 MAE=942.8940 MAE=909.9526 MAE=908.1254 MAE=867.1986 MAE=835.6025 MAE=813.5928 Epoch: 60/500MAE=810.7390 MAE=812.5594 MAE=765.8340 MAE=758.4703 MAE=752.7028 MAE=689.7579 MAE=648.7988 MAE=629.6954 MAE=605.4708 MAE=599.1606 Epoch: 70/500MAE=576.2894 MAE=605.7432 MAE=556.9024 MAE=557.0392 MAE=540.9389 MAE=462.8073 MAE=423.8942 MAE=402.8507 MAE=422.2735 MAE=354.2927 Epoch: 80/500MAE=354.1818 MAE=317.7361 MAE=319.1281 MAE=269.2229 MAE=263.4882 MAE=220.0421 MAE=237.8317 MAE=165.3542 MAE=218.6638 MAE=175.6078 Epoch: 90/500MAE=155.1920 MAE=141.4622 MAE=146.3225 MAE=126.0454 MAE=129.7607 MAE=114.9323 MAE=120.6890 MAE=111.0792 MAE=116.0292 MAE=100.4797 Epoch: 100/500MAE=94.9349 MAE=95.4296 MAE=92.3484 MAE=98.0054 MAE=93.9233 MAE=91.9994 MAE=93.7437 MAE=96.2212 MAE=91.1479 MAE=91.7583 Epoch: 110/500MAE=90.9473 MAE=92.6032 MAE=90.6033 MAE=89.3667 MAE=85.3811 MAE=86.6935 MAE=84.7208 MAE=84.7084 MAE=85.1008 MAE=87.8519 Epoch: 120/500MAE=87.2381 MAE=88.6055 MAE=85.0885 MAE=84.1854 MAE=84.3710 MAE=89.9737 MAE=86.8131 MAE=82.7545 MAE=82.4415 MAE=82.7092 Epoch: 130/500MAE=82.2298 MAE=80.8523 MAE=87.2109 MAE=79.9011 MAE=80.1966 MAE=81.5588 MAE=80.7208 MAE=79.3796 MAE=78.9606 MAE=80.8521 Epoch: 140/500MAE=83.6167 MAE=81.1769 MAE=82.8333 MAE=82.2330 MAE=79.3637 MAE=79.6442 MAE=80.3824 MAE=78.8397 MAE=78.3529 MAE=77.7224 Epoch: 150/500MAE=78.5900 MAE=78.1646 MAE=77.8791 MAE=78.0347 MAE=78.4005 MAE=77.9940 MAE=78.3977 MAE=78.4264 MAE=78.1422 MAE=78.1879 Epoch: 160/500MAE=78.0546 MAE=78.1974 MAE=77.8969 MAE=78.0036 MAE=78.0811 MAE=78.1640 MAE=78.2592 MAE=78.2485 MAE=78.4347 MAE=78.1286 Epoch: 170/500MAE=78.2382 MAE=78.1930 MAE=78.1571 MAE=78.1417 MAE=78.2948 MAE=78.0988 MAE=78.2007 MAE=78.2961 MAE=78.0895 MAE=78.0799 Epoch: 180/500MAE=78.1229 MAE=78.1945 MAE=78.1140 MAE=78.0920 MAE=78.0925 MAE=78.0498 MAE=78.0932 MAE=78.1042 MAE=78.1534 MAE=78.0779 Epoch: 190/500MAE=78.0899 MAE=78.1144 MAE=78.0841 MAE=78.1007 MAE=78.0676 MAE=77.9984 MAE=78.2910 MAE=78.0881 MAE=78.1488 MAE=78.1232 Epoch: 200/500MAE=78.1547 MAE=78.2496 MAE=78.0567 MAE=78.0918 MAE=78.0823 MAE=78.2837 MAE=78.0186 MAE=78.0693 MAE=78.2495 MAE=78.0724 Epoch: 210/500MAE=78.0458 MAE=78.0849 MAE=78.0429 MAE=78.0703 MAE=78.0518 MAE=78.0157 MAE=78.0879 MAE=78.0328 MAE=78.0843 MAE=77.9941 Epoch: 220/500MAE=78.0359 MAE=77.9128 MAE=78.0069 MAE=78.0038 MAE=77.9908 MAE=78.0160 MAE=78.0168 MAE=78.0341 MAE=78.0342 MAE=78.0041 Epoch: 230/500MAE=78.0035 MAE=78.1069 MAE=78.0146 MAE=77.9760 MAE=77.9904 MAE=77.9804 MAE=78.0255 MAE=78.0643 MAE=78.0061 MAE=78.1899 Epoch: 240/500MAE=77.9706 MAE=78.0345 MAE=78.0020 MAE=77.8602 MAE=78.0207 MAE=77.9899 MAE=77.9909 MAE=78.0675 MAE=78.0316 MAE=78.0849 Epoch: 250/500MAE=78.0175 MAE=77.8750 MAE=77.9666 MAE=77.9440 MAE=78.0435 MAE=78.0524 MAE=77.9981 MAE=78.1097 MAE=78.0939 MAE=78.1155 Epoch: 260/500MAE=78.1144 MAE=78.1309 MAE=78.1873 MAE=78.1193 MAE=78.1143 MAE=78.2685 MAE=78.1093 MAE=78.1332 MAE=78.1295 MAE=78.0246 Epoch: 270/500MAE=78.0255 MAE=78.2941 MAE=78.0846 MAE=78.1380 MAE=78.1438 MAE=78.0339 MAE=78.1088 MAE=77.9735 MAE=78.1305 MAE=78.0731 Epoch: 280/500MAE=78.0482 MAE=78.0350 MAE=78.0586 MAE=78.0370 MAE=77.9958 MAE=78.0126 MAE=78.0974 MAE=78.1591 MAE=78.1974 MAE=78.0629 Epoch: 290/500MAE=78.1361 MAE=78.0275 MAE=78.1064 MAE=78.1489 MAE=78.1390 MAE=78.1236 MAE=78.1790 MAE=78.2466 MAE=78.1511 MAE=78.2072 MAE=81.6570 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 126.559 +/- 43.642\n",
      "\n",
      "Epoch: 1/500MAE=1549.2953 MAE=1546.7559 MAE=1544.4117 MAE=1541.9828 MAE=1539.7546 MAE=1536.5959 MAE=1533.0717 MAE=1530.1803 MAE=1526.2070 Epoch: 10/500MAE=1521.4987 MAE=1516.1353 MAE=1511.8328 MAE=1504.1821 MAE=1500.9600 MAE=1492.3710 MAE=1490.7244 MAE=1480.8677 MAE=1473.5905 MAE=1464.3356 Epoch: 20/500MAE=1452.3317 MAE=1450.1364 MAE=1437.5469 MAE=1430.0806 MAE=1417.2371 MAE=1408.3958 MAE=1395.3147 MAE=1388.0668 MAE=1385.3574 MAE=1362.4325 Epoch: 30/500MAE=1352.4227 MAE=1345.7052 MAE=1329.0603 MAE=1313.7274 MAE=1305.9187 MAE=1284.9451 MAE=1267.2108 MAE=1252.4093 MAE=1242.9858 MAE=1223.3398 Epoch: 40/500MAE=1206.6189 MAE=1190.6235 MAE=1174.4910 MAE=1155.2737 MAE=1136.9600 MAE=1134.7322 MAE=1102.2002 MAE=1067.1853 MAE=1061.6948 MAE=1052.0503 Epoch: 50/500MAE=1033.1018 MAE=1013.8580 MAE=976.6132 MAE=975.0792 MAE=938.9313 MAE=912.8549 MAE=891.8088 MAE=877.1337 MAE=856.2276 MAE=867.0380 Epoch: 60/500MAE=831.1115 MAE=794.9999 MAE=782.4156 MAE=760.1260 MAE=719.9222 MAE=750.0194 MAE=664.3961 MAE=674.8230 MAE=638.6974 MAE=614.2872 Epoch: 70/500MAE=573.5788 MAE=557.9077 MAE=537.9041 MAE=538.3063 MAE=499.5331 MAE=474.4613 MAE=461.3916 MAE=421.3137 MAE=420.5068 MAE=390.2880 Epoch: 80/500MAE=344.7253 MAE=307.2830 MAE=304.4586 MAE=280.5055 MAE=301.5903 MAE=260.4123 MAE=240.5668 MAE=236.4895 MAE=211.5277 MAE=178.6186 Epoch: 90/500MAE=154.3435 MAE=155.6464 MAE=140.4579 MAE=126.6286 MAE=120.3165 MAE=124.2988 MAE=108.8129 MAE=115.8641 MAE=110.6157 MAE=110.1646 Epoch: 100/500MAE=183.2666 MAE=106.9117 MAE=106.0868 MAE=110.1554 MAE=102.8155 MAE=93.0804 MAE=94.6659 MAE=96.1061 MAE=91.0788 MAE=92.4814 Epoch: 110/500MAE=94.8095 MAE=89.7457 MAE=94.6046 MAE=90.8087 MAE=91.7193 MAE=91.0672 MAE=86.4213 MAE=86.3111 MAE=85.0709 MAE=85.5445 Epoch: 120/500MAE=83.7734 MAE=86.0968 MAE=85.4730 MAE=84.9994 MAE=88.3397 MAE=86.8420 MAE=85.9426 MAE=85.7048 MAE=85.8991 MAE=85.9699 Epoch: 130/500MAE=84.0296 MAE=85.3066 MAE=84.8916 MAE=85.4471 MAE=85.6906 MAE=85.2867 MAE=85.1452 MAE=85.4584 MAE=85.4635 MAE=85.2488 Epoch: 140/500MAE=85.7365 MAE=85.7770 MAE=86.0164 MAE=85.7551 MAE=86.0879 MAE=85.8964 MAE=86.1722 MAE=85.9915 MAE=86.2353 MAE=86.3309 Epoch: 150/500MAE=86.3685 MAE=86.3435 MAE=86.3026 MAE=86.2913 MAE=86.0762 MAE=86.1337 MAE=86.0729 MAE=86.0891 MAE=86.0942 MAE=86.1051 Epoch: 160/500MAE=86.0350 MAE=85.8402 MAE=86.0216 MAE=85.9189 MAE=85.9798 MAE=85.8382 MAE=86.0871 MAE=85.8665 MAE=85.8434 MAE=86.0483 Epoch: 170/500MAE=86.0720 MAE=86.1191 MAE=85.9985 MAE=86.0957 MAE=86.0976 MAE=85.9172 MAE=86.0758 MAE=86.0911 MAE=86.0142 MAE=85.8172 Epoch: 180/500MAE=85.7277 MAE=85.7313 MAE=85.7645 MAE=85.7926 MAE=86.0323 MAE=86.1202 MAE=86.0786 MAE=85.8560 MAE=85.9596 MAE=85.7566 Epoch: 190/500MAE=85.8555 MAE=85.8236 MAE=86.1202 MAE=86.1458 MAE=86.1125 MAE=86.1536 MAE=86.2431 MAE=86.2715 MAE=85.9118 MAE=86.0689 Epoch: 200/500MAE=86.0228 MAE=86.0291 MAE=85.7639 MAE=85.7614 MAE=85.7147 MAE=85.9544 MAE=86.0210 MAE=85.9717 MAE=85.9557 MAE=86.0953 Epoch: 210/500MAE=86.1245 MAE=86.1411 MAE=86.0727 MAE=85.9215 MAE=85.7380 MAE=86.0514 MAE=86.1847 MAE=85.8613 MAE=86.0816 MAE=85.8475 Epoch: 220/500MAE=86.0538 MAE=85.8270 MAE=85.8774 MAE=85.8284 MAE=86.0491 MAE=86.1168 MAE=85.8416 MAE=86.0778 MAE=85.8413 MAE=86.1116 Epoch: 230/500MAE=86.0996 MAE=85.9211 MAE=86.0491 MAE=85.8229 MAE=86.0513 MAE=85.9562 MAE=86.1269 MAE=85.8900 MAE=86.0523 MAE=86.0860 Epoch: 240/500MAE=86.1036 MAE=86.1604 MAE=86.1429 MAE=86.0809 MAE=86.0782 MAE=85.9523 MAE=85.8852 MAE=85.8818 MAE=86.1416 MAE=85.9204 Epoch: 250/500MAE=86.1544 MAE=86.1548 MAE=86.1962 MAE=85.9296 MAE=86.1250 MAE=86.1409 MAE=86.0214 MAE=86.1022 MAE=86.0891 MAE=86.0739 Epoch: 260/500MAE=86.0902 MAE=85.8025 MAE=85.9998 MAE=86.0306 MAE=86.0511 MAE=86.0523 MAE=85.7888 MAE=85.7221 MAE=86.0260 MAE=86.0541 Epoch: 270/500MAE=86.1622 MAE=196.0349 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 140.454 +/- 47.917\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1549.1379 MAE=1545.7874 MAE=1543.7170 MAE=1541.8948 MAE=1538.3516 MAE=1534.9131 MAE=1531.8756 MAE=1528.4148 MAE=1524.7236 Epoch: 10/500MAE=1522.7266 MAE=1513.6292 MAE=1507.8632 MAE=1504.0374 MAE=1498.8795 MAE=1493.3981 MAE=1482.6688 MAE=1479.8533 MAE=1474.1448 MAE=1465.6184 Epoch: 20/500MAE=1457.6523 MAE=1444.1964 MAE=1441.1992 MAE=1421.9688 MAE=1410.9808 MAE=1407.1954 MAE=1402.8997 MAE=1380.3784 MAE=1368.1042 MAE=1358.6733 Epoch: 30/500MAE=1339.4607 MAE=1326.7085 MAE=1317.9531 MAE=1297.8745 MAE=1288.6499 MAE=1276.1073 MAE=1264.9365 MAE=1242.5724 MAE=1224.9683 MAE=1219.1221 Epoch: 40/500MAE=1184.0330 MAE=1183.7219 MAE=1167.0286 MAE=1140.0867 MAE=1143.8354 MAE=1123.2697 MAE=1101.4128 MAE=1080.7026 MAE=1053.4116 MAE=1039.0491 Epoch: 50/500MAE=1057.1128 MAE=992.1333 MAE=976.2825 MAE=959.4343 MAE=958.9679 MAE=945.4196 MAE=893.3973 MAE=861.0932 MAE=846.2196 MAE=852.6763 Epoch: 60/500MAE=784.2678 MAE=743.3444 MAE=766.7988 MAE=732.6235 MAE=646.1711 MAE=765.6743 MAE=625.8350 MAE=668.4498 MAE=661.3281 MAE=588.2993 Epoch: 70/500MAE=577.5480 MAE=592.8107 MAE=481.0271 MAE=535.3188 MAE=510.3903 MAE=448.6591 MAE=406.9874 MAE=382.7299 MAE=346.8007 MAE=309.5987 Epoch: 80/500MAE=302.2270 MAE=372.8350 MAE=239.2337 MAE=292.3763 MAE=246.1076 MAE=212.2722 MAE=241.8317 MAE=162.1658 MAE=202.8322 MAE=172.3967 Epoch: 90/500MAE=153.2518 MAE=140.1003 MAE=157.1599 MAE=141.7402 MAE=199.9777 MAE=191.4650 MAE=157.8176 MAE=140.4142 MAE=131.3736 MAE=139.6154 Epoch: 100/500MAE=127.5408 MAE=131.8462 MAE=121.9910 MAE=123.5368 MAE=119.1792 MAE=118.9901 MAE=123.2187 MAE=117.9691 MAE=123.3976 MAE=117.8043 Epoch: 110/500MAE=111.6755 MAE=111.6540 MAE=110.8216 MAE=117.9072 MAE=117.8918 MAE=111.9600 MAE=118.3329 MAE=115.1067 MAE=105.7851 MAE=106.2959 Epoch: 120/500MAE=105.3694 MAE=104.6928 MAE=108.4526 MAE=103.6571 MAE=101.0996 MAE=109.1241 MAE=101.6389 MAE=105.9522 MAE=102.0258 MAE=99.0712 Epoch: 130/500MAE=98.7360 MAE=100.1079 MAE=97.7204 MAE=99.1813 MAE=97.9768 MAE=97.9909 MAE=97.7796 MAE=97.5582 MAE=97.0552 MAE=97.0182 Epoch: 140/500MAE=96.0589 MAE=97.2165 MAE=96.1319 MAE=96.1743 MAE=95.4711 MAE=95.8820 MAE=96.5838 MAE=96.0289 MAE=95.7800 MAE=96.1381 Epoch: 150/500MAE=96.7327 MAE=95.9745 MAE=96.3887 MAE=96.2543 MAE=96.4471 MAE=96.3692 MAE=96.2223 MAE=96.0752 MAE=96.2640 MAE=95.8837 Epoch: 160/500MAE=95.9749 MAE=96.0166 MAE=95.9714 MAE=95.9788 MAE=95.9944 MAE=95.9957 MAE=96.0211 MAE=95.9354 MAE=96.1386 MAE=96.0023 Epoch: 170/500MAE=96.0471 MAE=96.0135 MAE=96.2199 MAE=96.0193 MAE=96.0288 MAE=96.0007 MAE=95.9999 MAE=96.0317 MAE=96.0625 MAE=96.0699 Epoch: 180/500MAE=96.0717 MAE=96.0813 MAE=96.0645 MAE=96.0700 MAE=96.0285 MAE=96.1735 MAE=96.0618 MAE=96.0439 MAE=96.0758 MAE=96.0398 Epoch: 190/500MAE=96.0426 MAE=95.9519 MAE=96.0976 MAE=96.0088 MAE=96.1128 MAE=96.0321 MAE=95.9545 MAE=95.9640 MAE=96.0312 MAE=96.0344 Epoch: 200/500MAE=95.9773 MAE=95.9814 MAE=96.0114 MAE=96.0753 MAE=96.0650 MAE=96.1637 MAE=96.1500 MAE=96.1229 MAE=96.1702 MAE=96.0409 Epoch: 210/500MAE=96.0729 MAE=96.0477 MAE=95.9597 MAE=95.9947 MAE=96.0481 MAE=96.0177 MAE=96.1024 MAE=96.0686 MAE=96.0067 MAE=96.0584 Epoch: 220/500MAE=96.0633 MAE=96.0430 MAE=96.0845 MAE=95.9727 MAE=96.0538 MAE=96.0153 MAE=96.0313 MAE=96.0343 MAE=96.1245 MAE=96.1232 Epoch: 230/500MAE=96.1171 MAE=96.1459 MAE=96.1157 MAE=96.0883 MAE=96.2015 MAE=96.1064 MAE=96.0590 MAE=96.0492 MAE=96.1259 MAE=96.0368 Epoch: 240/500MAE=96.0605 MAE=96.0535 MAE=95.9955 MAE=96.0596 MAE=95.9867 MAE=96.1072 MAE=96.0092 MAE=96.0309 MAE=96.0937 MAE=96.0271 Epoch: 250/500MAE=96.0337 MAE=95.9525 MAE=96.0592 MAE=96.0598 MAE=95.9769 MAE=96.1285 MAE=96.1474 MAE=95.9189 MAE=95.9637 MAE=96.1075 Epoch: 260/500MAE=96.1084 MAE=95.9765 MAE=95.9619 MAE=96.2149 MAE=95.9683 MAE=95.9310 MAE=96.0250 MAE=96.0423 MAE=96.1138 MAE=96.0419 Epoch: 270/500MAE=95.9908 MAE=95.7324 MAE=95.9176 MAE=95.9472 MAE=95.8146 MAE=95.7259 MAE=95.7276 MAE=96.0158 MAE=96.0002 MAE=95.9626 Epoch: 280/500MAE=95.9493 MAE=96.0324 MAE=95.8608 MAE=95.7814 MAE=95.8990 MAE=95.9608 MAE=96.0028 MAE=96.0383 MAE=95.9782 MAE=96.0558 Epoch: 290/500MAE=95.9967 MAE=95.9344 MAE=95.8460 MAE=95.8911 MAE=95.7038 MAE=99.2300 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 99.230 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1549.1736 MAE=1546.9012 MAE=1544.2688 MAE=1542.0323 MAE=1539.0355 MAE=1535.5720 MAE=1532.0671 MAE=1527.9421 MAE=1525.1071 Epoch: 10/500MAE=1522.0935 MAE=1515.8068 MAE=1509.9055 MAE=1503.7053 MAE=1499.0131 MAE=1489.8521 MAE=1484.4888 MAE=1479.2123 MAE=1470.9344 MAE=1460.3831 Epoch: 20/500MAE=1456.3230 MAE=1446.4723 MAE=1432.6436 MAE=1426.9747 MAE=1414.8494 MAE=1410.1553 MAE=1392.8297 MAE=1381.0331 MAE=1374.5720 MAE=1366.0796 Epoch: 30/500MAE=1356.8026 MAE=1328.6622 MAE=1315.2795 MAE=1307.7480 MAE=1278.6899 MAE=1265.4100 MAE=1260.8604 MAE=1236.5629 MAE=1238.7673 MAE=1205.1106 Epoch: 40/500MAE=1186.4629 MAE=1148.9695 MAE=1190.0963 MAE=1110.5674 MAE=1122.3342 MAE=1090.5291 MAE=1092.6056 MAE=1074.0364 MAE=1049.1447 MAE=1025.9612 Epoch: 50/500MAE=1010.6499 MAE=1000.1658 MAE=945.4315 MAE=947.5325 MAE=948.0837 MAE=920.7603 MAE=878.1085 MAE=875.3276 MAE=773.4918 MAE=824.2158 Epoch: 60/500MAE=748.6250 MAE=787.2053 MAE=747.1595 MAE=739.3160 MAE=663.1862 MAE=652.6852 MAE=688.5676 MAE=616.4431 MAE=545.3655 MAE=575.9598 Epoch: 70/500MAE=586.6920 MAE=540.1687 MAE=484.5060 MAE=520.3728 MAE=436.8835 MAE=455.2650 MAE=359.5520 MAE=442.2707 MAE=352.4343 MAE=282.1144 Epoch: 80/500MAE=240.6487 MAE=303.2267 MAE=302.0580 MAE=272.3038 MAE=229.7179 MAE=254.9249 MAE=231.2260 MAE=202.2926 MAE=202.8734 MAE=179.0661 Epoch: 90/500MAE=155.3556 MAE=156.8517 MAE=178.4466 MAE=130.5993 MAE=127.9212 MAE=130.7430 MAE=137.5226 MAE=110.2032 MAE=118.1941 MAE=113.7558 Epoch: 100/500MAE=115.5146 MAE=116.7471 MAE=107.9231 MAE=103.7989 MAE=104.2501 MAE=110.8947 MAE=107.6063 MAE=114.3426 MAE=99.1752 MAE=103.3557 Epoch: 110/500MAE=102.7915 MAE=100.1354 MAE=104.5795 MAE=97.9365 MAE=96.2338 MAE=96.4578 MAE=97.4847 MAE=98.1030 MAE=98.9498 MAE=97.2374 Epoch: 120/500MAE=98.0280 MAE=97.2784 MAE=97.5346 MAE=97.3367 MAE=97.3072 MAE=97.2299 MAE=97.0130 MAE=97.5583 MAE=97.1733 MAE=96.9982 Epoch: 130/500MAE=96.7769 MAE=96.8778 MAE=97.0679 MAE=96.9987 MAE=97.3690 MAE=97.3453 MAE=97.2357 MAE=96.9870 MAE=97.1428 MAE=97.1897 Epoch: 140/500MAE=97.0330 MAE=97.1761 MAE=96.9904 MAE=97.1663 MAE=97.2586 MAE=97.1198 MAE=97.1963 MAE=97.2007 MAE=97.1743 MAE=97.3166 Epoch: 150/500MAE=96.9135 MAE=97.2015 MAE=97.2325 MAE=97.1210 MAE=97.1318 MAE=97.2182 MAE=96.9475 MAE=97.1599 MAE=97.1872 MAE=96.9824 Epoch: 160/500MAE=97.0263 MAE=97.1101 MAE=97.1394 MAE=97.1478 MAE=96.9969 MAE=97.2801 MAE=97.2468 MAE=97.0686 MAE=97.3565 MAE=97.1927 Epoch: 170/500MAE=97.0697 MAE=97.1654 MAE=97.2075 MAE=96.9705 MAE=97.1017 MAE=97.1618 MAE=97.1575 MAE=97.1879 MAE=97.2023 MAE=97.1492 Epoch: 180/500MAE=97.0666 MAE=97.1979 MAE=97.0486 MAE=97.1753 MAE=97.0939 MAE=97.0941 MAE=96.9775 MAE=97.2458 MAE=97.0895 MAE=97.2216 Epoch: 190/500MAE=97.1992 MAE=97.0222 MAE=97.1535 MAE=97.2643 MAE=97.1621 MAE=97.1416 MAE=97.2110 MAE=96.9994 MAE=97.1894 MAE=97.1405 Epoch: 200/500MAE=97.1144 MAE=97.1534 MAE=97.1509 MAE=97.1267 MAE=97.1769 MAE=97.1429 MAE=97.2155 MAE=97.1108 MAE=97.1631 MAE=97.0112 Epoch: 210/500MAE=97.0975 MAE=97.0818 MAE=97.0672 MAE=97.0822 MAE=97.0930 MAE=96.8515 MAE=97.0648 MAE=97.2596 MAE=96.8305 MAE=96.8243 Epoch: 220/500MAE=97.1272 MAE=97.0729 MAE=97.1623 MAE=97.0671 MAE=97.0431 MAE=97.1120 MAE=96.7902 MAE=96.8648 MAE=97.0933 MAE=97.0476 Epoch: 230/500MAE=96.7189 MAE=97.0160 MAE=96.8480 MAE=97.1358 MAE=97.0303 MAE=97.0561 MAE=97.0931 MAE=97.2080 MAE=97.1000 MAE=97.0650 Epoch: 240/500MAE=97.1390 MAE=96.9834 MAE=97.0905 MAE=96.9489 MAE=97.1428 MAE=97.0619 MAE=97.0794 MAE=97.1514 MAE=97.0857 MAE=96.8755 Epoch: 250/500MAE=97.0630 MAE=97.1377 MAE=97.0776 MAE=97.0618 MAE=96.9095 MAE=97.0509 MAE=97.2243 MAE=97.0718 MAE=96.9897 MAE=96.8356 Epoch: 260/500MAE=97.0831 MAE=96.7879 MAE=96.8593 MAE=97.1139 MAE=96.7610 MAE=92.2913 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 95.761 +/- 3.469\n",
      "\n",
      "Epoch: 1/500MAE=1548.2764 MAE=1546.1166 MAE=1543.9186 MAE=1541.3928 MAE=1538.7646 MAE=1536.6179 MAE=1534.5841 MAE=1528.0339 MAE=1524.0072 Epoch: 10/500MAE=1522.0734 MAE=1515.1016 MAE=1510.7720 MAE=1505.0406 MAE=1499.8264 MAE=1493.5977 MAE=1487.7561 MAE=1478.6765 MAE=1473.2593 MAE=1465.5642 Epoch: 20/500MAE=1458.4064 MAE=1452.5737 MAE=1441.1027 MAE=1434.7476 MAE=1426.7744 MAE=1416.8202 MAE=1403.9468 MAE=1390.9832 MAE=1369.5861 MAE=1362.9568 Epoch: 30/500MAE=1357.2422 MAE=1350.7341 MAE=1333.3365 MAE=1304.2607 MAE=1310.8925 MAE=1278.6580 MAE=1263.4885 MAE=1241.9590 MAE=1236.0448 MAE=1223.4492 Epoch: 40/500MAE=1236.8499 MAE=1186.6400 MAE=1166.2019 MAE=1154.9773 MAE=1117.5461 MAE=1131.2823 MAE=1089.7251 MAE=1071.9581 MAE=1044.7290 MAE=1032.3662 Epoch: 50/500MAE=1015.5301 MAE=996.4360 MAE=964.1843 MAE=949.6134 MAE=923.6894 MAE=892.1031 MAE=899.3473 MAE=862.9757 MAE=831.7270 MAE=816.3725 Epoch: 60/500MAE=792.2685 MAE=791.2057 MAE=745.2003 MAE=786.3961 MAE=687.3633 MAE=665.9610 MAE=689.5772 MAE=625.3044 MAE=610.9738 MAE=590.9973 Epoch: 70/500MAE=544.4056 MAE=553.1307 MAE=490.1911 MAE=528.8797 MAE=505.8275 MAE=430.4358 MAE=427.7869 MAE=399.2182 MAE=371.4848 MAE=377.0205 Epoch: 80/500MAE=343.8174 MAE=336.5504 MAE=311.1126 MAE=294.7476 MAE=294.6785 MAE=278.6698 MAE=283.6405 MAE=215.1975 MAE=245.8342 MAE=222.1657 Epoch: 90/500MAE=234.5838 MAE=234.0380 MAE=207.6066 MAE=211.8801 MAE=209.4312 MAE=220.3262 MAE=115.2265 MAE=108.5114 MAE=209.3747 MAE=205.5167 Epoch: 100/500MAE=115.6128 MAE=203.7707 MAE=206.0196 MAE=209.5667 MAE=207.9736 MAE=204.9195 MAE=196.0200 MAE=202.6810 MAE=91.8168 MAE=195.8789 Epoch: 110/500MAE=134.0241 MAE=201.1663 MAE=203.4018 MAE=203.0744 MAE=210.1091 MAE=199.7310 MAE=205.5020 MAE=208.5925 MAE=90.8878 MAE=95.6301 Epoch: 120/500MAE=83.8397 MAE=82.7327 MAE=83.2144 MAE=85.1836 MAE=83.9309 MAE=84.1334 MAE=83.1807 MAE=81.7011 MAE=82.0844 MAE=82.1790 Epoch: 130/500MAE=81.5205 MAE=81.9360 MAE=81.1861 MAE=81.6239 MAE=82.1386 MAE=82.3481 MAE=81.8851 MAE=82.3710 MAE=81.6792 MAE=81.4489 Epoch: 140/500MAE=82.0133 MAE=80.9043 MAE=81.1961 MAE=81.2433 MAE=81.4246 MAE=81.4506 MAE=81.2502 MAE=81.4119 MAE=81.2453 MAE=81.1248 Epoch: 150/500MAE=81.1386 MAE=81.1439 MAE=81.2263 MAE=81.3234 MAE=81.3099 MAE=81.3041 MAE=81.2893 MAE=81.2628 MAE=81.2564 MAE=81.1580 Epoch: 160/500MAE=81.2360 MAE=81.1985 MAE=81.1705 MAE=81.1787 MAE=81.1674 MAE=81.1492 MAE=81.1457 MAE=81.1310 MAE=81.1291 MAE=81.1527 Epoch: 170/500MAE=81.1446 MAE=81.1375 MAE=81.1290 MAE=81.1148 MAE=81.0992 MAE=81.1039 MAE=81.1052 MAE=81.0658 MAE=81.0644 MAE=81.0968 Epoch: 180/500MAE=81.0977 MAE=81.1083 MAE=81.0884 MAE=81.0900 MAE=81.0981 MAE=81.1115 MAE=81.0922 MAE=81.0827 MAE=81.0681 MAE=81.0774 Epoch: 190/500MAE=81.0744 MAE=81.0540 MAE=81.0676 MAE=81.0704 MAE=81.0682 MAE=81.0647 MAE=81.0824 MAE=81.0848 MAE=81.0601 MAE=81.0693 Epoch: 200/500MAE=81.0583 MAE=81.0569 MAE=81.0669 MAE=81.0565 MAE=81.0616 MAE=81.0699 MAE=81.0775 MAE=81.0763 MAE=81.0737 MAE=81.0800 Epoch: 210/500MAE=81.0702 MAE=81.0594 MAE=81.0683 MAE=81.0694 MAE=81.0462 MAE=81.0332 MAE=81.0432 MAE=81.0416 MAE=81.0657 MAE=81.0747 Epoch: 220/500MAE=81.0831 MAE=81.0619 MAE=81.0405 MAE=81.0458 MAE=81.0508 MAE=81.0607 MAE=81.0575 MAE=81.0521 MAE=81.0457 MAE=81.0622 Epoch: 230/500MAE=81.0447 MAE=81.0497 MAE=81.0692 MAE=81.0581 MAE=81.0589 MAE=81.0655 MAE=81.0443 MAE=81.0342 MAE=81.0335 MAE=81.0428 Epoch: 240/500MAE=81.0558 MAE=81.0353 MAE=81.0295 MAE=81.0454 MAE=81.0554 MAE=81.0464 MAE=81.0591 MAE=81.0685 MAE=81.0629 MAE=81.0745 Epoch: 250/500MAE=81.0805 MAE=81.0807 MAE=81.0602 MAE=81.1024 MAE=81.0542 MAE=81.0497 MAE=81.0479 MAE=81.0425 MAE=81.0306 MAE=81.0355 Epoch: 260/500MAE=81.0314 MAE=81.0250 MAE=81.0378 MAE=81.0365 MAE=81.0435 MAE=81.0286 MAE=81.0427 MAE=81.0541 MAE=81.0415 MAE=81.0447 Epoch: 270/500MAE=81.0509 MAE=81.0513 MAE=81.0580 MAE=81.0716 MAE=81.0442 MAE=81.0386 MAE=81.0364 MAE=81.0532 MAE=81.0480 MAE=81.0602 Epoch: 280/500MAE=81.0436 MAE=81.0450 MAE=80.9958 MAE=80.9906 MAE=80.9863 MAE=80.9751 MAE=80.9744 MAE=80.9676 MAE=80.9795 MAE=80.9697 Epoch: 290/500MAE=80.9624 MAE=80.9544 MAE=82.1899 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 91.237 +/- 6.996\n",
      "\n",
      "Epoch: 1/500MAE=1548.3279 MAE=1546.8613 MAE=1544.3567 MAE=1542.1343 MAE=1539.4917 MAE=1535.2800 MAE=1533.0073 MAE=1529.3361 MAE=1524.4799 Epoch: 10/500MAE=1519.3491 MAE=1515.0078 MAE=1509.5674 MAE=1503.2217 MAE=1497.7653 MAE=1491.2792 MAE=1484.4885 MAE=1478.1548 MAE=1470.8015 MAE=1463.3340 Epoch: 20/500MAE=1453.8802 MAE=1454.0916 MAE=1433.3008 MAE=1425.5370 MAE=1411.2292 MAE=1402.1058 MAE=1393.6160 MAE=1382.5626 MAE=1371.7532 MAE=1356.2441 Epoch: 30/500MAE=1348.5784 MAE=1334.9871 MAE=1315.4089 MAE=1301.5997 MAE=1287.1488 MAE=1277.7842 MAE=1260.7097 MAE=1236.5677 MAE=1230.2937 MAE=1212.0503 Epoch: 40/500MAE=1194.9109 MAE=1181.0212 MAE=1166.0859 MAE=1143.4487 MAE=1136.9688 MAE=1113.1486 MAE=1076.7068 MAE=1068.9839 MAE=1053.4646 MAE=1036.6919 Epoch: 50/500MAE=1016.1959 MAE=995.9790 MAE=981.9167 MAE=952.6823 MAE=931.5292 MAE=908.2581 MAE=886.4484 MAE=860.6556 MAE=837.0826 MAE=817.4075 Epoch: 60/500MAE=805.8889 MAE=772.5394 MAE=749.1610 MAE=717.3253 MAE=712.7784 MAE=670.5687 MAE=663.5121 MAE=628.6932 MAE=628.4718 MAE=582.0502 Epoch: 70/500MAE=569.9224 MAE=532.1938 MAE=516.5354 MAE=453.1666 MAE=449.5684 MAE=435.8142 MAE=395.9864 MAE=387.0093 MAE=358.6501 MAE=318.8392 Epoch: 80/500MAE=309.8010 MAE=268.6240 MAE=272.6931 MAE=250.6858 MAE=224.2942 MAE=195.9769 MAE=191.1588 MAE=164.9657 MAE=167.7424 MAE=144.6597 Epoch: 90/500MAE=170.0028 MAE=141.8258 MAE=113.7223 MAE=177.4971 MAE=113.8126 MAE=114.8276 MAE=114.8590 MAE=94.8568 MAE=99.4740 MAE=106.4150 Epoch: 100/500MAE=103.4289 MAE=97.6025 MAE=93.9746 MAE=93.1462 MAE=98.3318 MAE=93.6607 MAE=90.8552 MAE=92.4319 MAE=92.0354 MAE=96.0027 Epoch: 110/500MAE=87.1243 MAE=92.5118 MAE=91.7682 MAE=90.3259 MAE=90.5707 MAE=90.2916 MAE=89.5777 MAE=89.9646 MAE=90.9553 MAE=90.6118 Epoch: 120/500MAE=90.5101 MAE=90.0548 MAE=90.5651 MAE=90.2294 MAE=90.0353 MAE=90.2283 MAE=89.8689 MAE=89.8459 MAE=89.7103 MAE=90.1237 Epoch: 130/500MAE=90.1551 MAE=90.0358 MAE=89.7351 MAE=89.6904 MAE=90.1881 MAE=90.5155 MAE=90.3703 MAE=90.4912 MAE=90.2159 MAE=90.0181 Epoch: 140/500MAE=90.1057 MAE=90.3336 MAE=89.9421 MAE=90.2149 MAE=90.0201 MAE=90.2000 MAE=90.0936 MAE=89.9998 MAE=90.6384 MAE=89.9401 Epoch: 150/500MAE=90.0701 MAE=89.9378 MAE=90.2811 MAE=90.2952 MAE=90.2122 MAE=90.4361 MAE=89.9351 MAE=90.2422 MAE=90.2541 MAE=90.0958 Epoch: 160/500MAE=90.4992 MAE=90.1350 MAE=90.1593 MAE=89.8451 MAE=90.2509 MAE=90.0310 MAE=89.7884 MAE=90.0888 MAE=90.0759 MAE=89.9766 Epoch: 170/500MAE=90.0836 MAE=89.7943 MAE=90.1770 MAE=89.7763 MAE=89.9414 MAE=90.2094 MAE=90.0704 MAE=90.3314 MAE=89.9776 MAE=90.0448 Epoch: 180/500MAE=90.1185 MAE=89.9940 MAE=89.9258 MAE=90.2277 MAE=89.7307 MAE=89.7250 MAE=89.8717 MAE=89.9980 MAE=89.7824 MAE=89.7779 Epoch: 190/500MAE=90.2781 MAE=90.0503 MAE=89.9614 MAE=89.9839 MAE=90.3002 MAE=90.3744 MAE=89.5648 MAE=89.8401 MAE=89.5632 MAE=90.0122 Epoch: 200/500MAE=89.6294 MAE=89.5440 MAE=89.5554 MAE=89.9510 MAE=89.7669 MAE=89.5349 MAE=89.6243 MAE=89.6311 MAE=90.0342 MAE=89.7690 Epoch: 210/500MAE=89.3645 MAE=89.6403 MAE=89.6368 MAE=89.7161 MAE=89.7486 MAE=89.5605 MAE=89.4206 MAE=89.9780 MAE=89.7549 MAE=89.2998 Epoch: 220/500MAE=89.9752 MAE=89.5890 MAE=89.5139 MAE=89.3057 MAE=89.4505 MAE=89.5016 MAE=89.6352 MAE=89.5709 MAE=89.4926 MAE=89.6484 Epoch: 230/500MAE=89.6059 MAE=89.7160 MAE=89.4874 MAE=89.5654 MAE=90.0742 MAE=89.5256 MAE=89.8698 MAE=89.6561 MAE=89.9245 MAE=89.8160 Epoch: 240/500MAE=89.7811 MAE=89.4691 MAE=89.9141 MAE=89.4936 MAE=89.7584 MAE=89.4265 MAE=89.8717 MAE=89.9297 MAE=89.8183 MAE=89.4958 Epoch: 250/500MAE=89.7025 MAE=89.6782 MAE=89.7264 MAE=89.4838 MAE=89.5107 MAE=89.5373 MAE=89.3899 MAE=89.5870 MAE=89.6356 MAE=89.6020 Epoch: 260/500MAE=89.5239 MAE=127.7386 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 100.362 +/- 16.927\n",
      "\n",
      "Epoch: 1/500MAE=1548.8303 MAE=1546.8073 MAE=1544.0697 MAE=1542.0154 MAE=1539.2644 MAE=1536.2002 MAE=1533.2249 MAE=1530.6014 MAE=1526.0338 Epoch: 10/500MAE=1520.4949 MAE=1515.8507 MAE=1511.6063 MAE=1504.8983 MAE=1499.6831 MAE=1493.7881 MAE=1487.2791 MAE=1481.4584 MAE=1473.0493 MAE=1467.2965 Epoch: 20/500MAE=1460.4821 MAE=1452.9438 MAE=1439.8665 MAE=1433.3956 MAE=1417.6866 MAE=1407.4828 MAE=1404.5522 MAE=1389.7910 MAE=1377.1370 MAE=1372.2881 Epoch: 30/500MAE=1353.8074 MAE=1352.3312 MAE=1347.0259 MAE=1329.5826 MAE=1294.2736 MAE=1284.3385 MAE=1282.0769 MAE=1254.8785 MAE=1246.8239 MAE=1228.8917 Epoch: 40/500MAE=1213.3988 MAE=1205.4165 MAE=1169.5823 MAE=1141.4656 MAE=1151.8964 MAE=1130.4161 MAE=1108.2429 MAE=1075.6033 MAE=1066.3569 MAE=1050.7592 Epoch: 50/500MAE=1031.5432 MAE=1013.1907 MAE=1006.4100 MAE=941.4459 MAE=946.3643 MAE=938.7187 MAE=907.7908 MAE=885.4812 MAE=844.9413 MAE=837.5225 Epoch: 60/500MAE=886.4761 MAE=796.4203 MAE=836.6597 MAE=739.4796 MAE=698.3954 MAE=699.3158 MAE=660.2758 MAE=669.5812 MAE=628.0278 MAE=609.5110 Epoch: 70/500MAE=593.0115 MAE=550.9360 MAE=538.1562 MAE=545.1982 MAE=520.2040 MAE=468.0193 MAE=432.5861 MAE=422.6362 MAE=386.2791 MAE=428.2323 Epoch: 80/500MAE=405.1865 MAE=358.8820 MAE=322.0412 MAE=234.7421 MAE=239.6796 MAE=219.9877 MAE=224.6301 MAE=256.9856 MAE=239.4958 MAE=231.0854 Epoch: 90/500MAE=233.4696 MAE=225.8186 MAE=219.7474 MAE=222.5611 MAE=221.7542 MAE=216.9165 MAE=212.3774 MAE=211.4209 MAE=206.9908 MAE=212.2490 Epoch: 100/500MAE=204.2567 MAE=208.9516 MAE=219.5568 MAE=105.2578 MAE=102.5046 MAE=101.9894 MAE=99.8798 MAE=99.4956 MAE=97.6636 MAE=97.1320 Epoch: 110/500MAE=96.3840 MAE=95.5565 MAE=94.1132 MAE=94.6585 MAE=92.7853 MAE=92.5435 MAE=91.1568 MAE=91.5553 MAE=91.4594 MAE=91.2462 Epoch: 120/500MAE=89.8914 MAE=90.5350 MAE=88.6623 MAE=91.5379 MAE=89.4461 MAE=88.5052 MAE=88.9621 MAE=87.2083 MAE=87.3870 MAE=87.2635 Epoch: 130/500MAE=87.4764 MAE=88.0411 MAE=87.0222 MAE=86.7685 MAE=86.8713 MAE=86.2426 MAE=86.6339 MAE=86.5616 MAE=85.8815 MAE=86.5189 Epoch: 140/500MAE=86.7979 MAE=87.2271 MAE=85.0157 MAE=87.1248 MAE=85.5205 MAE=85.6003 MAE=86.4667 MAE=85.8960 MAE=85.5872 MAE=85.3680 Epoch: 150/500MAE=85.0185 MAE=85.5301 MAE=85.7421 MAE=85.6073 MAE=85.3756 MAE=85.6535 MAE=85.7134 MAE=85.7224 MAE=85.5443 MAE=85.6454 Epoch: 160/500MAE=85.6604 MAE=85.3107 MAE=85.1823 MAE=85.5180 MAE=85.3765 MAE=85.2145 MAE=85.1865 MAE=85.1625 MAE=85.2305 MAE=85.1992 Epoch: 170/500MAE=85.1218 MAE=84.9031 MAE=84.9789 MAE=85.2365 MAE=85.0755 MAE=85.0760 MAE=84.9942 MAE=85.1470 MAE=85.1478 MAE=84.9086 Epoch: 180/500MAE=85.0680 MAE=85.7369 MAE=85.0771 MAE=84.7994 MAE=85.1698 MAE=85.1800 MAE=85.2183 MAE=85.0939 MAE=84.8793 MAE=84.9792 Epoch: 190/500MAE=85.0512 MAE=84.9462 MAE=84.9785 MAE=85.0359 MAE=84.9739 MAE=85.0107 MAE=84.9238 MAE=84.9037 MAE=85.2082 MAE=85.0119 Epoch: 200/500MAE=84.9806 MAE=85.1129 MAE=85.1458 MAE=85.0759 MAE=84.9772 MAE=85.1081 MAE=85.4346 MAE=84.8804 MAE=85.0158 MAE=84.9711 Epoch: 210/500MAE=85.0305 MAE=84.9507 MAE=84.9848 MAE=85.1421 MAE=84.8963 MAE=85.0596 MAE=85.0698 MAE=84.8877 MAE=85.1021 MAE=84.9996 Epoch: 220/500MAE=85.0063 MAE=84.9368 MAE=84.9282 MAE=85.0946 MAE=84.9507 MAE=84.9999 MAE=84.9816 MAE=85.1237 MAE=85.2200 MAE=85.0267 Epoch: 230/500MAE=85.1213 MAE=85.4215 MAE=85.0073 MAE=84.9111 MAE=85.0690 MAE=84.9857 MAE=84.9568 MAE=85.0184 MAE=84.9996 MAE=85.0444 Epoch: 240/500MAE=84.9135 MAE=85.4732 MAE=85.1122 MAE=84.8669 MAE=84.9449 MAE=84.8661 MAE=84.8618 MAE=85.0257 MAE=85.0173 MAE=85.0934 Epoch: 250/500MAE=84.9482 MAE=85.1042 MAE=84.8651 MAE=84.9880 MAE=85.0222 MAE=85.0321 MAE=85.0142 MAE=84.9207 MAE=84.9579 MAE=85.3327 Epoch: 260/500MAE=84.9045 MAE=84.8842 MAE=84.9449 MAE=84.9750 MAE=85.4144 MAE=85.0246 MAE=85.0519 MAE=85.3267 MAE=84.9132 MAE=84.8979 Epoch: 270/500MAE=84.8826 MAE=84.9842 MAE=84.9906 MAE=84.9781 MAE=84.8884 MAE=84.8147 MAE=84.9017 MAE=85.0303 MAE=85.3041 MAE=84.8924 Epoch: 280/500MAE=85.0812 MAE=84.9745 MAE=85.0065 MAE=84.8004 MAE=84.9345 MAE=84.7635 MAE=84.8416 MAE=84.8993 MAE=84.8692 MAE=85.0717 Epoch: 290/500MAE=84.8238 MAE=84.7876 MAE=84.7960 MAE=84.9409 MAE=84.8706 MAE=84.8272 MAE=84.9486 MAE=84.8757 MAE=84.7544 MAE=84.8889 Epoch: 300/500MAE=84.8447 MAE=84.9786 MAE=84.7867 MAE=84.7347 MAE=84.9121 MAE=85.3912 MAE=85.0069 MAE=84.9428 MAE=84.8764 MAE=84.9011 Epoch: 310/500MAE=84.7568 MAE=84.7712 MAE=84.8036 MAE=84.9685 MAE=84.8850 MAE=85.3082 MAE=84.8119 MAE=84.8934 MAE=84.9862 MAE=84.9349 Epoch: 320/500MAE=84.8444 MAE=84.9669 MAE=84.7446 MAE=84.8675 MAE=85.0724 MAE=84.8293 MAE=84.9851 MAE=84.8860 MAE=84.8609 MAE=84.9598 Epoch: 330/500MAE=84.9473 MAE=85.0154 MAE=84.9165 MAE=85.4100 MAE=84.8968 MAE=84.9884 MAE=84.8900 MAE=84.9458 MAE=84.9275 MAE=84.8995 Epoch: 340/500MAE=84.9311 MAE=84.8291 MAE=85.0418 MAE=85.0471 MAE=84.9908 MAE=84.8724 MAE=85.3509 MAE=84.9161 MAE=84.9175 MAE=84.8093 Epoch: 350/500MAE=84.8333 MAE=84.7623 MAE=84.8917 MAE=84.8024 MAE=84.7612 MAE=84.8449 MAE=84.9136 MAE=84.8359 MAE=84.7619 MAE=84.8842 Epoch: 360/500MAE=84.8957 MAE=84.8025 MAE=84.7690 MAE=84.9646 MAE=85.1570 MAE=84.7711 MAE=84.7514 MAE=85.0095 MAE=84.8772 MAE=84.8925 Epoch: 370/500MAE=84.8904 MAE=84.8286 MAE=84.8720 MAE=84.8394 MAE=84.8997 MAE=84.8444 MAE=84.7107 MAE=85.5023 MAE=84.9017 MAE=84.8564 Epoch: 380/500MAE=84.8308 MAE=84.7849 MAE=84.8557 MAE=84.8409 MAE=84.8074 MAE=84.8481 MAE=84.7001 MAE=84.7995 MAE=84.9269 MAE=84.8726 Epoch: 390/500MAE=84.8865 MAE=84.7237 MAE=84.7094 MAE=84.8216 MAE=84.8028 MAE=84.8304 MAE=84.7829 MAE=84.9108 MAE=85.2425 MAE=84.8622 Epoch: 400/500MAE=84.7497 MAE=84.8733 MAE=84.9480 MAE=84.7897 MAE=84.8973 MAE=84.8293 MAE=84.8719 MAE=84.8386 MAE=84.8118 MAE=84.7800 Epoch: 410/500MAE=84.8667 MAE=84.8592 MAE=84.7123 MAE=84.7510 MAE=84.7501 MAE=84.7892 MAE=84.6396 MAE=84.6932 MAE=84.8007 MAE=84.6995 Epoch: 420/500MAE=84.7772 MAE=84.8694 MAE=85.3855 MAE=84.7941 MAE=84.7835 MAE=84.7953 MAE=84.7650 MAE=84.8720 MAE=84.6444 MAE=84.7879 Epoch: 430/500MAE=84.7751 MAE=84.6585 MAE=85.1642 MAE=84.6827 MAE=84.7979 MAE=84.9437 MAE=85.0596 MAE=84.7513 MAE=84.5862 MAE=85.2357 Epoch: 440/500MAE=85.1340 MAE=84.6308 MAE=84.5492 MAE=84.7132 MAE=84.7175 MAE=84.5739 MAE=84.6812 MAE=84.7655 MAE=84.6234 MAE=84.6405 Epoch: 450/500MAE=84.8342 MAE=85.2192 MAE=84.7778 MAE=84.6819 MAE=84.7492 MAE=84.6747 MAE=84.6405 MAE=84.8680 MAE=84.7051 MAE=84.8443 Epoch: 460/500MAE=84.5339 MAE=84.6175 MAE=84.7523 MAE=84.7368 MAE=85.1282 MAE=84.7668 MAE=84.8331 MAE=84.8538 MAE=84.7885 MAE=84.7641 Epoch: 470/500MAE=84.6701 MAE=84.7581 MAE=84.7851 MAE=84.8568 MAE=84.7469 MAE=84.8057 MAE=84.7577 MAE=84.5733 MAE=84.7902 MAE=84.6555 Epoch: 480/500MAE=84.6178 MAE=84.7610 MAE=84.7305 MAE=85.0935 MAE=84.6697 MAE=84.8079 MAE=84.7815 MAE=84.8234 MAE=84.7074 MAE=84.7907 Epoch: 490/500MAE=84.7482 MAE=85.3498 MAE=84.7626 MAE=84.7916 MAE=85.0259 MAE=84.6987 MAE=84.6879 MAE=84.6812 MAE=84.7266 MAE=84.8442 Epoch: 500/500MAE=84.6476 MAE=85.6052 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 97.411 +/- 16.250\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1549.4694 MAE=1546.8906 MAE=1544.4800 MAE=1541.8688 MAE=1539.5483 MAE=1536.5918 MAE=1533.1064 MAE=1528.9084 MAE=1524.3438 Epoch: 10/500MAE=1520.4844 MAE=1517.1669 MAE=1511.3920 MAE=1504.4324 MAE=1496.4932 MAE=1494.7880 MAE=1487.3413 MAE=1479.8049 MAE=1472.4553 MAE=1457.6982 Epoch: 20/500MAE=1455.7316 MAE=1443.8876 MAE=1437.6259 MAE=1430.7856 MAE=1411.3259 MAE=1401.8040 MAE=1388.8152 MAE=1386.1770 MAE=1367.3857 MAE=1352.2410 Epoch: 30/500MAE=1346.5129 MAE=1330.6931 MAE=1328.7634 MAE=1296.7911 MAE=1291.8579 MAE=1279.5840 MAE=1262.3838 MAE=1249.7628 MAE=1214.7136 MAE=1219.2356 Epoch: 40/500MAE=1197.5411 MAE=1181.1719 MAE=1153.2800 MAE=1172.7963 MAE=1132.7544 MAE=1093.5212 MAE=1081.7775 MAE=1080.1682 MAE=1051.7725 MAE=1025.0413 Epoch: 50/500MAE=1010.2015 MAE=991.0084 MAE=965.8711 MAE=957.7043 MAE=913.9287 MAE=891.6516 MAE=884.6290 MAE=864.6377 MAE=842.9091 MAE=816.1597 Epoch: 60/500MAE=787.4256 MAE=791.1536 MAE=775.2228 MAE=718.1544 MAE=714.1816 MAE=675.5938 MAE=661.0078 MAE=629.6925 MAE=589.1149 MAE=576.6190 Epoch: 70/500MAE=545.1155 MAE=558.5120 MAE=510.9142 MAE=490.1193 MAE=446.5753 MAE=435.4608 MAE=405.8412 MAE=363.6600 MAE=364.3551 MAE=340.4471 Epoch: 80/500MAE=319.8378 MAE=324.1246 MAE=267.4645 MAE=256.7686 MAE=226.5965 MAE=185.3171 MAE=257.8459 MAE=254.0308 MAE=174.3346 MAE=212.6184 Epoch: 90/500MAE=225.3092 MAE=224.4841 MAE=231.0566 MAE=209.4997 MAE=210.4968 MAE=199.8705 MAE=215.0118 MAE=208.7957 MAE=202.9066 MAE=203.0440 Epoch: 100/500MAE=203.8856 MAE=198.2368 MAE=214.6479 MAE=207.1497 MAE=102.9708 MAE=100.6758 MAE=88.5605 MAE=84.2258 MAE=87.5153 MAE=81.8406 Epoch: 110/500MAE=82.3145 MAE=77.3057 MAE=78.4341 MAE=74.8540 MAE=82.4636 MAE=74.6826 MAE=74.3375 MAE=76.0087 MAE=74.9292 MAE=79.2518 Epoch: 120/500MAE=72.8588 MAE=75.2057 MAE=72.9583 MAE=72.5531 MAE=71.9549 MAE=71.8863 MAE=73.4875 MAE=71.6518 MAE=72.2967 MAE=70.9377 Epoch: 130/500MAE=68.5242 MAE=70.5340 MAE=70.8027 MAE=69.5966 MAE=71.4298 MAE=69.3347 MAE=67.9236 MAE=68.1506 MAE=68.4298 MAE=67.7556 Epoch: 140/500MAE=68.8888 MAE=67.5701 MAE=67.5694 MAE=67.8911 MAE=67.6589 MAE=68.4931 MAE=67.5998 MAE=68.4647 MAE=67.7485 MAE=67.3654 Epoch: 150/500MAE=66.4646 MAE=66.3236 MAE=67.3835 MAE=68.3240 MAE=66.1575 MAE=66.9847 MAE=67.6235 MAE=67.7494 MAE=67.0039 MAE=67.6705 Epoch: 160/500MAE=67.3881 MAE=67.7297 MAE=67.4995 MAE=67.4376 MAE=67.4733 MAE=67.0961 MAE=67.0462 MAE=67.5866 MAE=67.5758 MAE=67.1234 Epoch: 170/500MAE=67.0702 MAE=67.3353 MAE=67.0385 MAE=67.3014 MAE=67.2762 MAE=67.2688 MAE=67.2400 MAE=67.1801 MAE=67.2105 MAE=67.5415 Epoch: 180/500MAE=67.3603 MAE=67.4430 MAE=67.3819 MAE=67.0872 MAE=67.1686 MAE=67.1532 MAE=67.2702 MAE=66.8747 MAE=67.0954 MAE=67.2290 Epoch: 190/500MAE=67.4382 MAE=67.2635 MAE=67.2547 MAE=67.2286 MAE=67.1237 MAE=67.1213 MAE=67.2797 MAE=67.3091 MAE=67.2026 MAE=67.2505 Epoch: 200/500MAE=67.2211 MAE=67.3388 MAE=67.3410 MAE=67.3430 MAE=67.1196 MAE=66.8528 MAE=66.9968 MAE=67.1974 MAE=67.1019 MAE=66.8017 Epoch: 210/500MAE=67.3662 MAE=67.3564 MAE=67.3222 MAE=67.5159 MAE=67.3321 MAE=67.3041 MAE=67.2793 MAE=67.5266 MAE=67.1079 MAE=67.2496 Epoch: 220/500MAE=66.9901 MAE=67.5280 MAE=67.3697 MAE=67.1589 MAE=67.1319 MAE=67.2304 MAE=67.1330 MAE=67.2694 MAE=67.3864 MAE=67.2358 Epoch: 230/500MAE=67.2965 MAE=67.1133 MAE=67.1765 MAE=67.5756 MAE=67.1607 MAE=67.0765 MAE=67.3714 MAE=67.4375 MAE=67.1334 MAE=67.2584 Epoch: 240/500MAE=67.4932 MAE=67.2599 MAE=67.2434 MAE=67.3998 MAE=67.3589 MAE=67.0549 MAE=67.4402 MAE=67.3258 MAE=67.4043 MAE=67.0444 Epoch: 250/500MAE=67.1987 MAE=67.3407 MAE=67.3291 MAE=67.0785 MAE=67.2201 MAE=67.2089 MAE=67.2404 MAE=67.2997 MAE=67.1847 MAE=67.1998 Epoch: 260/500MAE=67.2181 MAE=67.1754 MAE=67.1529 MAE=67.0614 MAE=67.2280 MAE=67.6560 MAE=67.2612 MAE=67.2304 MAE=67.4553 MAE=67.4133 Epoch: 270/500MAE=67.4220 MAE=67.0400 MAE=67.3955 MAE=67.5364 MAE=67.5066 MAE=67.3692 MAE=67.1501 MAE=67.2581 MAE=67.3569 MAE=67.1107 Epoch: 280/500MAE=67.5056 MAE=67.2479 MAE=67.2459 MAE=67.2244 MAE=67.4669 MAE=67.2650 MAE=67.2533 MAE=67.2924 MAE=67.0409 MAE=67.2939 Epoch: 290/500MAE=67.3059 MAE=67.1735 MAE=67.4220 MAE=67.4995 MAE=67.3062 MAE=67.2688 MAE=67.2905 MAE=67.0403 MAE=67.0639 MAE=67.2087 Epoch: 300/500MAE=67.0120 MAE=67.3207 MAE=67.0573 MAE=67.1911 MAE=67.4279 MAE=67.3917 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 67.392 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1549.1948 MAE=1547.0232 MAE=1543.7810 MAE=1542.0647 MAE=1538.9668 MAE=1536.6965 MAE=1532.7379 MAE=1529.2063 MAE=1524.1042 Epoch: 10/500MAE=1520.6510 MAE=1515.6790 MAE=1511.2406 MAE=1505.5519 MAE=1500.9554 MAE=1495.4174 MAE=1488.8730 MAE=1482.3870 MAE=1468.6006 MAE=1462.2986 Epoch: 20/500MAE=1457.4468 MAE=1446.5691 MAE=1433.8296 MAE=1432.1262 MAE=1407.4453 MAE=1408.6506 MAE=1397.3805 MAE=1384.5562 MAE=1371.8101 MAE=1353.6495 Epoch: 30/500MAE=1341.9255 MAE=1319.1271 MAE=1314.1672 MAE=1302.1302 MAE=1313.4847 MAE=1270.8975 MAE=1263.2725 MAE=1247.1178 MAE=1231.9486 MAE=1216.4381 Epoch: 40/500MAE=1202.3881 MAE=1166.7239 MAE=1163.7378 MAE=1140.9879 MAE=1160.5182 MAE=1084.8264 MAE=1096.3535 MAE=1091.5732 MAE=1066.7203 MAE=1028.6865 Epoch: 50/500MAE=1026.6787 MAE=951.0334 MAE=1010.4613 MAE=956.8048 MAE=955.3324 MAE=903.2497 MAE=888.9138 MAE=858.1171 MAE=798.9706 MAE=807.1938 Epoch: 60/500MAE=836.3251 MAE=743.1940 MAE=760.4657 MAE=723.2082 MAE=738.8156 MAE=685.4385 MAE=615.2257 MAE=620.8044 MAE=586.3063 MAE=621.9453 Epoch: 70/500MAE=541.5635 MAE=543.2709 MAE=508.7500 MAE=468.5692 MAE=448.9258 MAE=458.0084 MAE=423.4809 MAE=383.6765 MAE=394.2227 MAE=304.2323 Epoch: 80/500MAE=320.8850 MAE=305.8108 MAE=252.9127 MAE=267.3997 MAE=224.7220 MAE=217.1025 MAE=220.7129 MAE=177.0159 MAE=217.4747 MAE=153.5769 Epoch: 90/500MAE=138.0078 MAE=119.8900 MAE=124.1666 MAE=101.9959 MAE=105.8653 MAE=96.6140 MAE=102.5344 MAE=90.7737 MAE=91.9165 MAE=94.7629 Epoch: 100/500MAE=78.4119 MAE=88.7770 MAE=90.3684 MAE=76.3927 MAE=82.2098 MAE=77.3457 MAE=78.2676 MAE=74.5072 MAE=78.8243 MAE=80.4131 Epoch: 110/500MAE=83.6915 MAE=77.6535 MAE=73.1087 MAE=72.7663 MAE=69.7287 MAE=74.8385 MAE=70.0792 MAE=72.0972 MAE=70.7049 MAE=70.0117 Epoch: 120/500MAE=67.4689 MAE=68.4388 MAE=68.7794 MAE=67.5981 MAE=66.7420 MAE=68.2406 MAE=68.6809 MAE=68.2640 MAE=68.0459 MAE=67.2398 Epoch: 130/500MAE=66.7874 MAE=67.2802 MAE=65.3966 MAE=66.1521 MAE=66.0441 MAE=65.4807 MAE=65.7752 MAE=65.1178 MAE=65.7142 MAE=65.1205 Epoch: 140/500MAE=65.3691 MAE=65.0347 MAE=65.5983 MAE=66.1531 MAE=65.2369 MAE=65.3873 MAE=65.4454 MAE=65.2843 MAE=65.3107 MAE=64.9006 Epoch: 150/500MAE=65.4306 MAE=65.8133 MAE=65.8880 MAE=65.7867 MAE=65.4679 MAE=65.4215 MAE=65.5715 MAE=65.5414 MAE=65.3622 MAE=65.6116 Epoch: 160/500MAE=65.5086 MAE=65.1937 MAE=65.1588 MAE=65.1626 MAE=65.1797 MAE=65.3298 MAE=65.3177 MAE=65.1835 MAE=65.1715 MAE=65.3699 Epoch: 170/500MAE=65.0429 MAE=65.1333 MAE=65.2361 MAE=65.2760 MAE=65.1942 MAE=65.0049 MAE=65.1263 MAE=65.2702 MAE=65.4435 MAE=65.2328 Epoch: 180/500MAE=65.4822 MAE=65.4226 MAE=65.2757 MAE=65.1081 MAE=65.4027 MAE=65.3299 MAE=65.2111 MAE=65.4128 MAE=65.3437 MAE=65.1255 Epoch: 190/500MAE=65.2470 MAE=65.1677 MAE=65.2678 MAE=65.1636 MAE=65.2351 MAE=65.3308 MAE=65.2704 MAE=64.9805 MAE=65.1781 MAE=64.9977 Epoch: 200/500MAE=65.1125 MAE=65.0003 MAE=65.0960 MAE=64.8749 MAE=65.1161 MAE=65.1192 MAE=65.2659 MAE=65.3801 MAE=65.0989 MAE=65.2378 Epoch: 210/500MAE=65.3331 MAE=65.0359 MAE=65.2901 MAE=65.0316 MAE=65.3487 MAE=65.0564 MAE=65.1860 MAE=65.5776 MAE=65.2246 MAE=65.4908 Epoch: 220/500MAE=65.2699 MAE=65.2551 MAE=65.0692 MAE=65.2258 MAE=65.5108 MAE=65.2273 MAE=65.1352 MAE=65.1460 MAE=65.2361 MAE=65.3092 Epoch: 230/500MAE=65.4622 MAE=65.2462 MAE=65.2767 MAE=65.2523 MAE=65.3390 MAE=65.5610 MAE=65.3938 MAE=65.1483 MAE=65.1990 MAE=65.2538 Epoch: 240/500MAE=64.9945 MAE=65.0390 MAE=65.4076 MAE=65.2744 MAE=65.2138 MAE=65.0230 MAE=65.1720 MAE=65.2817 MAE=65.0540 MAE=65.2239 Epoch: 250/500MAE=65.3270 MAE=65.1960 MAE=65.2771 MAE=65.2011 MAE=65.2877 MAE=65.1948 MAE=65.3937 MAE=65.2930 MAE=65.2045 MAE=65.3186 Epoch: 260/500MAE=65.2066 MAE=65.2476 MAE=65.2181 MAE=65.2814 MAE=65.2367 MAE=65.3010 MAE=65.3229 MAE=65.2207 MAE=65.2114 MAE=65.4001 Epoch: 270/500MAE=65.4360 MAE=65.2159 MAE=65.3996 MAE=65.3381 MAE=65.3138 MAE=65.4930 MAE=65.4833 MAE=65.4101 MAE=65.4763 MAE=65.5437 Epoch: 280/500MAE=65.4745 MAE=65.2722 MAE=65.2206 MAE=65.5876 MAE=65.2641 MAE=65.4773 MAE=65.2608 MAE=65.4872 MAE=65.4526 MAE=65.2829 Epoch: 290/500MAE=65.4491 MAE=65.6699 MAE=65.4598 MAE=65.1721 MAE=65.6458 MAE=65.4335 MAE=65.4579 MAE=65.6229 MAE=65.5450 MAE=65.4395 Epoch: 300/500MAE=65.4762 MAE=65.4775 MAE=65.4669 MAE=65.3771 MAE=65.3553 MAE=65.3393 MAE=65.5469 MAE=65.4958 MAE=65.3281 MAE=65.3937 Epoch: 310/500MAE=65.7088 MAE=65.4362 MAE=65.3256 MAE=65.5735 MAE=65.3542 MAE=65.4210 MAE=65.3126 MAE=65.3847 MAE=65.4609 MAE=65.4863 Epoch: 320/500MAE=65.1825 MAE=65.2765 MAE=65.2543 MAE=65.1386 MAE=65.6330 MAE=65.2550 MAE=65.4216 MAE=65.2895 MAE=65.4215 MAE=65.3612 Epoch: 330/500MAE=65.4267 MAE=65.3226 MAE=65.2474 MAE=65.4187 MAE=65.5457 MAE=65.4299 MAE=65.3903 MAE=65.3872 MAE=65.3166 MAE=65.2426 Epoch: 340/500MAE=65.2524 MAE=65.3462 MAE=65.1767 MAE=65.2328 MAE=65.1275 MAE=65.4036 MAE=65.2863 MAE=65.3198 MAE=65.1729 MAE=65.4692 Epoch: 350/500MAE=65.2635 MAE=65.2816 MAE=65.4489 MAE=65.4337 MAE=66.3299 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 66.861 +/- 0.531\n",
      "\n",
      "Epoch: 1/500MAE=1549.4736 MAE=1546.3806 MAE=1542.8043 MAE=1541.3525 MAE=1539.7925 MAE=1536.5977 MAE=1533.4832 MAE=1527.2937 MAE=1524.8657 Epoch: 10/500MAE=1516.3253 MAE=1517.0861 MAE=1512.6759 MAE=1504.8787 MAE=1505.0386 MAE=1494.7463 MAE=1484.0900 MAE=1486.1658 MAE=1471.1193 MAE=1453.5227 Epoch: 20/500MAE=1458.7731 MAE=1442.8541 MAE=1440.1458 MAE=1417.6807 MAE=1416.0540 MAE=1413.6436 MAE=1396.5874 MAE=1385.1008 MAE=1367.5027 MAE=1360.5027 Epoch: 30/500MAE=1328.8357 MAE=1321.4939 MAE=1334.4705 MAE=1309.5669 MAE=1282.4971 MAE=1285.0216 MAE=1306.8341 MAE=1248.2581 MAE=1223.8175 MAE=1213.5292 Epoch: 40/500MAE=1213.2639 MAE=1196.5380 MAE=1191.7543 MAE=1167.1464 MAE=1123.0057 MAE=1128.7073 MAE=1103.9987 MAE=1051.7400 MAE=1072.2423 MAE=1034.1031 Epoch: 50/500MAE=1014.9332 MAE=1009.7062 MAE=977.5237 MAE=958.5817 MAE=933.9435 MAE=902.7085 MAE=894.7746 MAE=856.4930 MAE=849.4199 MAE=840.2389 Epoch: 60/500MAE=839.1919 MAE=769.7442 MAE=738.1813 MAE=761.6361 MAE=704.2457 MAE=637.5699 MAE=717.4416 MAE=586.8861 MAE=658.5873 MAE=612.2972 Epoch: 70/500MAE=607.5503 MAE=589.8503 MAE=506.4017 MAE=500.1613 MAE=469.4558 MAE=480.3666 MAE=476.6052 MAE=470.2652 MAE=434.8099 MAE=437.7224 Epoch: 80/500MAE=425.5846 MAE=469.1682 MAE=425.5964 MAE=399.1238 MAE=407.2055 MAE=417.9778 MAE=312.4839 MAE=344.0253 MAE=386.0945 MAE=317.6469 Epoch: 90/500MAE=314.2365 MAE=293.6328 MAE=294.4901 MAE=292.6398 MAE=272.4012 MAE=289.7901 MAE=287.9855 MAE=273.3739 MAE=290.1615 MAE=336.7730 Epoch: 100/500MAE=312.2635 MAE=233.6662 MAE=283.2661 MAE=250.9666 MAE=230.9368 MAE=255.5817 MAE=291.4598 MAE=276.9917 MAE=211.9135 MAE=227.8745 Epoch: 110/500MAE=286.7878 MAE=290.0166 MAE=280.6321 MAE=274.7032 MAE=274.7654 MAE=270.3517 MAE=270.9313 MAE=203.5492 MAE=273.9091 MAE=270.8172 Epoch: 120/500MAE=272.5381 MAE=268.9873 MAE=270.1641 MAE=268.0883 MAE=270.1723 MAE=270.1214 MAE=270.9623 MAE=271.1344 MAE=271.3460 MAE=268.1160 Epoch: 130/500MAE=267.2722 MAE=267.8518 MAE=268.0748 MAE=267.8449 MAE=266.5547 MAE=267.8413 MAE=267.6792 MAE=268.2962 MAE=266.7507 MAE=268.2741 Epoch: 140/500MAE=269.2528 MAE=268.0509 MAE=267.5439 MAE=267.1838 MAE=266.9167 MAE=267.6983 MAE=267.6078 MAE=267.1729 MAE=267.5542 MAE=267.2638 Epoch: 150/500MAE=267.6006 MAE=267.5450 MAE=267.3089 MAE=267.0804 MAE=266.5999 MAE=266.7299 MAE=266.7905 MAE=267.3596 MAE=267.2431 MAE=268.3451 Epoch: 160/500MAE=267.3101 MAE=266.7487 MAE=267.4572 MAE=266.6338 MAE=267.4146 MAE=267.3849 MAE=266.8079 MAE=267.3026 MAE=267.1315 MAE=267.0545 Epoch: 170/500MAE=266.9101 MAE=266.9016 MAE=266.9702 MAE=267.1778 MAE=266.4613 MAE=267.9172 MAE=267.0332 MAE=266.8150 MAE=266.4201 MAE=267.3193 Epoch: 180/500MAE=267.3430 MAE=266.4645 MAE=266.4954 MAE=266.8937 MAE=267.0353 MAE=266.9000 MAE=266.2067 MAE=266.7204 MAE=266.6023 MAE=267.0822 Epoch: 190/500MAE=267.4891 MAE=266.6195 MAE=266.3179 MAE=267.1582 MAE=266.9395 MAE=267.1565 MAE=266.7131 MAE=267.1658 MAE=267.3952 MAE=266.5872 Epoch: 200/500MAE=267.1007 MAE=266.7679 MAE=267.8078 MAE=267.0879 MAE=267.1701 MAE=267.8290 MAE=266.9246 MAE=267.1005 MAE=267.7555 MAE=267.0852 Epoch: 210/500MAE=266.9337 MAE=267.2047 MAE=266.6050 MAE=267.1395 MAE=266.9041 MAE=266.4002 MAE=267.0689 MAE=267.2876 MAE=266.3354 MAE=266.1784 Epoch: 220/500MAE=266.4319 MAE=267.0777 MAE=267.3898 MAE=267.1591 MAE=266.6290 MAE=267.0161 MAE=266.5520 MAE=267.2855 MAE=266.9040 MAE=266.5813 Epoch: 230/500MAE=266.5973 MAE=266.9197 MAE=266.5174 MAE=266.2323 MAE=266.9553 MAE=266.4507 MAE=266.1179 MAE=266.3082 MAE=266.5509 MAE=266.1293 Epoch: 240/500MAE=266.2108 MAE=265.7978 MAE=266.2995 MAE=266.5911 MAE=266.0305 MAE=266.7159 MAE=266.2128 MAE=266.2870 MAE=265.8640 MAE=265.7232 Epoch: 250/500MAE=265.9666 MAE=266.0677 MAE=266.9812 MAE=266.2668 MAE=266.2479 MAE=266.3537 MAE=266.5853 MAE=266.3528 MAE=266.4229 MAE=266.1408 Epoch: 260/500MAE=266.3106 MAE=265.5072 MAE=265.9622 MAE=266.7942 MAE=266.6084 MAE=266.3392 MAE=266.2926 MAE=266.9866 MAE=266.1364 MAE=266.0396 Epoch: 270/500MAE=266.4919 MAE=266.1687 MAE=266.4362 MAE=266.2278 MAE=265.5250 MAE=266.0815 MAE=265.7975 MAE=265.7749 MAE=265.3401 MAE=266.0512 Epoch: 280/500MAE=266.1273 MAE=266.3509 MAE=265.8190 MAE=266.7240 MAE=265.9782 MAE=266.5911 MAE=266.5783 MAE=265.9332 MAE=265.9579 MAE=265.9759 Epoch: 290/500MAE=266.2439 MAE=265.4786 MAE=266.5717 MAE=266.4315 MAE=266.1557 MAE=266.3116 MAE=266.7432 MAE=265.7966 MAE=266.1310 MAE=265.8331 Epoch: 300/500MAE=265.8158 MAE=266.4282 MAE=266.1710 MAE=266.1631 MAE=266.1950 MAE=265.4079 MAE=266.6579 MAE=265.9277 MAE=265.8975 MAE=266.2535 Epoch: 310/500MAE=266.0270 MAE=266.2466 MAE=265.6234 MAE=266.7085 MAE=265.9735 MAE=265.5703 MAE=265.5522 MAE=266.0636 MAE=266.3565 MAE=265.8301 Epoch: 320/500MAE=266.0677 MAE=265.2403 MAE=266.4670 MAE=264.9347 MAE=265.2416 MAE=265.8667 MAE=265.1151 MAE=265.8310 MAE=265.2949 MAE=266.0311 Epoch: 330/500MAE=265.5282 MAE=265.2664 MAE=265.5360 MAE=265.6426 MAE=265.7826 MAE=266.2148 MAE=265.5634 MAE=265.7235 MAE=265.7733 MAE=265.5985 Epoch: 340/500MAE=265.8690 MAE=266.5671 MAE=264.9912 MAE=265.7720 MAE=265.9700 MAE=266.3071 MAE=265.8448 MAE=265.7652 MAE=265.7173 MAE=265.6622 Epoch: 350/500MAE=265.9874 MAE=265.6825 MAE=265.6660 MAE=265.6329 MAE=265.0450 MAE=265.6812 MAE=265.3659 MAE=265.1469 MAE=265.7229 MAE=265.9005 Epoch: 360/500MAE=265.2915 MAE=266.7494 MAE=265.6145 MAE=264.9646 MAE=266.2537 MAE=265.6110 MAE=265.8768 MAE=265.5897 MAE=265.7101 MAE=265.1633 Epoch: 370/500MAE=265.3396 MAE=265.0573 MAE=265.5584 MAE=265.7672 MAE=266.0443 MAE=265.8397 MAE=265.5337 MAE=266.1963 MAE=265.6302 MAE=265.5342 Epoch: 380/500MAE=266.1687 MAE=265.8352 MAE=264.8448 MAE=264.9752 MAE=265.5667 MAE=265.7086 MAE=265.7283 MAE=265.5284 MAE=265.0316 MAE=265.0586 Epoch: 390/500MAE=265.7985 MAE=265.4522 MAE=266.0757 MAE=265.0434 MAE=265.0682 MAE=265.1190 MAE=264.7063 MAE=265.0768 MAE=264.6663 MAE=265.4724 Epoch: 400/500MAE=265.7141 MAE=265.6848 MAE=264.8632 MAE=265.6631 MAE=265.5711 MAE=264.8686 MAE=265.1297 MAE=265.4893 MAE=266.0375 MAE=265.0066 Epoch: 410/500MAE=265.2990 MAE=265.7834 MAE=265.4644 MAE=265.6436 MAE=265.3456 MAE=265.2854 MAE=265.4527 MAE=265.2838 MAE=265.2353 MAE=265.9198 Epoch: 420/500MAE=264.8765 MAE=265.2089 MAE=264.7266 MAE=264.5338 MAE=265.3543 MAE=264.7628 MAE=264.5580 MAE=264.9990 MAE=265.2196 MAE=265.4638 Epoch: 430/500MAE=265.2269 MAE=264.8851 MAE=264.9836 MAE=264.6185 MAE=265.1701 MAE=266.0175 MAE=264.8132 MAE=265.0802 MAE=264.2471 MAE=265.5449 Epoch: 440/500MAE=264.8204 MAE=265.1567 MAE=265.1774 MAE=265.3703 MAE=265.3713 MAE=265.8033 MAE=265.7667 MAE=265.0977 MAE=264.5492 MAE=264.9246 Epoch: 450/500MAE=264.7408 MAE=265.2848 MAE=265.4254 MAE=265.0085 MAE=265.2783 MAE=266.0988 MAE=265.5328 MAE=265.7838 MAE=265.3031 MAE=265.4025 Epoch: 460/500MAE=265.1419 MAE=265.4865 MAE=265.0305 MAE=264.8817 MAE=264.1701 MAE=265.4427 MAE=265.2033 MAE=265.2532 MAE=264.4374 MAE=265.4591 Epoch: 470/500MAE=265.8429 MAE=264.9227 MAE=264.9496 MAE=264.6570 MAE=265.1544 MAE=264.5574 MAE=264.8885 MAE=265.0992 MAE=265.2647 MAE=265.1517 Epoch: 480/500MAE=264.5291 MAE=264.1737 MAE=264.8605 MAE=265.1800 MAE=264.8446 MAE=264.4450 MAE=264.9925 MAE=264.3825 MAE=264.5783 MAE=264.2254 Epoch: 490/500MAE=265.2244 MAE=265.2308 MAE=264.7028 MAE=265.0757 MAE=264.2840 MAE=264.4161 MAE=264.6393 MAE=264.1997 MAE=264.9756 MAE=265.0515 Epoch: 500/500MAE=265.0690 MAE=193.6521 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 109.125 +/- 59.772\n",
      "\n",
      "Epoch: 1/500MAE=1548.9927 MAE=1546.7369 MAE=1543.8469 MAE=1540.9709 MAE=1539.5269 MAE=1536.6779 MAE=1533.3507 MAE=1530.0825 MAE=1526.9070 Epoch: 10/500MAE=1519.2644 MAE=1516.3893 MAE=1511.8289 MAE=1505.3907 MAE=1501.0781 MAE=1493.4746 MAE=1485.6617 MAE=1479.6738 MAE=1469.7827 MAE=1464.6152 Epoch: 20/500MAE=1457.7671 MAE=1438.9556 MAE=1432.2269 MAE=1419.8059 MAE=1410.8169 MAE=1402.1733 MAE=1388.8118 MAE=1374.9399 MAE=1366.1831 MAE=1354.9623 Epoch: 30/500MAE=1343.6091 MAE=1322.9463 MAE=1311.1508 MAE=1298.1261 MAE=1291.8019 MAE=1275.3425 MAE=1242.7699 MAE=1232.5479 MAE=1235.7783 MAE=1205.3124 Epoch: 40/500MAE=1185.2449 MAE=1163.8455 MAE=1160.3763 MAE=1143.9751 MAE=1094.2173 MAE=1110.3318 MAE=1084.5669 MAE=1053.1987 MAE=1030.8088 MAE=1017.8782 Epoch: 50/500MAE=995.2681 MAE=979.9617 MAE=968.4438 MAE=956.9231 MAE=912.5857 MAE=916.3762 MAE=897.5447 MAE=863.2498 MAE=805.8690 MAE=800.0710 Epoch: 60/500MAE=796.7972 MAE=779.0612 MAE=741.7349 MAE=713.5673 MAE=670.8105 MAE=677.6461 MAE=622.0713 MAE=617.4194 MAE=616.7316 MAE=539.8641 Epoch: 70/500MAE=542.6997 MAE=519.0935 MAE=449.3136 MAE=478.6399 MAE=402.8560 MAE=426.1887 MAE=400.4285 MAE=360.3040 MAE=336.8459 MAE=310.9806 Epoch: 80/500MAE=278.7718 MAE=268.1849 MAE=245.5718 MAE=264.9479 MAE=200.8609 MAE=235.0393 MAE=196.3883 MAE=183.4652 MAE=138.3491 MAE=128.1316 Epoch: 90/500MAE=149.9936 MAE=151.6888 MAE=114.7485 MAE=90.6274 MAE=84.6994 MAE=87.2658 MAE=88.4740 MAE=85.4470 MAE=89.0528 MAE=80.8639 Epoch: 100/500MAE=91.4413 MAE=85.5844 MAE=86.6278 MAE=71.9343 MAE=85.6258 MAE=70.3031 MAE=76.6930 MAE=69.3452 MAE=69.2617 MAE=67.6404 Epoch: 110/500MAE=69.7363 MAE=71.3130 MAE=71.4526 MAE=69.1180 MAE=66.8823 MAE=64.9724 MAE=64.8215 MAE=65.0384 MAE=64.5628 MAE=63.7735 Epoch: 120/500MAE=66.2619 MAE=64.6035 MAE=65.0106 MAE=67.6197 MAE=64.4222 MAE=63.7799 MAE=63.1076 MAE=63.5598 MAE=63.6872 MAE=63.1719 Epoch: 130/500MAE=63.9383 MAE=63.2334 MAE=62.7876 MAE=63.0101 MAE=62.1485 MAE=63.6184 MAE=62.4050 MAE=62.9640 MAE=62.9030 MAE=62.4817 Epoch: 140/500MAE=62.4520 MAE=62.8391 MAE=62.7060 MAE=62.6725 MAE=62.8529 MAE=62.7335 MAE=62.6926 MAE=62.5361 MAE=62.5906 MAE=62.7720 Epoch: 150/500MAE=62.5336 MAE=62.6700 MAE=62.7814 MAE=62.5334 MAE=62.5441 MAE=62.9399 MAE=62.5324 MAE=62.4703 MAE=62.5537 MAE=62.6840 Epoch: 160/500MAE=62.5568 MAE=62.5276 MAE=62.6822 MAE=62.7042 MAE=62.6664 MAE=62.6149 MAE=62.6418 MAE=62.5977 MAE=62.4839 MAE=62.6797 Epoch: 170/500MAE=62.6312 MAE=62.6203 MAE=62.6529 MAE=62.6667 MAE=62.7695 MAE=62.4960 MAE=62.6526 MAE=62.4583 MAE=62.6811 MAE=62.8263 Epoch: 180/500MAE=62.4593 MAE=62.6559 MAE=62.6808 MAE=62.6360 MAE=62.7406 MAE=62.6832 MAE=62.4950 MAE=62.6755 MAE=62.4830 MAE=62.4718 Epoch: 190/500MAE=62.6369 MAE=62.8122 MAE=62.6145 MAE=62.6188 MAE=62.4898 MAE=62.4579 MAE=62.4463 MAE=62.5903 MAE=62.4721 MAE=62.4404 Epoch: 200/500MAE=62.8198 MAE=62.4776 MAE=62.7593 MAE=62.6021 MAE=62.4304 MAE=62.5922 MAE=62.4298 MAE=62.3367 MAE=62.6504 MAE=62.3968 Epoch: 210/500MAE=62.4043 MAE=62.6722 MAE=62.4518 MAE=62.7390 MAE=62.4774 MAE=62.7240 MAE=62.6347 MAE=62.4185 MAE=62.4979 MAE=62.6072 Epoch: 220/500MAE=62.4864 MAE=62.6186 MAE=62.4296 MAE=62.4656 MAE=62.7529 MAE=62.4626 MAE=62.4633 MAE=62.4333 MAE=62.6261 MAE=62.4580 Epoch: 230/500MAE=62.3926 MAE=62.5364 MAE=62.5905 MAE=62.5320 MAE=62.5192 MAE=62.4925 MAE=62.3360 MAE=62.5942 MAE=62.3139 MAE=62.3772 Epoch: 240/500MAE=62.3832 MAE=62.4993 MAE=62.5241 MAE=62.3231 MAE=62.2841 MAE=62.4927 MAE=62.5037 MAE=62.6388 MAE=62.5548 MAE=62.3199 Epoch: 250/500MAE=62.3611 MAE=62.3287 MAE=62.5180 MAE=62.4912 MAE=62.5227 MAE=62.2564 MAE=62.3005 MAE=62.4642 MAE=62.3179 MAE=62.4689 Epoch: 260/500MAE=62.2820 MAE=62.5153 MAE=62.5874 MAE=62.3874 MAE=62.3385 MAE=62.4942 MAE=62.3284 MAE=62.3611 MAE=62.3339 MAE=62.3213 Epoch: 270/500MAE=62.3201 MAE=62.5315 MAE=62.5889 MAE=62.4126 MAE=62.6679 MAE=62.3922 MAE=62.4384 MAE=62.4399 MAE=62.5832 MAE=62.4047 Epoch: 280/500MAE=62.4218 MAE=62.5718 MAE=62.4069 MAE=62.5784 MAE=62.4475 Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 136, in <module>\n",
      "    _, metric_te = trainer.fit_and_test(loader_tr[i], loader_va[i], loader_te[i], log_train_results=args.log_train_results,\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 177, in _train_test_reg\n",
      "    self.model.load_state_dict(torch.load(save_path), strict=False)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for CGI:\n",
      "\tMissing key(s) in state_dict: \"pools.0.transform.lin_rel.weight\", \"pools.0.transform.lin_rel.bias\", \"pools.0.transform.lin_root.weight\", \"pools.0.pp_conv.lin_rel.weight\", \"pools.0.pp_conv.lin_rel.bias\", \"pools.0.pp_conv.lin_root.weight\", \"pools.0.np_conv.lin_rel.weight\", \"pools.0.np_conv.lin_rel.bias\", \"pools.0.np_conv.lin_root.weight\", \"pools.0.positive_pooling.lin_rel.weight\", \"pools.0.positive_pooling.lin_rel.bias\", \"pools.0.positive_pooling.lin_root.weight\", \"pools.0.negative_pooling.lin_rel.weight\", \"pools.0.negative_pooling.lin_rel.bias\", \"pools.0.negative_pooling.lin_root.weight\", \"pools.0.discriminator.fc1.weight\", \"pools.0.discriminator.fc1.bias\", \"pools.0.discriminator.fc2.weight\", \"pools.0.discriminator.fc2.bias\", \"pools.1.transform.lin_rel.weight\", \"pools.1.transform.lin_rel.bias\", \"pools.1.transform.lin_root.weight\", \"pools.1.pp_conv.lin_rel.weight\", \"pools.1.pp_conv.lin_rel.bias\", \"pools.1.pp_conv.lin_root.weight\", \"pools.1.np_conv.lin_rel.weight\", \"pools.1.np_conv.lin_rel.bias\", \"pools.1.np_conv.lin_root.weight\", \"pools.1.positive_pooling.lin_rel.weight\", \"pools.1.positive_pooling.lin_rel.bias\", \"pools.1.positive_pooling.lin_root.weight\", \"pools.1.negative_pooling.lin_rel.weight\", \"pools.1.negative_pooling.lin_rel.bias\", \"pools.1.negative_pooling.lin_root.weight\", \"pools.1.discriminator.fc1.weight\", \"pools.1.discriminator.fc1.bias\", \"pools.1.discriminator.fc2.weight\", \"pools.1.discriminator.fc2.bias\", \"pools.2.transform.lin_rel.weight\", \"pools.2.transform.lin_rel.bias\", \"pools.2.transform.lin_root.weight\", \"pools.2.pp_conv.lin_rel.weight\", \"pools.2.pp_conv.lin_rel.bias\", \"pools.2.pp_conv.lin_root.weight\", \"pools.2.np_conv.lin_rel.weight\", \"pools.2.np_conv.lin_rel.bias\", \"pools.2.np_conv.lin_root.weight\", \"pools.2.positive_pooling.lin_rel.weight\", \"pools.2.positive_pooling.lin_rel.bias\", \"pools.2.positive_pooling.lin_root.weight\", \"pools.2.negative_pooling.lin_rel.weight\", \"pools.2.negative_pooling.lin_rel.bias\", \"pools.2.negative_pooling.lin_root.weight\", \"pools.2.discriminator.fc1.weight\", \"pools.2.discriminator.fc1.bias\", \"pools.2.discriminator.fc2.weight\", \"pools.2.discriminator.fc2.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"pools.0.lin.weight\", \"pools.0.lin.bias\", \"pools.1.lin.weight\", \"pools.1.lin.bias\", \"pools.2.lin.weight\", \"pools.2.lin.bias\". \n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 0 --run_times=5 --patience=150 --epochs=500 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 0 --run_times=5 --patience=150 --epochs=500 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 0 --run_times=5 --patience=150 --epochs=500 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 0 --run_times=5 --patience=150 --epochs=500 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 0 --run_times=5 --patience=150 --epochs=500 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm8.pt\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.1306 MAE=0.0994 MAE=0.0766 MAE=0.0557 MAE=0.0419 MAE=0.0346 MAE=0.0345 MAE=0.0331 MAE=0.0376 Epoch: 10/150MAE=0.0352 MAE=0.0326 MAE=0.0297 MAE=0.0295 MAE=0.0282 MAE=0.0280 MAE=0.0276 MAE=0.0290 MAE=0.0291 MAE=0.0326 Epoch: 20/150MAE=0.0294 MAE=0.0271 MAE=0.0282 MAE=0.0266 MAE=0.0263 MAE=0.0270 MAE=0.0263 MAE=0.0265 MAE=0.0255 MAE=0.0252 Epoch: 30/150MAE=0.0257 MAE=0.0250 MAE=0.0253 MAE=0.0250 MAE=0.0249 MAE=0.0259 MAE=0.0255 MAE=0.0250 MAE=0.0261 MAE=0.0251 Epoch: 40/150MAE=0.0246 MAE=0.0250 MAE=0.0245 MAE=0.0248 MAE=0.0251 MAE=0.0245 MAE=0.0247 MAE=0.0248 MAE=0.0246 MAE=0.0244 Epoch: 50/150MAE=0.0245 MAE=0.0247 MAE=0.0246 MAE=0.0245 MAE=0.0244 MAE=0.0245 MAE=0.0245 MAE=0.0244 MAE=0.0244 MAE=0.0244 Epoch: 60/150MAE=0.0244 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0242 MAE=0.0243 MAE=0.0243 MAE=0.0242 MAE=0.0242 MAE=0.0242 Epoch: 70/150MAE=0.0242 MAE=0.0243 MAE=0.0243 MAE=0.0242 MAE=0.0242 MAE=0.0242 MAE=0.0242 MAE=0.0244 MAE=0.0247 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.025 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.1188 MAE=0.0989 MAE=0.0818 MAE=0.0618 MAE=0.0479 MAE=0.0440 MAE=0.0402 MAE=0.0377 MAE=0.0380 Epoch: 10/150MAE=0.0373 MAE=0.0367 MAE=0.0369 MAE=0.0384 MAE=0.0341 MAE=0.0355 MAE=0.0306 MAE=0.0296 MAE=0.0311 MAE=0.0330 Epoch: 20/150MAE=0.0322 MAE=0.0331 MAE=0.0327 MAE=0.0289 MAE=0.0280 MAE=0.0273 MAE=0.0271 MAE=0.0271 MAE=0.0262 MAE=0.0265 Epoch: 30/150MAE=0.0255 MAE=0.0270 MAE=0.0262 MAE=0.0269 MAE=0.0271 MAE=0.0258 MAE=0.0254 MAE=0.0257 MAE=0.0253 MAE=0.0252 Epoch: 40/150MAE=0.0257 MAE=0.0257 MAE=0.0262 MAE=0.0256 MAE=0.0252 MAE=0.0253 MAE=0.0254 MAE=0.0258 MAE=0.0253 MAE=0.0255 MAE=0.0258 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.025 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1064 MAE=0.0744 MAE=0.0570 MAE=0.0409 MAE=0.0367 MAE=0.0355 MAE=0.0384 MAE=0.0356 MAE=0.0331 Epoch: 10/150MAE=0.0351 MAE=0.0325 MAE=0.0321 MAE=0.0306 MAE=0.0314 MAE=0.0303 MAE=0.0303 MAE=0.0329 MAE=0.0332 MAE=0.0361 Epoch: 20/150MAE=0.0318 MAE=0.0312 MAE=0.0288 MAE=0.0274 MAE=0.0281 MAE=0.0281 MAE=0.0276 MAE=0.0275 MAE=0.0277 MAE=0.0277 Epoch: 30/150MAE=0.0272 MAE=0.0274 MAE=0.0272 MAE=0.0273 MAE=0.0273 MAE=0.0271 MAE=0.0268 MAE=0.0269 MAE=0.0263 MAE=0.0267 Epoch: 40/150MAE=0.0266 MAE=0.0271 MAE=0.0270 MAE=0.0271 MAE=0.0272 MAE=0.0271 MAE=0.0270 MAE=0.0269 MAE=0.0267 MAE=0.0262 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.026 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1182 MAE=0.0930 MAE=0.0744 MAE=0.0571 MAE=0.0416 MAE=0.0352 MAE=0.0331 MAE=0.0342 MAE=0.0333 Epoch: 10/150MAE=0.0364 MAE=0.0353 MAE=0.0328 MAE=0.0312 MAE=0.0309 MAE=0.0307 MAE=0.0297 MAE=0.0294 MAE=0.0291 MAE=0.0288 Epoch: 20/150MAE=0.0287 MAE=0.0367 MAE=0.0303 MAE=0.0303 MAE=0.0286 MAE=0.0325 MAE=0.0311 MAE=0.0280 MAE=0.0271 MAE=0.0300 Epoch: 30/150MAE=0.0295 MAE=0.0266 MAE=0.0292 MAE=0.0306 MAE=0.0324 MAE=0.0304 MAE=0.0283 MAE=0.0305 MAE=0.0297 MAE=0.0309 Epoch: 40/150MAE=0.0301 MAE=0.0306 MAE=0.0271 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.026 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1170 MAE=0.0944 MAE=0.0767 MAE=0.0554 MAE=0.0387 MAE=0.0349 MAE=0.0338 MAE=0.0336 MAE=0.0352 Epoch: 10/150MAE=0.0341 MAE=0.0332 MAE=0.0315 MAE=0.0307 MAE=0.0299 MAE=0.0332 MAE=0.0306 MAE=0.0363 MAE=0.0333 MAE=0.0321 Epoch: 20/150MAE=0.0296 MAE=0.0289 MAE=0.0295 MAE=0.0295 MAE=0.0276 MAE=0.0276 MAE=0.0284 MAE=0.0274 MAE=0.0263 MAE=0.0272 Epoch: 30/150MAE=0.0271 MAE=0.0306 MAE=0.0281 MAE=0.0277 MAE=0.0285 MAE=0.0266 MAE=0.0275 MAE=0.0271 MAE=0.0272 MAE=0.0271 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.026 +/- 0.001\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm8.pt\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.1010 MAE=0.0810 MAE=0.0643 MAE=0.0466 MAE=0.0442 MAE=0.0417 MAE=0.0367 MAE=0.0320 MAE=0.0563 Epoch: 10/150MAE=0.0558 MAE=0.0368 MAE=0.0320 MAE=0.0320 MAE=0.0310 MAE=0.0307 MAE=0.0300 MAE=0.0304 MAE=0.0302 MAE=0.0295 Epoch: 20/150MAE=0.0297 MAE=0.0299 MAE=0.0287 MAE=0.0279 MAE=0.0276 MAE=0.0276 MAE=0.0285 MAE=0.0286 MAE=0.0280 MAE=0.0266 Epoch: 30/150MAE=0.0265 MAE=0.0263 MAE=0.0340 MAE=0.0394 MAE=0.0332 MAE=0.0297 MAE=0.0280 MAE=0.0268 MAE=0.0270 MAE=0.0261 Epoch: 40/150MAE=0.0257 MAE=0.0263 MAE=0.0260 MAE=0.0265 MAE=0.0268 MAE=0.0257 MAE=0.0255 MAE=0.0258 MAE=0.0263 MAE=0.0262 Epoch: 50/150MAE=0.0260 MAE=0.0259 MAE=0.0260 MAE=0.0260 MAE=0.0258 MAE=0.0260 MAE=0.0262 MAE=0.0261 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.026 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.0821 MAE=0.0696 MAE=0.0576 MAE=0.0471 MAE=0.0416 MAE=0.0453 MAE=0.0346 MAE=0.0378 MAE=0.0330 Epoch: 10/150MAE=0.0312 MAE=0.0342 MAE=0.0344 MAE=0.0290 MAE=0.0288 MAE=0.0281 MAE=0.0305 MAE=0.0284 MAE=0.0271 MAE=0.0276 Epoch: 20/150MAE=0.0307 MAE=0.0261 MAE=0.0260 MAE=0.0263 MAE=0.0265 MAE=0.0277 MAE=0.0281 MAE=0.0292 MAE=0.0257 MAE=0.0253 Epoch: 30/150MAE=0.0257 MAE=0.0270 MAE=0.0282 MAE=0.0268 MAE=0.0255 MAE=0.0252 MAE=0.0252 MAE=0.0259 MAE=0.0258 MAE=0.0254 Epoch: 40/150MAE=0.0249 MAE=0.0250 MAE=0.0250 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0247 MAE=0.0248 MAE=0.0248 MAE=0.0247 Epoch: 50/150MAE=0.0247 MAE=0.0246 MAE=0.0246 MAE=0.0246 MAE=0.0248 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0247 Epoch: 60/150MAE=0.0247 MAE=0.0247 MAE=0.0247 MAE=0.0251 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.026 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.1102 MAE=0.0648 MAE=0.0550 MAE=0.0451 MAE=0.0368 MAE=0.0370 MAE=0.0339 MAE=0.0340 MAE=0.0347 Epoch: 10/150MAE=0.0354 MAE=0.0315 MAE=0.0312 MAE=0.0291 MAE=0.0298 MAE=0.0285 MAE=0.0273 MAE=0.0268 MAE=0.0270 MAE=0.0259 Epoch: 20/150MAE=0.0270 MAE=0.0267 MAE=0.0263 MAE=0.0259 MAE=0.0277 MAE=0.0260 MAE=0.0277 MAE=0.0265 MAE=0.0311 MAE=0.0289 Epoch: 30/150MAE=0.0269 MAE=0.0248 MAE=0.0244 MAE=0.0241 MAE=0.0251 MAE=0.0245 MAE=0.0242 MAE=0.0239 MAE=0.0238 MAE=0.0240 Epoch: 40/150MAE=0.0243 MAE=0.0233 MAE=0.0264 MAE=0.0254 MAE=0.0280 MAE=0.0246 MAE=0.0251 MAE=0.0237 MAE=0.0245 MAE=0.0243 Epoch: 50/150MAE=0.0246 MAE=0.0248 MAE=0.0240 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.025 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.0980 MAE=0.0806 MAE=0.0636 MAE=0.0512 MAE=0.0427 MAE=0.0375 MAE=0.0346 MAE=0.0354 MAE=0.0318 Epoch: 10/150MAE=0.0352 MAE=0.0311 MAE=0.0317 MAE=0.1010 MAE=0.0725 MAE=0.0443 MAE=0.0368 MAE=0.0343 MAE=0.0342 MAE=0.0332 Epoch: 20/150MAE=0.0320 MAE=0.0309 MAE=0.0304 MAE=0.0305 MAE=0.0311 MAE=0.0301 MAE=0.0296 MAE=0.0294 MAE=0.0300 MAE=0.0289 Epoch: 30/150MAE=0.0280 MAE=0.0280 MAE=0.0282 MAE=0.0295 MAE=0.0288 MAE=0.0282 MAE=0.0274 MAE=0.0272 MAE=0.0282 MAE=0.0280 Epoch: 40/150MAE=0.0274 MAE=0.0273 MAE=0.0272 MAE=0.0275 MAE=0.0274 MAE=0.0274 MAE=0.0275 MAE=0.0273 MAE=0.0278 MAE=0.0274 Epoch: 50/150MAE=0.0271 MAE=0.0268 MAE=0.0268 MAE=0.0266 MAE=0.0281 MAE=0.0276 MAE=0.0273 MAE=0.0270 MAE=0.0268 MAE=0.0272 Epoch: 60/150MAE=0.0269 MAE=0.0268 MAE=0.0283 MAE=0.0284 MAE=0.0271 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.026 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1082 MAE=0.0712 MAE=0.0611 MAE=0.0469 MAE=0.0397 MAE=0.0349 MAE=0.0346 MAE=0.0315 MAE=0.0329 Epoch: 10/150MAE=0.0329 MAE=0.0322 MAE=0.0485 MAE=0.0400 MAE=0.0321 MAE=0.0290 MAE=0.0306 MAE=0.0301 MAE=0.0280 MAE=0.0276 Epoch: 20/150MAE=0.0275 MAE=0.0271 MAE=0.0295 MAE=0.0280 MAE=0.0283 MAE=0.0285 MAE=0.0277 MAE=0.0269 MAE=0.0265 MAE=0.0266 Epoch: 30/150MAE=0.0260 MAE=0.0264 MAE=0.0271 MAE=0.0261 MAE=0.0262 MAE=0.0258 MAE=0.0259 MAE=0.0259 MAE=0.0258 MAE=0.0257 Epoch: 40/150MAE=0.0258 MAE=0.0257 MAE=0.0256 MAE=0.0260 MAE=0.0254 MAE=0.0256 MAE=0.0255 MAE=0.0252 MAE=0.0254 MAE=0.0253 Epoch: 50/150MAE=0.0251 MAE=0.0250 MAE=0.0251 MAE=0.0252 MAE=0.0301 MAE=0.0277 MAE=0.0264 MAE=0.0258 MAE=0.0251 MAE=0.0249 Epoch: 60/150MAE=0.0247 MAE=0.0247 MAE=0.0246 MAE=0.0247 MAE=0.0246 MAE=0.0246 MAE=0.0247 MAE=0.0246 MAE=0.0245 MAE=0.0245 Epoch: 70/150MAE=0.0246 MAE=0.0246 MAE=0.0245 MAE=0.0246 MAE=0.0245 MAE=0.0245 MAE=0.0245 MAE=0.0246 MAE=0.0246 MAE=0.0245 Epoch: 80/150MAE=0.0245 MAE=0.0245 MAE=0.0246 MAE=0.0249 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.025 +/- 0.001\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm8.pt\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.0866 MAE=0.0535 MAE=0.0581 MAE=0.0496 MAE=0.0397 MAE=0.0355 MAE=0.0420 MAE=0.0360 MAE=0.0322 Epoch: 10/150MAE=0.0323 MAE=0.0338 MAE=0.0402 MAE=0.0294 MAE=0.0287 MAE=0.0315 MAE=0.0282 MAE=0.0287 MAE=0.0284 MAE=0.0260 Epoch: 20/150MAE=0.0254 MAE=0.0277 MAE=0.0241 MAE=0.0251 MAE=0.0228 MAE=0.0230 MAE=0.0238 MAE=0.0290 MAE=0.0239 MAE=0.0227 Epoch: 30/150MAE=0.0237 MAE=0.0270 MAE=0.0241 MAE=0.0247 MAE=0.0228 MAE=0.0221 MAE=0.0223 MAE=0.0218 MAE=0.0218 MAE=0.0216 Epoch: 40/150MAE=0.0217 MAE=0.0222 MAE=0.0218 MAE=0.0219 MAE=0.0219 MAE=0.0217 MAE=0.0223 MAE=0.0221 MAE=0.0218 MAE=0.0217 MAE=0.0222 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.022 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.0823 MAE=0.0622 MAE=0.0510 MAE=0.0459 MAE=0.0466 MAE=0.0542 MAE=0.0394 MAE=0.0358 MAE=0.0381 Epoch: 10/150MAE=0.0368 MAE=0.0369 MAE=0.0338 MAE=0.0355 MAE=0.0376 MAE=0.0361 MAE=0.0320 MAE=0.0297 MAE=0.0286 MAE=0.0272 Epoch: 20/150MAE=0.0278 MAE=0.0265 MAE=0.0247 MAE=0.0251 MAE=0.0243 MAE=0.0259 MAE=0.0244 MAE=0.0244 MAE=0.0294 MAE=0.0233 Epoch: 30/150MAE=0.0246 MAE=0.0240 MAE=0.0242 MAE=0.0231 MAE=0.0256 MAE=0.0240 MAE=0.0247 MAE=0.0245 MAE=0.0245 MAE=0.0243 Epoch: 40/150MAE=0.0247 MAE=0.0239 MAE=0.0245 MAE=0.0244 MAE=0.0235 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.023 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.0921 MAE=0.0662 MAE=0.0478 MAE=0.0380 MAE=0.0375 MAE=0.0333 MAE=0.0350 MAE=0.0421 MAE=0.0453 Epoch: 10/150MAE=0.0326 MAE=0.0443 MAE=0.0497 MAE=0.0470 MAE=0.0429 MAE=0.0329 MAE=0.0392 MAE=0.0417 MAE=0.0318 MAE=0.0295 Epoch: 20/150MAE=0.0284 MAE=0.0289 MAE=0.0311 MAE=0.0306 MAE=0.0302 MAE=0.0293 MAE=0.0298 MAE=0.0287 MAE=0.0278 MAE=0.0277 Epoch: 30/150MAE=0.0279 MAE=0.0281 MAE=0.0288 MAE=0.0275 MAE=0.0265 MAE=0.0266 MAE=0.0277 MAE=0.0259 MAE=0.0248 MAE=0.0251 Epoch: 40/150MAE=0.0279 MAE=0.0283 MAE=0.0262 MAE=0.0263 MAE=0.0251 MAE=0.0241 MAE=0.0238 MAE=0.0246 MAE=0.0242 MAE=0.0240 Epoch: 50/150MAE=0.0240 MAE=0.0239 MAE=0.0237 MAE=0.0235 MAE=0.0235 MAE=0.0237 MAE=0.0238 MAE=0.0233 MAE=0.0232 MAE=0.0234 Epoch: 60/150MAE=0.0231 MAE=0.0233 MAE=0.0233 MAE=0.0233 MAE=0.0240 MAE=0.0241 MAE=0.0235 MAE=0.0232 MAE=0.0237 MAE=0.0239 Epoch: 70/150MAE=0.0240 MAE=0.0234 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.023 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1049 MAE=0.0721 MAE=0.0554 MAE=0.0466 MAE=0.0374 MAE=0.0339 MAE=0.0348 MAE=0.0355 MAE=0.0390 Epoch: 10/150MAE=0.0342 MAE=0.0315 MAE=0.0297 MAE=0.0297 MAE=0.0308 MAE=0.0306 MAE=0.0295 MAE=0.0326 MAE=0.0314 MAE=0.0289 Epoch: 20/150MAE=0.0273 MAE=0.0280 MAE=0.0309 MAE=0.0328 MAE=0.0339 MAE=0.0314 MAE=0.0289 MAE=0.0280 MAE=0.0272 MAE=0.0264 Epoch: 30/150MAE=0.0258 MAE=0.0284 MAE=0.0277 MAE=0.0261 MAE=0.0262 MAE=0.0262 MAE=0.0270 MAE=0.0275 MAE=0.0274 MAE=0.0270 Epoch: 40/150MAE=0.0266 MAE=0.0263 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.024 +/- 0.002\n",
      "\n",
      "Epoch: 1/150MAE=0.1014 MAE=0.0449 MAE=0.0474 MAE=0.0443 MAE=0.0445 MAE=0.0387 MAE=0.0409 MAE=0.0439 MAE=0.0404 Epoch: 10/150MAE=0.0354 MAE=0.0344 MAE=0.0337 MAE=0.0393 MAE=0.0419 MAE=0.0331 MAE=0.0334 MAE=0.0322 MAE=0.0308 MAE=0.0305 Epoch: 20/150MAE=0.0284 MAE=0.0286 MAE=0.0287 MAE=0.0272 MAE=0.0266 MAE=0.0259 MAE=0.0324 MAE=0.0306 MAE=0.0280 MAE=0.0269 Epoch: 30/150MAE=0.0254 MAE=0.0249 MAE=0.0263 MAE=0.0256 MAE=0.0250 MAE=0.0242 MAE=0.0244 MAE=0.0256 MAE=0.0269 MAE=0.0251 Epoch: 40/150MAE=0.0249 MAE=0.0243 MAE=0.0243 MAE=0.0240 MAE=0.0242 MAE=0.0239 MAE=0.0238 MAE=0.0235 MAE=0.0238 MAE=0.0238 Epoch: 50/150MAE=0.0235 MAE=0.0237 MAE=0.0237 MAE=0.0236 MAE=0.0236 MAE=0.0234 MAE=0.0235 MAE=0.0235 MAE=0.0233 MAE=0.0235 Epoch: 60/150MAE=0.0234 MAE=0.0234 MAE=0.0236 MAE=0.0233 MAE=0.0234 MAE=0.0234 MAE=0.0233 MAE=0.0233 MAE=0.0233 MAE=0.0233 Epoch: 70/150MAE=0.0232 MAE=0.0233 MAE=0.0233 MAE=0.0233 MAE=0.0232 MAE=0.0232 MAE=0.0231 MAE=0.0231 MAE=0.0232 MAE=0.0232 Epoch: 80/150MAE=0.0231 MAE=0.0231 MAE=0.0230 MAE=0.0231 MAE=0.0231 MAE=0.0231 MAE=0.0230 MAE=0.0230 MAE=0.0230 MAE=0.0229 Epoch: 90/150MAE=0.0229 MAE=0.0229 MAE=0.0228 MAE=0.0229 MAE=0.0229 MAE=0.0230 MAE=0.0229 MAE=0.0229 MAE=0.0228 MAE=0.0229 Epoch: 100/150MAE=0.0229 MAE=0.0229 MAE=0.0229 MAE=0.0234 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.024 +/- 0.001\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm8.pt\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.0799 MAE=0.0498 MAE=0.0452 MAE=0.0397 MAE=0.0432 MAE=0.0359 MAE=0.0395 MAE=0.0366 MAE=0.0316 Epoch: 10/150MAE=0.0300 MAE=0.0316 MAE=0.0278 MAE=0.0282 MAE=0.0282 MAE=0.0279 MAE=0.0271 MAE=0.0271 MAE=0.0279 MAE=0.0268 Epoch: 20/150MAE=0.0256 MAE=0.0258 MAE=0.0296 MAE=0.0275 MAE=0.0267 MAE=0.0253 MAE=0.0248 MAE=0.0249 MAE=0.0245 MAE=0.0241 Epoch: 30/150MAE=0.0247 MAE=0.0244 MAE=0.0239 MAE=0.0242 MAE=0.0239 MAE=0.0235 MAE=0.0235 MAE=0.0235 MAE=0.0239 MAE=0.0236 Epoch: 40/150MAE=0.0229 MAE=0.0233 MAE=0.0231 MAE=0.0227 MAE=0.0227 MAE=0.0229 MAE=0.0228 MAE=0.0226 MAE=0.0240 MAE=0.0229 Epoch: 50/150MAE=0.0229 MAE=0.0237 MAE=0.0224 MAE=0.0221 MAE=0.0219 MAE=0.0221 MAE=0.0219 MAE=0.0218 MAE=0.0217 MAE=0.0211 Epoch: 60/150MAE=0.0209 MAE=0.0213 MAE=0.0212 MAE=0.0215 MAE=0.0212 MAE=0.0212 MAE=0.0209 MAE=0.0210 MAE=0.0209 MAE=0.0211 Epoch: 70/150MAE=0.0208 MAE=0.0210 MAE=0.0207 MAE=0.0210 MAE=0.0207 MAE=0.0210 MAE=0.0206 MAE=0.0212 MAE=0.0207 MAE=0.0209 Epoch: 80/150MAE=0.0206 MAE=0.0209 MAE=0.0207 MAE=0.0209 MAE=0.0206 MAE=0.0207 MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0207 Epoch: 90/150MAE=0.0205 MAE=0.0206 MAE=0.0204 MAE=0.0207 MAE=0.0206 MAE=0.0205 MAE=0.0207 MAE=0.0205 MAE=0.0205 MAE=0.0206 Epoch: 100/150MAE=0.0204 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0206 MAE=0.0205 MAE=0.0205 MAE=0.0204 MAE=0.0206 MAE=0.0206 Epoch: 110/150MAE=0.0205 MAE=0.0210 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.021 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.0706 MAE=0.0535 MAE=0.0439 MAE=0.0382 MAE=0.0385 MAE=0.0350 MAE=0.0340 MAE=0.0319 MAE=0.0436 Epoch: 10/150MAE=0.0319 MAE=0.0293 MAE=0.0275 MAE=0.0276 MAE=0.0263 MAE=0.0270 MAE=0.0257 MAE=0.0255 MAE=0.0243 MAE=0.0240 Epoch: 20/150MAE=0.0234 MAE=0.0233 MAE=0.0233 MAE=0.0227 MAE=0.0225 MAE=0.0227 MAE=0.0225 MAE=0.0237 MAE=0.0220 MAE=0.0222 Epoch: 30/150MAE=0.0232 MAE=0.0220 MAE=0.0218 MAE=0.0224 MAE=0.0210 MAE=0.0216 MAE=0.0220 MAE=0.0216 MAE=0.0215 MAE=0.0214 Epoch: 40/150MAE=0.0208 MAE=0.0202 MAE=0.0199 MAE=0.0200 MAE=0.0199 MAE=0.0200 MAE=0.0208 MAE=0.0202 MAE=0.0199 MAE=0.0198 Epoch: 50/150MAE=0.0200 MAE=0.0197 MAE=0.0196 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0194 MAE=0.0194 MAE=0.0195 MAE=0.0194 Epoch: 60/150MAE=0.0193 MAE=0.0193 MAE=0.0194 MAE=0.0193 MAE=0.0193 MAE=0.0192 MAE=0.0192 MAE=0.0191 MAE=0.0192 MAE=0.0191 Epoch: 70/150MAE=0.0192 MAE=0.0191 MAE=0.0191 MAE=0.0191 MAE=0.0191 MAE=0.0192 MAE=0.0191 MAE=0.0191 MAE=0.0191 MAE=0.0191 Epoch: 80/150MAE=0.0191 MAE=0.0191 MAE=0.0190 MAE=0.0190 MAE=0.0190 MAE=0.0190 MAE=0.0190 MAE=0.0191 MAE=0.0191 MAE=0.0190 Epoch: 90/150MAE=0.0190 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0190 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 Epoch: 100/150MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0188 MAE=0.0189 MAE=0.0188 MAE=0.0188 MAE=0.0188 Epoch: 110/150MAE=0.0189 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 Epoch: 120/150MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0187 MAE=0.0188 MAE=0.0187 MAE=0.0187 MAE=0.0188 MAE=0.0187 Epoch: 130/150MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 Epoch: 140/150MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0186 MAE=0.0186 MAE=0.0186 MAE=0.0186 MAE=0.0186 MAE=0.0187 Epoch: 150/150MAE=0.0187 MAE=0.0190 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.020 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.0930 MAE=0.0439 MAE=0.0455 MAE=0.0386 MAE=0.0378 MAE=0.0370 MAE=0.0358 MAE=0.0396 MAE=0.0358 Epoch: 10/150MAE=0.0320 MAE=0.0311 MAE=0.0298 MAE=0.0296 MAE=0.0299 MAE=0.0278 MAE=0.0267 MAE=0.0263 MAE=0.0260 MAE=0.0270 Epoch: 20/150MAE=0.0490 MAE=0.0552 MAE=0.0363 MAE=0.0306 MAE=0.0282 MAE=0.0275 MAE=0.0268 MAE=0.0263 MAE=0.0259 MAE=0.0277 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.023 +/- 0.004\n",
      "\n",
      "Epoch: 1/150MAE=0.0687 MAE=0.0446 MAE=0.0441 MAE=0.0377 MAE=0.0349 MAE=0.0326 MAE=0.0317 MAE=0.0311 MAE=0.0323 Epoch: 10/150MAE=0.0320 MAE=0.0289 MAE=0.0308 MAE=0.0281 MAE=0.0266 MAE=0.0258 MAE=0.0249 MAE=0.0251 MAE=0.0265 MAE=0.0250 Epoch: 20/150MAE=0.0280 MAE=0.0254 MAE=0.0250 MAE=0.0236 MAE=0.0235 MAE=0.0233 MAE=0.0231 MAE=0.0228 MAE=0.0229 MAE=0.0225 Epoch: 30/150MAE=0.0225 MAE=0.0223 MAE=0.0224 MAE=0.0221 MAE=0.0241 MAE=0.0264 MAE=0.0254 MAE=0.0233 MAE=0.0225 MAE=0.0225 Epoch: 40/150MAE=0.0228 MAE=0.0223 MAE=0.0224 MAE=0.0223 MAE=0.0224 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.023 +/- 0.003\n",
      "\n",
      "Epoch: 1/150MAE=0.0765 MAE=0.0475 MAE=0.0427 MAE=0.0428 MAE=0.0409 MAE=0.0359 MAE=0.0344 MAE=0.0326 MAE=0.0345 Epoch: 10/150MAE=0.0324 MAE=0.0281 MAE=0.0268 MAE=0.0273 MAE=0.0259 MAE=0.0300 MAE=0.0270 MAE=0.0249 MAE=0.0260 MAE=0.0255 Epoch: 20/150MAE=0.0234 MAE=0.0232 MAE=0.0241 MAE=0.0246 MAE=0.0233 MAE=0.0242 MAE=0.0238 MAE=0.0223 MAE=0.0220 MAE=0.0217 Epoch: 30/150MAE=0.0216 MAE=0.0213 MAE=0.0218 MAE=0.0222 MAE=0.0214 MAE=0.0220 MAE=0.0216 MAE=0.0216 MAE=0.0216 MAE=0.0233 Epoch: 40/150MAE=0.0222 MAE=0.0214 MAE=0.0219 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.022 +/- 0.003\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm8.pt\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.0674 MAE=0.0503 MAE=0.0373 MAE=0.0376 MAE=0.0398 MAE=0.0352 MAE=0.0349 MAE=0.0339 MAE=0.0319 Epoch: 10/150MAE=0.0311 MAE=0.0290 MAE=0.0306 MAE=0.0332 MAE=0.0303 MAE=0.0277 MAE=0.0271 MAE=0.0259 MAE=0.0252 MAE=0.0269 Epoch: 20/150MAE=0.0275 MAE=0.0260 MAE=0.0246 MAE=0.0241 MAE=0.0246 MAE=0.0235 MAE=0.0240 MAE=0.0232 MAE=0.0231 MAE=0.0226 Epoch: 30/150MAE=0.0239 MAE=0.0221 MAE=0.0235 MAE=0.0229 MAE=0.0216 MAE=0.0219 MAE=0.0219 MAE=0.0216 MAE=0.0212 MAE=0.0209 Epoch: 40/150MAE=0.0211 MAE=0.0213 MAE=0.0215 MAE=0.0215 MAE=0.0203 MAE=0.0201 MAE=0.0198 MAE=0.0197 MAE=0.0197 MAE=0.0198 Epoch: 50/150MAE=0.0199 MAE=0.0198 MAE=0.0196 MAE=0.0194 MAE=0.0194 MAE=0.0193 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0193 Epoch: 60/150MAE=0.0193 MAE=0.0193 MAE=0.0193 MAE=0.0193 MAE=0.0192 MAE=0.0192 MAE=0.0193 MAE=0.0191 MAE=0.0192 MAE=0.0194 Epoch: 70/150MAE=0.0191 MAE=0.0191 MAE=0.0191 MAE=0.0192 MAE=0.0191 MAE=0.0191 MAE=0.0190 MAE=0.0190 MAE=0.0190 MAE=0.0190 Epoch: 80/150MAE=0.0190 MAE=0.0190 MAE=0.0189 MAE=0.0189 MAE=0.0190 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 Epoch: 90/150MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0189 MAE=0.0194 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.019 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.0901 MAE=0.0545 MAE=0.0440 MAE=0.0444 MAE=0.0392 MAE=0.0363 MAE=0.0362 MAE=0.0340 MAE=0.0307 Epoch: 10/150MAE=0.0292 MAE=0.0281 MAE=0.0274 MAE=0.0289 MAE=0.0289 MAE=0.0267 MAE=0.0278 MAE=0.0286 MAE=0.0253 MAE=0.0250 Epoch: 20/150MAE=0.0247 MAE=0.0246 MAE=0.0262 MAE=0.0244 MAE=0.0229 MAE=0.0249 MAE=0.0267 MAE=0.0244 MAE=0.0237 MAE=0.0223 Epoch: 30/150MAE=0.0218 MAE=0.0213 MAE=0.0211 MAE=0.0212 MAE=0.0210 MAE=0.0210 MAE=0.0209 MAE=0.0208 MAE=0.0209 MAE=0.0212 Epoch: 40/150MAE=0.0207 MAE=0.0207 MAE=0.0211 MAE=0.0206 MAE=0.0208 MAE=0.0204 MAE=0.0203 MAE=0.0206 MAE=0.0208 MAE=0.0203 Epoch: 50/150MAE=0.0202 MAE=0.0238 MAE=0.0222 MAE=0.0211 MAE=0.0205 MAE=0.0201 MAE=0.0201 MAE=0.0199 MAE=0.0200 MAE=0.0199 Epoch: 60/150MAE=0.0199 MAE=0.0200 MAE=0.0198 MAE=0.0200 MAE=0.0197 MAE=0.0198 MAE=0.0198 MAE=0.0197 MAE=0.0197 MAE=0.0197 Epoch: 70/150MAE=0.0198 MAE=0.0197 MAE=0.0196 MAE=0.0196 MAE=0.0196 MAE=0.0198 MAE=0.0197 MAE=0.0195 MAE=0.0195 MAE=0.0197 Epoch: 80/150MAE=0.0196 MAE=0.0205 MAE=0.0203 MAE=0.0202 MAE=0.0196 MAE=0.0198 MAE=0.0197 MAE=0.0196 MAE=0.0195 MAE=0.0201 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.020 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.0932 MAE=0.0473 MAE=0.0402 MAE=0.0418 MAE=0.0355 MAE=0.0358 MAE=0.0415 MAE=0.0342 MAE=0.0306 Epoch: 10/150MAE=0.0308 MAE=0.0290 MAE=0.0275 MAE=0.0258 MAE=0.0249 MAE=0.0363 MAE=0.0333 MAE=0.0266 MAE=0.0253 MAE=0.0245 Epoch: 20/150MAE=0.0234 MAE=0.0233 MAE=0.0226 MAE=0.0221 MAE=0.0219 MAE=0.0220 MAE=0.0216 MAE=0.0215 MAE=0.0213 MAE=0.0213 Epoch: 30/150MAE=0.0214 MAE=0.0211 MAE=0.0211 MAE=0.0213 MAE=0.0212 MAE=0.0209 MAE=0.0214 MAE=0.0218 MAE=0.0264 MAE=0.0281 Epoch: 40/150MAE=0.0226 MAE=0.0214 MAE=0.0210 MAE=0.0206 MAE=0.0207 MAE=0.0202 MAE=0.0201 MAE=0.0202 MAE=0.0201 MAE=0.0200 Epoch: 50/150MAE=0.0200 MAE=0.0207 MAE=0.0211 MAE=0.0201 MAE=0.0200 MAE=0.0199 MAE=0.0198 MAE=0.0198 MAE=0.0197 MAE=0.0197 Epoch: 60/150MAE=0.0197 MAE=0.0197 MAE=0.0197 MAE=0.0198 MAE=0.0197 MAE=0.0197 MAE=0.0197 MAE=0.0197 MAE=0.0196 MAE=0.0196 Epoch: 70/150MAE=0.0196 MAE=0.0196 MAE=0.0196 MAE=0.0196 MAE=0.0196 MAE=0.0195 MAE=0.0195 MAE=0.0196 MAE=0.0196 MAE=0.0195 Epoch: 80/150MAE=0.0196 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0194 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0195 Epoch: 90/150MAE=0.0194 MAE=0.0195 MAE=0.0195 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 Epoch: 100/150MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 Epoch: 110/150MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0200 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.020 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.0640 MAE=0.0775 MAE=0.0480 MAE=0.0450 MAE=0.0434 MAE=0.0402 MAE=0.0385 MAE=0.0349 MAE=0.0341 Epoch: 10/150MAE=0.0350 MAE=0.0609 MAE=0.0406 MAE=0.0345 MAE=0.0316 MAE=0.0301 MAE=0.0292 MAE=0.0279 MAE=0.0275 MAE=0.0272 Epoch: 20/150MAE=0.0268 MAE=0.0267 MAE=0.0275 MAE=0.0264 MAE=0.0249 MAE=0.0250 MAE=0.0274 MAE=0.0268 MAE=0.0272 MAE=0.0249 Epoch: 30/150MAE=0.0238 MAE=0.0232 MAE=0.0232 MAE=0.0230 MAE=0.0229 MAE=0.0229 MAE=0.0228 MAE=0.0226 MAE=0.0225 MAE=0.0224 Epoch: 40/150MAE=0.0224 MAE=0.0222 MAE=0.0220 MAE=0.0218 MAE=0.0217 MAE=0.0217 MAE=0.0218 MAE=0.0216 MAE=0.0215 MAE=0.0213 Epoch: 50/150MAE=0.0212 MAE=0.0212 MAE=0.0211 MAE=0.0210 MAE=0.0210 MAE=0.0235 MAE=0.0221 MAE=0.0223 MAE=0.0214 MAE=0.0209 Epoch: 60/150MAE=0.0208 MAE=0.0208 MAE=0.0207 MAE=0.0206 MAE=0.0206 MAE=0.0207 MAE=0.0207 MAE=0.0206 MAE=0.0204 MAE=0.0204 Epoch: 70/150MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0204 MAE=0.0204 MAE=0.0204 Epoch: 80/150MAE=0.0203 MAE=0.0203 MAE=0.0204 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 MAE=0.0203 Epoch: 90/150MAE=0.0203 MAE=0.0203 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0203 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0203 Epoch: 100/150MAE=0.0202 MAE=0.0203 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 Epoch: 110/150MAE=0.0202 MAE=0.0203 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0202 MAE=0.0208 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.020 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.0850 MAE=0.0491 MAE=0.0583 MAE=0.0407 MAE=0.0378 MAE=0.0412 MAE=0.0351 MAE=0.0634 MAE=0.0417 Epoch: 10/150MAE=0.0372 MAE=0.0333 MAE=0.0315 MAE=0.0285 MAE=0.0282 MAE=0.0266 MAE=0.0260 MAE=0.0588 MAE=0.0348 MAE=0.0323 Epoch: 20/150MAE=0.0283 MAE=0.0266 MAE=0.0253 MAE=0.0249 MAE=0.0244 MAE=0.0240 MAE=0.0238 MAE=0.0234 MAE=0.0233 MAE=0.0229 Epoch: 30/150MAE=0.0227 MAE=0.0225 MAE=0.0225 MAE=0.0223 MAE=0.0221 MAE=0.0219 MAE=0.0217 MAE=0.0216 MAE=0.0214 MAE=0.0211 Epoch: 40/150MAE=0.0211 MAE=0.0210 MAE=0.0208 MAE=0.0207 MAE=0.0205 MAE=0.0204 MAE=0.0204 MAE=0.0204 MAE=0.0201 MAE=0.0202 Epoch: 50/150MAE=0.0201 MAE=0.0201 MAE=0.0200 MAE=0.0199 MAE=0.0201 MAE=0.0198 MAE=0.0197 MAE=0.0199 MAE=0.0197 MAE=0.0195 Epoch: 60/150MAE=0.0195 MAE=0.0196 MAE=0.0196 MAE=0.0195 MAE=0.0195 MAE=0.0193 MAE=0.0195 MAE=0.0191 MAE=0.0190 MAE=0.0189 Epoch: 70/150MAE=0.0190 MAE=0.0190 MAE=0.0190 MAE=0.0189 MAE=0.0189 MAE=0.0188 MAE=0.0188 MAE=0.0187 MAE=0.0187 MAE=0.0187 Epoch: 80/150MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0186 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0188 MAE=0.0185 MAE=0.0185 Epoch: 90/150MAE=0.0185 MAE=0.0185 MAE=0.0185 MAE=0.0185 MAE=0.0185 MAE=0.0185 MAE=0.0184 MAE=0.0184 MAE=0.0185 MAE=0.0185 Epoch: 100/150MAE=0.0185 MAE=0.0185 MAE=0.0184 MAE=0.0184 MAE=0.0183 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0185 Epoch: 110/150MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0184 MAE=0.0188 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.020 +/- 0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --cuda_num 0 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --cuda_num 0 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --cuda_num 0 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --cuda_num 0 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --cuda_num 0 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/bace.pt\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.812 +/- 0.000\n",
      "AUPRC: 0.758 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.830 +/- 0.018\n",
      "AUPRC: 0.788 +/- 0.030\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.826 +/- 0.016\n",
      "AUPRC: 0.780 +/- 0.027\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.829 +/- 0.015\n",
      "AUPRC: 0.789 +/- 0.028\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150Epoch: 40/150Epoch: 50/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.820 +/- 0.022\n",
      "AUPRC: 0.777 +/- 0.035\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/bace.pt\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.837 +/- 0.000\n",
      "AUPRC: 0.832 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.837 +/- 0.000\n",
      "AUPRC: 0.803 +/- 0.029\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150Epoch: 40/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.810 +/- 0.039\n",
      "AUPRC: 0.777 +/- 0.043\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.792 +/- 0.046\n",
      "AUPRC: 0.750 +/- 0.060\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.804 +/- 0.048\n",
      "AUPRC: 0.764 +/- 0.060\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/bace.pt\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.797 +/- 0.000\n",
      "AUPRC: 0.758 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.828 +/- 0.031\n",
      "AUPRC: 0.788 +/- 0.030\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.839 +/- 0.030\n",
      "AUPRC: 0.801 +/- 0.031\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.831 +/- 0.029\n",
      "AUPRC: 0.796 +/- 0.027\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150Epoch: 40/150Epoch: 50/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.835 +/- 0.027\n",
      "AUPRC: 0.793 +/- 0.026\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/bace.pt\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.787 +/- 0.000\n",
      "AUPRC: 0.727 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150Epoch: 40/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.813 +/- 0.027\n",
      "AUPRC: 0.752 +/- 0.025\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.828 +/- 0.030\n",
      "AUPRC: 0.767 +/- 0.029\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.805 +/- 0.047\n",
      "AUPRC: 0.758 +/- 0.030\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.797 +/- 0.045\n",
      "AUPRC: 0.752 +/- 0.029\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/bace.pt\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.815 +/- 0.000\n",
      "AUPRC: 0.756 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.794 +/- 0.021\n",
      "AUPRC: 0.749 +/- 0.007\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.797 +/- 0.018\n",
      "AUPRC: 0.741 +/- 0.013\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.797 +/- 0.016\n",
      "AUPRC: 0.737 +/- 0.013\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.814 +/- 0.035\n",
      "AUPRC: 0.755 +/- 0.038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/esol.pt\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=2.0718 RMSE=1.8997 RMSE=1.7036 RMSE=1.6062 RMSE=2.2478 RMSE=1.6341 RMSE=2.5753 RMSE=2.0837 RMSE=3.1815 Epoch: 10/150RMSE=1.5465 RMSE=1.3421 RMSE=1.6950 RMSE=1.9844 RMSE=1.2863 RMSE=1.4566 RMSE=1.2869 RMSE=1.3106 RMSE=1.3919 RMSE=2.1109 Epoch: 20/150RMSE=2.2135 RMSE=2.1410 RMSE=2.2133 RMSE=2.1879 RMSE=2.2280 RMSE=1.2859 RMSE=1.3158 RMSE=2.2738 RMSE=2.2767 RMSE=2.2453 Epoch: 30/150RMSE=2.2386 RMSE=1.4225 RMSE=1.3738 RMSE=1.4043 RMSE=1.3963 RMSE=1.4424 RMSE=1.4113 RMSE=1.2289 RMSE=1.4463 RMSE=1.2472 Epoch: 40/150RMSE=1.2314 RMSE=1.2365 RMSE=1.2601 RMSE=1.2488 RMSE=1.2393 RMSE=1.2374 RMSE=1.2354 RMSE=1.2351 RMSE=1.2408 RMSE=1.2493 Epoch: 50/150RMSE=1.2474 RMSE=1.2419 RMSE=1.2380 RMSE=1.2424 RMSE=1.2471 RMSE=1.2397 RMSE=1.2384 RMSE=2.4182 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.418 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.7838 RMSE=2.3822 RMSE=1.9067 RMSE=1.8584 RMSE=1.8934 RMSE=1.4924 RMSE=1.5355 RMSE=1.7065 RMSE=1.4907 Epoch: 10/150RMSE=1.5800 RMSE=1.5677 RMSE=1.6378 RMSE=1.5384 RMSE=1.6118 RMSE=1.6416 RMSE=1.6822 RMSE=1.6873 RMSE=1.6583 RMSE=1.7097 Epoch: 20/150RMSE=1.7436 RMSE=1.7596 RMSE=1.7275 RMSE=1.7286 RMSE=1.7235 RMSE=1.7117 RMSE=1.7108 RMSE=1.7105 RMSE=1.7078 RMSE=1.7242 RMSE=1.3085 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.863 +/- 0.555\n",
      "\n",
      "Epoch: 1/150RMSE=1.8297 RMSE=1.7374 RMSE=1.6017 RMSE=1.3471 RMSE=1.8534 RMSE=1.6666 RMSE=1.7121 RMSE=1.7316 RMSE=1.6245 Epoch: 10/150RMSE=1.5062 RMSE=1.5119 RMSE=1.4578 RMSE=1.4957 RMSE=1.5155 RMSE=1.5139 RMSE=1.4316 RMSE=1.3947 RMSE=1.3933 RMSE=1.4441 Epoch: 20/150RMSE=1.4111 RMSE=1.4120 RMSE=1.4211 RMSE=1.4345 RMSE=1.4265 RMSE=1.3231 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.683 +/- 0.520\n",
      "\n",
      "Epoch: 1/150RMSE=1.8454 RMSE=1.9459 RMSE=1.8817 RMSE=1.9016 RMSE=1.9655 RMSE=1.8720 RMSE=1.9090 RMSE=1.8867 RMSE=1.8921 Epoch: 10/150RMSE=1.8875 RMSE=2.0681 RMSE=2.1271 RMSE=1.8581 RMSE=1.9609 RMSE=1.9357 RMSE=1.9970 RMSE=2.0572 RMSE=2.0373 RMSE=2.0328 Epoch: 20/150RMSE=2.0316 RMSE=2.0066 RMSE=2.0311 RMSE=2.0191 RMSE=2.0314 RMSE=2.0259 RMSE=2.0202 RMSE=1.9967 RMSE=2.0167 RMSE=2.0097 RMSE=2.3136 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.841 +/- 0.526\n",
      "\n",
      "Epoch: 1/150RMSE=1.8838 RMSE=2.5699 RMSE=2.0739 RMSE=2.4630 RMSE=2.3222 RMSE=2.0950 RMSE=2.1102 RMSE=2.0207 RMSE=2.2249 Epoch: 10/150RMSE=2.1509 RMSE=2.1340 RMSE=2.1759 RMSE=2.2140 RMSE=2.2517 RMSE=2.1736 RMSE=2.2622 RMSE=2.1662 RMSE=2.1601 RMSE=2.1253 Epoch: 20/150RMSE=2.4522 RMSE=2.1975 RMSE=2.1203 RMSE=2.1460 RMSE=2.1532 RMSE=2.1414 RMSE=2.1445 RMSE=2.1429 RMSE=2.1591 RMSE=2.1932 Epoch: 30/150RMSE=2.2000 RMSE=2.1872 RMSE=2.1546 RMSE=2.1537 RMSE=2.1627 RMSE=2.1758 RMSE=2.1791 RMSE=2.1692 RMSE=2.1758 RMSE=2.1714 Epoch: 40/150RMSE=2.1654 RMSE=2.3173 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.936 +/- 0.508\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/esol.pt\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=2.4513 RMSE=2.3229 RMSE=2.8047 RMSE=2.4439 RMSE=2.5474 RMSE=2.6778 RMSE=2.6752 RMSE=2.4246 RMSE=2.5110 Epoch: 10/150RMSE=2.3210 RMSE=2.5866 RMSE=2.3062 RMSE=2.2529 RMSE=2.3992 RMSE=2.1608 RMSE=2.5481 RMSE=2.2447 RMSE=2.3450 RMSE=2.2614 Epoch: 20/150RMSE=2.2267 RMSE=2.4809 RMSE=2.5793 RMSE=2.5925 RMSE=2.5332 RMSE=2.4665 RMSE=2.4628 RMSE=1.4411 RMSE=1.4191 RMSE=1.3992 Epoch: 30/150RMSE=2.1381 RMSE=2.1349 RMSE=2.1332 RMSE=2.1347 RMSE=2.1442 RMSE=2.1471 RMSE=2.3364 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.336 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=2.1507 RMSE=2.0143 RMSE=2.1684 RMSE=2.3649 RMSE=1.6365 RMSE=2.8503 RMSE=1.7217 RMSE=2.5250 RMSE=1.6241 Epoch: 10/150RMSE=1.7843 RMSE=1.6167 RMSE=2.4727 RMSE=1.5408 RMSE=1.6773 RMSE=1.7236 RMSE=1.6824 RMSE=1.5179 RMSE=1.5068 RMSE=1.5867 Epoch: 20/150RMSE=1.6286 RMSE=1.7936 RMSE=1.5517 RMSE=1.6133 RMSE=1.6114 RMSE=1.5857 RMSE=1.5886 RMSE=1.5861 RMSE=1.5647 RMSE=1.5899 Epoch: 30/150RMSE=1.6186 RMSE=1.6132 RMSE=1.6107 RMSE=1.6096 RMSE=1.6085 RMSE=1.6101 RMSE=1.5931 RMSE=1.5951 RMSE=1.5893 RMSE=1.9660 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 2.151 +/- 0.185\n",
      "\n",
      "Epoch: 1/150RMSE=2.0417 RMSE=2.0117 RMSE=1.9640 RMSE=2.3929 RMSE=2.1056 RMSE=2.2256 RMSE=2.1212 RMSE=2.2216 RMSE=2.0098 Epoch: 10/150RMSE=2.0360 RMSE=2.2437 RMSE=2.1330 RMSE=2.0466 RMSE=2.0632 RMSE=2.0765 RMSE=2.0573 RMSE=2.0388 RMSE=2.0843 RMSE=2.0633 Epoch: 20/150RMSE=2.1012 RMSE=2.1002 RMSE=2.0983 RMSE=1.7645 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 2.022 +/- 0.237\n",
      "\n",
      "Epoch: 1/150RMSE=2.5924 RMSE=1.6091 RMSE=1.6615 RMSE=2.8039 RMSE=2.6651 RMSE=2.5478 RMSE=2.5837 RMSE=2.6012 RMSE=2.6064 Epoch: 10/150RMSE=2.6311 RMSE=1.1608 RMSE=2.4380 RMSE=2.4319 RMSE=1.3417 RMSE=1.4200 RMSE=1.1124 RMSE=1.6667 RMSE=2.4257 RMSE=2.2893 Epoch: 20/150RMSE=2.3725 RMSE=2.3758 RMSE=2.4119 RMSE=2.3776 RMSE=2.3363 RMSE=2.3641 RMSE=2.3335 RMSE=2.3405 RMSE=2.3231 RMSE=1.2141 Epoch: 30/150RMSE=1.2432 RMSE=1.2252 RMSE=1.2322 RMSE=1.2277 RMSE=1.2208 RMSE=1.2158 RMSE=1.2098 RMSE=1.9314 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 2.000 +/- 0.209\n",
      "\n",
      "Epoch: 1/150RMSE=2.1101 RMSE=1.9984 RMSE=2.4213 RMSE=1.9079 RMSE=2.0092 RMSE=2.0381 RMSE=1.9874 RMSE=1.3668 RMSE=1.2613 Epoch: 10/150RMSE=2.0304 RMSE=2.0070 RMSE=1.2816 RMSE=2.2484 RMSE=1.7659 RMSE=1.7604 RMSE=2.1629 RMSE=1.8139 RMSE=2.2292 RMSE=2.5911 Epoch: 20/150RMSE=2.3667 RMSE=2.1486 RMSE=1.2944 RMSE=1.1526 RMSE=1.1733 RMSE=2.1406 RMSE=2.1558 RMSE=2.1687 RMSE=2.5004 RMSE=1.2087 Epoch: 30/150RMSE=2.2032 RMSE=2.1720 RMSE=1.2221 RMSE=2.2032 RMSE=1.2159 RMSE=1.2198 RMSE=1.2322 RMSE=1.2537 RMSE=2.2310 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 2.046 +/- 0.208\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/esol.pt\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=2.3010 RMSE=1.5764 RMSE=1.5958 RMSE=1.5487 RMSE=1.6045 RMSE=1.5713 RMSE=1.5998 RMSE=1.5404 RMSE=1.5019 Epoch: 10/150RMSE=1.4715 RMSE=1.3716 RMSE=1.4127 RMSE=1.3948 RMSE=1.3727 RMSE=1.2967 RMSE=1.4951 RMSE=1.4674 RMSE=1.3228 RMSE=1.2895 Epoch: 20/150RMSE=1.3283 RMSE=1.3777 RMSE=1.3575 RMSE=1.2615 RMSE=1.2317 RMSE=1.3661 RMSE=1.4374 RMSE=1.3501 RMSE=1.3813 RMSE=1.1145 Epoch: 30/150RMSE=1.1598 RMSE=1.1911 RMSE=1.1309 RMSE=1.1589 RMSE=1.1904 RMSE=1.2118 RMSE=1.2307 RMSE=1.2246 RMSE=1.2051 RMSE=1.2168 Epoch: 40/150RMSE=1.2528 RMSE=1.2217 RMSE=1.2553 RMSE=1.2583 RMSE=1.2238 RMSE=1.2438 RMSE=1.2425 RMSE=1.2452 RMSE=1.2521 RMSE=1.2467 RMSE=1.2706 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.271 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.8919 RMSE=1.6628 RMSE=1.8775 RMSE=1.8942 RMSE=1.9754 RMSE=1.9198 RMSE=1.9543 RMSE=1.9093 RMSE=2.0364 Epoch: 10/150RMSE=1.5656 RMSE=2.0228 RMSE=1.9978 RMSE=1.9065 RMSE=1.9679 RMSE=1.9413 RMSE=1.9232 RMSE=1.9863 RMSE=1.9091 RMSE=1.8869 Epoch: 20/150RMSE=1.9288 RMSE=1.8249 RMSE=1.8834 RMSE=1.8521 RMSE=1.8256 RMSE=1.5231 RMSE=1.7978 RMSE=1.7874 RMSE=1.7007 RMSE=1.7363 Epoch: 30/150RMSE=1.7233 RMSE=1.7219 RMSE=1.7095 RMSE=1.7237 RMSE=1.7614 RMSE=1.7971 RMSE=1.8137 RMSE=1.7648 RMSE=1.7847 RMSE=1.7730 Epoch: 40/150RMSE=1.7810 RMSE=1.7911 RMSE=1.8021 RMSE=1.7895 RMSE=1.7877 RMSE=1.7808 RMSE=1.7697 RMSE=1.7716 RMSE=1.7691 RMSE=1.7734 Epoch: 50/150RMSE=1.7712 RMSE=1.7694 RMSE=2.1842 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.727 +/- 0.457\n",
      "\n",
      "Epoch: 1/150RMSE=2.4872 RMSE=2.5516 RMSE=2.2201 RMSE=2.1127 RMSE=1.8484 RMSE=1.9269 RMSE=1.7951 RMSE=1.5661 RMSE=1.5339 Epoch: 10/150RMSE=1.7189 RMSE=1.3599 RMSE=1.5139 RMSE=1.5503 RMSE=1.7772 RMSE=1.5604 RMSE=1.0083 RMSE=0.9783 RMSE=1.0657 RMSE=1.0268 Epoch: 20/150RMSE=1.0926 RMSE=1.0399 RMSE=1.0501 RMSE=1.0598 RMSE=1.0265 RMSE=1.0641 RMSE=1.0505 RMSE=1.0427 RMSE=1.0455 RMSE=1.0863 Epoch: 30/150RMSE=1.0313 RMSE=1.0447 RMSE=1.0648 RMSE=1.0670 RMSE=1.0657 RMSE=1.0741 RMSE=1.0607 RMSE=1.0627 RMSE=0.8636 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.439 +/- 0.552\n",
      "\n",
      "Epoch: 1/150RMSE=1.6843 RMSE=2.7694 RMSE=2.1045 RMSE=2.6002 RMSE=1.9632 RMSE=1.9839 RMSE=1.6933 RMSE=1.4093 RMSE=1.7282 Epoch: 10/150RMSE=1.9269 RMSE=1.7091 RMSE=1.8624 RMSE=1.4719 RMSE=1.5530 RMSE=1.5382 RMSE=1.5030 RMSE=1.5359 RMSE=1.6444 RMSE=1.8580 Epoch: 20/150RMSE=1.7421 RMSE=1.9713 RMSE=3.7045 RMSE=3.2074 RMSE=2.2519 RMSE=2.1960 RMSE=2.1706 RMSE=1.6765 RMSE=1.7148 RMSE=1.1531 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.368 +/- 0.494\n",
      "\n",
      "Epoch: 1/150RMSE=2.7855 RMSE=2.0235 RMSE=2.5521 RMSE=1.4406 RMSE=2.9194 RMSE=1.5550 RMSE=1.6030 RMSE=1.7319 RMSE=1.4412 Epoch: 10/150RMSE=1.3036 RMSE=1.2252 RMSE=1.1645 RMSE=1.1966 RMSE=1.2615 RMSE=2.2536 RMSE=1.0188 RMSE=1.2537 RMSE=2.2186 RMSE=1.3626 Epoch: 20/150RMSE=2.1345 RMSE=2.2870 RMSE=2.1881 RMSE=2.2841 RMSE=1.2912 RMSE=1.4268 RMSE=1.3801 RMSE=1.3560 RMSE=1.3581 RMSE=1.4017 Epoch: 30/150RMSE=1.4672 RMSE=1.4298 RMSE=1.4065 RMSE=1.4840 RMSE=1.4590 RMSE=1.4047 RMSE=1.4075 RMSE=2.4623 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.587 +/- 0.622\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/esol.pt\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=2.1415 RMSE=2.0590 RMSE=1.7627 RMSE=1.8559 RMSE=1.7740 RMSE=1.8961 RMSE=1.8445 RMSE=1.8190 RMSE=1.8529 Epoch: 10/150RMSE=1.7981 RMSE=1.3821 RMSE=1.7904 RMSE=1.9064 RMSE=1.8963 RMSE=1.8453 RMSE=1.8380 RMSE=1.8556 RMSE=1.8954 RMSE=1.8390 Epoch: 20/150RMSE=1.8232 RMSE=1.7936 RMSE=1.8058 RMSE=1.8327 RMSE=1.8272 RMSE=1.8158 RMSE=1.7963 RMSE=1.3856 RMSE=1.8587 RMSE=1.8191 Epoch: 30/150RMSE=1.8202 RMSE=1.8404 RMSE=1.8394 RMSE=1.7795 RMSE=1.7835 RMSE=1.8458 RMSE=1.8341 RMSE=1.8247 RMSE=1.8506 RMSE=1.8169 Epoch: 40/150RMSE=1.8084 RMSE=1.8064 RMSE=1.8071 RMSE=1.8082 RMSE=2.5756 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.576 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=2.3059 RMSE=1.7776 RMSE=2.0599 RMSE=1.6985 RMSE=1.8766 RMSE=1.9691 RMSE=2.0626 RMSE=2.1700 RMSE=1.9706 Epoch: 10/150RMSE=1.7197 RMSE=2.2750 RMSE=2.2387 RMSE=1.3677 RMSE=2.2773 RMSE=1.3755 RMSE=2.1376 RMSE=2.1723 RMSE=2.2832 RMSE=1.3771 Epoch: 20/150RMSE=1.8209 RMSE=1.3745 RMSE=1.3801 RMSE=1.3840 RMSE=1.3717 RMSE=1.3525 RMSE=1.3822 RMSE=1.3819 RMSE=1.3684 RMSE=1.3903 Epoch: 30/150RMSE=1.3814 RMSE=1.3936 RMSE=1.3740 RMSE=1.3688 RMSE=1.3870 RMSE=1.3870 RMSE=1.3932 RMSE=1.3867 RMSE=1.3855 RMSE=1.3905 Epoch: 40/150RMSE=1.3905 RMSE=1.3758 RMSE=1.3852 RMSE=1.3813 RMSE=1.3837 RMSE=1.3869 RMSE=2.6905 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 2.633 +/- 0.057\n",
      "\n",
      "Epoch: 1/150RMSE=1.7829 RMSE=1.7254 RMSE=1.8315 RMSE=2.5955 RMSE=2.5978 RMSE=2.4261 RMSE=2.7916 RMSE=2.4015 RMSE=2.3630 Epoch: 10/150RMSE=2.6088 RMSE=2.0154 RMSE=2.4049 RMSE=1.5907 RMSE=2.3800 RMSE=2.3963 RMSE=2.5591 RMSE=2.4520 RMSE=2.5981 RMSE=2.5497 Epoch: 20/150RMSE=2.5688 RMSE=2.4803 RMSE=2.5477 RMSE=2.4589 RMSE=2.4786 RMSE=2.3910 RMSE=2.5138 RMSE=2.4977 RMSE=2.4655 RMSE=2.4630 Epoch: 30/150RMSE=2.4787 RMSE=2.4763 RMSE=2.4885 RMSE=2.4366 RMSE=2.5096 RMSE=2.4420 RMSE=2.4444 RMSE=2.4878 RMSE=2.6309 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 2.632 +/- 0.047\n",
      "\n",
      "Epoch: 1/150RMSE=2.1911 RMSE=3.5186 RMSE=1.3758 RMSE=1.4003 RMSE=1.2318 RMSE=1.4143 RMSE=1.1228 RMSE=1.2747 RMSE=1.0488 Epoch: 10/150RMSE=1.5434 RMSE=1.1687 RMSE=1.1590 RMSE=1.4885 RMSE=1.1425 RMSE=1.1506 RMSE=1.1677 RMSE=1.1554 RMSE=1.2163 RMSE=1.2083 Epoch: 20/150RMSE=1.2299 RMSE=1.2316 RMSE=1.2209 RMSE=1.2147 RMSE=1.2159 RMSE=1.1883 RMSE=1.1609 RMSE=1.1832 RMSE=1.1743 RMSE=1.1846 RMSE=2.4706 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 2.592 +/- 0.081\n",
      "\n",
      "Epoch: 1/150RMSE=1.7308 RMSE=1.5655 RMSE=1.5244 RMSE=1.4955 RMSE=1.7100 RMSE=1.5011 RMSE=1.4548 RMSE=1.3973 RMSE=1.4095 Epoch: 10/150RMSE=1.4437 RMSE=1.4802 RMSE=1.4246 RMSE=1.4226 RMSE=1.4191 RMSE=1.4154 RMSE=1.7600 RMSE=1.4491 RMSE=1.4215 RMSE=1.8142 Epoch: 20/150RMSE=1.8067 RMSE=1.8245 RMSE=1.8090 RMSE=1.8365 RMSE=1.8116 RMSE=1.8184 RMSE=1.8288 RMSE=1.8358 RMSE=1.8226 RMSE=1.7999 Epoch: 30/150RMSE=1.8001 RMSE=1.8104 RMSE=1.8296 RMSE=1.8237 RMSE=1.8305 RMSE=1.8207 RMSE=1.8030 RMSE=2.6729 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 2.608 +/- 0.079\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/esol.pt\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.8946 RMSE=1.6297 RMSE=1.5107 RMSE=1.8890 RMSE=1.9666 RMSE=1.9280 RMSE=1.9536 RMSE=1.8887 RMSE=1.8801 Epoch: 10/150RMSE=1.9652 RMSE=2.2865 RMSE=1.4221 RMSE=1.9407 RMSE=1.3661 RMSE=1.3634 RMSE=1.3837 RMSE=1.3911 RMSE=1.3761 RMSE=1.4112 Epoch: 20/150RMSE=1.3350 RMSE=1.3848 RMSE=1.4064 RMSE=1.4263 RMSE=1.3914 RMSE=1.3972 RMSE=2.2229 RMSE=1.3512 RMSE=1.3350 RMSE=1.3409 Epoch: 30/150RMSE=1.3487 RMSE=1.3437 RMSE=1.3345 RMSE=1.3416 RMSE=1.3333 RMSE=1.3413 RMSE=1.3372 RMSE=1.3441 RMSE=1.3451 RMSE=1.3742 Epoch: 40/150RMSE=1.3693 RMSE=1.3727 RMSE=1.3508 RMSE=1.3500 RMSE=1.3502 RMSE=1.3587 RMSE=1.3610 RMSE=1.3508 RMSE=1.3528 RMSE=1.3600 Epoch: 50/150RMSE=1.3609 RMSE=1.3578 RMSE=1.3606 RMSE=1.3609 RMSE=1.3589 RMSE=2.6861 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.686 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=2.2509 RMSE=2.8419 RMSE=2.2195 RMSE=2.7071 RMSE=2.8763 RMSE=2.3355 RMSE=1.9755 RMSE=2.2369 RMSE=2.6912 Epoch: 10/150RMSE=2.2218 RMSE=2.3931 RMSE=2.4791 RMSE=2.5707 RMSE=2.3552 RMSE=2.2586 RMSE=2.3136 RMSE=2.2216 RMSE=2.2019 RMSE=2.4179 Epoch: 20/150RMSE=2.1735 RMSE=2.1402 RMSE=2.2196 RMSE=2.1789 RMSE=2.1925 RMSE=2.1998 RMSE=2.2979 RMSE=2.3091 RMSE=2.8334 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 2.760 +/- 0.074\n",
      "\n",
      "Epoch: 1/150RMSE=1.6373 RMSE=1.5610 RMSE=2.0600 RMSE=1.4894 RMSE=2.0156 RMSE=1.9728 RMSE=1.8370 RMSE=2.0541 RMSE=2.2052 Epoch: 10/150RMSE=1.7825 RMSE=1.8802 RMSE=1.4156 RMSE=1.4177 RMSE=1.9030 RMSE=1.7660 RMSE=1.8540 RMSE=1.8401 RMSE=1.7904 RMSE=1.9240 Epoch: 20/150RMSE=1.9557 RMSE=1.9021 RMSE=1.8754 RMSE=1.9128 RMSE=1.9204 RMSE=1.8789 RMSE=1.8923 RMSE=1.9195 RMSE=1.8800 RMSE=1.9224 Epoch: 30/150RMSE=1.8956 RMSE=1.8794 RMSE=1.8896 RMSE=1.8822 RMSE=1.8855 RMSE=1.9033 RMSE=2.4826 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 2.667 +/- 0.144\n",
      "\n",
      "Epoch: 1/150RMSE=1.8905 RMSE=1.9897 RMSE=2.0100 RMSE=1.8755 RMSE=1.7905 RMSE=1.7486 RMSE=1.7802 RMSE=1.8789 RMSE=1.8298 Epoch: 10/150RMSE=1.8875 RMSE=1.9080 RMSE=1.9503 RMSE=1.7728 RMSE=1.9920 RMSE=1.9381 RMSE=1.9749 RMSE=1.9165 RMSE=1.8815 RMSE=1.9270 Epoch: 20/150RMSE=1.7467 RMSE=1.9388 RMSE=1.8838 RMSE=1.8293 RMSE=1.8391 RMSE=1.8961 RMSE=2.0546 RMSE=1.9179 RMSE=1.9909 RMSE=1.9057 Epoch: 30/150RMSE=1.8232 RMSE=1.8811 RMSE=1.8104 RMSE=1.8251 RMSE=2.6771 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 2.670 +/- 0.125\n",
      "\n",
      "Epoch: 1/150RMSE=2.2704 RMSE=2.4952 RMSE=1.8946 RMSE=2.4480 RMSE=2.6861 RMSE=1.8069 RMSE=2.5444 RMSE=2.5432 RMSE=2.6330 Epoch: 10/150RMSE=2.6811 RMSE=2.8279 RMSE=2.8165 RMSE=2.7873 RMSE=2.7098 RMSE=2.8254 RMSE=2.8758 RMSE=2.8448 RMSE=2.8603 RMSE=2.8934 Epoch: 20/150RMSE=2.8649 RMSE=2.8763 RMSE=2.3941 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 2.615 +/- 0.157\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freesolv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.3008 RMSE=4.9119 RMSE=3.8208 RMSE=2.5792 RMSE=2.5959 RMSE=2.4530 RMSE=2.2121 RMSE=2.1744 RMSE=2.1863 Epoch: 10/150RMSE=2.0348 RMSE=2.3669 RMSE=2.1571 RMSE=2.2296 RMSE=2.0895 RMSE=2.1718 RMSE=2.2484 RMSE=2.1528 RMSE=2.2124 RMSE=2.2672 Epoch: 20/150RMSE=2.2282 RMSE=2.2321 RMSE=2.2446 RMSE=2.2236 RMSE=2.1834 RMSE=2.2173 RMSE=2.1675 RMSE=2.1655 RMSE=2.1718 RMSE=2.1673 Epoch: 30/150RMSE=2.1664 RMSE=2.2518 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.252 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.1971 RMSE=4.7546 RMSE=3.5074 RMSE=2.5835 RMSE=2.3011 RMSE=2.1394 RMSE=2.0396 RMSE=1.9223 RMSE=2.0062 Epoch: 10/150RMSE=1.9697 RMSE=1.9506 RMSE=1.9269 RMSE=1.8852 RMSE=1.7383 RMSE=1.7836 RMSE=1.8263 RMSE=1.7406 RMSE=1.8362 RMSE=1.7515 Epoch: 20/150RMSE=1.7326 RMSE=1.7348 RMSE=1.7668 RMSE=1.7659 RMSE=1.7968 RMSE=1.7990 RMSE=1.7962 RMSE=1.7825 RMSE=1.7740 RMSE=1.7673 Epoch: 30/150RMSE=1.7760 RMSE=1.7726 RMSE=1.7764 RMSE=1.7731 RMSE=1.7643 RMSE=1.7555 RMSE=1.7627 RMSE=1.7656 RMSE=1.7554 RMSE=1.7582 Epoch: 40/150RMSE=1.7580 RMSE=2.3612 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 2.307 +/- 0.055\n",
      "\n",
      "Epoch: 1/150RMSE=5.1809 RMSE=4.8197 RMSE=3.9933 RMSE=3.6493 RMSE=3.8165 RMSE=3.4151 RMSE=3.4071 RMSE=3.4788 RMSE=3.2638 Epoch: 10/150RMSE=3.3256 RMSE=3.5935 RMSE=3.2301 RMSE=3.6901 RMSE=3.5768 RMSE=3.5186 RMSE=3.5679 RMSE=3.5478 RMSE=3.5212 RMSE=3.4452 Epoch: 20/150RMSE=3.5731 RMSE=3.5069 RMSE=3.4524 RMSE=3.4633 RMSE=3.4705 RMSE=3.4660 RMSE=3.4860 RMSE=3.4760 RMSE=3.4691 RMSE=3.4773 RMSE=3.8816 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 2.832 +/- 0.744\n",
      "\n",
      "Epoch: 1/150RMSE=5.2112 RMSE=4.6745 RMSE=4.0070 RMSE=3.6316 RMSE=3.4014 RMSE=3.4168 RMSE=4.0045 RMSE=3.3404 RMSE=3.3120 Epoch: 10/150RMSE=3.2522 RMSE=3.2997 RMSE=3.2708 RMSE=3.2830 RMSE=3.2252 RMSE=3.1902 RMSE=3.0875 RMSE=3.1833 RMSE=3.0869 RMSE=3.0932 Epoch: 20/150RMSE=3.1775 RMSE=3.0951 RMSE=3.1138 RMSE=3.1183 RMSE=3.1442 RMSE=3.1291 RMSE=3.1460 RMSE=3.1348 RMSE=3.1388 RMSE=3.1436 Epoch: 30/150RMSE=3.1478 RMSE=3.1662 RMSE=3.1666 RMSE=3.1655 RMSE=3.1385 RMSE=3.1357 RMSE=3.1642 RMSE=4.5915 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 3.272 +/- 0.998\n",
      "\n",
      "Epoch: 1/150RMSE=5.1454 RMSE=4.6717 RMSE=3.9569 RMSE=3.6129 RMSE=3.3587 RMSE=3.3532 RMSE=3.3451 RMSE=3.3613 RMSE=3.2997 Epoch: 10/150RMSE=3.3042 RMSE=3.3139 RMSE=3.2910 RMSE=3.3830 RMSE=3.3379 RMSE=3.3609 RMSE=3.3590 RMSE=3.3564 RMSE=3.3742 RMSE=3.3789 Epoch: 20/150RMSE=3.3673 RMSE=3.3647 RMSE=3.3485 RMSE=3.3490 RMSE=3.3488 RMSE=3.3538 RMSE=3.3481 RMSE=3.3493 RMSE=3.3480 RMSE=3.3465 Epoch: 30/150RMSE=3.3472 RMSE=3.6700 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 3.351 +/- 0.907\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.2119 RMSE=4.3147 RMSE=3.6126 RMSE=3.4135 RMSE=3.2845 RMSE=2.8914 RMSE=3.4229 RMSE=3.1865 RMSE=2.5271 Epoch: 10/150RMSE=3.0045 RMSE=2.6345 RMSE=2.5625 RMSE=2.3748 RMSE=2.2719 RMSE=2.3277 RMSE=2.3245 RMSE=2.2760 RMSE=2.7180 RMSE=2.6051 Epoch: 20/150RMSE=2.5669 RMSE=2.3831 RMSE=2.3095 RMSE=2.2124 RMSE=2.2501 RMSE=2.2587 RMSE=2.2710 RMSE=2.2920 RMSE=2.3117 RMSE=2.3314 Epoch: 30/150RMSE=2.3132 RMSE=2.3248 RMSE=2.3393 RMSE=2.3495 RMSE=2.3508 RMSE=2.3538 RMSE=2.3461 RMSE=2.3479 RMSE=2.3505 RMSE=2.3549 Epoch: 40/150RMSE=2.3560 RMSE=2.3544 RMSE=2.3537 RMSE=2.3545 RMSE=2.7365 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.737 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.2054 RMSE=4.8377 RMSE=4.0827 RMSE=3.7778 RMSE=3.8495 RMSE=3.6142 RMSE=3.6136 RMSE=3.5222 RMSE=3.5900 Epoch: 10/150RMSE=3.4884 RMSE=3.4560 RMSE=3.5453 RMSE=3.2944 RMSE=3.2792 RMSE=3.3626 RMSE=3.1287 RMSE=3.1823 RMSE=3.0808 RMSE=3.0890 Epoch: 20/150RMSE=3.2257 RMSE=3.1452 RMSE=3.2162 RMSE=3.3204 RMSE=3.2796 RMSE=3.3013 RMSE=3.3559 RMSE=3.5542 RMSE=3.5169 RMSE=3.6348 Epoch: 30/150RMSE=3.6337 RMSE=3.4904 RMSE=3.5963 RMSE=3.6288 RMSE=3.6270 RMSE=3.5709 RMSE=3.6862 RMSE=3.7148 RMSE=3.5543 RMSE=3.5851 Epoch: 40/150RMSE=3.7629 RMSE=4.1027 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 3.420 +/- 0.683\n",
      "\n",
      "Epoch: 1/150RMSE=5.2443 RMSE=4.5753 RMSE=3.5983 RMSE=3.5074 RMSE=3.0704 RMSE=2.4403 RMSE=2.3912 RMSE=2.6139 RMSE=2.3239 Epoch: 10/150RMSE=2.2189 RMSE=2.3525 RMSE=2.0819 RMSE=2.6429 RMSE=2.3938 RMSE=1.9921 RMSE=2.9707 RMSE=3.3856 RMSE=2.9198 RMSE=3.2053 Epoch: 20/150RMSE=3.1423 RMSE=3.2436 RMSE=3.1934 RMSE=3.1833 RMSE=3.3607 RMSE=3.3008 RMSE=3.2842 RMSE=3.3032 RMSE=3.2928 RMSE=3.2958 Epoch: 30/150RMSE=3.3309 RMSE=3.2975 RMSE=3.3061 RMSE=3.2945 RMSE=3.2848 RMSE=3.2660 RMSE=3.2702 RMSE=2.1905 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 3.010 +/- 0.804\n",
      "\n",
      "Epoch: 1/150RMSE=5.0743 RMSE=4.3602 RMSE=3.6484 RMSE=2.6018 RMSE=3.3565 RMSE=3.2782 RMSE=2.4296 RMSE=3.0816 RMSE=2.9156 Epoch: 10/150RMSE=3.0247 RMSE=2.2166 RMSE=2.0987 RMSE=3.1298 RMSE=3.0731 RMSE=3.0976 RMSE=3.0256 RMSE=3.0283 RMSE=3.0689 RMSE=3.0729 Epoch: 20/150RMSE=3.0422 RMSE=3.0541 RMSE=3.0484 RMSE=3.0634 RMSE=3.0939 RMSE=3.1133 RMSE=3.0282 RMSE=2.9880 RMSE=3.1461 RMSE=2.9803 Epoch: 30/150RMSE=3.0576 RMSE=2.9383 RMSE=3.0802 RMSE=3.0604 RMSE=3.0419 RMSE=3.0515 RMSE=3.0086 RMSE=3.0233 RMSE=3.0376 RMSE=2.9970 Epoch: 40/150RMSE=3.0416 RMSE=3.0115 RMSE=3.0359 RMSE=3.0269 RMSE=3.0213 RMSE=3.0082 RMSE=3.0121 RMSE=3.0387 RMSE=3.0724 RMSE=3.0301 Epoch: 50/150RMSE=3.0260 RMSE=3.0183 RMSE=3.0407 RMSE=3.0357 RMSE=3.0376 RMSE=3.0429 RMSE=3.0475 RMSE=3.0472 RMSE=3.0461 RMSE=3.0445 Epoch: 60/150RMSE=3.0177 RMSE=5.2244 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 3.564 +/- 1.185\n",
      "\n",
      "Epoch: 1/150RMSE=5.0822 RMSE=4.6712 RMSE=3.7734 RMSE=3.5030 RMSE=3.3127 RMSE=3.4449 RMSE=3.3198 RMSE=3.2731 RMSE=3.6332 Epoch: 10/150RMSE=3.2922 RMSE=3.2004 RMSE=3.7858 RMSE=3.3314 RMSE=3.3377 RMSE=3.6004 RMSE=3.3854 RMSE=3.6819 RMSE=3.4913 RMSE=3.4830 Epoch: 20/150RMSE=3.5041 RMSE=3.5104 RMSE=3.5553 RMSE=3.5081 RMSE=3.4960 RMSE=3.5542 RMSE=3.4733 RMSE=3.5305 RMSE=3.5433 RMSE=3.4867 Epoch: 30/150RMSE=3.5534 RMSE=3.9176 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 3.634 +/- 1.069\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=4.9905 RMSE=4.3963 RMSE=3.4206 RMSE=3.1690 RMSE=3.6646 RMSE=3.1270 RMSE=3.2650 RMSE=3.2176 RMSE=3.2893 Epoch: 10/150RMSE=3.4143 RMSE=3.5974 RMSE=3.4392 RMSE=3.6177 RMSE=3.1891 RMSE=3.2487 RMSE=3.6458 RMSE=3.5334 RMSE=3.3153 RMSE=3.2011 Epoch: 20/150RMSE=3.2953 RMSE=3.2531 RMSE=3.2563 RMSE=3.5236 RMSE=3.6127 RMSE=3.2791 RMSE=3.2796 RMSE=3.6931 RMSE=3.2736 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 3.274 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.0649 RMSE=4.5242 RMSE=3.8758 RMSE=3.6285 RMSE=3.3240 RMSE=3.2001 RMSE=3.3125 RMSE=3.2149 RMSE=3.2245 Epoch: 10/150RMSE=3.1548 RMSE=2.4430 RMSE=3.7544 RMSE=2.9788 RMSE=3.0762 RMSE=3.0523 RMSE=3.0037 RMSE=3.1034 RMSE=3.1553 RMSE=3.1955 Epoch: 20/150RMSE=3.1811 RMSE=3.2126 RMSE=3.1542 RMSE=3.0703 RMSE=3.1575 RMSE=3.1528 RMSE=3.1825 RMSE=3.1853 RMSE=3.1874 RMSE=3.2200 Epoch: 30/150RMSE=3.2130 RMSE=3.2169 RMSE=3.3768 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 3.325 +/- 0.052\n",
      "\n",
      "Epoch: 1/150RMSE=5.0133 RMSE=3.8838 RMSE=3.6960 RMSE=3.0182 RMSE=2.7196 RMSE=2.5483 RMSE=2.3757 RMSE=2.4496 RMSE=2.1388 Epoch: 10/150RMSE=2.1121 RMSE=1.9835 RMSE=1.9806 RMSE=2.2557 RMSE=1.9372 RMSE=1.8006 RMSE=1.8643 RMSE=1.8682 RMSE=1.8020 RMSE=1.8408 Epoch: 20/150RMSE=1.7571 RMSE=1.8922 RMSE=1.6484 RMSE=1.6823 RMSE=1.7887 RMSE=1.8134 RMSE=1.7062 RMSE=1.6456 RMSE=1.6605 RMSE=1.6306 Epoch: 30/150RMSE=1.5987 RMSE=1.6411 RMSE=1.6364 RMSE=1.5071 RMSE=1.5461 RMSE=1.5161 RMSE=1.5303 RMSE=1.5712 RMSE=1.5695 RMSE=1.5611 Epoch: 40/150RMSE=1.5306 RMSE=1.5184 RMSE=1.5136 RMSE=1.5225 RMSE=1.5299 RMSE=1.5085 RMSE=1.5040 RMSE=1.5139 RMSE=1.5199 RMSE=1.5013 Epoch: 50/150RMSE=1.5063 RMSE=1.5044 RMSE=1.5076 RMSE=1.5006 RMSE=1.4977 RMSE=1.4989 RMSE=1.5132 RMSE=1.5205 RMSE=1.5149 RMSE=1.5065 Epoch: 60/150RMSE=1.4904 RMSE=1.4906 RMSE=1.4914 RMSE=1.4908 RMSE=1.4905 RMSE=1.4896 RMSE=1.4901 RMSE=1.4889 RMSE=1.4936 RMSE=1.4959 Epoch: 70/150RMSE=1.5002 RMSE=1.5002 RMSE=1.5012 RMSE=1.5037 RMSE=1.5029 RMSE=1.5021 RMSE=1.5014 RMSE=1.5009 RMSE=1.5002 RMSE=1.5010 Epoch: 80/150RMSE=1.5010 RMSE=1.5012 RMSE=1.5008 RMSE=1.5006 RMSE=1.5001 RMSE=1.4999 RMSE=1.5001 RMSE=1.4994 RMSE=2.0484 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 2.900 +/- 0.603\n",
      "\n",
      "Epoch: 1/150RMSE=5.2928 RMSE=4.5500 RMSE=3.7728 RMSE=3.7436 RMSE=3.6617 RMSE=3.4979 RMSE=2.5069 RMSE=2.8263 RMSE=2.6570 Epoch: 10/150RMSE=2.7299 RMSE=2.4437 RMSE=2.6046 RMSE=2.4763 RMSE=2.5489 RMSE=2.1622 RMSE=2.2916 RMSE=2.2590 RMSE=2.5619 RMSE=2.5079 Epoch: 20/150RMSE=2.2951 RMSE=2.1482 RMSE=2.1825 RMSE=1.9212 RMSE=2.2303 RMSE=2.0455 RMSE=2.2557 RMSE=2.2367 RMSE=2.1958 RMSE=2.1955 Epoch: 30/150RMSE=2.2394 RMSE=2.2158 RMSE=2.1829 RMSE=2.1919 RMSE=2.1676 RMSE=2.1816 RMSE=2.1537 RMSE=2.1790 RMSE=2.1565 RMSE=2.1679 Epoch: 40/150RMSE=2.1729 RMSE=2.1797 RMSE=2.1860 RMSE=2.1737 RMSE=2.5964 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 2.824 +/- 0.539\n",
      "\n",
      "Epoch: 1/150RMSE=5.1887 RMSE=4.6129 RMSE=3.7923 RMSE=3.8091 RMSE=3.6039 RMSE=3.5381 RMSE=2.7226 RMSE=3.5119 RMSE=3.4542 Epoch: 10/150RMSE=2.8466 RMSE=2.6293 RMSE=2.8513 RMSE=2.6328 RMSE=2.8629 RMSE=2.5760 RMSE=2.5640 RMSE=2.6136 RMSE=2.7953 RMSE=3.7765 Epoch: 20/150RMSE=2.5646 RMSE=2.4637 RMSE=2.5066 RMSE=2.4218 RMSE=2.3647 RMSE=2.4949 RMSE=2.5044 RMSE=2.2818 RMSE=2.4350 RMSE=2.4194 Epoch: 30/150RMSE=2.4433 RMSE=2.4163 RMSE=2.4774 RMSE=2.3465 RMSE=2.3791 RMSE=2.3365 RMSE=2.3270 RMSE=2.3187 RMSE=2.3218 RMSE=2.3255 Epoch: 40/150RMSE=2.3203 RMSE=2.3288 RMSE=2.3341 RMSE=2.3282 RMSE=2.3466 RMSE=2.3525 RMSE=2.3312 RMSE=2.3208 RMSE=6.0523 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 3.470 +/- 1.378\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=4.9849 RMSE=4.3630 RMSE=3.8146 RMSE=3.4065 RMSE=3.1555 RMSE=3.3540 RMSE=3.1280 RMSE=3.2184 RMSE=3.0904 Epoch: 10/150RMSE=3.2134 RMSE=3.4020 RMSE=3.2177 RMSE=3.3038 RMSE=3.3384 RMSE=3.1667 RMSE=3.6079 RMSE=3.4592 RMSE=3.4102 RMSE=3.6786 Epoch: 20/150RMSE=3.3022 RMSE=3.5696 RMSE=4.2225 RMSE=3.5788 RMSE=4.1183 RMSE=3.9852 RMSE=3.8494 RMSE=4.0143 RMSE=4.0486 RMSE=3.8864 Epoch: 30/150RMSE=3.7501 RMSE=3.7841 RMSE=3.5465 RMSE=3.5668 RMSE=3.4955 RMSE=4.2496 RMSE=4.2420 RMSE=4.3269 RMSE=4.3562 RMSE=4.3531 Epoch: 40/150RMSE=4.3644 RMSE=4.1255 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 4.126 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.0960 RMSE=4.4710 RMSE=3.6230 RMSE=2.7845 RMSE=3.3952 RMSE=3.1846 RMSE=3.1519 RMSE=3.0559 RMSE=3.1229 Epoch: 10/150RMSE=3.1298 RMSE=3.2118 RMSE=2.9519 RMSE=3.1095 RMSE=3.1133 RMSE=3.1069 RMSE=2.9754 RMSE=3.0055 RMSE=3.0714 RMSE=3.0496 Epoch: 20/150RMSE=3.0437 RMSE=2.9484 RMSE=3.0620 RMSE=3.0978 RMSE=2.9875 RMSE=2.9467 RMSE=2.9632 RMSE=2.9880 RMSE=2.9795 RMSE=2.9781 Epoch: 30/150RMSE=2.9863 RMSE=2.9744 RMSE=2.9780 RMSE=2.9769 RMSE=2.9647 RMSE=2.9606 RMSE=2.9993 RMSE=2.9707 RMSE=2.9942 RMSE=2.9889 Epoch: 40/150RMSE=2.9718 RMSE=2.9709 RMSE=2.9590 RMSE=2.9872 RMSE=2.9929 RMSE=2.9909 RMSE=2.9959 RMSE=2.9947 RMSE=2.9862 RMSE=2.9840 Epoch: 50/150RMSE=2.9838 RMSE=2.9822 RMSE=2.9855 RMSE=2.9916 RMSE=2.9906 RMSE=2.9880 RMSE=2.9875 RMSE=1.7513 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 2.938 +/- 1.187\n",
      "\n",
      "Epoch: 1/150RMSE=4.9367 RMSE=4.0561 RMSE=3.3418 RMSE=3.1555 RMSE=3.2268 RMSE=3.2963 RMSE=3.4246 RMSE=3.0911 RMSE=3.1126 Epoch: 10/150RMSE=3.3206 RMSE=3.2230 RMSE=3.2725 RMSE=3.1180 RMSE=3.2197 RMSE=3.0810 RMSE=3.0691 RMSE=2.9735 RMSE=1.5269 RMSE=1.6187 Epoch: 20/150RMSE=1.5338 RMSE=1.4211 RMSE=1.4423 RMSE=1.4153 RMSE=1.4041 RMSE=1.5948 RMSE=3.2348 RMSE=3.0929 RMSE=3.1519 RMSE=3.1300 Epoch: 30/150RMSE=3.1070 RMSE=3.1626 RMSE=3.1277 RMSE=3.0277 RMSE=1.3793 RMSE=1.3808 RMSE=1.3689 RMSE=1.3413 RMSE=1.4134 RMSE=1.3887 Epoch: 40/150RMSE=1.4617 RMSE=1.4848 RMSE=1.4967 RMSE=1.4618 RMSE=1.4308 RMSE=1.4320 RMSE=1.4415 RMSE=1.4377 RMSE=1.4517 RMSE=1.4297 Epoch: 50/150RMSE=1.4220 RMSE=1.4267 RMSE=1.4541 RMSE=1.4632 RMSE=1.4644 RMSE=1.4613 RMSE=1.4548 RMSE=1.4535 RMSE=1.5961 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 2.491 +/- 1.158\n",
      "\n",
      "Epoch: 1/150RMSE=4.9640 RMSE=4.5424 RMSE=3.4479 RMSE=3.0490 RMSE=3.0409 RMSE=3.3053 RMSE=3.5879 RMSE=3.1943 RMSE=3.1371 Epoch: 10/150RMSE=2.9137 RMSE=2.9681 RMSE=3.2979 RMSE=3.6594 RMSE=3.3684 RMSE=3.1172 RMSE=3.2078 RMSE=3.0326 RMSE=3.6148 RMSE=3.5144 Epoch: 20/150RMSE=3.2210 RMSE=3.0697 RMSE=3.0666 RMSE=3.1140 RMSE=3.0407 RMSE=3.0259 RMSE=3.1203 RMSE=3.1989 RMSE=3.1042 RMSE=3.1066 Epoch: 30/150RMSE=3.0715 RMSE=3.2055 RMSE=3.0634 RMSE=3.1577 RMSE=3.1187 RMSE=3.1438 RMSE=3.0969 RMSE=3.1927 RMSE=3.1448 RMSE=3.1222 Epoch: 40/150RMSE=3.1201 RMSE=3.0862 RMSE=3.0770 RMSE=3.0863 RMSE=3.1002 RMSE=3.0912 RMSE=3.0835 RMSE=3.0708 RMSE=3.0861 RMSE=3.0933 RMSE=1.8994 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 2.343 +/- 1.035\n",
      "\n",
      "Epoch: 1/150RMSE=4.9445 RMSE=4.2446 RMSE=3.8692 RMSE=3.6974 RMSE=3.3131 RMSE=2.7536 RMSE=2.7320 RMSE=2.6985 RMSE=2.8670 Epoch: 10/150RMSE=2.9796 RMSE=3.0920 RMSE=3.4438 RMSE=3.1646 RMSE=2.0467 RMSE=2.2196 RMSE=2.3762 RMSE=3.1409 RMSE=2.3063 RMSE=2.2402 Epoch: 20/150RMSE=2.0901 RMSE=2.3152 RMSE=2.1100 RMSE=2.1709 RMSE=2.1877 RMSE=2.3519 RMSE=2.4298 RMSE=2.3731 RMSE=2.3787 RMSE=2.4419 Epoch: 30/150RMSE=2.3773 RMSE=2.4910 RMSE=2.4699 RMSE=2.4560 RMSE=2.3945 RMSE=3.0765 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 2.490 +/- 0.971\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.1631 RMSE=4.4240 RMSE=3.4062 RMSE=2.8880 RMSE=3.0154 RMSE=2.2527 RMSE=1.9405 RMSE=2.3242 RMSE=3.0202 Epoch: 10/150RMSE=3.0610 RMSE=2.9224 RMSE=2.2034 RMSE=1.8687 RMSE=3.0894 RMSE=2.0347 RMSE=1.9430 RMSE=1.8820 RMSE=1.7007 RMSE=1.7806 Epoch: 20/150RMSE=1.8045 RMSE=1.8639 RMSE=1.6734 RMSE=1.9519 RMSE=1.6707 RMSE=1.9828 RMSE=1.6324 RMSE=1.7355 RMSE=1.5250 RMSE=1.8181 Epoch: 30/150RMSE=1.5809 RMSE=1.5305 RMSE=1.5908 RMSE=1.5713 RMSE=1.4570 RMSE=1.4717 RMSE=1.4708 RMSE=1.4645 RMSE=1.5574 RMSE=1.5220 Epoch: 40/150RMSE=1.4844 RMSE=1.5271 RMSE=1.5481 RMSE=1.5764 RMSE=1.5629 RMSE=1.5396 RMSE=1.5508 RMSE=1.5739 RMSE=1.5312 RMSE=1.5389 Epoch: 50/150RMSE=1.5757 RMSE=1.5723 RMSE=1.5633 RMSE=1.5563 RMSE=1.5433 RMSE=1.6755 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.676 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.1090 RMSE=4.0123 RMSE=3.8093 RMSE=3.4039 RMSE=3.2535 RMSE=3.6865 RMSE=3.2197 RMSE=3.0951 RMSE=3.0094 Epoch: 10/150RMSE=2.9336 RMSE=3.1124 RMSE=3.0303 RMSE=3.1678 RMSE=3.0621 RMSE=3.0182 RMSE=3.0595 RMSE=3.1179 RMSE=3.1533 RMSE=3.0359 Epoch: 20/150RMSE=3.0054 RMSE=3.0422 RMSE=2.9747 RMSE=2.9678 RMSE=2.9505 RMSE=3.0152 RMSE=3.0442 RMSE=3.0398 RMSE=3.0355 RMSE=3.0270 Epoch: 30/150RMSE=3.0410 RMSE=3.0530 RMSE=3.0385 RMSE=3.0421 RMSE=3.0351 RMSE=3.0564 RMSE=3.0357 RMSE=3.0356 RMSE=3.0223 RMSE=3.0248 Epoch: 40/150RMSE=3.0504 RMSE=3.0527 RMSE=3.0193 RMSE=3.0322 RMSE=3.0182 RMSE=2.9325 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 2.304 +/- 0.628\n",
      "\n",
      "Epoch: 1/150RMSE=4.7802 RMSE=3.9566 RMSE=3.4385 RMSE=3.2316 RMSE=2.8661 RMSE=3.0292 RMSE=2.8864 RMSE=3.0861 RMSE=3.2295 Epoch: 10/150RMSE=2.6938 RMSE=2.4900 RMSE=2.5041 RMSE=2.5060 RMSE=2.5392 RMSE=2.9227 RMSE=2.4130 RMSE=2.4478 RMSE=2.5291 RMSE=2.5318 Epoch: 20/150RMSE=2.5566 RMSE=2.5610 RMSE=2.5656 RMSE=2.5533 RMSE=2.5799 RMSE=2.6086 RMSE=2.6272 RMSE=2.6330 RMSE=2.6571 RMSE=2.5191 Epoch: 30/150RMSE=2.6223 RMSE=2.5332 RMSE=2.6619 RMSE=2.6465 RMSE=2.6367 RMSE=2.6279 RMSE=2.6445 RMSE=3.3797 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 2.663 +/- 0.721\n",
      "\n",
      "Epoch: 1/150RMSE=4.8075 RMSE=4.4129 RMSE=3.8092 RMSE=3.1899 RMSE=3.3776 RMSE=3.5249 RMSE=3.2028 RMSE=3.0977 RMSE=3.3673 Epoch: 10/150RMSE=2.6131 RMSE=2.3944 RMSE=2.5611 RMSE=2.6200 RMSE=2.5621 RMSE=2.4431 RMSE=2.2844 RMSE=2.3901 RMSE=2.3488 RMSE=2.4901 Epoch: 20/150RMSE=2.2899 RMSE=2.3276 RMSE=2.2763 RMSE=2.3249 RMSE=2.3241 RMSE=2.3798 RMSE=2.2158 RMSE=2.2621 RMSE=2.1728 RMSE=2.2378 Epoch: 30/150RMSE=2.2834 RMSE=2.2435 RMSE=2.2693 RMSE=2.2620 RMSE=2.2130 RMSE=2.1419 RMSE=2.2011 RMSE=2.1636 RMSE=2.1597 RMSE=2.2297 Epoch: 40/150RMSE=2.2222 RMSE=2.1982 RMSE=2.2033 RMSE=2.2126 RMSE=2.2266 RMSE=2.1923 RMSE=2.2282 RMSE=2.2512 RMSE=2.2619 RMSE=2.2607 Epoch: 50/150RMSE=2.2469 RMSE=2.2486 RMSE=2.2502 RMSE=2.2455 RMSE=2.2640 RMSE=2.2692 RMSE=4.7495 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 3.184 +/- 1.099\n",
      "\n",
      "Epoch: 1/150RMSE=4.8313 RMSE=4.4210 RMSE=3.7213 RMSE=2.9601 RMSE=2.5982 RMSE=2.4076 RMSE=2.9385 RMSE=2.8899 RMSE=2.8886 Epoch: 10/150RMSE=3.2296 RMSE=2.9996 RMSE=3.2324 RMSE=3.2039 RMSE=3.2762 RMSE=3.2007 RMSE=3.3142 RMSE=3.3671 RMSE=3.3169 RMSE=3.2414 Epoch: 20/150RMSE=3.3093 RMSE=3.2043 RMSE=3.2381 RMSE=3.2327 RMSE=3.1593 RMSE=3.2768 RMSE=3.2424 RMSE=3.2203 RMSE=3.2845 RMSE=3.2429 RMSE=3.3846 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 3.224 +/- 0.986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipophilicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/lipo.pt\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.4377 RMSE=1.2825 RMSE=1.3415 RMSE=1.3214 RMSE=1.3303 RMSE=1.2929 RMSE=1.3297 RMSE=1.2325 RMSE=1.2157 Epoch: 10/150RMSE=1.2426 RMSE=1.3559 RMSE=1.3393 RMSE=1.2666 RMSE=1.2662 RMSE=1.2719 RMSE=1.2364 RMSE=1.2612 RMSE=1.3486 RMSE=1.3924 Epoch: 20/150RMSE=1.2633 RMSE=1.2863 RMSE=1.2886 RMSE=1.1201 RMSE=1.1211 RMSE=1.1271 RMSE=1.1190 RMSE=1.2668 RMSE=1.2434 RMSE=1.1201 Epoch: 30/150RMSE=1.1192 RMSE=1.1222 RMSE=1.1246 RMSE=1.1250 RMSE=1.1269 RMSE=1.1188 RMSE=1.3982 RMSE=1.3945 RMSE=1.3867 RMSE=1.3915 Epoch: 40/150RMSE=1.3925 RMSE=1.3952 RMSE=1.3952 RMSE=1.3949 RMSE=1.3946 RMSE=1.3951 RMSE=1.3970 RMSE=1.1440 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.144 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.3863 RMSE=1.2456 RMSE=1.2249 RMSE=1.1782 RMSE=1.3677 RMSE=1.4398 RMSE=1.3274 RMSE=1.3988 RMSE=1.3707 Epoch: 10/150RMSE=1.3319 RMSE=1.3940 RMSE=1.3300 RMSE=1.3425 RMSE=1.3403 RMSE=1.3519 RMSE=1.3732 RMSE=1.3949 RMSE=1.3686 RMSE=1.3745 Epoch: 20/150RMSE=1.3974 RMSE=1.3857 RMSE=1.3952 RMSE=1.4047 RMSE=1.4178 RMSE=1.3917 RMSE=1.3976 RMSE=1.4154 RMSE=1.4224 RMSE=1.4017 Epoch: 30/150RMSE=1.4157 RMSE=1.4201 RMSE=1.4115 RMSE=1.4112 RMSE=1.4214 RMSE=1.4331 RMSE=1.4564 RMSE=1.4334 RMSE=1.4717 RMSE=1.4345 Epoch: 40/150RMSE=1.4282 RMSE=1.4302 RMSE=1.4388 RMSE=1.4279 RMSE=1.4391 RMSE=1.4341 RMSE=1.4382 RMSE=1.4472 RMSE=1.4437 RMSE=1.0707 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.107 +/- 0.037\n",
      "\n",
      "Epoch: 1/150RMSE=1.3458 RMSE=1.3385 RMSE=1.3705 RMSE=1.3161 RMSE=1.3286 RMSE=1.3517 RMSE=1.3432 RMSE=1.3452 RMSE=1.3734 Epoch: 10/150RMSE=1.3334 RMSE=1.3387 RMSE=1.3649 RMSE=1.4012 RMSE=1.3733 RMSE=1.3912 RMSE=1.3816 RMSE=1.3827 RMSE=1.3821 RMSE=1.4022 Epoch: 20/150RMSE=1.3965 RMSE=1.3998 RMSE=1.4040 RMSE=1.4015 RMSE=1.4147 RMSE=1.4197 RMSE=1.4261 RMSE=1.4246 RMSE=1.4237 RMSE=1.4268 Epoch: 30/150RMSE=1.4281 RMSE=1.4332 RMSE=1.1130 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.109 +/- 0.030\n",
      "\n",
      "Epoch: 1/150RMSE=1.2376 RMSE=1.2043 RMSE=1.1957 RMSE=1.2035 RMSE=1.2147 RMSE=1.1340 RMSE=1.1477 RMSE=1.1371 RMSE=1.1566 Epoch: 10/150RMSE=1.1208 RMSE=1.1840 RMSE=1.1779 RMSE=1.1277 RMSE=1.1316 RMSE=1.1005 RMSE=1.0891 RMSE=1.1110 RMSE=1.1092 RMSE=1.1400 Epoch: 20/150RMSE=1.1229 RMSE=1.1276 RMSE=1.1226 RMSE=1.1083 RMSE=1.1177 RMSE=1.1127 RMSE=1.1050 RMSE=1.1176 RMSE=1.1165 RMSE=1.1053 Epoch: 30/150RMSE=1.1059 RMSE=1.1044 RMSE=1.1118 RMSE=1.1141 RMSE=1.1137 RMSE=1.1177 RMSE=1.1148 RMSE=1.0113 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.085 +/- 0.050\n",
      "\n",
      "Epoch: 1/150RMSE=1.2243 RMSE=1.1752 RMSE=1.1587 RMSE=1.2363 RMSE=1.1956 RMSE=1.1069 RMSE=1.2191 RMSE=1.1970 RMSE=1.1874 Epoch: 10/150RMSE=1.3191 RMSE=1.3340 RMSE=1.3596 RMSE=1.2201 RMSE=1.3555 RMSE=1.3314 RMSE=1.2194 RMSE=1.3759 RMSE=1.1857 RMSE=1.1807 Epoch: 20/150RMSE=1.1891 RMSE=1.2002 RMSE=1.1986 RMSE=1.1826 RMSE=1.1927 RMSE=1.1928 RMSE=1.1924 RMSE=1.1886 RMSE=1.1928 RMSE=1.1908 Epoch: 30/150RMSE=1.1998 RMSE=1.1978 RMSE=1.1973 RMSE=1.2103 RMSE=1.2121 RMSE=1.2001 RMSE=1.2078 RMSE=1.2084 RMSE=1.2087 RMSE=1.0390 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.076 +/- 0.048\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/lipo.pt\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.3322 RMSE=1.2502 RMSE=1.1881 RMSE=1.1912 RMSE=1.1653 RMSE=1.1807 RMSE=1.1112 RMSE=1.1445 RMSE=1.1410 Epoch: 10/150RMSE=1.1250 RMSE=1.1303 RMSE=1.0987 RMSE=1.0908 RMSE=1.0894 RMSE=1.1354 RMSE=1.1209 RMSE=1.1277 RMSE=1.1163 RMSE=1.0953 Epoch: 20/150RMSE=1.0950 RMSE=1.0971 RMSE=1.0934 RMSE=1.0918 RMSE=1.1046 RMSE=1.0889 RMSE=1.1171 RMSE=1.1123 RMSE=1.1082 RMSE=1.1126 Epoch: 30/150RMSE=1.1120 RMSE=1.1109 RMSE=1.1110 RMSE=1.1087 RMSE=1.1075 RMSE=1.0966 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.097 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.3511 RMSE=1.2920 RMSE=1.3011 RMSE=1.2834 RMSE=1.2604 RMSE=1.3334 RMSE=1.3327 RMSE=1.3659 RMSE=1.2566 Epoch: 10/150RMSE=1.3417 RMSE=1.2050 RMSE=1.1736 RMSE=1.4130 RMSE=1.4135 RMSE=1.4288 RMSE=1.4329 RMSE=1.3692 RMSE=1.3700 RMSE=1.3522 Epoch: 20/150RMSE=1.3665 RMSE=1.3763 RMSE=1.3726 RMSE=1.3883 RMSE=1.4607 RMSE=1.4778 RMSE=1.3941 RMSE=1.4799 RMSE=1.3966 RMSE=1.4002 Epoch: 30/150RMSE=1.2021 RMSE=1.2078 RMSE=1.2100 RMSE=1.2040 RMSE=1.2153 RMSE=1.2062 RMSE=1.2143 RMSE=1.2026 RMSE=1.1989 RMSE=1.2132 Epoch: 40/150RMSE=1.2046 RMSE=1.1927 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.145 +/- 0.048\n",
      "\n",
      "Epoch: 1/150RMSE=1.4079 RMSE=1.3503 RMSE=1.3482 RMSE=1.1320 RMSE=1.3586 RMSE=1.3901 RMSE=1.3814 RMSE=1.4252 RMSE=1.2357 Epoch: 10/150RMSE=1.2776 RMSE=1.2488 RMSE=1.2874 RMSE=1.2886 RMSE=1.2813 RMSE=1.2460 RMSE=1.2892 RMSE=1.4630 RMSE=1.3601 RMSE=1.2671 Epoch: 20/150RMSE=1.2777 RMSE=1.2794 RMSE=1.3214 RMSE=1.3133 RMSE=1.2848 RMSE=1.2975 RMSE=1.2606 RMSE=1.4170 RMSE=1.2927 RMSE=1.2991 Epoch: 30/150RMSE=1.4003 RMSE=1.5568 RMSE=1.2831 RMSE=1.4894 RMSE=1.5475 RMSE=1.3002 RMSE=1.2961 RMSE=1.5856 RMSE=0.9002 RMSE=1.5549 Epoch: 40/150RMSE=1.5052 RMSE=1.5701 RMSE=1.5003 RMSE=1.4950 RMSE=0.9816 RMSE=1.4922 RMSE=1.5092 RMSE=1.5031 RMSE=1.2435 RMSE=1.2379 Epoch: 50/150RMSE=1.2434 RMSE=1.5115 RMSE=1.5153 RMSE=0.9185 RMSE=0.9077 RMSE=1.2440 RMSE=0.9089 RMSE=0.9061 RMSE=1.5175 RMSE=1.5186 Epoch: 60/150RMSE=1.5155 RMSE=1.5118 RMSE=1.5135 RMSE=0.9069 RMSE=1.5135 RMSE=1.5108 RMSE=1.5132 RMSE=1.5111 RMSE=1.5134 RMSE=1.5132 RMSE=1.3756 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.222 +/- 0.116\n",
      "\n",
      "Epoch: 1/150RMSE=1.3584 RMSE=1.2622 RMSE=1.5597 RMSE=1.3064 RMSE=1.3421 RMSE=1.3145 RMSE=1.2707 RMSE=1.3138 RMSE=1.2930 Epoch: 10/150RMSE=1.3319 RMSE=1.3506 RMSE=1.3497 RMSE=1.3290 RMSE=1.3136 RMSE=1.2469 RMSE=1.2538 RMSE=1.3606 RMSE=1.3718 RMSE=1.3095 Epoch: 20/150RMSE=1.3255 RMSE=1.3293 RMSE=1.3265 RMSE=1.2853 RMSE=1.3225 RMSE=1.3414 RMSE=1.3298 RMSE=1.3083 RMSE=1.2488 RMSE=1.2771 Epoch: 30/150RMSE=1.3219 RMSE=1.3131 RMSE=1.2703 RMSE=1.2756 RMSE=1.2726 RMSE=1.3410 RMSE=1.2931 RMSE=1.3245 RMSE=1.3418 RMSE=1.3008 Epoch: 40/150RMSE=1.3046 RMSE=1.2952 RMSE=1.2980 RMSE=1.2898 RMSE=1.3039 RMSE=1.2960 RMSE=1.3081 RMSE=1.2960 RMSE=1.3029 RMSE=1.1667 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.208 +/- 0.103\n",
      "\n",
      "Epoch: 1/150RMSE=1.2576 RMSE=1.2048 RMSE=1.1663 RMSE=1.1643 RMSE=1.1572 RMSE=1.1263 RMSE=1.1695 RMSE=1.1750 RMSE=1.1577 Epoch: 10/150RMSE=1.1682 RMSE=1.2093 RMSE=1.0630 RMSE=1.0790 RMSE=1.0580 RMSE=1.0160 RMSE=1.1361 RMSE=1.1305 RMSE=1.0339 RMSE=1.1229 Epoch: 20/150RMSE=1.0172 RMSE=1.0208 RMSE=1.0318 RMSE=1.0266 RMSE=1.0162 RMSE=1.0189 RMSE=1.0250 RMSE=1.0100 RMSE=1.1332 RMSE=1.0106 Epoch: 30/150RMSE=1.0048 RMSE=1.0201 RMSE=1.0130 RMSE=0.9955 RMSE=0.9972 RMSE=0.9995 RMSE=0.9977 RMSE=0.9960 RMSE=1.0008 RMSE=1.0028 Epoch: 40/150RMSE=0.9947 RMSE=0.9912 RMSE=1.0036 RMSE=1.0032 RMSE=0.9996 RMSE=0.9958 RMSE=0.9961 RMSE=1.0012 RMSE=1.0002 RMSE=0.9989 Epoch: 50/150RMSE=1.0011 RMSE=1.0015 RMSE=0.9973 RMSE=1.0009 RMSE=0.9974 RMSE=0.9991 RMSE=0.9986 RMSE=1.0047 RMSE=1.0046 RMSE=1.0009 Epoch: 60/150RMSE=1.0042 RMSE=0.9989 RMSE=0.9592 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.158 +/- 0.136\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/lipo.pt\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.3208 RMSE=1.1897 RMSE=1.1783 RMSE=1.2383 RMSE=1.1894 RMSE=1.1064 RMSE=1.1056 RMSE=1.2462 RMSE=1.0699 Epoch: 10/150RMSE=1.1054 RMSE=1.0412 RMSE=1.0489 RMSE=1.0118 RMSE=1.0395 RMSE=1.1668 RMSE=0.9800 RMSE=1.0974 RMSE=1.3490 RMSE=1.0285 Epoch: 20/150RMSE=0.9876 RMSE=0.9170 RMSE=0.9347 RMSE=0.9550 RMSE=0.9051 RMSE=0.9285 RMSE=0.9108 RMSE=0.9289 RMSE=0.9390 RMSE=0.9343 Epoch: 30/150RMSE=0.9512 RMSE=0.9444 RMSE=0.9229 RMSE=0.9205 RMSE=0.9317 RMSE=0.9228 RMSE=0.9260 RMSE=0.9044 RMSE=0.9058 RMSE=0.9037 Epoch: 40/150RMSE=0.9018 RMSE=0.9087 RMSE=0.9058 RMSE=0.9072 RMSE=0.9005 RMSE=0.9094 RMSE=0.8955 RMSE=0.9034 RMSE=0.9008 RMSE=0.8957 Epoch: 50/150RMSE=0.9075 RMSE=0.8982 RMSE=0.8977 RMSE=0.8969 RMSE=0.8961 RMSE=0.8969 RMSE=0.9001 RMSE=0.9009 RMSE=0.8971 RMSE=0.8955 Epoch: 60/150RMSE=0.8964 RMSE=0.8961 RMSE=0.8964 RMSE=0.8962 RMSE=0.8967 RMSE=0.8974 RMSE=0.8971 RMSE=0.8747 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.875 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.3679 RMSE=1.2993 RMSE=1.2661 RMSE=1.2248 RMSE=1.2371 RMSE=1.2520 RMSE=1.2040 RMSE=1.1792 RMSE=1.1947 Epoch: 10/150RMSE=1.1824 RMSE=1.1771 RMSE=1.1650 RMSE=1.1836 RMSE=1.1622 RMSE=1.1672 RMSE=1.1605 RMSE=1.1544 RMSE=1.1832 RMSE=1.1617 Epoch: 20/150RMSE=1.1341 RMSE=1.1554 RMSE=1.1790 RMSE=1.1633 RMSE=1.1933 RMSE=1.1573 RMSE=1.1749 RMSE=1.1801 RMSE=1.1693 RMSE=1.1535 Epoch: 30/150RMSE=1.1607 RMSE=1.1707 RMSE=1.1634 RMSE=1.1609 RMSE=1.1662 RMSE=1.1506 RMSE=1.1511 RMSE=1.1262 RMSE=1.1300 RMSE=1.1382 Epoch: 40/150RMSE=1.1379 RMSE=1.1330 RMSE=1.1329 RMSE=1.1303 RMSE=1.1326 RMSE=1.1272 RMSE=1.1321 RMSE=1.1343 RMSE=1.1283 RMSE=1.1317 Epoch: 50/150RMSE=1.1323 RMSE=1.1338 RMSE=1.1346 RMSE=1.1319 RMSE=1.1329 RMSE=1.1312 RMSE=1.1318 RMSE=1.1321 RMSE=1.0352 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 0.955 +/- 0.080\n",
      "\n",
      "Epoch: 1/150RMSE=1.2937 RMSE=1.2270 RMSE=1.2968 RMSE=1.1939 RMSE=1.2666 RMSE=1.1793 RMSE=1.2835 RMSE=1.3125 RMSE=1.3136 Epoch: 10/150RMSE=1.3089 RMSE=1.3311 RMSE=1.3321 RMSE=1.1461 RMSE=1.3297 RMSE=1.3160 RMSE=1.3393 RMSE=1.1757 RMSE=1.3495 RMSE=1.3375 Epoch: 20/150RMSE=1.3399 RMSE=1.3262 RMSE=1.3315 RMSE=1.3372 RMSE=1.3302 RMSE=1.3334 RMSE=1.3543 RMSE=1.3406 RMSE=1.3559 RMSE=1.0940 Epoch: 30/150RMSE=1.1184 RMSE=1.1185 RMSE=1.1379 RMSE=1.3480 RMSE=1.3597 RMSE=1.3471 RMSE=1.3617 RMSE=1.0990 RMSE=1.1191 RMSE=1.1061 Epoch: 40/150RMSE=1.0988 RMSE=1.1038 RMSE=1.3362 RMSE=1.0978 RMSE=1.1040 RMSE=1.0987 RMSE=1.0969 RMSE=1.1001 RMSE=1.1008 RMSE=1.0933 Epoch: 50/150RMSE=1.0921 RMSE=1.0951 RMSE=1.0975 RMSE=1.0910 RMSE=1.0944 RMSE=1.0895 RMSE=1.0934 RMSE=1.0924 RMSE=1.3413 RMSE=1.0922 Epoch: 60/150RMSE=1.0905 RMSE=1.0932 RMSE=1.0927 RMSE=1.0899 RMSE=1.0930 RMSE=1.0930 RMSE=1.0922 RMSE=1.0916 RMSE=1.0902 RMSE=1.0918 Epoch: 70/150RMSE=1.0917 RMSE=1.0895 RMSE=1.3420 RMSE=1.3405 RMSE=1.3416 RMSE=1.3460 RMSE=1.3378 RMSE=1.3401 RMSE=1.3415 RMSE=1.3394 Epoch: 80/150RMSE=1.3391 RMSE=1.3403 RMSE=1.3411 RMSE=1.3412 RMSE=1.3405 RMSE=1.3403 RMSE=1.3399 RMSE=1.3402 RMSE=1.3411 RMSE=1.3400 Epoch: 90/150RMSE=1.3401 RMSE=1.3410 RMSE=1.3401 RMSE=1.3438 RMSE=1.3433 RMSE=1.3438 RMSE=1.3442 RMSE=1.3441 RMSE=1.3436 RMSE=1.3435 Epoch: 100/150RMSE=1.3407 RMSE=1.3414 RMSE=1.3405 RMSE=1.3439 RMSE=1.3442 RMSE=1.3450 RMSE=1.3445 RMSE=1.3440 RMSE=1.3450 RMSE=1.3432 RMSE=0.8549 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 0.922 +/- 0.081\n",
      "\n",
      "Epoch: 1/150RMSE=1.3090 RMSE=1.2736 RMSE=1.2632 RMSE=1.2757 RMSE=1.2980 RMSE=1.2953 RMSE=1.2900 RMSE=1.3003 RMSE=1.2926 Epoch: 10/150RMSE=1.2967 RMSE=1.3181 RMSE=1.3446 RMSE=1.3176 RMSE=1.3171 RMSE=1.3280 RMSE=1.3519 RMSE=1.3509 RMSE=1.3434 RMSE=1.3823 Epoch: 20/150RMSE=1.3478 RMSE=1.3500 RMSE=1.3650 RMSE=1.3558 RMSE=1.3797 RMSE=1.3549 RMSE=1.3538 RMSE=1.3549 RMSE=1.3669 RMSE=1.2815 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.012 +/- 0.171\n",
      "\n",
      "Epoch: 1/150RMSE=1.3285 RMSE=1.3841 RMSE=1.3678 RMSE=1.3602 RMSE=1.3540 RMSE=1.3556 RMSE=1.3842 RMSE=1.3690 RMSE=1.3603 Epoch: 10/150RMSE=1.3595 RMSE=1.3766 RMSE=1.3439 RMSE=1.3650 RMSE=1.3401 RMSE=1.3417 RMSE=1.3772 RMSE=1.3581 RMSE=1.3476 RMSE=1.3868 Epoch: 20/150RMSE=1.3397 RMSE=1.3406 RMSE=1.3537 RMSE=1.3894 RMSE=1.3730 RMSE=1.3839 RMSE=1.3543 RMSE=1.3670 RMSE=1.3728 RMSE=1.3667 Epoch: 30/150RMSE=1.3657 RMSE=1.3760 RMSE=1.3618 RMSE=1.3615 RMSE=1.3586 RMSE=1.3525 RMSE=1.3604 RMSE=1.3578 RMSE=1.3567 RMSE=1.3590 Epoch: 40/150RMSE=1.3572 RMSE=1.3561 RMSE=1.3557 RMSE=1.3584 RMSE=1.3582 RMSE=1.3582 RMSE=1.3592 RMSE=1.3586 RMSE=1.3568 RMSE=1.3592 Epoch: 50/150RMSE=1.3547 RMSE=1.3535 RMSE=1.3573 RMSE=1.3569 RMSE=1.3564 RMSE=1.3561 RMSE=1.3577 RMSE=1.3590 RMSE=1.3565 RMSE=1.3585 Epoch: 60/150RMSE=1.3574 RMSE=1.3604 RMSE=1.3566 RMSE=1.3620 RMSE=1.3570 RMSE=1.3579 RMSE=1.3576 RMSE=1.3581 RMSE=1.3599 RMSE=1.3575 Epoch: 70/150RMSE=1.3566 RMSE=1.3568 RMSE=1.3570 RMSE=1.3579 RMSE=1.3592 RMSE=1.3525 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.080 +/- 0.205\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/lipo.pt\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.3387 RMSE=1.3017 RMSE=1.2277 RMSE=1.2202 RMSE=1.2464 RMSE=1.3026 RMSE=1.2562 RMSE=1.2377 RMSE=1.2336 Epoch: 10/150RMSE=1.1433 RMSE=1.1163 RMSE=1.2361 RMSE=1.2370 RMSE=1.2353 RMSE=1.2378 RMSE=1.2016 RMSE=1.2285 RMSE=1.2134 RMSE=1.2252 Epoch: 20/150RMSE=1.2069 RMSE=1.2230 RMSE=1.2112 RMSE=1.2183 RMSE=1.2121 RMSE=1.2490 RMSE=1.2670 RMSE=1.2452 RMSE=1.2291 RMSE=1.2284 Epoch: 30/150RMSE=1.2257 RMSE=1.2431 RMSE=1.2327 RMSE=1.2293 RMSE=1.2348 RMSE=1.2210 RMSE=1.2264 RMSE=1.2431 RMSE=1.2270 RMSE=1.2369 Epoch: 40/150RMSE=1.2365 RMSE=1.2265 RMSE=1.2435 RMSE=1.2359 RMSE=1.2401 RMSE=1.2368 RMSE=1.2424 RMSE=1.2287 RMSE=1.2315 RMSE=1.2365 Epoch: 50/150RMSE=1.2351 RMSE=1.2352 RMSE=1.2381 RMSE=1.2386 RMSE=1.2338 RMSE=1.2385 RMSE=1.2359 RMSE=1.2367 RMSE=1.2404 RMSE=1.2386 Epoch: 60/150RMSE=1.2428 RMSE=1.2393 RMSE=1.2385 RMSE=1.2398 RMSE=1.2385 RMSE=1.2390 RMSE=1.2442 RMSE=1.2408 RMSE=1.2395 RMSE=0.9437 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.944 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.3668 RMSE=1.2126 RMSE=1.1844 RMSE=1.1774 RMSE=1.1415 RMSE=1.1423 RMSE=1.2497 RMSE=1.1054 RMSE=1.1198 Epoch: 10/150RMSE=1.1435 RMSE=1.3099 RMSE=1.3132 RMSE=1.1605 RMSE=1.2946 RMSE=1.3094 RMSE=1.3571 RMSE=1.3517 RMSE=1.3693 RMSE=1.3321 Epoch: 20/150RMSE=1.3214 RMSE=1.3183 RMSE=1.3396 RMSE=1.3779 RMSE=1.3501 RMSE=1.3868 RMSE=1.3634 RMSE=1.3731 RMSE=1.3672 RMSE=1.3706 Epoch: 30/150RMSE=1.3796 RMSE=1.3799 RMSE=1.3737 RMSE=1.3697 RMSE=1.3666 RMSE=1.3658 RMSE=1.3776 RMSE=1.3603 RMSE=1.3684 RMSE=1.3697 Epoch: 40/150RMSE=1.3725 RMSE=1.3732 RMSE=1.3741 RMSE=1.3764 RMSE=1.3746 RMSE=1.3709 RMSE=1.3729 RMSE=1.3754 RMSE=1.3683 RMSE=1.3705 Epoch: 50/150RMSE=1.3731 RMSE=1.3735 RMSE=1.3736 RMSE=1.3718 RMSE=1.3740 RMSE=1.3745 RMSE=1.3749 RMSE=1.3750 RMSE=1.3756 RMSE=1.3763 Epoch: 60/150RMSE=1.3755 RMSE=1.3760 RMSE=1.3752 RMSE=1.3761 RMSE=1.3763 RMSE=1.3756 RMSE=0.8115 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 0.878 +/- 0.066\n",
      "\n",
      "Epoch: 1/150RMSE=1.3234 RMSE=1.2272 RMSE=1.1675 RMSE=1.1344 RMSE=1.1109 RMSE=1.2613 RMSE=1.1044 RMSE=1.0820 RMSE=1.3380 Epoch: 10/150RMSE=1.3329 RMSE=1.0624 RMSE=1.1378 RMSE=1.0701 RMSE=1.0245 RMSE=1.5075 RMSE=1.4059 RMSE=1.0747 RMSE=1.3799 RMSE=1.3953 Epoch: 20/150RMSE=0.9802 RMSE=1.3464 RMSE=1.4054 RMSE=1.3958 RMSE=1.0080 RMSE=1.0001 RMSE=1.4392 RMSE=1.4159 RMSE=0.9599 RMSE=0.9481 Epoch: 30/150RMSE=0.9628 RMSE=0.9662 RMSE=0.9814 RMSE=0.9457 RMSE=0.9691 RMSE=0.9553 RMSE=0.9844 RMSE=0.9539 RMSE=0.9594 RMSE=0.9611 Epoch: 40/150RMSE=0.9401 RMSE=0.9566 RMSE=0.9573 RMSE=0.9679 RMSE=0.9540 RMSE=0.9469 RMSE=0.9486 RMSE=0.9392 RMSE=0.9290 RMSE=0.9429 Epoch: 50/150RMSE=0.9542 RMSE=0.9320 RMSE=0.9306 RMSE=0.9398 RMSE=0.9348 RMSE=0.9306 RMSE=0.9338 RMSE=0.9353 RMSE=0.9388 RMSE=0.9316 Epoch: 60/150RMSE=0.9363 RMSE=0.9356 RMSE=0.9345 RMSE=0.9353 RMSE=0.9368 RMSE=0.9379 RMSE=0.9373 RMSE=0.9357 RMSE=0.9371 RMSE=0.8446 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 0.867 +/- 0.056\n",
      "\n",
      "Epoch: 1/150RMSE=1.2779 RMSE=1.2096 RMSE=1.1133 RMSE=1.1620 RMSE=1.0213 RMSE=1.0187 RMSE=0.9079 RMSE=1.0566 RMSE=0.8845 Epoch: 10/150RMSE=0.9235 RMSE=0.9177 RMSE=0.8728 RMSE=0.8592 RMSE=0.8623 RMSE=0.8493 RMSE=0.8436 RMSE=0.8663 RMSE=0.8121 RMSE=0.8392 Epoch: 20/150RMSE=0.8288 RMSE=0.8502 RMSE=0.8132 RMSE=0.8239 RMSE=0.8191 RMSE=0.7757 RMSE=0.7786 RMSE=0.7817 RMSE=0.8095 RMSE=0.7983 Epoch: 30/150RMSE=0.7809 RMSE=0.7652 RMSE=0.7739 RMSE=0.7599 RMSE=0.7659 RMSE=0.7605 RMSE=0.7755 RMSE=0.7754 RMSE=0.7653 RMSE=0.7814 Epoch: 40/150RMSE=0.7726 RMSE=0.7661 RMSE=0.7648 RMSE=0.7662 RMSE=0.7710 RMSE=0.7679 RMSE=0.7607 RMSE=0.7585 RMSE=0.7626 RMSE=0.7593 Epoch: 50/150RMSE=0.7594 RMSE=0.7613 RMSE=0.7622 RMSE=0.7603 RMSE=0.7588 RMSE=0.7589 RMSE=0.7604 RMSE=0.7611 RMSE=0.7608 RMSE=0.7600 Epoch: 60/150RMSE=0.7614 RMSE=0.7615 RMSE=0.7610 RMSE=0.7614 RMSE=0.7618 RMSE=0.7618 RMSE=0.7611 RMSE=0.7621 RMSE=0.7349 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 0.834 +/- 0.075\n",
      "\n",
      "Epoch: 1/150RMSE=1.2852 RMSE=1.1625 RMSE=1.1024 RMSE=1.1086 RMSE=1.0419 RMSE=1.0220 RMSE=0.9781 RMSE=1.0995 RMSE=0.9309 Epoch: 10/150RMSE=0.8875 RMSE=1.0396 RMSE=0.9463 RMSE=0.8703 RMSE=0.9635 RMSE=0.8734 RMSE=0.8359 RMSE=0.8603 RMSE=1.1528 RMSE=1.0026 Epoch: 20/150RMSE=0.8990 RMSE=0.9324 RMSE=0.9272 RMSE=0.8892 RMSE=0.9887 RMSE=0.8386 RMSE=0.8512 RMSE=0.8841 RMSE=0.8523 RMSE=0.8470 Epoch: 30/150RMSE=0.8707 RMSE=0.8807 RMSE=0.8834 RMSE=0.8887 RMSE=0.8705 RMSE=0.8790 RMSE=0.8741 RMSE=0.7999 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 0.827 +/- 0.068\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/lipo.pt\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.3270 RMSE=1.4691 RMSE=1.3286 RMSE=1.3234 RMSE=1.2133 RMSE=1.3586 RMSE=1.3646 RMSE=1.3746 RMSE=1.3690 Epoch: 10/150RMSE=1.1768 RMSE=1.3543 RMSE=1.3415 RMSE=1.1496 RMSE=1.4161 RMSE=1.3742 RMSE=1.5054 RMSE=1.2219 RMSE=1.2577 RMSE=1.2567 Epoch: 20/150RMSE=1.2650 RMSE=1.1675 RMSE=1.1427 RMSE=1.1285 RMSE=1.1578 RMSE=1.1299 RMSE=1.1129 RMSE=1.1280 RMSE=1.1430 RMSE=1.1390 Epoch: 30/150RMSE=1.1431 RMSE=1.0840 RMSE=1.0904 RMSE=1.0994 RMSE=1.1025 RMSE=1.0914 RMSE=1.0880 RMSE=1.0889 RMSE=1.0925 RMSE=1.0942 Epoch: 40/150RMSE=1.0896 RMSE=1.0891 RMSE=1.0899 RMSE=1.0914 RMSE=1.0920 RMSE=1.0930 RMSE=1.0907 RMSE=1.0891 RMSE=1.0864 RMSE=1.0927 Epoch: 50/150RMSE=1.0934 RMSE=1.0886 RMSE=0.9737 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.974 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.3001 RMSE=1.3408 RMSE=1.3195 RMSE=1.3308 RMSE=1.1581 RMSE=1.3672 RMSE=1.3324 RMSE=1.1195 RMSE=1.1233 Epoch: 10/150RMSE=1.3187 RMSE=1.3093 RMSE=1.3919 RMSE=1.0804 RMSE=1.3461 RMSE=1.3345 RMSE=1.0532 RMSE=1.1032 RMSE=1.0785 RMSE=1.0781 Epoch: 20/150RMSE=1.0457 RMSE=1.0561 RMSE=1.0374 RMSE=1.0231 RMSE=1.0272 RMSE=1.0255 RMSE=1.0396 RMSE=1.0186 RMSE=1.0230 RMSE=1.0118 Epoch: 30/150RMSE=1.0252 RMSE=1.0583 RMSE=1.0386 RMSE=1.0281 RMSE=1.0433 RMSE=1.0364 RMSE=1.0540 RMSE=1.0518 RMSE=1.0176 RMSE=1.0121 Epoch: 40/150RMSE=1.0197 RMSE=1.0245 RMSE=1.0149 RMSE=1.0312 RMSE=1.0128 RMSE=1.0173 RMSE=1.0181 RMSE=1.0160 RMSE=1.0160 RMSE=1.0090 Epoch: 50/150RMSE=1.0094 RMSE=1.0164 RMSE=1.0171 RMSE=1.0129 RMSE=1.0083 RMSE=1.0120 RMSE=1.0151 RMSE=1.0195 RMSE=1.0197 RMSE=1.0128 Epoch: 60/150RMSE=1.0129 RMSE=1.0155 RMSE=1.0154 RMSE=1.0149 RMSE=1.0113 RMSE=1.0115 RMSE=1.0176 RMSE=1.0148 RMSE=1.0134 RMSE=1.0134 Epoch: 70/150RMSE=1.0169 RMSE=1.0208 RMSE=1.0241 RMSE=1.0214 RMSE=1.0179 RMSE=0.9465 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 0.960 +/- 0.014\n",
      "\n",
      "Epoch: 1/150RMSE=1.2174 RMSE=1.1567 RMSE=1.1825 RMSE=1.1665 RMSE=1.1807 RMSE=1.1506 RMSE=1.0902 RMSE=1.0729 RMSE=1.1627 Epoch: 10/150RMSE=1.1336 RMSE=1.3484 RMSE=1.0573 RMSE=1.0590 RMSE=1.3287 RMSE=1.0577 RMSE=1.0156 RMSE=1.0351 RMSE=1.0323 RMSE=1.0516 Epoch: 20/150RMSE=1.0731 RMSE=1.3127 RMSE=1.0056 RMSE=1.3194 RMSE=1.3259 RMSE=0.9927 RMSE=1.3215 RMSE=1.0450 RMSE=1.0492 RMSE=1.0353 Epoch: 30/150RMSE=0.9851 RMSE=0.9739 RMSE=1.0037 RMSE=0.9900 RMSE=0.9762 RMSE=0.9793 RMSE=0.9604 RMSE=0.9730 RMSE=0.9855 RMSE=0.9579 Epoch: 40/150RMSE=0.9835 RMSE=0.9577 RMSE=0.9657 RMSE=0.9671 RMSE=0.9628 RMSE=0.9735 RMSE=0.9816 RMSE=0.9727 RMSE=0.9812 RMSE=0.9888 Epoch: 50/150RMSE=0.9881 RMSE=0.9793 RMSE=0.9796 RMSE=0.9778 RMSE=0.9856 RMSE=0.9851 RMSE=0.9805 RMSE=0.9877 RMSE=0.9872 RMSE=0.9786 Epoch: 60/150RMSE=0.9810 RMSE=0.9806 RMSE=1.3559 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.092 +/- 0.187\n",
      "\n",
      "Epoch: 1/150RMSE=1.2785 RMSE=1.2028 RMSE=1.2899 RMSE=1.1388 RMSE=1.1087 RMSE=1.0963 RMSE=1.0527 RMSE=1.0496 RMSE=1.0195 Epoch: 10/150RMSE=1.0399 RMSE=1.1321 RMSE=1.0032 RMSE=1.0169 RMSE=1.0439 RMSE=1.0595 RMSE=1.0114 RMSE=1.1329 RMSE=1.0288 RMSE=1.0125 Epoch: 20/150RMSE=1.0654 RMSE=1.0104 RMSE=0.9883 RMSE=0.9864 RMSE=0.9980 RMSE=0.9804 RMSE=1.0032 RMSE=0.9723 RMSE=1.0217 RMSE=1.0182 Epoch: 30/150RMSE=0.9827 RMSE=1.0160 RMSE=0.9750 RMSE=0.9746 RMSE=0.9816 RMSE=0.9911 RMSE=0.9894 RMSE=0.9972 RMSE=1.0052 RMSE=0.9917 Epoch: 40/150RMSE=0.9934 RMSE=0.9909 RMSE=0.9916 RMSE=1.0006 RMSE=0.9921 RMSE=0.9922 RMSE=0.9933 RMSE=0.9923 RMSE=0.9286 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.051 +/- 0.177\n",
      "\n",
      "Epoch: 1/150RMSE=1.2427 RMSE=1.1784 RMSE=1.0881 RMSE=1.0924 RMSE=1.0956 RMSE=1.0247 RMSE=1.0020 RMSE=0.9880 RMSE=0.9708 Epoch: 10/150RMSE=0.9660 RMSE=0.9529 RMSE=0.9583 RMSE=1.0069 RMSE=0.9526 RMSE=1.0020 RMSE=1.0024 RMSE=0.9935 RMSE=0.9792 RMSE=0.9295 Epoch: 20/150RMSE=0.9272 RMSE=0.9057 RMSE=0.9250 RMSE=0.9331 RMSE=0.9410 RMSE=0.9147 RMSE=0.9343 RMSE=0.9420 RMSE=1.1020 RMSE=1.0907 Epoch: 30/150RMSE=0.9070 RMSE=0.8935 RMSE=0.9027 RMSE=0.8962 RMSE=0.8948 RMSE=0.8944 RMSE=0.8969 RMSE=0.8983 RMSE=0.8972 RMSE=0.8945 Epoch: 40/150RMSE=0.8919 RMSE=0.8921 RMSE=0.8966 RMSE=0.8991 RMSE=0.8999 RMSE=0.9008 RMSE=0.8970 RMSE=0.8986 RMSE=0.8974 RMSE=0.8953 Epoch: 50/150RMSE=0.8931 RMSE=0.8976 RMSE=0.8939 RMSE=0.8972 RMSE=0.8972 RMSE=0.8926 RMSE=0.8924 RMSE=0.8959 RMSE=0.8957 RMSE=0.8944 Epoch: 60/150RMSE=0.8953 RMSE=0.8973 RMSE=0.8508 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.011 +/- 0.177\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --cuda_num 0 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CG-ODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
