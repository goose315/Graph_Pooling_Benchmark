{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch_scatter import scatter, scatter_add, scatter_min\n",
    "from torch_sparse import SparseTensor, remove_diag\n",
    "\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.nn.dense import Linear\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\n",
    "\n",
    "Scorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch_scatter import scatter_max, scatter_min\n",
    "\n",
    "from torch_geometric.typing import Adj, OptTensor, SparseTensor, Tensor\n",
    "\n",
    "\n",
    "def maximal_independent_set(edge_index: Adj, k: int = 1,\n",
    "                            perm: OptTensor = None) -> Tensor:\n",
    "    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n",
    "    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n",
    "    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n",
    "    returned set.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method follows `Blelloch's Alogirithm\n",
    "    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n",
    "    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n",
    "    higher values of :math:`k`.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: :class:`ByteTensor`\n",
    "    \"\"\"\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "        device = edge_index.device()\n",
    "        n = edge_index.size(0)\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        device = row.device\n",
    "        n = edge_index.max().item() + 1\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    mis = torch.zeros(n, dtype=torch.bool, device=device)\n",
    "    mask = mis.clone()\n",
    "    min_rank = rank.clone()\n",
    "\n",
    "    while not mask.all():\n",
    "        for _ in range(k):\n",
    "            min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "            scatter_min(min_rank[row], col, out=min_neigh)\n",
    "            torch.minimum(min_neigh, min_rank, out=min_rank)  # self-loops\n",
    "\n",
    "        mis = mis | torch.eq(rank, min_rank)\n",
    "        mask = mis.clone().byte()\n",
    "\n",
    "        for _ in range(k):\n",
    "            max_neigh = torch.full_like(mask, fill_value=0)\n",
    "            scatter_max(mask[row], col, out=max_neigh)\n",
    "            torch.maximum(max_neigh, mask, out=mask)  # self-loops\n",
    "\n",
    "        mask = mask.to(dtype=torch.bool)\n",
    "        min_rank = rank.clone()\n",
    "        min_rank[mask] = n\n",
    "\n",
    "    return mis\n",
    "\n",
    "def maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n",
    "                                    perm: OptTensor = None) -> PairTensor:\n",
    "    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n",
    "    clustering of a graph, as defined in `\"Generalizing Downsampling from\n",
    "    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method returns both the :math:`k`-MIS and the clustering, where the\n",
    "    :math:`c`-th cluster refers to the :math:`c`-th element of the\n",
    "    :math:`k`-MIS.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n",
    "    \"\"\"\n",
    "    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n",
    "    n, device = mis.size(0), mis.device\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n",
    "    rank_mis = rank[mis]\n",
    "    min_rank[mis] = rank_mis\n",
    "\n",
    "    for _ in range(k):\n",
    "        min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "        scatter_min(min_rank[row], col, out=min_neigh)\n",
    "        torch.minimum(min_neigh, min_rank, out=min_rank)\n",
    "\n",
    "    _, clusters = torch.unique(min_rank, return_inverse=True)\n",
    "    perm = torch.argsort(rank_mis)\n",
    "    return mis, perm[clusters]\n",
    "\n",
    "\n",
    "class KMISPooling(Module):\n",
    "\n",
    "    _heuristics = {None, 'greedy', 'w-greedy'}\n",
    "    _passthroughs = {None, 'before', 'after'}\n",
    "    _scorers = {\n",
    "        'linear',\n",
    "        'random',\n",
    "        'constant',\n",
    "        'canonical',\n",
    "        'first',\n",
    "        'last',\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n",
    "                 scorer: Union[Scorer, str] = 'linear',\n",
    "                 score_heuristic: Optional[str] = 'greedy',\n",
    "                 score_passthrough: Optional[str] = 'before',\n",
    "                 aggr_x: Optional[Union[str, Aggregation]] = None,\n",
    "                 aggr_edge: str = 'sum',\n",
    "                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n",
    "                 remove_self_loops: bool = True) -> None:\n",
    "        super(KMISPooling, self).__init__()\n",
    "        assert score_heuristic in self._heuristics, \\\n",
    "            \"Unrecognized `score_heuristic` value.\"\n",
    "        assert score_passthrough in self._passthroughs, \\\n",
    "            \"Unrecognized `score_passthrough` value.\"\n",
    "\n",
    "        if not callable(scorer):\n",
    "            assert scorer in self._scorers, \\\n",
    "                \"Unrecognized `scorer` value.\"\n",
    "\n",
    "        self.k = k\n",
    "        self.scorer = scorer\n",
    "        self.score_heuristic = score_heuristic\n",
    "        self.score_passthrough = score_passthrough\n",
    "\n",
    "        self.aggr_x = aggr_x\n",
    "        self.aggr_edge = aggr_edge\n",
    "        self.aggr_score = aggr_score\n",
    "        self.remove_self_loops = remove_self_loops\n",
    "\n",
    "        if scorer == 'linear':\n",
    "            assert self.score_passthrough is not None, \\\n",
    "                \"`'score_passthrough'` must not be `None`\" \\\n",
    "                \" when using `'linear'` scorer\"\n",
    "\n",
    "            self.lin = Linear(in_features=in_channels, out_features=1)\n",
    "\n",
    "    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n",
    "        if self.score_heuristic is None:\n",
    "            return x\n",
    "\n",
    "        row, col, _ = adj.coo()\n",
    "        x = x.view(-1)\n",
    "\n",
    "        if self.score_heuristic == 'greedy':\n",
    "            k_sums = torch.ones_like(x)\n",
    "        else:\n",
    "            k_sums = x.clone()\n",
    "\n",
    "        for _ in range(self.k):\n",
    "            scatter_add(k_sums[row], col, out=k_sums)\n",
    "\n",
    "        return x / k_sums\n",
    "\n",
    "    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) -> Tensor:\n",
    "        if self.scorer == 'linear':\n",
    "            return self.lin(x).sigmoid()\n",
    "\n",
    "        if self.scorer == 'random':\n",
    "            return torch.rand((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'constant':\n",
    "            return torch.ones((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'canonical':\n",
    "            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n",
    "\n",
    "        if self.scorer == 'first':\n",
    "            return x[..., [0]]\n",
    "\n",
    "        if self.scorer == 'last':\n",
    "            return x[..., [-1]]\n",
    "\n",
    "        return self.scorer(x, edge_index, edge_attr, batch)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) \\\n",
    "            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n",
    "        \"\"\"\"\"\"\n",
    "        edge_index = edge_index.long()\n",
    "        adj, n = edge_index, x.size(0)\n",
    "\n",
    "        if not isinstance(edge_index, SparseTensor):\n",
    "            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n",
    "\n",
    "        score = self._scorer(x, edge_index, edge_attr, batch)\n",
    "        updated_score = self._apply_heuristic(score, adj)\n",
    "        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n",
    "\n",
    "        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n",
    "\n",
    "        row, col, val = adj.coo()\n",
    "        c = mis.sum()\n",
    "\n",
    "        if val is None:\n",
    "            val = torch.ones_like(row, dtype=torch.float)\n",
    "\n",
    "        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n",
    "                           is_sorted=False,\n",
    "                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n",
    "\n",
    "        if self.remove_self_loops:\n",
    "            adj = remove_diag(adj)\n",
    "\n",
    "        if self.score_passthrough == 'before':\n",
    "            x = self.aggr_score(x, score)\n",
    "\n",
    "        if self.aggr_x is None:\n",
    "            x = x[mis]\n",
    "        elif isinstance(self.aggr_x, str):\n",
    "            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n",
    "                        reduce=self.aggr_x)\n",
    "        else:\n",
    "            x = self.aggr_x(x, cluster, dim_size=c)\n",
    "\n",
    "        if self.score_passthrough == 'after':\n",
    "            x = self.aggr_score(x, score[mis])\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_index, edge_attr = adj, None\n",
    "\n",
    "        else:\n",
    "            row, col, edge_attr = adj.coo()\n",
    "            edge_index = torch.stack([row, col])\n",
    "\n",
    "        if batch is not None:\n",
    "            batch = batch[mis]\n",
    "\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, mis, cluster\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.scorer == 'linear':\n",
    "            channels = f\"in_channels={self.lin.in_channels}, \"\n",
    "        else:\n",
    "            channels = \"\"\n",
    "\n",
    "        return f'{self.__class__.__name__}({channels}k={self.k})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROTEINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time: 25.64 seconds\n",
      "Var Time: 0.45 seconds\n",
      "Average Memory: 144.00 MB\n",
      "Average Best Val Acc: 0.7430\n",
      "Std Best Test Acc: 0.0101\n",
      "Average Test Acc: 0.7063\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 150\n",
    "data_path = \"/data/Zeyu/Pooling/\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"PROTEINS\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = KMISPooling(64, k=1, aggr_x='sum')\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = KMISPooling(64, k=1, aggr_x='sum')\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCI1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time: 76.19 seconds\n",
      "Var Time: 0.46 seconds\n",
      "Average Memory: 100.67 MB\n",
      "Average Best Val Acc: 0.7587\n",
      "Std Best Test Acc: 0.0219\n",
      "Average Test Acc: 0.7315\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 150\n",
    "data_path = \"/data/Zeyu/Pooling/\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"NCI1\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = KMISPooling(64, k=2, aggr_x='sum')\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = KMISPooling(64, k=2, aggr_x='sum')\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCI109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time: 77.94 seconds\n",
      "Var Time: 4.55 seconds\n",
      "Average Memory: 102.67 MB\n",
      "Average Best Val Acc: 0.7426\n",
      "Std Best Test Acc: 0.0110\n",
      "Average Test Acc: 0.7317\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 150\n",
    "data_path = \"/data/Zeyu/Pooling/\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"NCI109\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = KMISPooling(64, k=2, aggr_x='sum')\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = KMISPooling(64, k=2, aggr_x='sum')\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 156 for seed 42\n",
      "Early stopping at epoch 165 for seed 43\n",
      "Early stopping at epoch 163 for seed 44\n",
      "Average Time: 7.32 seconds\n",
      "Var Time: 0.03 seconds\n",
      "Average Memory: 32.67 MB\n",
      "Average Best Val Acc: 0.8690\n",
      "Std Best Test Acc: 0.0430\n",
      "Average Test Acc: 0.8046\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 150\n",
    "data_path = \"/data/Zeyu/Pooling/1\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"MUTAG\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = KMISPooling(64, k=1, aggr_x='sum')\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = KMISPooling(64, k=1, aggr_x='sum')\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 151 for seed 44\n",
      "Average Time: 25.64 seconds\n",
      "Var Time: 9.66 seconds\n",
      "Average Memory: 575.33 MB\n",
      "Average Best Val Acc: 0.7658\n",
      "Std Best Test Acc: 0.0170\n",
      "Average Test Acc: 0.7057\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"DD\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = KMISPooling(64, k=2, aggr_x='sum')\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = KMISPooling(64, k=2, aggr_x='sum')\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 170 for seed 42\n",
      "Early stopping at epoch 175 for seed 44\n",
      "Average Time: 27.48 seconds\n",
      "Var Time: 3.11 seconds\n",
      "Average Memory: 138.67 MB\n",
      "Average Best Val Acc: 0.7778\n",
      "Std Best Test Acc: 0.0362\n",
      "Average Test Acc: 0.7289\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-BINARY\", transform=T.Compose([T.OneHotDegree(136)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = KMISPooling(64, k=2, aggr_x='sum')\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = KMISPooling(64, k=2, aggr_x='sum')\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 173 for seed 42\n",
      "Early stopping at epoch 190 for seed 43\n",
      "Average Time: 39.87 seconds\n",
      "Var Time: 5.52 seconds\n",
      "Average Memory: 131.33 MB\n",
      "Average Best Val Acc: 0.5200\n",
      "Std Best Test Acc: 0.0247\n",
      "Average Test Acc: 0.4696\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-MULTI\", transform=T.Compose([T.OneHotDegree(88)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = KMISPooling(64, k=1, aggr_x='sum')\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = KMISPooling(64, k=1, aggr_x='sum')\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 1.0305, Val Acc: 0.6147, Test Acc: 0.6067\n",
      "Seed: 42, Epoch: 002, Loss: 0.9300, Val Acc: 0.6480, Test Acc: 0.6440\n",
      "Seed: 42, Epoch: 003, Loss: 0.8238, Val Acc: 0.6667, Test Acc: 0.6587\n",
      "Seed: 42, Epoch: 004, Loss: 0.7196, Val Acc: 0.6813, Test Acc: 0.6733\n",
      "Seed: 42, Epoch: 005, Loss: 0.6395, Val Acc: 0.7080, Test Acc: 0.6827\n",
      "Seed: 42, Epoch: 006, Loss: 0.5887, Val Acc: 0.7013, Test Acc: 0.6947\n",
      "Seed: 42, Epoch: 007, Loss: 0.5590, Val Acc: 0.7107, Test Acc: 0.6947\n",
      "Seed: 42, Epoch: 008, Loss: 0.5398, Val Acc: 0.6773, Test Acc: 0.6733\n",
      "Seed: 42, Epoch: 009, Loss: 0.5180, Val Acc: 0.7547, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 010, Loss: 0.4898, Val Acc: 0.7760, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 011, Loss: 0.4732, Val Acc: 0.7627, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 012, Loss: 0.4609, Val Acc: 0.7773, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 013, Loss: 0.4544, Val Acc: 0.7600, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 014, Loss: 0.4494, Val Acc: 0.7787, Test Acc: 0.7560\n",
      "Seed: 42, Epoch: 015, Loss: 0.4405, Val Acc: 0.7800, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 016, Loss: 0.4349, Val Acc: 0.7867, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 017, Loss: 0.4269, Val Acc: 0.7707, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 018, Loss: 0.4236, Val Acc: 0.7787, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 019, Loss: 0.4195, Val Acc: 0.7907, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 020, Loss: 0.4143, Val Acc: 0.7853, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 021, Loss: 0.4096, Val Acc: 0.7787, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 022, Loss: 0.4003, Val Acc: 0.7813, Test Acc: 0.7547\n",
      "Seed: 42, Epoch: 023, Loss: 0.3957, Val Acc: 0.7960, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 024, Loss: 0.3898, Val Acc: 0.7973, Test Acc: 0.7693\n",
      "Seed: 42, Epoch: 025, Loss: 0.3831, Val Acc: 0.7867, Test Acc: 0.7627\n",
      "Seed: 42, Epoch: 026, Loss: 0.3788, Val Acc: 0.7933, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 027, Loss: 0.3712, Val Acc: 0.8000, Test Acc: 0.7653\n",
      "Seed: 42, Epoch: 028, Loss: 0.3660, Val Acc: 0.8053, Test Acc: 0.7787\n",
      "Seed: 42, Epoch: 029, Loss: 0.3628, Val Acc: 0.7893, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 030, Loss: 0.3618, Val Acc: 0.7933, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 031, Loss: 0.3503, Val Acc: 0.7987, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 032, Loss: 0.3443, Val Acc: 0.8053, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 033, Loss: 0.3395, Val Acc: 0.8080, Test Acc: 0.7920\n",
      "Seed: 42, Epoch: 034, Loss: 0.3296, Val Acc: 0.8040, Test Acc: 0.7827\n",
      "Seed: 42, Epoch: 035, Loss: 0.3265, Val Acc: 0.8053, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 036, Loss: 0.3208, Val Acc: 0.8160, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 037, Loss: 0.3196, Val Acc: 0.7987, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 038, Loss: 0.3124, Val Acc: 0.8107, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 039, Loss: 0.3044, Val Acc: 0.8107, Test Acc: 0.7853\n",
      "Seed: 42, Epoch: 040, Loss: 0.3053, Val Acc: 0.8173, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 041, Loss: 0.3002, Val Acc: 0.8040, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 042, Loss: 0.2949, Val Acc: 0.8053, Test Acc: 0.7907\n",
      "Seed: 42, Epoch: 043, Loss: 0.2873, Val Acc: 0.7920, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 044, Loss: 0.2852, Val Acc: 0.8120, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 045, Loss: 0.2801, Val Acc: 0.8053, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 046, Loss: 0.2763, Val Acc: 0.7933, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 047, Loss: 0.2794, Val Acc: 0.8040, Test Acc: 0.8133\n",
      "Seed: 42, Epoch: 048, Loss: 0.2792, Val Acc: 0.8040, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 049, Loss: 0.2778, Val Acc: 0.8133, Test Acc: 0.7920\n",
      "Seed: 42, Epoch: 050, Loss: 0.2692, Val Acc: 0.8067, Test Acc: 0.8120\n",
      "Seed: 42, Epoch: 051, Loss: 0.2617, Val Acc: 0.8107, Test Acc: 0.8120\n",
      "Seed: 42, Epoch: 052, Loss: 0.2583, Val Acc: 0.8053, Test Acc: 0.8080\n",
      "Seed: 42, Epoch: 053, Loss: 0.2563, Val Acc: 0.8107, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 054, Loss: 0.2652, Val Acc: 0.8107, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 055, Loss: 0.2614, Val Acc: 0.8000, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 056, Loss: 0.2553, Val Acc: 0.8053, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 057, Loss: 0.2447, Val Acc: 0.8067, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 058, Loss: 0.2409, Val Acc: 0.8040, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 059, Loss: 0.2407, Val Acc: 0.8013, Test Acc: 0.8080\n",
      "Seed: 42, Epoch: 060, Loss: 0.2394, Val Acc: 0.8053, Test Acc: 0.8227\n",
      "Seed: 42, Epoch: 061, Loss: 0.2348, Val Acc: 0.7947, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 062, Loss: 0.2281, Val Acc: 0.8093, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 063, Loss: 0.2284, Val Acc: 0.7760, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 064, Loss: 0.2289, Val Acc: 0.8053, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 065, Loss: 0.2286, Val Acc: 0.8040, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 066, Loss: 0.2254, Val Acc: 0.7947, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 067, Loss: 0.2187, Val Acc: 0.8107, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 068, Loss: 0.2169, Val Acc: 0.7907, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 069, Loss: 0.2117, Val Acc: 0.7987, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 070, Loss: 0.2072, Val Acc: 0.7987, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 071, Loss: 0.2075, Val Acc: 0.7973, Test Acc: 0.8107\n",
      "Seed: 42, Epoch: 072, Loss: 0.1969, Val Acc: 0.7933, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 073, Loss: 0.1995, Val Acc: 0.8067, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 074, Loss: 0.2077, Val Acc: 0.7960, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 075, Loss: 0.2019, Val Acc: 0.7800, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 076, Loss: 0.1986, Val Acc: 0.7907, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 077, Loss: 0.1950, Val Acc: 0.7987, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 078, Loss: 0.1936, Val Acc: 0.8027, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 079, Loss: 0.1875, Val Acc: 0.7893, Test Acc: 0.8120\n",
      "Seed: 42, Epoch: 080, Loss: 0.1828, Val Acc: 0.7920, Test Acc: 0.7920\n",
      "Seed: 42, Epoch: 081, Loss: 0.1792, Val Acc: 0.7920, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 082, Loss: 0.1949, Val Acc: 0.7880, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 083, Loss: 0.1919, Val Acc: 0.7773, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 084, Loss: 0.2003, Val Acc: 0.7920, Test Acc: 0.8107\n",
      "Seed: 42, Epoch: 085, Loss: 0.1919, Val Acc: 0.7960, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 086, Loss: 0.1812, Val Acc: 0.7773, Test Acc: 0.7773\n",
      "Seed: 42, Epoch: 087, Loss: 0.1763, Val Acc: 0.7893, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 088, Loss: 0.1641, Val Acc: 0.7707, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 089, Loss: 0.1711, Val Acc: 0.7893, Test Acc: 0.7853\n",
      "Seed: 42, Epoch: 090, Loss: 0.1581, Val Acc: 0.7947, Test Acc: 0.7907\n",
      "Seed: 42, Epoch: 091, Loss: 0.1565, Val Acc: 0.7907, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 092, Loss: 0.1572, Val Acc: 0.7933, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 093, Loss: 0.1514, Val Acc: 0.7907, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 094, Loss: 0.1484, Val Acc: 0.7960, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 095, Loss: 0.1491, Val Acc: 0.7920, Test Acc: 0.7947\n",
      "Seed: 42, Epoch: 096, Loss: 0.1413, Val Acc: 0.7933, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 097, Loss: 0.1449, Val Acc: 0.7933, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 098, Loss: 0.1502, Val Acc: 0.8000, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 099, Loss: 0.1508, Val Acc: 0.7920, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 100, Loss: 0.1413, Val Acc: 0.7853, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 101, Loss: 0.1409, Val Acc: 0.7987, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 102, Loss: 0.1369, Val Acc: 0.7947, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 103, Loss: 0.1398, Val Acc: 0.7880, Test Acc: 0.7747\n",
      "Seed: 42, Epoch: 104, Loss: 0.1371, Val Acc: 0.7853, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 105, Loss: 0.1388, Val Acc: 0.7960, Test Acc: 0.7880\n",
      "Seed: 42, Epoch: 106, Loss: 0.1355, Val Acc: 0.7867, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 107, Loss: 0.1258, Val Acc: 0.7867, Test Acc: 0.7880\n",
      "Seed: 42, Epoch: 108, Loss: 0.1292, Val Acc: 0.7867, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 109, Loss: 0.1259, Val Acc: 0.7667, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 110, Loss: 0.1336, Val Acc: 0.7840, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 111, Loss: 0.1265, Val Acc: 0.7760, Test Acc: 0.7947\n",
      "Seed: 42, Epoch: 112, Loss: 0.1298, Val Acc: 0.7867, Test Acc: 0.8107\n",
      "Seed: 42, Epoch: 113, Loss: 0.1218, Val Acc: 0.7800, Test Acc: 0.7853\n",
      "Seed: 42, Epoch: 114, Loss: 0.1177, Val Acc: 0.7987, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 115, Loss: 0.1151, Val Acc: 0.7920, Test Acc: 0.7787\n",
      "Seed: 42, Epoch: 116, Loss: 0.1149, Val Acc: 0.7973, Test Acc: 0.7907\n",
      "Seed: 42, Epoch: 117, Loss: 0.1147, Val Acc: 0.7840, Test Acc: 0.7813\n",
      "Seed: 42, Epoch: 118, Loss: 0.1123, Val Acc: 0.7787, Test Acc: 0.7827\n",
      "Seed: 42, Epoch: 119, Loss: 0.1193, Val Acc: 0.7893, Test Acc: 0.7853\n",
      "Seed: 42, Epoch: 120, Loss: 0.1185, Val Acc: 0.7867, Test Acc: 0.7880\n",
      "Seed: 42, Epoch: 121, Loss: 0.1179, Val Acc: 0.7893, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 122, Loss: 0.1210, Val Acc: 0.7827, Test Acc: 0.7827\n",
      "Seed: 42, Epoch: 123, Loss: 0.1194, Val Acc: 0.7960, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 124, Loss: 0.1097, Val Acc: 0.7867, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 125, Loss: 0.1035, Val Acc: 0.7827, Test Acc: 0.7880\n",
      "Seed: 42, Epoch: 126, Loss: 0.1069, Val Acc: 0.7827, Test Acc: 0.7747\n",
      "Seed: 42, Epoch: 127, Loss: 0.1043, Val Acc: 0.7947, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 128, Loss: 0.1072, Val Acc: 0.7813, Test Acc: 0.7853\n",
      "Seed: 42, Epoch: 129, Loss: 0.1112, Val Acc: 0.8000, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 130, Loss: 0.1049, Val Acc: 0.7907, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 131, Loss: 0.1024, Val Acc: 0.7813, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 132, Loss: 0.0951, Val Acc: 0.7907, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 133, Loss: 0.0951, Val Acc: 0.7840, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 134, Loss: 0.0948, Val Acc: 0.7840, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 135, Loss: 0.0995, Val Acc: 0.7520, Test Acc: 0.7613\n",
      "Seed: 42, Epoch: 136, Loss: 0.1070, Val Acc: 0.7773, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 137, Loss: 0.1034, Val Acc: 0.7760, Test Acc: 0.7707\n",
      "Seed: 42, Epoch: 138, Loss: 0.0982, Val Acc: 0.7853, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 139, Loss: 0.0984, Val Acc: 0.7853, Test Acc: 0.7827\n",
      "Seed: 42, Epoch: 140, Loss: 0.0928, Val Acc: 0.7827, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 141, Loss: 0.0928, Val Acc: 0.7827, Test Acc: 0.7920\n",
      "Seed: 42, Epoch: 142, Loss: 0.0892, Val Acc: 0.7920, Test Acc: 0.7947\n",
      "Seed: 42, Epoch: 143, Loss: 0.0880, Val Acc: 0.7773, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 144, Loss: 0.0913, Val Acc: 0.7773, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 145, Loss: 0.0867, Val Acc: 0.7773, Test Acc: 0.7920\n",
      "Seed: 42, Epoch: 146, Loss: 0.0885, Val Acc: 0.7640, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 147, Loss: 0.0869, Val Acc: 0.7880, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 148, Loss: 0.0843, Val Acc: 0.7667, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 149, Loss: 0.0822, Val Acc: 0.7787, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 150, Loss: 0.0841, Val Acc: 0.7880, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 151, Loss: 0.0857, Val Acc: 0.7773, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 152, Loss: 0.0933, Val Acc: 0.7667, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 153, Loss: 0.0982, Val Acc: 0.7800, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 154, Loss: 0.1134, Val Acc: 0.7800, Test Acc: 0.7747\n",
      "Seed: 42, Epoch: 155, Loss: 0.1207, Val Acc: 0.7773, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 156, Loss: 0.1151, Val Acc: 0.7880, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 157, Loss: 0.1788, Val Acc: 0.7947, Test Acc: 0.7880\n",
      "Seed: 42, Epoch: 158, Loss: 0.1270, Val Acc: 0.7933, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 159, Loss: 0.1232, Val Acc: 0.7693, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 160, Loss: 0.1160, Val Acc: 0.7867, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 161, Loss: 0.1002, Val Acc: 0.7787, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 162, Loss: 0.0946, Val Acc: 0.7853, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 163, Loss: 0.0882, Val Acc: 0.7973, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 164, Loss: 0.0837, Val Acc: 0.7880, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 165, Loss: 0.0829, Val Acc: 0.7840, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 166, Loss: 0.0841, Val Acc: 0.7893, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 167, Loss: 0.0794, Val Acc: 0.7853, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 168, Loss: 0.0846, Val Acc: 0.7867, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 169, Loss: 0.0857, Val Acc: 0.7827, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 170, Loss: 0.0753, Val Acc: 0.7813, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 171, Loss: 0.0794, Val Acc: 0.7867, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 172, Loss: 0.0797, Val Acc: 0.7613, Test Acc: 0.7920\n",
      "Seed: 42, Epoch: 173, Loss: 0.0828, Val Acc: 0.7840, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 174, Loss: 0.0796, Val Acc: 0.7693, Test Acc: 0.7920\n",
      "Seed: 42, Epoch: 175, Loss: 0.0830, Val Acc: 0.7907, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 176, Loss: 0.0766, Val Acc: 0.7600, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 177, Loss: 0.0772, Val Acc: 0.7747, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 178, Loss: 0.0777, Val Acc: 0.7707, Test Acc: 0.7920\n",
      "Seed: 42, Epoch: 179, Loss: 0.0739, Val Acc: 0.7800, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 180, Loss: 0.0736, Val Acc: 0.7893, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 181, Loss: 0.0725, Val Acc: 0.7947, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 182, Loss: 0.0750, Val Acc: 0.7907, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 183, Loss: 0.0733, Val Acc: 0.7880, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 184, Loss: 0.0725, Val Acc: 0.7827, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 185, Loss: 0.0720, Val Acc: 0.7787, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 186, Loss: 0.0729, Val Acc: 0.7787, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 187, Loss: 0.0770, Val Acc: 0.7800, Test Acc: 0.7920\n",
      "Seed: 42, Epoch: 188, Loss: 0.0758, Val Acc: 0.7893, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 189, Loss: 0.0738, Val Acc: 0.7827, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 190, Loss: 0.0759, Val Acc: 0.7813, Test Acc: 0.8013\n",
      "Early stopping at epoch 190 for seed 42\n",
      "Seed: 43, Epoch: 001, Loss: 1.1025, Val Acc: 0.3440, Test Acc: 0.3307\n",
      "Seed: 43, Epoch: 002, Loss: 1.0012, Val Acc: 0.4893, Test Acc: 0.5040\n",
      "Seed: 43, Epoch: 003, Loss: 0.8639, Val Acc: 0.6907, Test Acc: 0.6800\n",
      "Seed: 43, Epoch: 004, Loss: 0.7561, Val Acc: 0.7080, Test Acc: 0.7040\n",
      "Seed: 43, Epoch: 005, Loss: 0.6608, Val Acc: 0.7107, Test Acc: 0.7080\n",
      "Seed: 43, Epoch: 006, Loss: 0.5966, Val Acc: 0.7413, Test Acc: 0.7320\n",
      "Seed: 43, Epoch: 007, Loss: 0.5486, Val Acc: 0.7693, Test Acc: 0.7493\n",
      "Seed: 43, Epoch: 008, Loss: 0.5142, Val Acc: 0.7773, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 009, Loss: 0.4926, Val Acc: 0.7747, Test Acc: 0.7480\n",
      "Seed: 43, Epoch: 010, Loss: 0.4835, Val Acc: 0.7787, Test Acc: 0.7587\n",
      "Seed: 43, Epoch: 011, Loss: 0.4706, Val Acc: 0.7800, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 012, Loss: 0.4590, Val Acc: 0.7800, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 013, Loss: 0.4508, Val Acc: 0.7947, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 014, Loss: 0.4434, Val Acc: 0.7880, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 015, Loss: 0.4387, Val Acc: 0.7933, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 016, Loss: 0.4296, Val Acc: 0.7907, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 017, Loss: 0.4226, Val Acc: 0.8027, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 018, Loss: 0.4140, Val Acc: 0.8053, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 019, Loss: 0.4097, Val Acc: 0.8067, Test Acc: 0.7853\n",
      "Seed: 43, Epoch: 020, Loss: 0.4045, Val Acc: 0.8027, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 021, Loss: 0.3994, Val Acc: 0.8080, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 022, Loss: 0.3860, Val Acc: 0.8067, Test Acc: 0.7947\n",
      "Seed: 43, Epoch: 023, Loss: 0.3855, Val Acc: 0.8187, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 024, Loss: 0.3848, Val Acc: 0.8187, Test Acc: 0.7987\n",
      "Seed: 43, Epoch: 025, Loss: 0.3710, Val Acc: 0.8227, Test Acc: 0.7920\n",
      "Seed: 43, Epoch: 026, Loss: 0.3631, Val Acc: 0.8147, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 027, Loss: 0.3604, Val Acc: 0.8187, Test Acc: 0.7947\n",
      "Seed: 43, Epoch: 028, Loss: 0.3467, Val Acc: 0.8267, Test Acc: 0.8027\n",
      "Seed: 43, Epoch: 029, Loss: 0.3391, Val Acc: 0.8267, Test Acc: 0.8027\n",
      "Seed: 43, Epoch: 030, Loss: 0.3351, Val Acc: 0.8333, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 031, Loss: 0.3279, Val Acc: 0.8307, Test Acc: 0.8053\n",
      "Seed: 43, Epoch: 032, Loss: 0.3269, Val Acc: 0.8293, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 033, Loss: 0.3212, Val Acc: 0.8307, Test Acc: 0.8053\n",
      "Seed: 43, Epoch: 034, Loss: 0.3189, Val Acc: 0.8213, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 035, Loss: 0.3181, Val Acc: 0.8320, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 036, Loss: 0.3040, Val Acc: 0.8280, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 037, Loss: 0.3002, Val Acc: 0.8240, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 038, Loss: 0.3001, Val Acc: 0.8293, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 039, Loss: 0.2988, Val Acc: 0.8280, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 040, Loss: 0.2891, Val Acc: 0.8213, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 041, Loss: 0.2840, Val Acc: 0.8280, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 042, Loss: 0.2779, Val Acc: 0.8147, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 043, Loss: 0.2760, Val Acc: 0.8240, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 044, Loss: 0.2740, Val Acc: 0.8267, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 045, Loss: 0.2688, Val Acc: 0.8200, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 046, Loss: 0.2695, Val Acc: 0.8133, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 047, Loss: 0.2706, Val Acc: 0.8293, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 048, Loss: 0.2609, Val Acc: 0.8227, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 049, Loss: 0.2569, Val Acc: 0.8080, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 050, Loss: 0.2597, Val Acc: 0.8200, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 051, Loss: 0.2548, Val Acc: 0.8187, Test Acc: 0.8267\n",
      "Seed: 43, Epoch: 052, Loss: 0.2476, Val Acc: 0.8187, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 053, Loss: 0.2512, Val Acc: 0.8253, Test Acc: 0.8320\n",
      "Seed: 43, Epoch: 054, Loss: 0.2400, Val Acc: 0.8147, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 055, Loss: 0.2407, Val Acc: 0.8213, Test Acc: 0.8293\n",
      "Seed: 43, Epoch: 056, Loss: 0.2346, Val Acc: 0.8147, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 057, Loss: 0.2311, Val Acc: 0.8093, Test Acc: 0.8267\n",
      "Seed: 43, Epoch: 058, Loss: 0.2243, Val Acc: 0.8227, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 059, Loss: 0.2255, Val Acc: 0.8227, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 060, Loss: 0.2213, Val Acc: 0.8320, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 061, Loss: 0.2167, Val Acc: 0.8227, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 062, Loss: 0.2109, Val Acc: 0.8213, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 063, Loss: 0.2072, Val Acc: 0.8227, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 064, Loss: 0.2112, Val Acc: 0.8240, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 065, Loss: 0.2044, Val Acc: 0.8133, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 066, Loss: 0.1992, Val Acc: 0.8280, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 067, Loss: 0.1941, Val Acc: 0.8173, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 068, Loss: 0.1938, Val Acc: 0.8160, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 069, Loss: 0.1940, Val Acc: 0.8173, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 070, Loss: 0.2043, Val Acc: 0.8320, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 071, Loss: 0.2053, Val Acc: 0.8133, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 072, Loss: 0.2101, Val Acc: 0.8080, Test Acc: 0.8040\n",
      "Seed: 43, Epoch: 073, Loss: 0.2249, Val Acc: 0.8160, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 074, Loss: 0.2069, Val Acc: 0.8200, Test Acc: 0.8293\n",
      "Seed: 43, Epoch: 075, Loss: 0.1975, Val Acc: 0.8333, Test Acc: 0.8320\n",
      "Seed: 43, Epoch: 076, Loss: 0.1884, Val Acc: 0.8267, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 077, Loss: 0.1845, Val Acc: 0.8280, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 078, Loss: 0.1835, Val Acc: 0.8227, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 079, Loss: 0.1755, Val Acc: 0.8333, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 080, Loss: 0.1750, Val Acc: 0.8253, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 081, Loss: 0.1739, Val Acc: 0.8227, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 082, Loss: 0.1750, Val Acc: 0.8213, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 083, Loss: 0.1658, Val Acc: 0.8280, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 084, Loss: 0.1554, Val Acc: 0.8173, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 085, Loss: 0.1610, Val Acc: 0.8240, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 086, Loss: 0.1603, Val Acc: 0.8187, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 087, Loss: 0.1559, Val Acc: 0.8213, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 088, Loss: 0.1579, Val Acc: 0.8253, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 089, Loss: 0.1570, Val Acc: 0.8240, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 090, Loss: 0.1482, Val Acc: 0.8160, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 091, Loss: 0.1459, Val Acc: 0.8187, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 092, Loss: 0.1486, Val Acc: 0.8213, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 093, Loss: 0.1445, Val Acc: 0.8200, Test Acc: 0.8040\n",
      "Seed: 43, Epoch: 094, Loss: 0.1431, Val Acc: 0.8187, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 095, Loss: 0.1430, Val Acc: 0.8107, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 096, Loss: 0.1514, Val Acc: 0.8040, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 097, Loss: 0.1489, Val Acc: 0.8173, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 098, Loss: 0.1518, Val Acc: 0.8200, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 099, Loss: 0.1419, Val Acc: 0.8253, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 100, Loss: 0.1335, Val Acc: 0.8133, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 101, Loss: 0.1297, Val Acc: 0.8200, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 102, Loss: 0.1369, Val Acc: 0.8200, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 103, Loss: 0.1426, Val Acc: 0.8200, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 104, Loss: 0.1368, Val Acc: 0.8227, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 105, Loss: 0.1344, Val Acc: 0.8107, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 106, Loss: 0.1339, Val Acc: 0.8227, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 107, Loss: 0.1312, Val Acc: 0.8227, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 108, Loss: 0.1305, Val Acc: 0.8093, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 109, Loss: 0.1281, Val Acc: 0.8067, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 110, Loss: 0.1348, Val Acc: 0.8160, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 111, Loss: 0.1257, Val Acc: 0.8147, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 112, Loss: 0.1260, Val Acc: 0.8160, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 113, Loss: 0.1279, Val Acc: 0.8080, Test Acc: 0.7920\n",
      "Seed: 43, Epoch: 114, Loss: 0.1280, Val Acc: 0.8120, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 115, Loss: 0.1329, Val Acc: 0.7987, Test Acc: 0.8040\n",
      "Seed: 43, Epoch: 116, Loss: 0.1180, Val Acc: 0.8187, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 117, Loss: 0.1106, Val Acc: 0.8173, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 118, Loss: 0.1109, Val Acc: 0.8160, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 119, Loss: 0.1141, Val Acc: 0.8093, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 120, Loss: 0.1139, Val Acc: 0.8173, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 121, Loss: 0.1125, Val Acc: 0.8227, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 122, Loss: 0.1109, Val Acc: 0.8120, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 123, Loss: 0.1065, Val Acc: 0.8080, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 124, Loss: 0.1157, Val Acc: 0.8213, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 125, Loss: 0.1132, Val Acc: 0.8213, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 126, Loss: 0.1112, Val Acc: 0.8173, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 127, Loss: 0.1036, Val Acc: 0.8147, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 128, Loss: 0.1078, Val Acc: 0.7987, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 129, Loss: 0.1087, Val Acc: 0.8133, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 130, Loss: 0.1033, Val Acc: 0.8093, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 131, Loss: 0.0967, Val Acc: 0.8120, Test Acc: 0.8267\n",
      "Seed: 43, Epoch: 132, Loss: 0.0993, Val Acc: 0.8160, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 133, Loss: 0.1032, Val Acc: 0.8027, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 134, Loss: 0.1031, Val Acc: 0.8067, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 135, Loss: 0.1017, Val Acc: 0.8027, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 136, Loss: 0.0966, Val Acc: 0.8053, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 137, Loss: 0.0974, Val Acc: 0.8040, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 138, Loss: 0.0913, Val Acc: 0.8093, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 139, Loss: 0.0915, Val Acc: 0.8120, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 140, Loss: 0.0894, Val Acc: 0.8147, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 141, Loss: 0.0910, Val Acc: 0.8067, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 142, Loss: 0.0966, Val Acc: 0.8067, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 143, Loss: 0.0976, Val Acc: 0.8013, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 144, Loss: 0.0924, Val Acc: 0.8107, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 145, Loss: 0.0864, Val Acc: 0.8147, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 146, Loss: 0.0849, Val Acc: 0.8120, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 147, Loss: 0.0872, Val Acc: 0.8093, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 148, Loss: 0.1004, Val Acc: 0.8080, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 149, Loss: 0.0874, Val Acc: 0.8120, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 150, Loss: 0.0864, Val Acc: 0.8000, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 151, Loss: 0.0889, Val Acc: 0.8107, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 152, Loss: 0.0878, Val Acc: 0.8133, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 153, Loss: 0.0885, Val Acc: 0.8147, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 154, Loss: 0.0833, Val Acc: 0.8080, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 155, Loss: 0.0835, Val Acc: 0.8160, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 156, Loss: 0.0889, Val Acc: 0.8187, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 157, Loss: 0.0883, Val Acc: 0.8093, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 158, Loss: 0.0906, Val Acc: 0.8013, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 159, Loss: 0.0859, Val Acc: 0.8080, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 160, Loss: 0.0837, Val Acc: 0.8053, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 161, Loss: 0.0904, Val Acc: 0.8147, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 162, Loss: 0.0849, Val Acc: 0.8120, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 163, Loss: 0.0897, Val Acc: 0.8040, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 164, Loss: 0.0821, Val Acc: 0.8107, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 165, Loss: 0.0843, Val Acc: 0.8093, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 166, Loss: 0.0818, Val Acc: 0.8160, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 167, Loss: 0.0824, Val Acc: 0.8093, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 168, Loss: 0.0854, Val Acc: 0.8067, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 169, Loss: 0.0854, Val Acc: 0.7947, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 170, Loss: 0.0974, Val Acc: 0.8173, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 171, Loss: 0.0962, Val Acc: 0.8053, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 172, Loss: 0.0746, Val Acc: 0.8067, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 173, Loss: 0.0824, Val Acc: 0.8027, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 174, Loss: 0.0878, Val Acc: 0.7960, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 175, Loss: 0.0772, Val Acc: 0.7947, Test Acc: 0.8053\n",
      "Seed: 43, Epoch: 176, Loss: 0.0813, Val Acc: 0.8067, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 177, Loss: 0.0764, Val Acc: 0.7960, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 178, Loss: 0.0792, Val Acc: 0.8027, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 179, Loss: 0.0881, Val Acc: 0.8053, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 180, Loss: 0.0782, Val Acc: 0.7960, Test Acc: 0.8173\n",
      "Early stopping at epoch 180 for seed 43\n",
      "Seed: 44, Epoch: 001, Loss: 1.0463, Val Acc: 0.6413, Test Acc: 0.6133\n",
      "Seed: 44, Epoch: 002, Loss: 0.9242, Val Acc: 0.6893, Test Acc: 0.6560\n",
      "Seed: 44, Epoch: 003, Loss: 0.8120, Val Acc: 0.6680, Test Acc: 0.6680\n",
      "Seed: 44, Epoch: 004, Loss: 0.7164, Val Acc: 0.7080, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 005, Loss: 0.6337, Val Acc: 0.7173, Test Acc: 0.7107\n",
      "Seed: 44, Epoch: 006, Loss: 0.5814, Val Acc: 0.7147, Test Acc: 0.7053\n",
      "Seed: 44, Epoch: 007, Loss: 0.5490, Val Acc: 0.7307, Test Acc: 0.7320\n",
      "Seed: 44, Epoch: 008, Loss: 0.5311, Val Acc: 0.7547, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 009, Loss: 0.5097, Val Acc: 0.7573, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 010, Loss: 0.4904, Val Acc: 0.7640, Test Acc: 0.7667\n",
      "Seed: 44, Epoch: 011, Loss: 0.4750, Val Acc: 0.7560, Test Acc: 0.7667\n",
      "Seed: 44, Epoch: 012, Loss: 0.4631, Val Acc: 0.7600, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 013, Loss: 0.4562, Val Acc: 0.7733, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 014, Loss: 0.4492, Val Acc: 0.7693, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 015, Loss: 0.4404, Val Acc: 0.7760, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 016, Loss: 0.4351, Val Acc: 0.7773, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 017, Loss: 0.4280, Val Acc: 0.7707, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 018, Loss: 0.4194, Val Acc: 0.7707, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 019, Loss: 0.4150, Val Acc: 0.7760, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 020, Loss: 0.4075, Val Acc: 0.7827, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 021, Loss: 0.4030, Val Acc: 0.7760, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 022, Loss: 0.3918, Val Acc: 0.7813, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 023, Loss: 0.3838, Val Acc: 0.7853, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 024, Loss: 0.3791, Val Acc: 0.7933, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 025, Loss: 0.3704, Val Acc: 0.8067, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 026, Loss: 0.3656, Val Acc: 0.8040, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 027, Loss: 0.3566, Val Acc: 0.8013, Test Acc: 0.7933\n",
      "Seed: 44, Epoch: 028, Loss: 0.3496, Val Acc: 0.7987, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 029, Loss: 0.3438, Val Acc: 0.8053, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 030, Loss: 0.3347, Val Acc: 0.8027, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 031, Loss: 0.3303, Val Acc: 0.7893, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 032, Loss: 0.3300, Val Acc: 0.7920, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 033, Loss: 0.3205, Val Acc: 0.8013, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 034, Loss: 0.3166, Val Acc: 0.7933, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 035, Loss: 0.3095, Val Acc: 0.8027, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 036, Loss: 0.3039, Val Acc: 0.8120, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 037, Loss: 0.2943, Val Acc: 0.8013, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 038, Loss: 0.3037, Val Acc: 0.8000, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 039, Loss: 0.2975, Val Acc: 0.8040, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 040, Loss: 0.2935, Val Acc: 0.8040, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 041, Loss: 0.2894, Val Acc: 0.8120, Test Acc: 0.8187\n",
      "Seed: 44, Epoch: 042, Loss: 0.2809, Val Acc: 0.8067, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 043, Loss: 0.2884, Val Acc: 0.8027, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 044, Loss: 0.2738, Val Acc: 0.7987, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 045, Loss: 0.2708, Val Acc: 0.7973, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 046, Loss: 0.2690, Val Acc: 0.8000, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 047, Loss: 0.2715, Val Acc: 0.8080, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 048, Loss: 0.2607, Val Acc: 0.8053, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 049, Loss: 0.2508, Val Acc: 0.8000, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 050, Loss: 0.2528, Val Acc: 0.8080, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 051, Loss: 0.2434, Val Acc: 0.8040, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 052, Loss: 0.2438, Val Acc: 0.8027, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 053, Loss: 0.2381, Val Acc: 0.8107, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 054, Loss: 0.2352, Val Acc: 0.8053, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 055, Loss: 0.2335, Val Acc: 0.8000, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 056, Loss: 0.2389, Val Acc: 0.7960, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 057, Loss: 0.2320, Val Acc: 0.7987, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 058, Loss: 0.2272, Val Acc: 0.8120, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 059, Loss: 0.2233, Val Acc: 0.8053, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 060, Loss: 0.2215, Val Acc: 0.8027, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 061, Loss: 0.2203, Val Acc: 0.8093, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 062, Loss: 0.2149, Val Acc: 0.8000, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 063, Loss: 0.2121, Val Acc: 0.8000, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 064, Loss: 0.2050, Val Acc: 0.7973, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 065, Loss: 0.2049, Val Acc: 0.7973, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 066, Loss: 0.2037, Val Acc: 0.7987, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 067, Loss: 0.2026, Val Acc: 0.8000, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 068, Loss: 0.2003, Val Acc: 0.8053, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 069, Loss: 0.1935, Val Acc: 0.8000, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 070, Loss: 0.1925, Val Acc: 0.8013, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 071, Loss: 0.1860, Val Acc: 0.8053, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 072, Loss: 0.1882, Val Acc: 0.7947, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 073, Loss: 0.1879, Val Acc: 0.7813, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 074, Loss: 0.1877, Val Acc: 0.8120, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 075, Loss: 0.1909, Val Acc: 0.8013, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 076, Loss: 0.1840, Val Acc: 0.7933, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 077, Loss: 0.1785, Val Acc: 0.8053, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 078, Loss: 0.1868, Val Acc: 0.8013, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 079, Loss: 0.1835, Val Acc: 0.8053, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 080, Loss: 0.1926, Val Acc: 0.8080, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 081, Loss: 0.1773, Val Acc: 0.8027, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 082, Loss: 0.1723, Val Acc: 0.8040, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 083, Loss: 0.1726, Val Acc: 0.8013, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 084, Loss: 0.1676, Val Acc: 0.8027, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 085, Loss: 0.1650, Val Acc: 0.7920, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 086, Loss: 0.1621, Val Acc: 0.8000, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 087, Loss: 0.1565, Val Acc: 0.8027, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 088, Loss: 0.1595, Val Acc: 0.7973, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 089, Loss: 0.1557, Val Acc: 0.7933, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 090, Loss: 0.1493, Val Acc: 0.8000, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 091, Loss: 0.1456, Val Acc: 0.7920, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 092, Loss: 0.1495, Val Acc: 0.8027, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 093, Loss: 0.1452, Val Acc: 0.8040, Test Acc: 0.7933\n",
      "Seed: 44, Epoch: 094, Loss: 0.1432, Val Acc: 0.8027, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 095, Loss: 0.1398, Val Acc: 0.8040, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 096, Loss: 0.1423, Val Acc: 0.7973, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 097, Loss: 0.1404, Val Acc: 0.7947, Test Acc: 0.7760\n",
      "Seed: 44, Epoch: 098, Loss: 0.1434, Val Acc: 0.7987, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 099, Loss: 0.1458, Val Acc: 0.7987, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 100, Loss: 0.1546, Val Acc: 0.7893, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 101, Loss: 0.1556, Val Acc: 0.8053, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 102, Loss: 0.1435, Val Acc: 0.8040, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 103, Loss: 0.1442, Val Acc: 0.8000, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 104, Loss: 0.1412, Val Acc: 0.7947, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 105, Loss: 0.1309, Val Acc: 0.8027, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 106, Loss: 0.1353, Val Acc: 0.7893, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 107, Loss: 0.1336, Val Acc: 0.8120, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 108, Loss: 0.1259, Val Acc: 0.8027, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 109, Loss: 0.1200, Val Acc: 0.8040, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 110, Loss: 0.1225, Val Acc: 0.7987, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 111, Loss: 0.1205, Val Acc: 0.8000, Test Acc: 0.7880\n",
      "Seed: 44, Epoch: 112, Loss: 0.1133, Val Acc: 0.8000, Test Acc: 0.7933\n",
      "Seed: 44, Epoch: 113, Loss: 0.1149, Val Acc: 0.7947, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 114, Loss: 0.1138, Val Acc: 0.7987, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 115, Loss: 0.1170, Val Acc: 0.7973, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 116, Loss: 0.1199, Val Acc: 0.7920, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 117, Loss: 0.1180, Val Acc: 0.8067, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 118, Loss: 0.1117, Val Acc: 0.7987, Test Acc: 0.7880\n",
      "Seed: 44, Epoch: 119, Loss: 0.1062, Val Acc: 0.7973, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 120, Loss: 0.1105, Val Acc: 0.7827, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 121, Loss: 0.1106, Val Acc: 0.7907, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 122, Loss: 0.1043, Val Acc: 0.7973, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 123, Loss: 0.1108, Val Acc: 0.7867, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 124, Loss: 0.1242, Val Acc: 0.7987, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 125, Loss: 0.1114, Val Acc: 0.8040, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 126, Loss: 0.1225, Val Acc: 0.7960, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 127, Loss: 0.1088, Val Acc: 0.7933, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 128, Loss: 0.1030, Val Acc: 0.7933, Test Acc: 0.7880\n",
      "Seed: 44, Epoch: 129, Loss: 0.1033, Val Acc: 0.8000, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 130, Loss: 0.1036, Val Acc: 0.7947, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 131, Loss: 0.0950, Val Acc: 0.8000, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 132, Loss: 0.0981, Val Acc: 0.7880, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 133, Loss: 0.0947, Val Acc: 0.7973, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 134, Loss: 0.0968, Val Acc: 0.7947, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 135, Loss: 0.1008, Val Acc: 0.7933, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 136, Loss: 0.1031, Val Acc: 0.8040, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 137, Loss: 0.1062, Val Acc: 0.7960, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 138, Loss: 0.1000, Val Acc: 0.8000, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 139, Loss: 0.0993, Val Acc: 0.7893, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 140, Loss: 0.1019, Val Acc: 0.7893, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 141, Loss: 0.0935, Val Acc: 0.7920, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 142, Loss: 0.0966, Val Acc: 0.8027, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 143, Loss: 0.0959, Val Acc: 0.8013, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 144, Loss: 0.0902, Val Acc: 0.7973, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 145, Loss: 0.0887, Val Acc: 0.8013, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 146, Loss: 0.0862, Val Acc: 0.7987, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 147, Loss: 0.0789, Val Acc: 0.7973, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 148, Loss: 0.0782, Val Acc: 0.7947, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 149, Loss: 0.0818, Val Acc: 0.7987, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 150, Loss: 0.0779, Val Acc: 0.7933, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 151, Loss: 0.0807, Val Acc: 0.7973, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 152, Loss: 0.0822, Val Acc: 0.7840, Test Acc: 0.7760\n",
      "Seed: 44, Epoch: 153, Loss: 0.0829, Val Acc: 0.7907, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 154, Loss: 0.0815, Val Acc: 0.7960, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 155, Loss: 0.0797, Val Acc: 0.7933, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 156, Loss: 0.0776, Val Acc: 0.7987, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 157, Loss: 0.0751, Val Acc: 0.7947, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 158, Loss: 0.0762, Val Acc: 0.7947, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 159, Loss: 0.0763, Val Acc: 0.8013, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 160, Loss: 0.0763, Val Acc: 0.7960, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 161, Loss: 0.0922, Val Acc: 0.7933, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 162, Loss: 0.0877, Val Acc: 0.7827, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 163, Loss: 0.0834, Val Acc: 0.7947, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 164, Loss: 0.0737, Val Acc: 0.7933, Test Acc: 0.7880\n",
      "Seed: 44, Epoch: 165, Loss: 0.0754, Val Acc: 0.7867, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 166, Loss: 0.0713, Val Acc: 0.8013, Test Acc: 0.7933\n",
      "Seed: 44, Epoch: 167, Loss: 0.0713, Val Acc: 0.7987, Test Acc: 0.7933\n",
      "Seed: 44, Epoch: 168, Loss: 0.0690, Val Acc: 0.7947, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 169, Loss: 0.0652, Val Acc: 0.7933, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 170, Loss: 0.0649, Val Acc: 0.7867, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 171, Loss: 0.0686, Val Acc: 0.7907, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 172, Loss: 0.0699, Val Acc: 0.7987, Test Acc: 0.7933\n",
      "Seed: 44, Epoch: 173, Loss: 0.0700, Val Acc: 0.7960, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 174, Loss: 0.0667, Val Acc: 0.7960, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 175, Loss: 0.0644, Val Acc: 0.7893, Test Acc: 0.7760\n",
      "Seed: 44, Epoch: 176, Loss: 0.0689, Val Acc: 0.7960, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 177, Loss: 0.0623, Val Acc: 0.7960, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 178, Loss: 0.0616, Val Acc: 0.7973, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 179, Loss: 0.0634, Val Acc: 0.7973, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 180, Loss: 0.0592, Val Acc: 0.7907, Test Acc: 0.7880\n",
      "Seed: 44, Epoch: 181, Loss: 0.0626, Val Acc: 0.7960, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 182, Loss: 0.0672, Val Acc: 0.7827, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 183, Loss: 0.0694, Val Acc: 0.7907, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 184, Loss: 0.0612, Val Acc: 0.7947, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 185, Loss: 0.0601, Val Acc: 0.7960, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 186, Loss: 0.0632, Val Acc: 0.7933, Test Acc: 0.7947\n",
      "Early stopping at epoch 186 for seed 44\n",
      "Average Time: 363.94 seconds\n",
      "Var Time: 171.63 seconds\n",
      "Average Memory: 12022.67 MB\n",
      "Average Best Val Acc: 0.8209\n",
      "Std Best Test Acc: 0.0049\n",
      "Average Test Acc: 0.8071\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"COLLAB\", transform=T.Compose([T.OneHotDegree(491)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = KMISPooling(64, k=2, aggr_x='sum')\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = KMISPooling(64, k=2, aggr_x='sum')\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_KMIS(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1549.5618 MAE=1547.0464 MAE=1544.4448 MAE=1542.1294 MAE=1539.6531 MAE=1536.8313 MAE=1531.3667 MAE=1527.2961 MAE=1524.4912 Epoch: 10/500MAE=1520.7839 MAE=1513.5902 MAE=1508.8425 MAE=1503.6182 MAE=1495.1584 MAE=1492.0415 MAE=1484.4242 MAE=1470.2502 MAE=1463.0957 MAE=1451.1276 Epoch: 20/500MAE=1442.1366 MAE=1432.0121 MAE=1428.1873 MAE=1423.5129 MAE=1406.6204 MAE=1398.7534 MAE=1386.5073 MAE=1373.7421 MAE=1367.6544 MAE=1353.0543 Epoch: 30/500MAE=1334.3445 MAE=1326.7229 MAE=1320.0898 MAE=1284.9170 MAE=1279.4819 MAE=1283.1274 MAE=1277.3868 MAE=1250.5691 MAE=1229.1216 MAE=1231.8153 Epoch: 40/500MAE=1196.9365 MAE=1201.6630 MAE=1164.7148 MAE=1161.7800 MAE=1148.6885 MAE=1129.2488 MAE=1131.0776 MAE=1109.1387 MAE=1077.2673 MAE=1087.8845 Epoch: 50/500MAE=1055.1865 MAE=1016.9828 MAE=1028.6208 MAE=991.5339 MAE=971.1177 MAE=932.2491 MAE=943.5845 MAE=893.9925 MAE=882.5664 MAE=886.9164 Epoch: 60/500MAE=844.5480 MAE=807.6670 MAE=824.0829 MAE=688.9279 MAE=731.0027 MAE=735.5696 MAE=720.2940 MAE=603.4985 MAE=639.6513 MAE=536.9354 Epoch: 70/500MAE=603.3289 MAE=513.1767 MAE=547.2217 MAE=546.7775 MAE=473.3467 MAE=463.9140 MAE=531.1007 MAE=422.3050 MAE=372.8833 MAE=425.4742 Epoch: 80/500MAE=415.7281 MAE=296.6500 MAE=296.7064 MAE=338.6704 MAE=288.6492 MAE=255.8693 MAE=201.3510 MAE=261.4659 MAE=214.8619 MAE=213.6005 Epoch: 90/500MAE=196.1111 MAE=212.8594 MAE=151.8360 MAE=132.2241 MAE=119.7720 MAE=127.4088 MAE=147.1627 MAE=107.3116 MAE=136.6499 MAE=87.3125 Epoch: 100/500MAE=109.1721 MAE=102.9871 MAE=94.8891 MAE=110.4979 MAE=86.6228 MAE=89.3082 MAE=83.2135 MAE=94.5338 MAE=88.0337 MAE=79.0770 Epoch: 110/500MAE=87.5941 MAE=82.1731 MAE=79.5439 MAE=76.8098 MAE=80.6039 MAE=79.6176 MAE=75.0734 MAE=72.9480 MAE=78.5711 MAE=74.4298 Epoch: 120/500MAE=78.9324 MAE=76.9585 MAE=70.4836 MAE=69.7940 MAE=70.0827 MAE=68.0199 MAE=69.9521 MAE=66.4660 MAE=69.9906 MAE=67.8736 Epoch: 130/500MAE=69.4750 MAE=67.7659 MAE=67.1529 MAE=66.5914 MAE=66.9300 MAE=66.2354 MAE=66.7797 MAE=66.5573 MAE=66.4699 MAE=66.6064 Epoch: 140/500MAE=66.2926 MAE=65.4109 MAE=66.1526 MAE=66.3719 MAE=65.7147 MAE=65.3827 MAE=64.5589 MAE=65.1486 MAE=65.0824 MAE=65.0480 Epoch: 150/500MAE=65.0613 MAE=64.8194 MAE=64.5116 MAE=64.4162 MAE=65.6796 MAE=65.3193 MAE=65.2402 MAE=65.6013 MAE=65.1598 MAE=65.0825 Epoch: 160/500MAE=64.8182 MAE=64.7513 MAE=64.6213 MAE=64.8184 MAE=64.8548 MAE=64.7407 MAE=64.8127 MAE=64.9235 MAE=64.9614 MAE=65.1163 Epoch: 170/500MAE=65.0127 MAE=64.8873 MAE=65.0510 MAE=64.9265 MAE=64.9820 MAE=64.9435 MAE=64.9918 MAE=64.9754 MAE=65.0299 MAE=64.8830 Epoch: 180/500MAE=65.0891 MAE=65.0827 MAE=65.1149 MAE=64.9822 MAE=65.0424 MAE=64.9656 MAE=65.0415 MAE=65.1459 MAE=64.9414 MAE=64.9794 Epoch: 190/500MAE=65.1529 MAE=64.9305 MAE=65.0725 MAE=64.9918 MAE=64.9227 MAE=64.7780 MAE=65.0284 MAE=65.1682 MAE=64.9240 MAE=65.1413 Epoch: 200/500MAE=65.0842 MAE=64.9487 MAE=64.9535 MAE=64.9568 MAE=65.0527 MAE=64.9085 MAE=65.1073 MAE=65.1314 MAE=64.9354 MAE=64.9545 Epoch: 210/500MAE=64.8552 MAE=65.1356 MAE=65.1061 MAE=64.9178 MAE=64.8486 MAE=64.9095 MAE=64.8366 MAE=65.0621 MAE=64.9903 MAE=64.8976 Epoch: 220/500MAE=64.9393 MAE=65.0787 MAE=64.9727 MAE=65.0058 MAE=64.9298 MAE=65.0899 MAE=65.1675 MAE=64.9084 MAE=65.1607 MAE=65.1764 Epoch: 230/500MAE=65.0364 MAE=65.1624 MAE=65.1221 MAE=64.9485 MAE=64.9166 MAE=65.0854 MAE=64.8968 MAE=65.1389 MAE=65.1523 MAE=65.2380 Epoch: 240/500MAE=64.9516 MAE=64.8628 MAE=64.9078 MAE=65.1395 MAE=65.0842 MAE=64.9498 MAE=65.1009 MAE=65.1192 MAE=64.9160 MAE=64.9812 Epoch: 250/500MAE=65.0093 MAE=65.1096 MAE=65.1373 MAE=64.9979 MAE=64.9708 MAE=65.0125 MAE=64.9887 MAE=64.9811 MAE=64.9106 MAE=64.9719 Epoch: 260/500MAE=65.0047 MAE=64.8738 MAE=64.9819 MAE=64.8512 MAE=64.8134 MAE=64.9853 MAE=64.8744 MAE=65.0177 MAE=65.0655 MAE=64.9035 Epoch: 270/500MAE=64.7426 MAE=64.8940 MAE=64.8941 MAE=65.0054 MAE=64.8663 MAE=65.0983 MAE=65.0371 MAE=64.9108 MAE=65.0894 MAE=64.8705 Epoch: 280/500MAE=64.9063 MAE=64.9261 MAE=64.9619 MAE=64.9184 MAE=65.0724 MAE=64.8321 MAE=64.7968 MAE=65.0626 MAE=64.9237 MAE=64.8574 Epoch: 290/500MAE=64.9035 MAE=65.1172 MAE=64.8863 MAE=64.8290 MAE=64.7088 MAE=64.6805 MAE=64.8260 MAE=65.0262 MAE=64.8190 MAE=64.8769 Epoch: 300/500MAE=65.1732 MAE=65.0820 MAE=64.8191 MAE=64.9631 MAE=67.6224 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 67.622 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1549.7936 MAE=1547.5437 MAE=1544.8566 MAE=1542.5486 MAE=1540.4929 MAE=1536.1466 MAE=1533.8964 MAE=1530.7375 MAE=1528.3123 Epoch: 10/500MAE=1520.1243 MAE=1518.0005 MAE=1509.4825 MAE=1504.7977 MAE=1496.4739 MAE=1489.7233 MAE=1483.0386 MAE=1475.1357 MAE=1469.9044 MAE=1469.1304 Epoch: 20/500MAE=1455.6206 MAE=1442.9146 MAE=1436.0994 MAE=1426.9817 MAE=1416.8462 MAE=1406.5769 MAE=1396.8745 MAE=1385.7166 MAE=1362.8530 MAE=1360.5059 Epoch: 30/500MAE=1337.1085 MAE=1332.4216 MAE=1318.4030 MAE=1307.0063 MAE=1296.7256 MAE=1294.7590 MAE=1280.7773 MAE=1250.3110 MAE=1234.9114 MAE=1210.0454 Epoch: 40/500MAE=1188.9288 MAE=1208.5547 MAE=1143.8403 MAE=1144.6262 MAE=1163.2407 MAE=1134.3713 MAE=1125.0435 MAE=1087.7155 MAE=1096.0662 MAE=1097.1130 Epoch: 50/500MAE=1018.0410 MAE=1044.1890 MAE=999.8147 MAE=1000.7953 MAE=965.9471 MAE=942.1910 MAE=926.5647 MAE=946.2562 MAE=853.4960 MAE=875.6238 Epoch: 60/500MAE=874.0266 MAE=825.8971 MAE=780.5331 MAE=769.4415 MAE=752.1182 MAE=695.0297 MAE=706.6981 MAE=765.7324 MAE=646.3535 MAE=548.6853 Epoch: 70/500MAE=579.8375 MAE=601.9399 MAE=579.7089 MAE=528.9886 MAE=433.0717 MAE=527.7957 MAE=467.7358 MAE=409.8710 MAE=407.5073 MAE=373.8976 Epoch: 80/500MAE=430.6461 MAE=343.0218 MAE=324.8368 MAE=269.4651 MAE=266.5757 MAE=254.6219 MAE=243.3640 MAE=235.1114 MAE=188.2841 MAE=197.7609 Epoch: 90/500MAE=165.8436 MAE=183.0807 MAE=197.2148 MAE=97.7114 MAE=120.9737 MAE=106.3578 MAE=119.3797 MAE=151.2074 MAE=121.6906 MAE=127.6081 Epoch: 100/500MAE=119.0397 MAE=118.9176 MAE=96.9311 MAE=97.0684 MAE=93.6342 MAE=95.0019 MAE=96.0599 MAE=89.8477 MAE=93.9756 MAE=93.2604 Epoch: 110/500MAE=86.1621 MAE=83.2844 MAE=94.1904 MAE=94.3212 MAE=92.8774 MAE=81.4392 MAE=78.2948 MAE=86.7346 MAE=77.9281 MAE=80.6870 Epoch: 120/500MAE=79.9407 MAE=82.4125 MAE=90.3398 MAE=78.3372 MAE=75.0855 MAE=74.5202 MAE=77.1499 MAE=77.3260 MAE=77.6737 MAE=74.4268 Epoch: 130/500MAE=72.9925 MAE=75.9169 MAE=73.0861 MAE=75.2699 MAE=74.7080 MAE=75.5003 MAE=74.5391 MAE=72.6633 MAE=73.5451 MAE=74.7711 Epoch: 140/500MAE=74.0060 MAE=72.6820 MAE=71.8912 MAE=72.0031 MAE=72.0920 MAE=71.7126 MAE=72.6065 MAE=72.6744 MAE=73.2296 MAE=72.8841 Epoch: 150/500MAE=71.9985 MAE=71.6959 MAE=72.4603 MAE=72.5373 MAE=71.9132 MAE=72.4229 MAE=71.7715 MAE=72.1171 MAE=71.7283 MAE=71.9766 Epoch: 160/500MAE=72.2088 MAE=71.8390 MAE=72.1157 MAE=71.7355 MAE=71.6954 MAE=71.8789 MAE=71.6552 MAE=71.8931 MAE=71.4519 MAE=71.9871 Epoch: 170/500MAE=71.7194 MAE=72.0032 MAE=71.6169 MAE=71.9188 MAE=71.8932 MAE=71.5106 MAE=71.7198 MAE=71.6501 MAE=71.8453 MAE=71.7430 Epoch: 180/500MAE=71.7256 MAE=71.7576 MAE=71.6925 MAE=71.8318 MAE=71.7940 MAE=71.6311 MAE=71.7076 MAE=71.8334 MAE=71.7234 MAE=71.8448 Epoch: 190/500MAE=71.8445 MAE=71.8835 MAE=71.6443 MAE=71.7143 MAE=71.7575 MAE=71.5955 MAE=71.7788 MAE=71.7720 MAE=71.5951 MAE=71.7416 Epoch: 200/500MAE=71.6654 MAE=71.7590 MAE=71.6921 MAE=71.7925 MAE=71.5878 MAE=71.5655 MAE=71.5949 MAE=71.6646 MAE=71.7666 MAE=71.7092 Epoch: 210/500MAE=71.6437 MAE=71.6011 MAE=71.7287 MAE=71.5716 MAE=71.7134 MAE=71.6462 MAE=71.7311 MAE=71.7903 MAE=71.7415 MAE=71.7204 Epoch: 220/500MAE=71.7883 MAE=71.6314 MAE=71.5330 MAE=71.5760 MAE=71.7476 MAE=71.5923 MAE=71.8136 MAE=71.6306 MAE=71.5765 MAE=71.6511 Epoch: 230/500MAE=71.8251 MAE=71.7090 MAE=71.8669 MAE=71.5063 MAE=71.6802 MAE=71.7458 MAE=71.7205 MAE=71.6448 MAE=71.5943 MAE=71.8307 Epoch: 240/500MAE=71.6830 MAE=71.8894 MAE=71.6832 MAE=71.6798 MAE=71.5395 MAE=71.6099 MAE=71.5806 MAE=71.6570 MAE=71.6496 MAE=71.5956 Epoch: 250/500MAE=71.4849 MAE=71.5671 MAE=71.5143 MAE=71.4879 MAE=71.5726 MAE=71.5417 MAE=71.6299 MAE=71.4754 MAE=71.5167 MAE=71.5160 Epoch: 260/500MAE=71.6419 MAE=71.6475 MAE=71.5460 MAE=71.4230 MAE=71.5137 MAE=71.5736 MAE=71.5191 MAE=71.7071 MAE=71.6060 MAE=71.6962 Epoch: 270/500MAE=71.6633 MAE=71.4098 MAE=71.5475 MAE=71.5944 MAE=71.7354 MAE=71.7180 MAE=71.6690 MAE=71.6120 MAE=71.5222 MAE=71.5139 Epoch: 280/500MAE=71.5449 MAE=71.4765 MAE=71.4675 MAE=71.5208 MAE=71.5164 MAE=71.4859 MAE=71.5962 MAE=71.4882 MAE=71.4505 MAE=71.4500 Epoch: 290/500MAE=71.3963 MAE=71.4452 MAE=71.4797 MAE=71.3411 MAE=71.5548 MAE=71.5137 MAE=71.5482 MAE=71.4659 MAE=71.6750 MAE=71.7179 Epoch: 300/500MAE=71.5804 MAE=71.4796 MAE=71.6467 MAE=71.3793 MAE=71.4823 MAE=71.5628 MAE=71.4759 MAE=71.4039 MAE=71.5696 MAE=71.5563 Epoch: 310/500MAE=71.8689 MAE=71.5331 MAE=71.5112 MAE=71.4311 MAE=71.4173 MAE=71.4535 MAE=71.4742 MAE=71.6358 MAE=71.6187 MAE=71.5496 Epoch: 320/500MAE=71.6131 MAE=71.5632 MAE=71.5844 MAE=71.5366 MAE=71.5276 MAE=71.5589 MAE=71.4154 MAE=71.4990 MAE=71.7181 MAE=71.5101 Epoch: 330/500MAE=71.3717 MAE=71.5435 MAE=71.4445 MAE=71.4788 MAE=71.5677 MAE=71.5250 MAE=71.5495 MAE=71.4881 MAE=71.5816 MAE=71.6300 Epoch: 340/500MAE=71.6716 MAE=71.6231 MAE=71.4491 MAE=71.6681 MAE=71.3874 MAE=71.6055 MAE=71.3590 MAE=71.5513 MAE=71.6414 MAE=71.4242 Epoch: 350/500MAE=71.7118 MAE=71.5414 MAE=71.4869 MAE=71.5645 MAE=71.6002 MAE=71.3100 MAE=71.5098 MAE=71.3946 MAE=71.5563 MAE=71.4446 Epoch: 360/500MAE=71.4111 MAE=71.4381 MAE=71.4442 MAE=71.4368 MAE=71.5975 MAE=71.4041 MAE=71.4678 MAE=71.4088 MAE=71.5311 MAE=71.3237 Epoch: 370/500MAE=71.4692 MAE=71.5239 MAE=71.3052 MAE=71.4900 MAE=71.5350 MAE=71.5355 MAE=71.5231 MAE=71.5206 MAE=71.3831 MAE=71.4626 Epoch: 380/500MAE=71.5107 MAE=71.3725 MAE=71.5981 MAE=71.3749 MAE=71.5488 MAE=71.4102 MAE=71.3693 MAE=71.4082 MAE=71.5493 MAE=71.4252 Epoch: 390/500MAE=71.3568 MAE=71.2489 MAE=71.4694 MAE=71.4346 MAE=71.4165 MAE=71.2645 MAE=71.3568 MAE=71.3382 MAE=71.3232 MAE=71.5642 Epoch: 400/500MAE=71.4515 MAE=71.4626 MAE=71.5408 MAE=71.3183 MAE=71.3822 MAE=71.4640 MAE=71.2905 MAE=71.2852 MAE=71.3866 MAE=71.3223 Epoch: 410/500MAE=71.3880 MAE=71.2941 MAE=71.3991 MAE=71.2839 MAE=71.4543 MAE=71.3447 MAE=71.2284 MAE=71.4848 MAE=71.3258 MAE=71.2716 Epoch: 420/500MAE=71.5442 MAE=71.3395 MAE=71.2982 MAE=71.4929 MAE=71.2450 MAE=71.2717 MAE=71.4122 MAE=71.3324 MAE=71.3447 MAE=71.2474 Epoch: 430/500MAE=71.2537 MAE=71.1188 MAE=71.2662 MAE=71.1939 MAE=71.2584 MAE=71.2398 MAE=71.3210 MAE=71.4095 MAE=71.3762 MAE=71.3176 Epoch: 440/500MAE=71.1861 MAE=71.1796 MAE=71.2368 MAE=71.3484 MAE=71.1939 MAE=71.2078 MAE=71.1521 MAE=71.3787 MAE=71.3583 MAE=71.1612 Epoch: 450/500MAE=71.3063 MAE=71.3785 MAE=71.1488 MAE=71.1939 MAE=71.1497 MAE=71.1553 MAE=71.2447 MAE=71.2653 MAE=71.2598 MAE=71.2134 Epoch: 460/500MAE=71.1211 MAE=71.2323 MAE=71.2626 MAE=71.2883 MAE=71.1365 MAE=71.2221 MAE=71.2962 MAE=71.2224 MAE=71.2676 MAE=71.0760 Epoch: 470/500MAE=71.0607 MAE=71.1076 MAE=71.0426 MAE=71.1299 MAE=71.3278 MAE=71.2895 MAE=71.2000 MAE=71.2359 MAE=71.1930 MAE=71.0215 Epoch: 480/500MAE=71.1454 MAE=71.1985 MAE=71.1918 MAE=71.0242 MAE=71.0455 MAE=71.0045 MAE=71.3022 MAE=71.1909 MAE=71.1019 MAE=71.1061 Epoch: 490/500MAE=71.0157 MAE=71.0666 MAE=71.2143 MAE=71.2293 MAE=71.1069 MAE=71.1745 MAE=71.1819 MAE=71.1614 MAE=71.1080 MAE=71.0784 Epoch: 500/500MAE=71.2065 MAE=76.8677 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 72.245 +/- 4.623\n",
      "\n",
      "Epoch: 1/500MAE=1549.8566 MAE=1547.9609 MAE=1545.4341 MAE=1542.8921 MAE=1540.7540 MAE=1537.7981 MAE=1533.0336 MAE=1530.5256 MAE=1527.2346 Epoch: 10/500MAE=1520.8638 MAE=1517.8738 MAE=1512.7485 MAE=1503.3733 MAE=1498.8765 MAE=1494.9075 MAE=1484.2365 MAE=1473.0499 MAE=1468.5043 MAE=1458.5891 Epoch: 20/500MAE=1446.8510 MAE=1440.7297 MAE=1428.1292 MAE=1415.8586 MAE=1416.0701 MAE=1401.7800 MAE=1388.1460 MAE=1373.8960 MAE=1361.5535 MAE=1342.6875 Epoch: 30/500MAE=1340.4622 MAE=1315.3782 MAE=1325.5559 MAE=1284.2559 MAE=1272.5531 MAE=1262.2595 MAE=1246.8317 MAE=1241.5488 MAE=1212.5247 MAE=1188.8322 Epoch: 40/500MAE=1176.6311 MAE=1158.4779 MAE=1137.2212 MAE=1135.7600 MAE=1092.6711 MAE=1085.4940 MAE=1034.1873 MAE=1039.4175 MAE=1020.9949 MAE=1007.6573 Epoch: 50/500MAE=978.0144 MAE=961.2268 MAE=963.4130 MAE=935.6584 MAE=883.9735 MAE=921.7428 MAE=870.2368 MAE=871.5955 MAE=799.8519 MAE=775.4835 Epoch: 60/500MAE=794.0454 MAE=775.9111 MAE=755.5977 MAE=682.0741 MAE=712.3785 MAE=707.0045 MAE=673.9009 MAE=601.6980 MAE=623.7609 MAE=573.7614 Epoch: 70/500MAE=594.2742 MAE=475.9153 MAE=497.2335 MAE=529.6499 MAE=480.9084 MAE=525.1251 MAE=416.7533 MAE=393.2900 MAE=410.6337 MAE=416.0817 Epoch: 80/500MAE=372.2417 MAE=380.5710 MAE=368.4273 MAE=366.9955 MAE=331.2749 MAE=360.4267 MAE=354.4125 MAE=307.6281 MAE=303.9469 MAE=245.1673 Epoch: 90/500MAE=346.1985 MAE=258.9470 MAE=235.3597 MAE=278.8120 MAE=226.8658 MAE=234.5421 MAE=180.7917 MAE=206.9862 MAE=222.6518 MAE=206.0098 Epoch: 100/500MAE=200.4730 MAE=196.4484 MAE=176.8821 MAE=208.3555 MAE=187.6998 MAE=169.9113 MAE=163.7457 MAE=171.1160 MAE=179.4294 MAE=182.2880 Epoch: 110/500MAE=173.9061 MAE=151.3935 MAE=153.1344 MAE=144.2131 MAE=161.3367 MAE=142.1199 MAE=140.6430 MAE=136.3976 MAE=140.0970 MAE=139.4510 Epoch: 120/500MAE=136.6019 MAE=140.9234 MAE=130.4971 MAE=133.2433 MAE=135.7514 MAE=126.2425 MAE=130.8919 MAE=132.0154 MAE=127.1442 MAE=123.2893 Epoch: 130/500MAE=128.7515 MAE=131.3279 MAE=123.8073 MAE=123.4211 MAE=121.5681 MAE=120.4607 MAE=125.1630 MAE=120.6796 MAE=122.0140 MAE=119.5974 Epoch: 140/500MAE=122.0361 MAE=122.1265 MAE=119.4270 MAE=118.9876 MAE=119.3768 MAE=121.7690 MAE=116.3348 MAE=119.5574 MAE=119.4312 MAE=117.3215 Epoch: 150/500MAE=117.8691 MAE=114.3638 MAE=115.2304 MAE=117.0698 MAE=116.1110 MAE=115.7872 MAE=114.9303 MAE=115.3632 MAE=115.4864 MAE=115.9397 Epoch: 160/500MAE=114.8745 MAE=114.7188 MAE=114.8883 MAE=114.9443 MAE=114.8318 MAE=114.6540 MAE=114.4372 MAE=114.8497 MAE=114.5926 MAE=114.8577 Epoch: 170/500MAE=114.4270 MAE=114.9546 MAE=114.5710 MAE=114.6783 MAE=114.8551 MAE=114.5980 MAE=114.5260 MAE=114.4535 MAE=114.6885 MAE=114.4595 Epoch: 180/500MAE=114.7674 MAE=114.4218 MAE=114.3933 MAE=114.6166 MAE=114.4339 MAE=114.5430 MAE=114.3471 MAE=114.3915 MAE=114.7662 MAE=114.3106 Epoch: 190/500MAE=114.3124 MAE=114.4018 MAE=114.6400 MAE=114.5007 MAE=114.4968 MAE=114.0970 MAE=114.3690 MAE=114.3400 MAE=114.1866 MAE=114.4659 Epoch: 200/500MAE=114.5025 MAE=114.2182 MAE=114.2382 MAE=114.0509 MAE=114.3474 MAE=114.4999 MAE=114.2678 MAE=114.5448 MAE=114.6152 MAE=114.2269 Epoch: 210/500MAE=114.1139 MAE=114.1445 MAE=113.9931 MAE=114.0880 MAE=114.4439 MAE=114.1924 MAE=113.8394 MAE=114.3047 MAE=114.1933 MAE=114.0326 Epoch: 220/500MAE=113.8813 MAE=113.9794 MAE=113.6169 MAE=113.9415 MAE=113.5777 MAE=114.1248 MAE=113.7018 MAE=113.6283 MAE=113.7447 MAE=113.6170 Epoch: 230/500MAE=113.8427 MAE=113.6613 MAE=113.8937 MAE=113.8786 MAE=113.7537 MAE=113.9852 MAE=113.6180 MAE=113.5868 MAE=113.7110 MAE=113.6179 Epoch: 240/500MAE=113.7063 MAE=113.9851 MAE=113.7368 MAE=113.5873 MAE=113.4929 MAE=113.9780 MAE=113.7910 MAE=113.8638 MAE=114.0146 MAE=113.7645 Epoch: 250/500MAE=113.3963 MAE=113.4035 MAE=113.5796 MAE=113.3646 MAE=113.6038 MAE=113.6047 MAE=113.5011 MAE=113.9915 MAE=113.4693 MAE=113.6002 Epoch: 260/500MAE=113.5708 MAE=113.5716 MAE=113.4261 MAE=113.2429 MAE=113.3546 MAE=113.1573 MAE=113.4867 MAE=113.4126 MAE=113.3225 MAE=113.4389 Epoch: 270/500MAE=113.4173 MAE=113.5555 MAE=113.1714 MAE=113.2129 MAE=113.4642 MAE=113.4203 MAE=112.7682 MAE=113.0257 MAE=113.1254 MAE=113.2017 Epoch: 280/500MAE=112.8715 MAE=113.1249 MAE=113.3367 MAE=112.9663 MAE=113.5119 MAE=113.1237 MAE=113.3895 MAE=112.9153 MAE=113.0985 MAE=113.0769 Epoch: 290/500MAE=112.8962 MAE=112.9338 MAE=113.0321 MAE=112.8194 MAE=112.9932 MAE=112.9108 MAE=113.0342 MAE=112.9508 MAE=112.8204 MAE=112.9982 Epoch: 300/500MAE=112.9530 MAE=112.7128 MAE=112.7193 MAE=112.6855 MAE=112.7795 MAE=112.8432 MAE=112.5979 MAE=112.8170 MAE=112.8281 MAE=112.7645 Epoch: 310/500MAE=112.6387 MAE=112.6492 MAE=112.9042 MAE=112.6913 MAE=112.5814 MAE=112.7088 MAE=112.5061 MAE=113.0582 MAE=112.9770 MAE=112.8331 Epoch: 320/500MAE=112.5696 MAE=112.9742 MAE=112.9179 MAE=112.7671 MAE=113.1691 MAE=112.4776 MAE=112.7498 MAE=113.1036 MAE=112.6447 MAE=112.8076 Epoch: 330/500MAE=112.9192 MAE=112.8753 MAE=112.8008 MAE=112.7991 MAE=112.5025 MAE=112.9076 MAE=112.8650 MAE=112.7323 MAE=112.8518 MAE=112.6282 Epoch: 340/500MAE=112.3765 MAE=112.5821 MAE=112.6038 MAE=112.5724 MAE=112.6743 MAE=113.1513 MAE=112.6711 MAE=112.9320 MAE=112.6308 MAE=112.5390 Epoch: 350/500MAE=112.8125 MAE=112.6033 MAE=112.4497 MAE=112.7257 MAE=112.4216 MAE=112.5837 MAE=112.0755 MAE=112.0965 MAE=112.7944 MAE=112.3824 Epoch: 360/500MAE=112.3701 MAE=112.1839 MAE=112.1593 MAE=112.3973 MAE=112.2019 MAE=112.0918 MAE=112.1334 MAE=112.3352 MAE=111.9833 MAE=112.4164 Epoch: 370/500MAE=112.4507 MAE=112.3093 MAE=112.6044 MAE=112.3188 MAE=112.2113 MAE=112.1573 MAE=112.5843 MAE=112.4073 MAE=112.2899 MAE=112.2845 Epoch: 380/500MAE=111.8468 MAE=112.0747 MAE=112.0945 MAE=112.0334 MAE=112.2822 MAE=112.2676 MAE=111.6958 MAE=111.8051 MAE=112.0245 MAE=111.7246 Epoch: 390/500MAE=111.6607 MAE=112.1058 MAE=112.1141 MAE=111.7780 MAE=112.1176 MAE=111.8340 MAE=111.9221 MAE=111.8425 MAE=112.0523 MAE=111.7158 Epoch: 400/500MAE=111.7262 MAE=111.5875 MAE=111.7826 MAE=111.9678 MAE=111.9379 MAE=111.8935 MAE=111.6071 MAE=111.6970 MAE=111.8910 MAE=111.6988 Epoch: 410/500MAE=111.9666 MAE=111.7928 MAE=111.5429 MAE=111.2502 MAE=111.5046 MAE=111.8878 MAE=111.4126 MAE=111.1701 MAE=111.8361 MAE=111.9237 Epoch: 420/500MAE=111.3895 MAE=111.5394 MAE=111.3021 MAE=111.7024 MAE=111.6206 MAE=111.5135 MAE=111.4637 MAE=111.5257 MAE=111.6029 MAE=111.5102 Epoch: 430/500MAE=111.2642 MAE=111.6349 MAE=111.7745 MAE=111.6127 MAE=111.6492 MAE=111.7776 MAE=111.4786 MAE=111.6715 MAE=111.2964 MAE=111.4512 Epoch: 440/500MAE=111.3882 MAE=111.6053 MAE=111.2810 MAE=110.8851 MAE=111.1286 MAE=111.0612 MAE=111.6044 MAE=111.1629 MAE=111.4621 MAE=111.1083 Epoch: 450/500MAE=111.0952 MAE=111.4873 MAE=111.2616 MAE=111.2587 MAE=111.3666 MAE=111.0735 MAE=111.5156 MAE=110.8792 MAE=110.9026 MAE=110.7567 Epoch: 460/500MAE=111.2391 MAE=110.7404 MAE=111.2327 MAE=111.1753 MAE=110.9213 MAE=111.1842 MAE=110.9282 MAE=110.6863 MAE=111.0901 MAE=110.7317 Epoch: 470/500MAE=110.5057 MAE=110.8375 MAE=110.8789 MAE=110.9216 MAE=110.7256 MAE=110.4866 MAE=111.2097 MAE=110.6472 MAE=110.7145 MAE=110.5978 Epoch: 480/500MAE=110.6027 MAE=110.5356 MAE=110.9610 MAE=110.1139 MAE=110.6637 MAE=110.4785 MAE=110.4030 MAE=110.5465 MAE=110.2648 MAE=110.1706 Epoch: 490/500MAE=110.1297 MAE=110.3003 MAE=110.7478 MAE=110.1479 MAE=109.8959 MAE=110.4796 MAE=110.4271 MAE=110.1293 MAE=110.1720 MAE=110.3753 Epoch: 500/500MAE=110.1750 MAE=122.4016 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 88.964 +/- 23.943\n",
      "\n",
      "Epoch: 1/500MAE=1549.6604 MAE=1547.4661 MAE=1545.1667 MAE=1542.8940 MAE=1540.0675 MAE=1536.1305 MAE=1534.2523 MAE=1530.5103 MAE=1525.1641 Epoch: 10/500MAE=1524.2861 MAE=1518.6345 MAE=1511.3561 MAE=1503.3231 MAE=1498.9888 MAE=1493.0460 MAE=1486.4285 MAE=1480.4971 MAE=1475.3823 MAE=1461.1854 Epoch: 20/500MAE=1442.2457 MAE=1436.2540 MAE=1433.2830 MAE=1413.7913 MAE=1415.2473 MAE=1406.4166 MAE=1394.0515 MAE=1372.3550 MAE=1361.8572 MAE=1347.5815 Epoch: 30/500MAE=1328.7266 MAE=1344.6736 MAE=1314.8751 MAE=1290.2673 MAE=1290.2872 MAE=1286.1064 MAE=1267.4562 MAE=1231.4445 MAE=1232.7814 MAE=1185.8695 Epoch: 40/500MAE=1178.1189 MAE=1173.4158 MAE=1172.1304 MAE=1136.8004 MAE=1113.4802 MAE=1117.8342 MAE=1088.1362 MAE=1091.5540 MAE=1058.9512 MAE=1026.1737 Epoch: 50/500MAE=985.1423 MAE=948.4128 MAE=961.4991 MAE=946.4543 MAE=933.1688 MAE=862.8865 MAE=913.9087 MAE=900.9907 MAE=817.9626 MAE=835.4619 Epoch: 60/500MAE=824.5972 MAE=863.8401 MAE=786.8569 MAE=735.8062 MAE=694.5348 MAE=711.7555 MAE=688.1296 MAE=611.5098 MAE=734.1160 MAE=597.9626 Epoch: 70/500MAE=517.6583 MAE=559.4200 MAE=480.0605 MAE=464.0401 MAE=550.5513 MAE=474.3009 MAE=390.4416 MAE=385.8082 MAE=406.0158 MAE=372.6153 Epoch: 80/500MAE=374.7371 MAE=339.7941 MAE=292.7545 MAE=236.5113 MAE=234.0212 MAE=263.2762 MAE=248.5005 MAE=186.8064 MAE=164.4614 MAE=145.0344 Epoch: 90/500MAE=161.1960 MAE=189.0098 MAE=140.7674 MAE=122.3446 MAE=167.2322 MAE=137.5629 MAE=123.2768 MAE=113.2176 MAE=105.2145 MAE=88.7466 Epoch: 100/500MAE=89.5861 MAE=165.6688 MAE=101.4074 MAE=93.9860 MAE=98.6268 MAE=91.5924 MAE=74.6939 MAE=89.1170 MAE=86.4861 MAE=90.9735 Epoch: 110/500MAE=82.2974 MAE=69.7966 MAE=69.5458 MAE=69.9127 MAE=68.8251 MAE=74.5792 MAE=71.1291 MAE=67.3576 MAE=67.5572 MAE=68.5411 Epoch: 120/500MAE=68.9382 MAE=69.1038 MAE=74.1682 MAE=69.4114 MAE=67.9821 MAE=68.0630 MAE=67.2021 MAE=66.8713 MAE=68.0479 MAE=67.3653 Epoch: 130/500MAE=66.8311 MAE=67.7742 MAE=68.4847 MAE=67.2414 MAE=68.7160 MAE=68.4408 MAE=66.7821 MAE=66.9642 MAE=68.2948 MAE=68.5888 Epoch: 140/500MAE=68.4529 MAE=68.4967 MAE=68.0229 MAE=67.6114 MAE=67.5434 MAE=67.1822 MAE=67.4228 MAE=67.2734 MAE=67.1543 MAE=67.6305 Epoch: 150/500MAE=67.4523 MAE=67.7796 MAE=67.8235 MAE=67.8375 MAE=67.7669 MAE=67.7803 MAE=67.8127 MAE=67.8517 MAE=67.7253 MAE=67.8893 Epoch: 160/500MAE=67.6916 MAE=67.8008 MAE=67.7814 MAE=67.7654 MAE=67.7092 MAE=67.7788 MAE=67.7533 MAE=67.7645 MAE=67.8799 MAE=67.8889 Epoch: 170/500MAE=67.7945 MAE=67.8284 MAE=67.7259 MAE=67.8137 MAE=67.8311 MAE=68.0207 MAE=67.7195 MAE=67.7922 MAE=67.9088 MAE=67.8353 Epoch: 180/500MAE=67.7787 MAE=67.8411 MAE=67.6190 MAE=67.5380 MAE=67.6396 MAE=67.7714 MAE=67.8082 MAE=67.7299 MAE=67.7888 MAE=67.7667 Epoch: 190/500MAE=67.7504 MAE=67.6996 MAE=67.8496 MAE=67.5910 MAE=67.6406 MAE=67.6903 MAE=67.6342 MAE=67.6448 MAE=67.6513 MAE=67.7235 Epoch: 200/500MAE=67.7059 MAE=67.7713 MAE=67.6911 MAE=67.5537 MAE=67.6261 MAE=67.6021 MAE=67.6877 MAE=67.5621 MAE=67.7034 MAE=67.6806 Epoch: 210/500MAE=67.5979 MAE=67.5091 MAE=67.5591 MAE=67.5736 MAE=67.6807 MAE=67.8298 MAE=67.8040 MAE=67.8293 MAE=67.8321 MAE=67.9218 Epoch: 220/500MAE=67.9521 MAE=67.7796 MAE=67.8786 MAE=67.7671 MAE=67.7332 MAE=67.7531 MAE=67.7437 MAE=67.6990 MAE=67.7814 MAE=67.8560 Epoch: 230/500MAE=67.7728 MAE=67.6705 MAE=67.7094 MAE=67.6888 MAE=67.7609 MAE=67.8378 MAE=67.7427 MAE=67.6903 MAE=67.6947 MAE=67.7126 Epoch: 240/500MAE=67.8324 MAE=67.6158 MAE=67.6931 MAE=67.6988 MAE=67.7016 MAE=67.6691 MAE=67.3872 MAE=67.4871 MAE=67.6432 MAE=67.7159 Epoch: 250/500MAE=67.6563 MAE=67.6718 MAE=67.5953 MAE=67.4969 MAE=67.4776 MAE=67.7666 MAE=67.5048 MAE=67.3460 MAE=67.3487 MAE=67.3837 Epoch: 260/500MAE=67.4398 MAE=67.5994 MAE=67.4239 MAE=67.3324 MAE=67.4586 MAE=67.4030 MAE=67.4905 MAE=67.6704 MAE=67.7274 MAE=67.6442 Epoch: 270/500MAE=67.4012 MAE=67.4419 MAE=67.4200 MAE=67.4547 MAE=67.5214 MAE=67.4415 MAE=67.6259 MAE=67.4376 MAE=67.4012 MAE=67.4905 Epoch: 280/500MAE=67.4585 MAE=67.4768 MAE=67.4110 MAE=67.3921 MAE=67.5269 MAE=67.4519 MAE=67.2846 MAE=64.5356 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 82.857 +/- 23.278\n",
      "\n",
      "Epoch: 1/500MAE=1549.6172 MAE=1547.1279 MAE=1544.9840 MAE=1542.5504 MAE=1539.8469 MAE=1536.6573 MAE=1534.0704 MAE=1530.3535 MAE=1525.6990 Epoch: 10/500MAE=1522.3896 MAE=1516.6292 MAE=1509.2528 MAE=1505.3309 MAE=1499.2202 MAE=1492.6748 MAE=1485.8967 MAE=1479.1489 MAE=1468.4189 MAE=1465.7874 Epoch: 20/500MAE=1453.9912 MAE=1456.0859 MAE=1433.3624 MAE=1432.3210 MAE=1424.4045 MAE=1407.4351 MAE=1398.6510 MAE=1385.6274 MAE=1377.4270 MAE=1363.2664 Epoch: 30/500MAE=1358.7385 MAE=1322.1758 MAE=1329.4031 MAE=1317.7463 MAE=1287.5599 MAE=1285.5878 MAE=1276.4319 MAE=1248.9595 MAE=1245.3647 MAE=1221.0140 Epoch: 40/500MAE=1210.7053 MAE=1187.6515 MAE=1170.1357 MAE=1144.6919 MAE=1126.7104 MAE=1143.8940 MAE=1111.2920 MAE=1112.4646 MAE=1087.3665 MAE=1039.1387 Epoch: 50/500MAE=1052.6626 MAE=1001.7648 MAE=977.2982 MAE=947.8511 MAE=952.6117 MAE=931.5494 MAE=876.2835 MAE=877.7913 MAE=861.4938 MAE=863.8805 Epoch: 60/500MAE=791.9087 MAE=804.4017 MAE=752.5362 MAE=813.7014 MAE=761.0470 MAE=718.2896 MAE=726.5055 MAE=665.8915 MAE=620.3512 MAE=611.9437 Epoch: 70/500MAE=560.9775 MAE=545.9699 MAE=496.7500 MAE=509.2119 MAE=522.7806 MAE=524.7102 MAE=480.8470 MAE=451.8224 MAE=381.7928 MAE=342.5143 Epoch: 80/500MAE=394.7757 MAE=369.8569 MAE=261.3477 MAE=324.7538 MAE=277.3132 MAE=244.3528 MAE=199.6217 MAE=231.0205 MAE=264.1576 MAE=170.7259 Epoch: 90/500MAE=227.7262 MAE=165.5000 MAE=207.0196 MAE=147.3909 MAE=128.5622 MAE=145.3289 MAE=96.7288 MAE=108.8879 MAE=93.2077 MAE=116.1584 Epoch: 100/500MAE=95.2428 MAE=86.9197 MAE=113.7610 MAE=87.5787 MAE=101.3352 MAE=129.1206 MAE=100.0309 MAE=94.9953 MAE=81.5348 MAE=87.0471 Epoch: 110/500MAE=98.0916 MAE=90.7479 MAE=92.8360 MAE=85.7094 MAE=88.6290 MAE=76.1730 MAE=79.2938 MAE=79.1990 MAE=82.1617 MAE=75.0882 Epoch: 120/500MAE=79.4770 MAE=79.5609 MAE=81.7899 MAE=77.2220 MAE=77.9903 MAE=75.3194 MAE=74.2898 MAE=74.5035 MAE=74.0358 MAE=77.9201 Epoch: 130/500MAE=76.6690 MAE=75.4659 MAE=74.5140 MAE=74.5966 MAE=74.5680 MAE=76.5055 MAE=73.6485 MAE=74.0364 MAE=74.3425 MAE=74.1495 Epoch: 140/500MAE=73.1483 MAE=74.0767 MAE=74.3423 MAE=72.6060 MAE=73.3367 MAE=73.5492 MAE=72.6886 MAE=73.8548 MAE=72.9190 MAE=73.1527 Epoch: 150/500MAE=73.1266 MAE=73.5078 MAE=73.3473 MAE=72.6804 MAE=72.5643 MAE=72.7633 MAE=72.2514 MAE=72.1638 MAE=72.2823 MAE=72.9409 Epoch: 160/500MAE=73.0203 MAE=72.9834 MAE=72.2724 MAE=72.3669 MAE=72.3239 MAE=72.8907 MAE=72.7391 MAE=72.6775 MAE=72.6841 MAE=72.6521 Epoch: 170/500MAE=72.6363 MAE=72.6574 MAE=72.6205 MAE=72.5191 MAE=72.5118 MAE=72.5816 MAE=72.4985 MAE=72.5014 MAE=72.6376 MAE=72.6349 Epoch: 180/500MAE=72.7339 MAE=72.5361 MAE=72.5751 MAE=72.6464 MAE=72.8760 MAE=72.6693 MAE=72.6618 MAE=72.5883 MAE=72.4770 MAE=72.6992 Epoch: 190/500MAE=72.7579 MAE=72.6124 MAE=72.8595 MAE=72.6292 MAE=72.7947 MAE=73.0794 MAE=73.0717 MAE=72.9190 MAE=72.7468 MAE=72.8666 Epoch: 200/500MAE=72.9273 MAE=73.0289 MAE=73.0913 MAE=72.8007 MAE=72.7051 MAE=72.5351 MAE=72.7417 MAE=72.5951 MAE=72.6357 MAE=72.5090 Epoch: 210/500MAE=72.5264 MAE=72.5967 MAE=72.6610 MAE=72.5650 MAE=72.7151 MAE=72.7796 MAE=72.6303 MAE=72.5098 MAE=72.6955 MAE=72.6043 Epoch: 220/500MAE=72.5542 MAE=72.6467 MAE=72.6957 MAE=72.7627 MAE=72.8398 MAE=72.6343 MAE=72.8044 MAE=72.7075 MAE=72.8008 MAE=72.9617 Epoch: 230/500MAE=72.5693 MAE=72.6311 MAE=72.3476 MAE=72.7399 MAE=72.6931 MAE=72.7555 MAE=72.4505 MAE=72.5542 MAE=72.6814 MAE=72.6439 Epoch: 240/500MAE=72.4563 MAE=72.5455 MAE=72.4856 MAE=72.4069 MAE=72.3040 MAE=72.4472 MAE=72.6664 MAE=72.5602 MAE=72.5990 MAE=72.4449 Epoch: 250/500MAE=72.4083 MAE=72.5642 MAE=72.5668 MAE=72.4753 MAE=72.3461 MAE=72.6905 MAE=72.5841 MAE=72.5709 MAE=72.5989 MAE=72.7041 Epoch: 260/500MAE=72.8924 MAE=72.5729 MAE=72.6731 MAE=72.5254 MAE=72.5223 MAE=72.5533 MAE=72.6317 MAE=72.4310 MAE=72.5262 MAE=72.5856 Epoch: 270/500MAE=72.5278 MAE=72.6099 MAE=72.6235 MAE=72.5625 MAE=72.4606 MAE=72.4762 MAE=72.6259 MAE=72.7625 MAE=72.6086 MAE=72.5348 Epoch: 280/500MAE=72.6097 MAE=72.6499 MAE=72.5779 MAE=72.4797 MAE=72.2318 MAE=72.2208 MAE=72.2711 MAE=72.3411 MAE=72.4576 MAE=72.5801 Epoch: 290/500MAE=72.6600 MAE=72.5625 MAE=72.6981 MAE=72.4436 MAE=72.6233 MAE=72.3243 MAE=72.5200 MAE=72.5953 MAE=72.5572 MAE=72.6598 Epoch: 300/500MAE=72.6719 MAE=72.6839 MAE=72.6144 MAE=72.5930 MAE=72.4945 MAE=72.4669 MAE=72.5907 MAE=72.5331 MAE=71.1336 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 80.512 +/- 21.342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num -1 --run_times=5 --patience=150 --epochs=500 --pooling='KMIS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/qm8.pt\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.1314 MAE=0.1341 MAE=0.1101 MAE=0.0809 MAE=0.0622 MAE=0.0515 MAE=0.0466 MAE=0.0420 MAE=0.0390 Epoch: 10/150MAE=0.0373 MAE=0.0357 MAE=0.0359 MAE=0.0323 MAE=0.0307 MAE=0.0300 MAE=0.0289 MAE=0.0281 MAE=0.0282 MAE=0.0269 Epoch: 20/150MAE=0.0255 MAE=0.0253 MAE=0.0248 MAE=0.0258 MAE=0.0238 MAE=0.0267 MAE=0.0277 MAE=0.0237 MAE=0.0231 MAE=0.0225 Epoch: 30/150MAE=0.0218 MAE=0.0220 MAE=0.0216 MAE=0.0212 MAE=0.0208 MAE=0.0210 MAE=0.0206 MAE=0.0203 MAE=0.0233 MAE=0.0207 Epoch: 40/150MAE=0.0201 MAE=0.0224 MAE=0.0210 MAE=0.0199 MAE=0.0198 MAE=0.0228 MAE=0.0218 MAE=0.0274 MAE=0.0213 MAE=0.0191 Epoch: 50/150MAE=0.0188 MAE=0.0206 MAE=0.0193 MAE=0.0199 MAE=0.0197 MAE=0.0186 MAE=0.0181 MAE=0.0183 MAE=0.0182 MAE=0.0184 Epoch: 60/150MAE=0.0181 MAE=0.0184 MAE=0.0181 MAE=0.0181 MAE=0.0179 MAE=0.0183 MAE=0.0179 MAE=0.0184 MAE=0.0182 MAE=0.0178 Epoch: 70/150MAE=0.0178 MAE=0.0177 MAE=0.0178 MAE=0.0180 MAE=0.0180 MAE=0.0177 MAE=0.0179 MAE=0.0178 MAE=0.0181 MAE=0.0176 Epoch: 80/150MAE=0.0179 MAE=0.0178 MAE=0.0177 MAE=0.0178 MAE=0.0177 MAE=0.0177 MAE=0.0177 MAE=0.0176 MAE=0.0178 MAE=0.0176 Epoch: 90/150MAE=0.0176 MAE=0.0176 MAE=0.0176 MAE=0.0177 MAE=0.0176 MAE=0.0175 MAE=0.0176 MAE=0.0176 MAE=0.0175 MAE=0.0175 Epoch: 100/150MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 MAE=0.0175 Epoch: 110/150MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0175 MAE=0.0175 MAE=0.0174 MAE=0.0174 MAE=0.0174 Epoch: 120/150MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 Epoch: 130/150MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0179 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.018 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.1263 MAE=0.1180 MAE=0.1029 MAE=0.0791 MAE=0.0664 MAE=0.0587 MAE=0.0517 MAE=0.0474 MAE=0.0473 Epoch: 10/150MAE=0.0449 MAE=0.0421 MAE=0.0387 MAE=0.0351 MAE=0.0333 MAE=0.0321 MAE=0.0300 MAE=0.0360 MAE=0.0432 MAE=0.0301 Epoch: 20/150MAE=0.0270 MAE=0.0277 MAE=0.0258 MAE=0.0255 MAE=0.0245 MAE=0.0246 MAE=0.0236 MAE=0.0237 MAE=0.0231 MAE=0.0234 Epoch: 30/150MAE=0.0225 MAE=0.0232 MAE=0.0222 MAE=0.0224 MAE=0.0221 MAE=0.0223 MAE=0.0224 MAE=0.0219 MAE=0.0224 MAE=0.0220 Epoch: 40/150MAE=0.0222 MAE=0.0213 MAE=0.0214 MAE=0.0209 MAE=0.0205 MAE=0.0207 MAE=0.0208 MAE=0.0203 MAE=0.0201 MAE=0.0200 Epoch: 50/150MAE=0.0199 MAE=0.0201 MAE=0.0198 MAE=0.0200 MAE=0.0194 MAE=0.0194 MAE=0.0195 MAE=0.0196 MAE=0.0194 MAE=0.0192 Epoch: 60/150MAE=0.0187 MAE=0.0186 MAE=0.0186 MAE=0.0186 MAE=0.0186 MAE=0.0185 MAE=0.0184 MAE=0.0184 MAE=0.0183 MAE=0.0182 Epoch: 70/150MAE=0.0183 MAE=0.0182 MAE=0.0183 MAE=0.0181 MAE=0.0181 MAE=0.0181 MAE=0.0180 MAE=0.0181 MAE=0.0180 MAE=0.0180 Epoch: 80/150MAE=0.0179 MAE=0.0180 MAE=0.0179 MAE=0.0178 MAE=0.0178 MAE=0.0178 MAE=0.0178 MAE=0.0176 MAE=0.0176 MAE=0.0174 Epoch: 90/150MAE=0.0174 MAE=0.0175 MAE=0.0173 MAE=0.0172 MAE=0.0173 MAE=0.0172 MAE=0.0173 MAE=0.0172 MAE=0.0172 MAE=0.0171 Epoch: 100/150MAE=0.0171 MAE=0.0170 MAE=0.0172 MAE=0.0171 MAE=0.0170 MAE=0.0170 MAE=0.0170 MAE=0.0169 MAE=0.0168 MAE=0.0170 Epoch: 110/150MAE=0.0169 MAE=0.0168 MAE=0.0168 MAE=0.0169 MAE=0.0168 MAE=0.0168 MAE=0.0166 MAE=0.0165 MAE=0.0165 MAE=0.0165 Epoch: 120/150MAE=0.0164 MAE=0.0164 MAE=0.0164 MAE=0.0165 MAE=0.0164 MAE=0.0164 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 Epoch: 130/150MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0162 MAE=0.0162 MAE=0.0163 MAE=0.0163 MAE=0.0163 MAE=0.0162 Epoch: 140/150MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 Epoch: 150/150MAE=0.0162 MAE=0.0169 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.017 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.1631 MAE=0.1518 MAE=0.1280 MAE=0.0995 MAE=0.0743 MAE=0.0593 MAE=0.0519 MAE=0.0467 MAE=0.0414 Epoch: 10/150MAE=0.0399 MAE=0.0388 MAE=0.0372 MAE=0.0346 MAE=0.0320 MAE=0.0318 MAE=0.0291 MAE=0.0282 MAE=0.0276 MAE=0.0270 Epoch: 20/150MAE=0.0278 MAE=0.0261 MAE=0.0250 MAE=0.0270 MAE=0.0295 MAE=0.0275 MAE=0.0273 MAE=0.0245 MAE=0.0235 MAE=0.0226 Epoch: 30/150MAE=0.0226 MAE=0.0229 MAE=0.0224 MAE=0.0219 MAE=0.0224 MAE=0.0215 MAE=0.0214 MAE=0.0215 MAE=0.0215 MAE=0.0207 Epoch: 40/150MAE=0.0208 MAE=0.0209 MAE=0.0210 MAE=0.0205 MAE=0.0203 MAE=0.0207 MAE=0.0205 MAE=0.0204 MAE=0.0202 MAE=0.0204 Epoch: 50/150MAE=0.0202 MAE=0.0198 MAE=0.0198 MAE=0.0200 MAE=0.0194 MAE=0.0194 MAE=0.0196 MAE=0.0191 MAE=0.0193 MAE=0.0191 Epoch: 60/150MAE=0.0192 MAE=0.0189 MAE=0.0191 MAE=0.0187 MAE=0.0190 MAE=0.0190 MAE=0.0187 MAE=0.0190 MAE=0.0183 MAE=0.0183 Epoch: 70/150MAE=0.0182 MAE=0.0185 MAE=0.0179 MAE=0.0180 MAE=0.0181 MAE=0.0178 MAE=0.0180 MAE=0.0179 MAE=0.0178 MAE=0.0178 Epoch: 80/150MAE=0.0175 MAE=0.0175 MAE=0.0176 MAE=0.0176 MAE=0.0173 MAE=0.0175 MAE=0.0175 MAE=0.0174 MAE=0.0174 MAE=0.0175 Epoch: 90/150MAE=0.0174 MAE=0.0174 MAE=0.0174 MAE=0.0173 MAE=0.0172 MAE=0.0172 MAE=0.0173 MAE=0.0172 MAE=0.0172 MAE=0.0172 Epoch: 100/150MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0172 MAE=0.0171 Epoch: 110/150MAE=0.0171 MAE=0.0171 MAE=0.0172 MAE=0.0171 MAE=0.0171 MAE=0.0172 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 Epoch: 120/150MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 Epoch: 130/150MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 Epoch: 140/150MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0171 MAE=0.0176 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.017 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.1435 MAE=0.1238 MAE=0.0921 MAE=0.0743 MAE=0.0601 MAE=0.0540 MAE=0.0497 MAE=0.0483 MAE=0.0458 Epoch: 10/150MAE=0.0436 MAE=0.0406 MAE=0.0374 MAE=0.0351 MAE=0.0331 MAE=0.0311 MAE=0.0309 MAE=0.0284 MAE=0.0281 MAE=0.0268 Epoch: 20/150MAE=0.0266 MAE=0.0259 MAE=0.0259 MAE=0.0251 MAE=0.0253 MAE=0.0241 MAE=0.0239 MAE=0.0241 MAE=0.0238 MAE=0.0240 Epoch: 30/150MAE=0.0230 MAE=0.0261 MAE=0.0227 MAE=0.0227 MAE=0.0277 MAE=0.0230 MAE=0.0215 MAE=0.0214 MAE=0.0212 MAE=0.0213 Epoch: 40/150MAE=0.0206 MAE=0.0213 MAE=0.0205 MAE=0.0207 MAE=0.0206 MAE=0.0203 MAE=0.0200 MAE=0.0197 MAE=0.0201 MAE=0.0199 Epoch: 50/150MAE=0.0194 MAE=0.0191 MAE=0.0201 MAE=0.0192 MAE=0.0193 MAE=0.0189 MAE=0.0185 MAE=0.0190 MAE=0.0185 MAE=0.0185 Epoch: 60/150MAE=0.0183 MAE=0.0181 MAE=0.0179 MAE=0.0177 MAE=0.0196 MAE=0.0178 MAE=0.0181 MAE=0.0176 MAE=0.0174 MAE=0.0178 Epoch: 70/150MAE=0.0193 MAE=0.0178 MAE=0.0175 MAE=0.0172 MAE=0.0172 MAE=0.0170 MAE=0.0174 MAE=0.0169 MAE=0.0170 MAE=0.0170 Epoch: 80/150MAE=0.0168 MAE=0.0169 MAE=0.0168 MAE=0.0168 MAE=0.0169 MAE=0.0165 MAE=0.0164 MAE=0.0166 MAE=0.0165 MAE=0.0166 Epoch: 90/150MAE=0.0164 MAE=0.0163 MAE=0.0163 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0162 MAE=0.0163 Epoch: 100/150MAE=0.0162 MAE=0.0161 MAE=0.0162 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0162 MAE=0.0161 MAE=0.0161 MAE=0.0161 Epoch: 110/150MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0160 MAE=0.0161 Epoch: 120/150MAE=0.0161 MAE=0.0160 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0161 MAE=0.0164 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.017 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1374 MAE=0.1108 MAE=0.0939 MAE=0.0745 MAE=0.0648 MAE=0.0570 MAE=0.0505 MAE=0.0481 MAE=0.0457 Epoch: 10/150MAE=0.0421 MAE=0.0395 MAE=0.0377 MAE=0.0389 MAE=0.0353 MAE=0.0364 MAE=0.0326 MAE=0.0312 MAE=0.0308 MAE=0.0289 Epoch: 20/150MAE=0.0271 MAE=0.0267 MAE=0.0280 MAE=0.0256 MAE=0.0263 MAE=0.0270 MAE=0.0253 MAE=0.0245 MAE=0.0251 MAE=0.0246 Epoch: 30/150MAE=0.0238 MAE=0.0237 MAE=0.0231 MAE=0.0238 MAE=0.0232 MAE=0.0226 MAE=0.0216 MAE=0.0217 MAE=0.0209 MAE=0.0217 Epoch: 40/150MAE=0.0231 MAE=0.0212 MAE=0.0213 MAE=0.0199 MAE=0.0195 MAE=0.0196 MAE=0.0200 MAE=0.0201 MAE=0.0199 MAE=0.0193 Epoch: 50/150MAE=0.0194 MAE=0.0192 MAE=0.0190 MAE=0.0188 MAE=0.0191 MAE=0.0186 MAE=0.0187 MAE=0.0192 MAE=0.0188 MAE=0.0189 Epoch: 60/150MAE=0.0186 MAE=0.0189 MAE=0.0191 MAE=0.0182 MAE=0.0182 MAE=0.0183 MAE=0.0183 MAE=0.0184 MAE=0.0182 MAE=0.0181 Epoch: 70/150MAE=0.0181 MAE=0.0185 MAE=0.0181 MAE=0.0180 MAE=0.0180 MAE=0.0180 MAE=0.0182 MAE=0.0184 MAE=0.0184 MAE=0.0182 Epoch: 80/150MAE=0.0181 MAE=0.0180 MAE=0.0180 MAE=0.0180 MAE=0.0180 MAE=0.0185 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.017 +/- 0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --cuda_num -1 --run_times=5 --patience=10 --epochs=150 --pooling='KMIS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/bace.pt\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.829 +/- 0.000\n",
      "AUPRC: 0.786 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.837 +/- 0.008\n",
      "AUPRC: 0.791 +/- 0.005\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.852 +/- 0.022\n",
      "AUPRC: 0.804 +/- 0.019\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.855 +/- 0.020\n",
      "AUPRC: 0.811 +/- 0.020\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.851 +/- 0.020\n",
      "AUPRC: 0.810 +/- 0.018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --pooling='KMIS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/esol.pt\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.5328 RMSE=1.3712 RMSE=1.1044 RMSE=1.1627 RMSE=1.1003 RMSE=1.0397 RMSE=1.0112 RMSE=0.9336 RMSE=0.9707 Epoch: 10/150RMSE=1.0671 RMSE=0.9404 RMSE=1.0075 RMSE=0.9318 RMSE=0.9162 RMSE=0.9720 RMSE=0.9732 RMSE=0.9573 RMSE=0.9403 RMSE=0.9709 Epoch: 20/150RMSE=0.9256 RMSE=0.9589 RMSE=0.9281 RMSE=0.9137 RMSE=0.9101 RMSE=0.8870 RMSE=0.9015 RMSE=0.8946 RMSE=0.9023 RMSE=0.8860 Epoch: 30/150RMSE=0.8911 RMSE=0.8926 RMSE=0.8933 RMSE=0.8995 RMSE=0.8965 RMSE=0.8918 RMSE=0.8967 RMSE=0.8899 RMSE=0.8960 RMSE=0.8949 Epoch: 40/150RMSE=0.8958 RMSE=0.8972 RMSE=0.8922 RMSE=0.8936 RMSE=0.8936 RMSE=0.8944 RMSE=0.9242 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.924 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=2.0167 RMSE=1.5934 RMSE=1.4871 RMSE=1.4690 RMSE=1.3988 RMSE=1.6020 RMSE=1.1155 RMSE=1.0913 RMSE=1.0293 Epoch: 10/150RMSE=1.1555 RMSE=1.4259 RMSE=1.3403 RMSE=1.4242 RMSE=1.2600 RMSE=1.3162 RMSE=1.2726 RMSE=1.2989 RMSE=1.2928 RMSE=1.3045 Epoch: 20/150RMSE=1.3328 RMSE=1.2858 RMSE=1.3114 RMSE=1.3063 RMSE=1.3347 RMSE=1.2963 RMSE=1.2925 RMSE=1.2959 RMSE=1.3256 RMSE=1.2955 RMSE=0.9451 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 0.935 +/- 0.010\n",
      "\n",
      "Epoch: 1/150RMSE=1.6924 RMSE=1.2660 RMSE=1.2061 RMSE=1.1110 RMSE=1.1554 RMSE=1.0843 RMSE=1.1913 RMSE=0.9648 RMSE=1.0419 Epoch: 10/150RMSE=1.0922 RMSE=1.0096 RMSE=0.9439 RMSE=0.9071 RMSE=0.9813 RMSE=0.8396 RMSE=0.9279 RMSE=1.0141 RMSE=0.9885 RMSE=1.1014 Epoch: 20/150RMSE=0.9047 RMSE=0.8518 RMSE=0.8464 RMSE=0.8685 RMSE=0.8398 RMSE=0.8761 RMSE=0.8200 RMSE=0.8045 RMSE=0.8224 RMSE=0.8506 Epoch: 30/150RMSE=0.8000 RMSE=0.8393 RMSE=0.8807 RMSE=0.8308 RMSE=0.8866 RMSE=0.8555 RMSE=0.8774 RMSE=0.8760 RMSE=0.8776 RMSE=0.8516 Epoch: 40/150RMSE=0.8582 RMSE=0.8508 RMSE=0.8578 RMSE=0.8601 RMSE=0.8506 RMSE=0.8595 RMSE=0.8573 RMSE=0.8535 RMSE=0.8550 RMSE=0.8572 Epoch: 50/150RMSE=0.8614 RMSE=0.9839 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 0.951 +/- 0.025\n",
      "\n",
      "Epoch: 1/150RMSE=1.6701 RMSE=1.1284 RMSE=1.1814 RMSE=1.2982 RMSE=0.8990 RMSE=0.9404 RMSE=0.9327 RMSE=1.1697 RMSE=0.9927 Epoch: 10/150RMSE=0.8425 RMSE=0.8648 RMSE=0.8729 RMSE=0.8591 RMSE=0.8724 RMSE=0.8702 RMSE=0.8673 RMSE=0.8806 RMSE=0.9152 RMSE=0.8709 Epoch: 20/150RMSE=0.8973 RMSE=0.8768 RMSE=0.9017 RMSE=0.8780 RMSE=0.8810 RMSE=0.8830 RMSE=0.8859 RMSE=0.8724 RMSE=0.8708 RMSE=0.8718 Epoch: 30/150RMSE=0.8702 RMSE=1.0071 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 0.965 +/- 0.032\n",
      "\n",
      "Epoch: 1/150RMSE=1.9406 RMSE=1.5064 RMSE=1.2170 RMSE=1.2738 RMSE=1.5385 RMSE=1.2267 RMSE=1.7410 RMSE=1.2022 RMSE=1.3433 Epoch: 10/150RMSE=0.9696 RMSE=1.0506 RMSE=0.9597 RMSE=1.2257 RMSE=1.0167 RMSE=0.9431 RMSE=0.9239 RMSE=0.9684 RMSE=1.0275 RMSE=0.9861 Epoch: 20/150RMSE=1.0963 RMSE=1.0560 RMSE=1.1924 RMSE=1.1589 RMSE=1.0800 RMSE=0.9662 RMSE=0.9327 RMSE=0.9760 RMSE=0.9251 RMSE=0.9744 Epoch: 30/150RMSE=1.0165 RMSE=0.9943 RMSE=0.9890 RMSE=0.9629 RMSE=0.9798 RMSE=0.9543 RMSE=0.9625 RMSE=0.9539 RMSE=0.9558 RMSE=0.9683 Epoch: 40/150RMSE=0.9484 RMSE=0.9590 RMSE=0.9766 RMSE=0.9671 RMSE=0.9652 RMSE=0.9613 RMSE=0.9532 RMSE=0.9563 RMSE=0.9536 RMSE=0.8966 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 0.951 +/- 0.040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --pooling='KMIS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freesolv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.0398 RMSE=4.2502 RMSE=3.4489 RMSE=2.2976 RMSE=1.8993 RMSE=2.1993 RMSE=1.8940 RMSE=1.6753 RMSE=1.9713 Epoch: 10/150RMSE=1.8214 RMSE=2.1092 RMSE=2.0908 RMSE=1.6954 RMSE=1.7761 RMSE=1.6625 RMSE=1.6235 RMSE=1.6587 RMSE=1.5381 RMSE=1.7271 Epoch: 20/150RMSE=1.6846 RMSE=1.6177 RMSE=1.5772 RMSE=1.6836 RMSE=1.6554 RMSE=1.6195 RMSE=1.6511 RMSE=1.6449 RMSE=1.6653 RMSE=1.7070 Epoch: 30/150RMSE=1.7297 RMSE=1.7472 RMSE=1.7410 RMSE=1.7282 RMSE=1.7363 RMSE=1.7367 RMSE=1.7313 RMSE=1.7279 RMSE=1.7427 RMSE=1.6409 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.641 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.1072 RMSE=4.2130 RMSE=3.1025 RMSE=2.6589 RMSE=2.4019 RMSE=2.0466 RMSE=2.1816 RMSE=2.2785 RMSE=1.9813 Epoch: 10/150RMSE=1.9025 RMSE=2.1090 RMSE=1.8004 RMSE=1.6068 RMSE=1.5191 RMSE=1.6012 RMSE=1.5312 RMSE=1.5982 RMSE=1.5553 RMSE=1.3823 Epoch: 20/150RMSE=1.3800 RMSE=1.3932 RMSE=1.3276 RMSE=1.4317 RMSE=1.4069 RMSE=1.3046 RMSE=1.3105 RMSE=1.3402 RMSE=1.4016 RMSE=1.2989 Epoch: 30/150RMSE=1.3356 RMSE=1.3387 RMSE=1.3114 RMSE=1.3318 RMSE=1.3033 RMSE=1.2915 RMSE=1.3117 RMSE=1.3194 RMSE=1.2961 RMSE=1.3105 Epoch: 40/150RMSE=1.3196 RMSE=1.3154 RMSE=1.3216 RMSE=1.2817 RMSE=1.2858 RMSE=1.2980 RMSE=1.2853 RMSE=1.3045 RMSE=1.3122 RMSE=1.3096 Epoch: 50/150RMSE=1.3080 RMSE=1.3061 RMSE=1.3021 RMSE=1.3051 RMSE=1.3055 RMSE=1.3000 RMSE=1.2956 RMSE=1.2952 RMSE=1.2984 RMSE=1.2999 Epoch: 60/150RMSE=1.3006 RMSE=1.3003 RMSE=1.3020 RMSE=1.3027 RMSE=1.2397 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.440 +/- 0.201\n",
      "\n",
      "Epoch: 1/150RMSE=5.2174 RMSE=4.6899 RMSE=3.6828 RMSE=2.5340 RMSE=2.2985 RMSE=1.7199 RMSE=1.6630 RMSE=1.5671 RMSE=1.3464 Epoch: 10/150RMSE=1.6757 RMSE=1.4668 RMSE=1.3686 RMSE=1.4210 RMSE=1.2044 RMSE=1.4232 RMSE=1.3109 RMSE=1.4941 RMSE=1.2972 RMSE=1.2437 Epoch: 20/150RMSE=1.3393 RMSE=1.2912 RMSE=1.3621 RMSE=1.3629 RMSE=1.3565 RMSE=1.3415 RMSE=1.2942 RMSE=1.2974 RMSE=1.3271 RMSE=1.3169 Epoch: 30/150RMSE=1.2881 RMSE=1.2927 RMSE=1.3016 RMSE=1.3196 RMSE=1.3335 RMSE=1.2269 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.369 +/- 0.192\n",
      "\n",
      "Epoch: 1/150RMSE=5.1552 RMSE=4.4488 RMSE=3.2979 RMSE=2.4176 RMSE=2.1123 RMSE=1.8303 RMSE=1.6490 RMSE=1.4300 RMSE=1.6949 Epoch: 10/150RMSE=1.6927 RMSE=1.3576 RMSE=1.8310 RMSE=1.5992 RMSE=1.7130 RMSE=1.6124 RMSE=1.4976 RMSE=1.4254 RMSE=1.3974 RMSE=1.3384 Epoch: 20/150RMSE=1.5268 RMSE=1.4921 RMSE=1.4069 RMSE=1.3566 RMSE=1.4198 RMSE=1.3253 RMSE=1.3429 RMSE=1.3425 RMSE=1.3290 RMSE=1.3467 Epoch: 30/150RMSE=1.3552 RMSE=1.3949 RMSE=1.3967 RMSE=1.4123 RMSE=1.4072 RMSE=1.4078 RMSE=1.4195 RMSE=1.4104 RMSE=1.4048 RMSE=1.4001 Epoch: 40/150RMSE=1.3884 RMSE=1.3811 RMSE=1.3771 RMSE=1.3756 RMSE=1.3701 RMSE=1.3650 RMSE=1.1545 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.315 +/- 0.191\n",
      "\n",
      "Epoch: 1/150RMSE=5.1589 RMSE=4.4007 RMSE=3.4505 RMSE=2.5297 RMSE=2.1934 RMSE=1.6987 RMSE=2.2174 RMSE=1.6643 RMSE=1.7354 Epoch: 10/150RMSE=1.5165 RMSE=1.8650 RMSE=1.9502 RMSE=1.4819 RMSE=1.5431 RMSE=1.5440 RMSE=1.4921 RMSE=1.4198 RMSE=1.4842 RMSE=1.5436 Epoch: 20/150RMSE=1.3918 RMSE=1.5213 RMSE=1.4267 RMSE=1.5292 RMSE=1.4937 RMSE=1.4495 RMSE=1.3173 RMSE=1.2831 RMSE=1.3103 RMSE=1.2795 Epoch: 30/150RMSE=1.2891 RMSE=1.2448 RMSE=1.2801 RMSE=1.2981 RMSE=1.2724 RMSE=1.2577 RMSE=1.2321 RMSE=1.2586 RMSE=1.2764 RMSE=1.2663 Epoch: 40/150RMSE=1.2840 RMSE=1.2692 RMSE=1.2545 RMSE=1.2915 RMSE=1.2707 RMSE=1.2637 RMSE=1.2648 RMSE=1.2586 RMSE=1.2485 RMSE=1.2522 Epoch: 50/150RMSE=1.2551 RMSE=1.2516 RMSE=1.2517 RMSE=1.2529 RMSE=1.2539 RMSE=1.2534 RMSE=1.2540 RMSE=1.1799 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.288 +/- 0.179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --pooling='KMIS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipophilicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/lipo.pt\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.3965 RMSE=1.1359 RMSE=1.0442 RMSE=0.9356 RMSE=0.9455 RMSE=0.9673 RMSE=0.8954 RMSE=0.9330 RMSE=1.1580 Epoch: 10/150RMSE=0.9578 RMSE=0.9415 RMSE=0.8825 RMSE=0.8082 RMSE=0.8100 RMSE=0.8197 RMSE=0.8417 RMSE=0.8081 RMSE=0.7925 RMSE=0.7617 Epoch: 20/150RMSE=0.7719 RMSE=0.7698 RMSE=0.7524 RMSE=0.7560 RMSE=0.7675 RMSE=0.7570 RMSE=0.7756 RMSE=0.7595 RMSE=0.7557 RMSE=0.7542 Epoch: 30/150RMSE=0.7451 RMSE=0.7749 RMSE=0.7620 RMSE=0.7572 RMSE=0.7619 RMSE=0.7500 RMSE=0.7510 RMSE=0.7545 RMSE=0.7615 RMSE=0.7635 Epoch: 40/150RMSE=0.7494 RMSE=0.7531 RMSE=0.7542 RMSE=0.7520 RMSE=0.7520 RMSE=0.7512 RMSE=0.7508 RMSE=0.7511 RMSE=0.7509 RMSE=0.7501 Epoch: 50/150RMSE=0.7498 RMSE=0.8584 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.858 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.5931 RMSE=1.2750 RMSE=1.1560 RMSE=1.0585 RMSE=1.0685 RMSE=0.9993 RMSE=1.0364 RMSE=0.9077 RMSE=0.9530 Epoch: 10/150RMSE=0.9274 RMSE=1.0535 RMSE=0.9531 RMSE=0.8468 RMSE=0.8606 RMSE=0.8245 RMSE=0.8457 RMSE=0.8194 RMSE=0.8527 RMSE=0.8526 Epoch: 20/150RMSE=1.0136 RMSE=0.8298 RMSE=0.8727 RMSE=0.8589 RMSE=0.8707 RMSE=0.8595 RMSE=0.8013 RMSE=0.8057 RMSE=0.8120 RMSE=0.8014 Epoch: 30/150RMSE=0.8009 RMSE=0.8104 RMSE=0.8090 RMSE=0.8065 RMSE=0.7939 RMSE=0.8024 RMSE=0.8030 RMSE=0.8201 RMSE=0.8035 RMSE=0.7937 Epoch: 40/150RMSE=0.7924 RMSE=0.7974 RMSE=0.7951 RMSE=0.7973 RMSE=0.7957 RMSE=0.8019 RMSE=0.8021 RMSE=0.7957 RMSE=0.7998 RMSE=0.8020 Epoch: 50/150RMSE=0.8029 RMSE=0.7991 RMSE=0.8036 RMSE=0.7999 RMSE=0.7994 RMSE=0.7980 RMSE=0.7990 RMSE=0.7998 RMSE=0.8010 RMSE=0.7998 Epoch: 60/150RMSE=0.8011 RMSE=0.7904 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 0.824 +/- 0.034\n",
      "\n",
      "Epoch: 1/150RMSE=1.4412 RMSE=1.3246 RMSE=1.0893 RMSE=1.0468 RMSE=1.0106 RMSE=0.9808 RMSE=0.9478 RMSE=1.0025 RMSE=0.9887 Epoch: 10/150RMSE=0.9135 RMSE=0.9146 RMSE=0.9130 RMSE=0.9141 RMSE=0.9208 RMSE=0.9433 RMSE=0.8647 RMSE=0.9100 RMSE=0.8909 RMSE=0.9091 Epoch: 20/150RMSE=0.8960 RMSE=0.7950 RMSE=0.8404 RMSE=0.8022 RMSE=0.8103 RMSE=0.8331 RMSE=0.7937 RMSE=0.7911 RMSE=0.7920 RMSE=0.7919 Epoch: 30/150RMSE=0.7938 RMSE=0.7842 RMSE=0.7879 RMSE=0.7891 RMSE=0.7944 RMSE=0.7908 RMSE=0.7889 RMSE=0.7818 RMSE=0.7843 RMSE=0.7795 Epoch: 40/150RMSE=0.7778 RMSE=0.7746 RMSE=0.7813 RMSE=0.7704 RMSE=0.7867 RMSE=0.7814 RMSE=0.7803 RMSE=0.8028 RMSE=0.8008 RMSE=0.7819 Epoch: 50/150RMSE=0.7751 RMSE=0.7806 RMSE=0.7802 RMSE=0.7789 RMSE=0.7771 RMSE=0.7778 RMSE=0.7766 RMSE=0.7750 RMSE=0.7762 RMSE=0.7766 Epoch: 60/150RMSE=0.7776 RMSE=0.7786 RMSE=0.7780 RMSE=0.7773 RMSE=0.7796 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 0.809 +/- 0.035\n",
      "\n",
      "Epoch: 1/150RMSE=1.6403 RMSE=1.1391 RMSE=1.0345 RMSE=1.0321 RMSE=0.9947 RMSE=0.9743 RMSE=1.0899 RMSE=0.9395 RMSE=0.8931 Epoch: 10/150RMSE=0.8678 RMSE=0.9166 RMSE=0.8509 RMSE=0.9034 RMSE=0.8853 RMSE=0.8861 RMSE=0.8509 RMSE=0.8144 RMSE=0.8338 RMSE=0.8714 Epoch: 20/150RMSE=0.8296 RMSE=0.8771 RMSE=0.8156 RMSE=0.8280 RMSE=0.7970 RMSE=0.7977 RMSE=0.8044 RMSE=0.8190 RMSE=0.8329 RMSE=0.8070 Epoch: 30/150RMSE=0.7943 RMSE=0.8050 RMSE=0.8167 RMSE=0.8004 RMSE=0.8177 RMSE=0.7918 RMSE=0.7913 RMSE=0.7916 RMSE=0.7891 RMSE=0.7972 Epoch: 40/150RMSE=0.7902 RMSE=0.8064 RMSE=0.8037 RMSE=0.7967 RMSE=0.7977 RMSE=0.7960 RMSE=0.8025 RMSE=0.7988 RMSE=0.7960 RMSE=0.7971 Epoch: 50/150RMSE=0.7980 RMSE=0.7987 RMSE=0.7963 RMSE=0.7965 RMSE=0.7969 RMSE=0.7972 RMSE=0.7974 RMSE=0.7972 RMSE=0.7966 RMSE=0.8169 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 0.811 +/- 0.030\n",
      "\n",
      "Epoch: 1/150RMSE=1.2947 RMSE=1.1347 RMSE=1.0871 RMSE=1.0309 RMSE=0.9445 RMSE=0.9797 RMSE=0.9675 RMSE=0.9699 RMSE=0.9114 Epoch: 10/150RMSE=0.8988 RMSE=0.8614 RMSE=0.8896 RMSE=0.8702 RMSE=0.9616 RMSE=0.8634 RMSE=0.8590 RMSE=0.8095 RMSE=0.8464 RMSE=0.8237 Epoch: 20/150RMSE=0.8228 RMSE=0.8190 RMSE=0.8031 RMSE=0.8172 RMSE=0.8216 RMSE=0.8150 RMSE=0.8343 RMSE=0.8261 RMSE=0.8182 RMSE=0.8079 Epoch: 30/150RMSE=0.8185 RMSE=0.8153 RMSE=0.8100 RMSE=0.8053 RMSE=0.8133 RMSE=0.8068 RMSE=0.8104 RMSE=0.8131 RMSE=0.8112 RMSE=0.8120 Epoch: 40/150RMSE=0.8091 RMSE=0.8122 RMSE=0.8090 RMSE=0.7897 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 0.807 +/- 0.029\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --pooling='KMIS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Tuple, Union\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\n",
    "Scorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\n",
    "from torch_sparse import SparseTensor, remove_diag\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.nn.dense import Linear\n",
    "from torch.nn import Module\n",
    "from torch_scatter import scatter_max, scatter_min\n",
    "\n",
    "def maximal_independent_set(edge_index: Adj, k: int = 1,\n",
    "                            perm: OptTensor = None) -> Tensor:\n",
    "    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n",
    "    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n",
    "    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n",
    "    returned set.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method follows `Blelloch's Alogirithm\n",
    "    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n",
    "    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n",
    "    higher values of :math:`k`.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: :class:`ByteTensor`\n",
    "    \"\"\"\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "        device = edge_index.device()\n",
    "        n = edge_index.size(0)\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        device = row.device\n",
    "        n = edge_index.max().item() + 1\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    mis = torch.zeros(n, dtype=torch.bool, device=device)\n",
    "    mask = mis.clone()\n",
    "    min_rank = rank.clone()\n",
    "\n",
    "    while not mask.all():\n",
    "        for _ in range(k):\n",
    "            min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "            scatter_min(min_rank[row], col, out=min_neigh)\n",
    "            torch.minimum(min_neigh, min_rank, out=min_rank)  # self-loops\n",
    "\n",
    "        mis = mis | torch.eq(rank, min_rank)\n",
    "        mask = mis.clone().byte()\n",
    "\n",
    "        for _ in range(k):\n",
    "            max_neigh = torch.full_like(mask, fill_value=0)\n",
    "            scatter_max(mask[row], col, out=max_neigh)\n",
    "            torch.maximum(max_neigh, mask, out=mask)  # self-loops\n",
    "\n",
    "        mask = mask.to(dtype=torch.bool)\n",
    "        min_rank = rank.clone()\n",
    "        min_rank[mask] = n\n",
    "\n",
    "    return mis\n",
    "\n",
    "def maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n",
    "                                    perm: OptTensor = None) -> PairTensor:\n",
    "    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n",
    "    clustering of a graph, as defined in `\"Generalizing Downsampling from\n",
    "    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method returns both the :math:`k`-MIS and the clustering, where the\n",
    "    :math:`c`-th cluster refers to the :math:`c`-th element of the\n",
    "    :math:`k`-MIS.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n",
    "    \"\"\"\n",
    "    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n",
    "    n, device = mis.size(0), mis.device\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n",
    "    rank_mis = rank[mis]\n",
    "    min_rank[mis] = rank_mis\n",
    "\n",
    "    for _ in range(k):\n",
    "        min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "        scatter_min(min_rank[row], col, out=min_neigh)\n",
    "        torch.minimum(min_neigh, min_rank, out=min_rank)\n",
    "\n",
    "    _, clusters = torch.unique(min_rank, return_inverse=True)\n",
    "    perm = torch.argsort(rank_mis)\n",
    "    return mis, perm[clusters]\n",
    "\n",
    "\n",
    "class KMISPooling(Module):\n",
    "\n",
    "    _heuristics = {None, 'greedy', 'w-greedy'}\n",
    "    _passthroughs = {None, 'before', 'after'}\n",
    "    _scorers = {\n",
    "        'linear',\n",
    "        'random',\n",
    "        'constant',\n",
    "        'canonical',\n",
    "        'first',\n",
    "        'last',\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n",
    "                 scorer: Union[Scorer, str] = 'linear',\n",
    "                 score_heuristic: Optional[str] = 'greedy',\n",
    "                 score_passthrough: Optional[str] = 'before',\n",
    "                 aggr_x: Optional[Union[str, Aggregation]] = None,\n",
    "                 aggr_edge: str = 'sum',\n",
    "                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n",
    "                 remove_self_loops: bool = True) -> None:\n",
    "        super(KMISPooling, self).__init__()\n",
    "        assert score_heuristic in self._heuristics, \\\n",
    "            \"Unrecognized `score_heuristic` value.\"\n",
    "        assert score_passthrough in self._passthroughs, \\\n",
    "            \"Unrecognized `score_passthrough` value.\"\n",
    "\n",
    "        if not callable(scorer):\n",
    "            assert scorer in self._scorers, \\\n",
    "                \"Unrecognized `scorer` value.\"\n",
    "\n",
    "        self.k = k\n",
    "        self.scorer = scorer\n",
    "        self.score_heuristic = score_heuristic\n",
    "        self.score_passthrough = score_passthrough\n",
    "\n",
    "        self.aggr_x = aggr_x\n",
    "        self.aggr_edge = aggr_edge\n",
    "        self.aggr_score = aggr_score\n",
    "        self.remove_self_loops = remove_self_loops\n",
    "\n",
    "        if scorer == 'linear':\n",
    "            assert self.score_passthrough is not None, \\\n",
    "                \"`'score_passthrough'` must not be `None`\" \\\n",
    "                \" when using `'linear'` scorer\"\n",
    "\n",
    "            self.lin = Linear(in_features=in_channels, out_features=1)\n",
    "\n",
    "    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n",
    "        if self.score_heuristic is None:\n",
    "            return x\n",
    "\n",
    "        row, col, _ = adj.coo()\n",
    "        x = x.view(-1)\n",
    "\n",
    "        if self.score_heuristic == 'greedy':\n",
    "            k_sums = torch.ones_like(x)\n",
    "        else:\n",
    "            k_sums = x.clone()\n",
    "\n",
    "        for _ in range(self.k):\n",
    "            scatter_add(k_sums[row], col, out=k_sums)\n",
    "\n",
    "        return x / k_sums\n",
    "\n",
    "    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) -> Tensor:\n",
    "        if self.scorer == 'linear':\n",
    "            return self.lin(x).sigmoid()\n",
    "\n",
    "        if self.scorer == 'random':\n",
    "            return torch.rand((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'constant':\n",
    "            return torch.ones((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'canonical':\n",
    "            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n",
    "\n",
    "        if self.scorer == 'first':\n",
    "            return x[..., [0]]\n",
    "\n",
    "        if self.scorer == 'last':\n",
    "            return x[..., [-1]]\n",
    "\n",
    "        return self.scorer(x, edge_index, edge_attr, batch)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) \\\n",
    "            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n",
    "        \"\"\"\"\"\"\n",
    "        edge_index = edge_index.long()\n",
    "        adj, n = edge_index, x.size(0)\n",
    "\n",
    "        if not isinstance(edge_index, SparseTensor):\n",
    "            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n",
    "\n",
    "        score = self._scorer(x, edge_index, edge_attr, batch)\n",
    "        updated_score = self._apply_heuristic(score, adj)\n",
    "        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n",
    "\n",
    "        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n",
    "\n",
    "        row, col, val = adj.coo()\n",
    "        c = mis.sum()\n",
    "\n",
    "        if val is None:\n",
    "            val = torch.ones_like(row, dtype=torch.float)\n",
    "\n",
    "        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n",
    "                           is_sorted=False,\n",
    "                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n",
    "\n",
    "        if self.remove_self_loops:\n",
    "            adj = remove_diag(adj)\n",
    "\n",
    "        if self.score_passthrough == 'before':\n",
    "            x = self.aggr_score(x, score)\n",
    "\n",
    "        if self.aggr_x is None:\n",
    "            x = x[mis]\n",
    "        elif isinstance(self.aggr_x, str):\n",
    "            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n",
    "                        reduce=self.aggr_x)\n",
    "        else:\n",
    "            x = self.aggr_x(x, cluster, dim_size=c)\n",
    "\n",
    "        if self.score_passthrough == 'after':\n",
    "            x = self.aggr_score(x, score[mis])\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_index, edge_attr = adj, None\n",
    "\n",
    "        else:\n",
    "            row, col, edge_attr = adj.coo()\n",
    "            edge_index = torch.stack([row, col])\n",
    "\n",
    "        if batch is not None:\n",
    "            batch = batch[mis]\n",
    "\n",
    "        #attn = x\n",
    "        #select_out = topk(attn, batch)\n",
    "        perm = perm[mis]\n",
    "\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, mis, cluster, perm\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.scorer == 'linear':\n",
    "            channels = f\"in_channels={self.lin.in_channels}, \"\n",
    "        else:\n",
    "            channels = \"\"\n",
    "\n",
    "        return f'{self.__class__.__name__}({channels}k={self.k})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "Seed 42, Fold 1 Val Acc: 0.897\n",
      "Seed 42, Fold 2 Val Acc: 0.900\n",
      "Seed 42, Fold 3 Val Acc: 0.878\n",
      "Seed 42, Fold 4 Val Acc: 0.895\n",
      "Early stopping at epoch 189 for fold 5\n",
      "Seed 42, Fold 5 Val Acc: 0.911\n",
      "Seed 42 Results: Mean Val Acc: 0.896, Time: 37.200 seconds, Memory: 0.363 MB, GPU Memory: 603.980 MB\n",
      "Early stopping at epoch 190 for fold 1\n",
      "Seed 123, Fold 1 Val Acc: 0.895\n",
      "Seed 123, Fold 2 Val Acc: 0.900\n",
      "Seed 123, Fold 3 Val Acc: 0.908\n",
      "Seed 123, Fold 4 Val Acc: 0.900\n",
      "Seed 123, Fold 5 Val Acc: 0.885\n",
      "Seed 123 Results: Mean Val Acc: 0.898, Time: 36.886 seconds, Memory: 0.343 MB, GPU Memory: 603.980 MB\n",
      "Seed 456, Fold 1 Val Acc: 0.900\n",
      "Seed 456, Fold 2 Val Acc: 0.871\n",
      "Seed 456, Fold 3 Val Acc: 0.900\n",
      "Early stopping at epoch 200 for fold 4\n",
      "Seed 456, Fold 4 Val Acc: 0.898\n",
      "Seed 456, Fold 5 Val Acc: 0.909\n",
      "Seed 456 Results: Mean Val Acc: 0.896, Time: 35.205 seconds, Memory: 0.345 MB, GPU Memory: 603.980 MB\n",
      "Seed 789, Fold 1 Val Acc: 0.869\n",
      "Seed 789, Fold 2 Val Acc: 0.906\n",
      "Seed 789, Fold 3 Val Acc: 0.904\n",
      "Seed 789, Fold 4 Val Acc: 0.911\n",
      "Seed 789, Fold 5 Val Acc: 0.908\n",
      "Seed 789 Results: Mean Val Acc: 0.900, Time: 36.774 seconds, Memory: 0.341 MB, GPU Memory: 603.980 MB\n",
      "Early stopping at epoch 199 for fold 1\n",
      "Seed 101, Fold 1 Val Acc: 0.915\n",
      "Seed 101, Fold 2 Val Acc: 0.897\n",
      "Seed 101, Fold 3 Val Acc: 0.897\n",
      "Seed 101, Fold 4 Val Acc: 0.896\n",
      "Seed 101, Fold 5 Val Acc: 0.885\n",
      "Seed 101 Results: Mean Val Acc: 0.898, Time: 32.771 seconds, Memory: 0.341 MB, GPU Memory: 603.980 MB\n",
      "{'seed': 42, 'mean_val_acc': 0.896238345008219, 'time': 37.20007371902466, 'memory': 0.362924, 'gpu_memory': 603.979776}\n",
      "{'seed': 123, 'mean_val_acc': 0.897706856920695, 'time': 36.886072874069214, 'memory': 0.343205, 'gpu_memory': 603.979776}\n",
      "{'seed': 456, 'mean_val_acc': 0.8958700233952432, 'time': 35.20477747917175, 'memory': 0.34486, 'gpu_memory': 603.979776}\n",
      "{'seed': 789, 'mean_val_acc': 0.8995641527579787, 'time': 36.7742440700531, 'memory': 0.340523, 'gpu_memory': 603.979776}\n",
      "{'seed': 101, 'mean_val_acc': 0.8980744964566097, 'time': 32.7708306312561, 'memory': 0.340745, 'gpu_memory': 603.979776}\n",
      "Total Mean Val Acc: 0.90\n",
      "Standard Deviation: 0.00\n"
     ]
    }
   ],
   "source": [
    "dataset_sparse = Planetoid(root=\"/data/Zeyu/Pooling\", name='Cora')\n",
    "dataset_sparse = dataset_sparse.shuffle()\n",
    "print(dataset_sparse[0])\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(64, k=1, aggr_x='sum'))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_KMIS(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# \n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CiteSeer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3327, 3703], edge_index=[2, 9104], y=[3327], train_mask=[3327], val_mask=[3327], test_mask=[3327])\n",
      "Early stopping at epoch 191 for fold 1\n",
      "Seed 42, Fold 1 Val Acc: 0.791\n",
      "Early stopping at epoch 198 for fold 2\n",
      "Seed 42, Fold 2 Val Acc: 0.791\n",
      "Early stopping at epoch 180 for fold 3\n",
      "Seed 42, Fold 3 Val Acc: 0.779\n",
      "Early stopping at epoch 183 for fold 4\n",
      "Seed 42, Fold 4 Val Acc: 0.762\n",
      "Early stopping at epoch 190 for fold 5\n",
      "Seed 42, Fold 5 Val Acc: 0.761\n",
      "Seed 42 Results: Mean Val Acc: 0.777, Time: 40.396 seconds, Memory: 3.783 MB, GPU Memory: 603.980 MB\n",
      "Early stopping at epoch 180 for fold 1\n",
      "Seed 123, Fold 1 Val Acc: 0.764\n",
      "Early stopping at epoch 190 for fold 2\n",
      "Seed 123, Fold 2 Val Acc: 0.779\n",
      "Early stopping at epoch 179 for fold 3\n",
      "Seed 123, Fold 3 Val Acc: 0.786\n",
      "Seed 123, Fold 4 Val Acc: 0.779\n",
      "Early stopping at epoch 187 for fold 5\n",
      "Seed 123, Fold 5 Val Acc: 0.776\n",
      "Seed 123 Results: Mean Val Acc: 0.777, Time: 37.863 seconds, Memory: 0.361 MB, GPU Memory: 603.980 MB\n",
      "Early stopping at epoch 192 for fold 1\n",
      "Seed 456, Fold 1 Val Acc: 0.773\n",
      "Early stopping at epoch 189 for fold 2\n",
      "Seed 456, Fold 2 Val Acc: 0.775\n",
      "Early stopping at epoch 177 for fold 3\n",
      "Seed 456, Fold 3 Val Acc: 0.761\n",
      "Early stopping at epoch 178 for fold 4\n",
      "Seed 456, Fold 4 Val Acc: 0.777\n",
      "Early stopping at epoch 184 for fold 5\n",
      "Seed 456, Fold 5 Val Acc: 0.795\n",
      "Seed 456 Results: Mean Val Acc: 0.776, Time: 37.591 seconds, Memory: 0.356 MB, GPU Memory: 603.980 MB\n",
      "Seed 789, Fold 1 Val Acc: 0.766\n",
      "Early stopping at epoch 188 for fold 2\n",
      "Seed 789, Fold 2 Val Acc: 0.779\n",
      "Seed 789, Fold 3 Val Acc: 0.780\n",
      "Early stopping at epoch 180 for fold 4\n",
      "Seed 789, Fold 4 Val Acc: 0.765\n",
      "Early stopping at epoch 189 for fold 5\n",
      "Seed 789, Fold 5 Val Acc: 0.797\n",
      "Seed 789 Results: Mean Val Acc: 0.778, Time: 38.139 seconds, Memory: 0.358 MB, GPU Memory: 603.980 MB\n",
      "Early stopping at epoch 185 for fold 1\n",
      "Seed 101, Fold 1 Val Acc: 0.794\n",
      "Early stopping at epoch 180 for fold 2\n",
      "Seed 101, Fold 2 Val Acc: 0.775\n",
      "Early stopping at epoch 186 for fold 3\n",
      "Seed 101, Fold 3 Val Acc: 0.776\n",
      "Early stopping at epoch 179 for fold 4\n",
      "Seed 101, Fold 4 Val Acc: 0.788\n",
      "Early stopping at epoch 189 for fold 5\n",
      "Seed 101, Fold 5 Val Acc: 0.765\n",
      "Seed 101 Results: Mean Val Acc: 0.780, Time: 37.914 seconds, Memory: 0.355 MB, GPU Memory: 603.980 MB\n",
      "{'seed': 42, 'mean_val_acc': 0.7769676443360652, 'time': 40.39581918716431, 'memory': 3.782939, 'gpu_memory': 603.979776}\n",
      "{'seed': 123, 'mean_val_acc': 0.776979385400438, 'time': 37.862605810165405, 'memory': 0.360921, 'gpu_memory': 603.979776}\n",
      "{'seed': 456, 'mean_val_acc': 0.7763765269028425, 'time': 37.59080672264099, 'memory': 0.355906, 'gpu_memory': 603.979776}\n",
      "{'seed': 789, 'mean_val_acc': 0.7775804375804376, 'time': 38.13895130157471, 'memory': 0.358223, 'gpu_memory': 603.979776}\n",
      "{'seed': 101, 'mean_val_acc': 0.7796784754679491, 'time': 37.91438150405884, 'memory': 0.355156, 'gpu_memory': 603.979776}\n",
      "Total Mean Val Acc: 0.78\n",
      "Standard Deviation: 0.00\n"
     ]
    }
   ],
   "source": [
    "dataset_sparse = Planetoid(root=\"/data/Zeyu/Pooling\", name='CiteSeer')\n",
    "dataset_sparse = dataset_sparse.shuffle()\n",
    "print(dataset_sparse[0])\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(64, k=2, aggr_x='sum'))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_KMIS(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# \n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n",
      "Seed 42, Fold 1 Val Acc: 0.882\n",
      "Seed 42, Fold 2 Val Acc: 0.881\n",
      "Seed 42, Fold 3 Val Acc: 0.866\n",
      "Seed 42, Fold 4 Val Acc: 0.880\n",
      "Seed 42, Fold 5 Val Acc: 0.879\n",
      "Seed 42 Results: Mean Val Acc: 0.878, Time: 47.855 seconds, Memory: 13.105 MB, GPU Memory: 782.238 MB\n",
      "Seed 123, Fold 1 Val Acc: 0.874\n",
      "Seed 123, Fold 2 Val Acc: 0.870\n",
      "Seed 123, Fold 3 Val Acc: 0.883\n",
      "Seed 123, Fold 4 Val Acc: 0.880\n",
      "Seed 123, Fold 5 Val Acc: 0.880\n",
      "Seed 123 Results: Mean Val Acc: 0.877, Time: 50.107 seconds, Memory: 0.821 MB, GPU Memory: 782.238 MB\n",
      "Seed 456, Fold 1 Val Acc: 0.886\n",
      "Seed 456, Fold 2 Val Acc: 0.874\n",
      "Seed 456, Fold 3 Val Acc: 0.880\n",
      "Seed 456, Fold 4 Val Acc: 0.881\n",
      "Seed 456, Fold 5 Val Acc: 0.870\n",
      "Seed 456 Results: Mean Val Acc: 0.878, Time: 44.027 seconds, Memory: 0.820 MB, GPU Memory: 782.238 MB\n",
      "Seed 789, Fold 1 Val Acc: 0.879\n",
      "Seed 789, Fold 2 Val Acc: 0.878\n",
      "Seed 789, Fold 3 Val Acc: 0.878\n",
      "Seed 789, Fold 4 Val Acc: 0.880\n",
      "Seed 789, Fold 5 Val Acc: 0.881\n",
      "Seed 789 Results: Mean Val Acc: 0.879, Time: 49.603 seconds, Memory: 0.817 MB, GPU Memory: 782.238 MB\n",
      "Seed 101, Fold 1 Val Acc: 0.888\n",
      "Seed 101, Fold 2 Val Acc: 0.871\n",
      "Seed 101, Fold 3 Val Acc: 0.874\n",
      "Seed 101, Fold 4 Val Acc: 0.873\n",
      "Seed 101, Fold 5 Val Acc: 0.884\n",
      "Seed 101 Results: Mean Val Acc: 0.878, Time: 48.428 seconds, Memory: 0.816 MB, GPU Memory: 782.238 MB\n",
      "{'seed': 42, 'mean_val_acc': 0.8776179086464884, 'time': 47.854684829711914, 'memory': 13.105207, 'gpu_memory': 782.237696}\n",
      "{'seed': 123, 'mean_val_acc': 0.8771116580645393, 'time': 50.10705494880676, 'memory': 0.82073, 'gpu_memory': 782.237696}\n",
      "{'seed': 456, 'mean_val_acc': 0.8782267108527758, 'time': 44.02650046348572, 'memory': 0.820119, 'gpu_memory': 782.237696}\n",
      "{'seed': 789, 'mean_val_acc': 0.8792920439796512, 'time': 49.603272438049316, 'memory': 0.816786, 'gpu_memory': 782.237696}\n",
      "{'seed': 101, 'mean_val_acc': 0.8779731997392869, 'time': 48.42775344848633, 'memory': 0.816164, 'gpu_memory': 782.237696}\n",
      "Total Mean Val Acc: 0.88\n",
      "Standard Deviation: 0.00\n"
     ]
    }
   ],
   "source": [
    "dataset_sparse = Planetoid(root=\"/data/Zeyu/Pooling\", name='PubMed')\n",
    "dataset_sparse = dataset_sparse.shuffle()\n",
    "print(dataset_sparse[0])\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(64, k=3, aggr_x='sum'))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_KMIS(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# \n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cornell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 1703], edge_index=[2, 298], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10])\n",
      "Seed 42, Fold 1 Val Acc: 0.811\n",
      "Seed 42, Fold 2 Val Acc: 0.757\n",
      "Seed 42, Fold 3 Val Acc: 0.838\n",
      "Seed 42, Fold 4 Val Acc: 0.833\n",
      "Seed 42, Fold 5 Val Acc: 0.694\n",
      "Seed 42 Results: Mean Val Acc: 0.787, Time: 32.437 seconds, Memory: 0.295 MB, GPU Memory: 782.238 MB\n",
      "Seed 123, Fold 1 Val Acc: 0.838\n",
      "Seed 123, Fold 2 Val Acc: 0.703\n",
      "Seed 123, Fold 3 Val Acc: 0.703\n",
      "Seed 123, Fold 4 Val Acc: 0.861\n",
      "Seed 123, Fold 5 Val Acc: 0.861\n",
      "Seed 123 Results: Mean Val Acc: 0.793, Time: 33.114 seconds, Memory: 0.279 MB, GPU Memory: 782.238 MB\n",
      "Seed 456, Fold 1 Val Acc: 0.811\n",
      "Seed 456, Fold 2 Val Acc: 0.757\n",
      "Early stopping at epoch 188 for fold 3\n",
      "Seed 456, Fold 3 Val Acc: 0.784\n",
      "Seed 456, Fold 4 Val Acc: 0.833\n",
      "Seed 456, Fold 5 Val Acc: 0.861\n",
      "Seed 456 Results: Mean Val Acc: 0.809, Time: 32.043 seconds, Memory: 0.277 MB, GPU Memory: 782.238 MB\n",
      "Seed 789, Fold 1 Val Acc: 0.784\n",
      "Seed 789, Fold 2 Val Acc: 0.730\n",
      "Seed 789, Fold 3 Val Acc: 0.946\n",
      "Seed 789, Fold 4 Val Acc: 0.778\n",
      "Seed 789, Fold 5 Val Acc: 0.667\n",
      "Seed 789 Results: Mean Val Acc: 0.781, Time: 31.811 seconds, Memory: 0.281 MB, GPU Memory: 782.238 MB\n",
      "Seed 101, Fold 1 Val Acc: 0.973\n",
      "Seed 101, Fold 2 Val Acc: 0.811\n",
      "Seed 101, Fold 3 Val Acc: 0.703\n",
      "Seed 101, Fold 4 Val Acc: 0.694\n",
      "Seed 101, Fold 5 Val Acc: 0.861\n",
      "Seed 101 Results: Mean Val Acc: 0.808, Time: 32.028 seconds, Memory: 0.279 MB, GPU Memory: 782.238 MB\n",
      "{'seed': 42, 'mean_val_acc': 0.7866366366366366, 'time': 32.4373242855072, 'memory': 0.294948, 'gpu_memory': 782.237696}\n",
      "{'seed': 123, 'mean_val_acc': 0.7930930930930932, 'time': 33.114471197128296, 'memory': 0.278775, 'gpu_memory': 782.237696}\n",
      "{'seed': 456, 'mean_val_acc': 0.8091591591591591, 'time': 32.04323053359985, 'memory': 0.276791, 'gpu_memory': 782.237696}\n",
      "{'seed': 789, 'mean_val_acc': 0.7807807807807807, 'time': 31.811376810073853, 'memory': 0.280771, 'gpu_memory': 782.237696}\n",
      "{'seed': 101, 'mean_val_acc': 0.8084084084084082, 'time': 32.02784204483032, 'memory': 0.279095, 'gpu_memory': 782.237696}\n",
      "Total Mean Val Acc: 0.80\n",
      "Standard Deviation: 0.01\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import WebKB\n",
    "dataset_sparse = WebKB(root=\"/data/Zeyu/Pooling\", name='Cornell')\n",
    "dataset_sparse = dataset_sparse.shuffle()\n",
    "print(dataset_sparse[0])\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(64, k=1, aggr_x='sum'))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_KMIS(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# \n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[183, 1703], edge_index=[2, 325], y=[183], train_mask=[183, 10], val_mask=[183, 10], test_mask=[183, 10])\n",
      "Seed 42, Fold 1 Val Acc: 0.811\n",
      "Seed 42, Fold 2 Val Acc: 0.838\n",
      "Seed 42, Fold 3 Val Acc: 0.811\n",
      "Early stopping at epoch 180 for fold 4\n",
      "Seed 42, Fold 4 Val Acc: 0.750\n",
      "Early stopping at epoch 199 for fold 5\n",
      "Seed 42, Fold 5 Val Acc: 0.750\n",
      "Seed 42 Results: Mean Val Acc: 0.792, Time: 30.321 seconds, Memory: 0.297 MB, GPU Memory: 782.238 MB\n",
      "Seed 123, Fold 1 Val Acc: 0.811\n",
      "Seed 123, Fold 2 Val Acc: 0.865\n",
      "Seed 123, Fold 3 Val Acc: 0.838\n",
      "Early stopping at epoch 183 for fold 4\n",
      "Seed 123, Fold 4 Val Acc: 0.833\n",
      "Seed 123, Fold 5 Val Acc: 0.750\n",
      "Seed 123 Results: Mean Val Acc: 0.819, Time: 34.745 seconds, Memory: 0.280 MB, GPU Memory: 782.238 MB\n",
      "Seed 456, Fold 1 Val Acc: 0.838\n",
      "Early stopping at epoch 196 for fold 2\n",
      "Seed 456, Fold 2 Val Acc: 0.784\n",
      "Seed 456, Fold 3 Val Acc: 0.838\n",
      "Seed 456, Fold 4 Val Acc: 0.833\n",
      "Seed 456, Fold 5 Val Acc: 0.778\n",
      "Seed 456 Results: Mean Val Acc: 0.814, Time: 32.229 seconds, Memory: 0.279 MB, GPU Memory: 782.238 MB\n",
      "Seed 789, Fold 1 Val Acc: 0.838\n",
      "Seed 789, Fold 2 Val Acc: 0.784\n",
      "Seed 789, Fold 3 Val Acc: 0.757\n",
      "Seed 789, Fold 4 Val Acc: 0.806\n",
      "Early stopping at epoch 189 for fold 5\n",
      "Seed 789, Fold 5 Val Acc: 0.889\n",
      "Seed 789 Results: Mean Val Acc: 0.815, Time: 29.493 seconds, Memory: 0.277 MB, GPU Memory: 782.238 MB\n",
      "Seed 101, Fold 1 Val Acc: 0.649\n",
      "Seed 101, Fold 2 Val Acc: 0.892\n",
      "Seed 101, Fold 3 Val Acc: 0.811\n",
      "Seed 101, Fold 4 Val Acc: 0.972\n",
      "Seed 101, Fold 5 Val Acc: 0.833\n",
      "Seed 101 Results: Mean Val Acc: 0.831, Time: 27.937 seconds, Memory: 0.280 MB, GPU Memory: 782.238 MB\n",
      "{'seed': 42, 'mean_val_acc': 0.7918918918918919, 'time': 30.320999145507812, 'memory': 0.297412, 'gpu_memory': 782.237696}\n",
      "{'seed': 123, 'mean_val_acc': 0.8193693693693694, 'time': 34.745489835739136, 'memory': 0.279542, 'gpu_memory': 782.237696}\n",
      "{'seed': 456, 'mean_val_acc': 0.814114114114114, 'time': 32.22861456871033, 'memory': 0.278795, 'gpu_memory': 782.237696}\n",
      "{'seed': 789, 'mean_val_acc': 0.8145645645645644, 'time': 29.493155241012573, 'memory': 0.276635, 'gpu_memory': 782.237696}\n",
      "{'seed': 101, 'mean_val_acc': 0.8313813813813814, 'time': 27.937294006347656, 'memory': 0.279789, 'gpu_memory': 782.237696}\n",
      "Total Mean Val Acc: 0.81\n",
      "Standard Deviation: 0.01\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import WebKB\n",
    "dataset_sparse = WebKB(root=\"/data/Zeyu/Pooling\", name='texas')\n",
    "dataset_sparse = dataset_sparse.shuffle()\n",
    "print(dataset_sparse[0])\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(64, k=1, aggr_x='sum'))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_KMIS(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# \n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wisconsin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[251, 1703], edge_index=[2, 515], y=[251], train_mask=[251, 10], val_mask=[251, 10], test_mask=[251, 10])\n",
      "Early stopping at epoch 188 for fold 1\n",
      "Seed 42, Fold 1 Val Acc: 0.804\n",
      "Early stopping at epoch 173 for fold 2\n",
      "Seed 42, Fold 2 Val Acc: 0.840\n",
      "Seed 42, Fold 3 Val Acc: 0.800\n",
      "Seed 42, Fold 4 Val Acc: 0.820\n",
      "Seed 42, Fold 5 Val Acc: 0.800\n",
      "Seed 42 Results: Mean Val Acc: 0.813, Time: 23.927 seconds, Memory: 0.297 MB, GPU Memory: 713.032 MB\n",
      "Early stopping at epoch 176 for fold 1\n",
      "Seed 123, Fold 1 Val Acc: 0.745\n",
      "Seed 123, Fold 2 Val Acc: 0.840\n",
      "Seed 123, Fold 3 Val Acc: 0.940\n",
      "Seed 123, Fold 4 Val Acc: 0.780\n",
      "Seed 123, Fold 5 Val Acc: 0.860\n",
      "Seed 123 Results: Mean Val Acc: 0.833, Time: 26.273 seconds, Memory: 0.283 MB, GPU Memory: 715.129 MB\n",
      "Seed 456, Fold 1 Val Acc: 0.824\n",
      "Early stopping at epoch 194 for fold 2\n",
      "Seed 456, Fold 2 Val Acc: 0.840\n",
      "Seed 456, Fold 3 Val Acc: 0.900\n",
      "Seed 456, Fold 4 Val Acc: 0.820\n",
      "Early stopping at epoch 178 for fold 5\n",
      "Seed 456, Fold 5 Val Acc: 0.760\n",
      "Seed 456 Results: Mean Val Acc: 0.829, Time: 27.798 seconds, Memory: 0.279 MB, GPU Memory: 715.129 MB\n",
      "Seed 789, Fold 1 Val Acc: 0.863\n",
      "Seed 789, Fold 2 Val Acc: 0.840\n",
      "Early stopping at epoch 185 for fold 3\n",
      "Seed 789, Fold 3 Val Acc: 0.760\n",
      "Early stopping at epoch 196 for fold 4\n",
      "Seed 789, Fold 4 Val Acc: 0.880\n",
      "Seed 789, Fold 5 Val Acc: 0.760\n",
      "Seed 789 Results: Mean Val Acc: 0.821, Time: 27.251 seconds, Memory: 0.280 MB, GPU Memory: 715.129 MB\n",
      "Seed 101, Fold 1 Val Acc: 0.882\n",
      "Early stopping at epoch 178 for fold 2\n",
      "Seed 101, Fold 2 Val Acc: 0.700\n",
      "Seed 101, Fold 3 Val Acc: 0.780\n",
      "Early stopping at epoch 194 for fold 4\n",
      "Seed 101, Fold 4 Val Acc: 0.820\n",
      "Seed 101, Fold 5 Val Acc: 0.920\n",
      "Seed 101 Results: Mean Val Acc: 0.820, Time: 26.300 seconds, Memory: 0.279 MB, GPU Memory: 715.129 MB\n",
      "{'seed': 42, 'mean_val_acc': 0.8127843137254903, 'time': 23.927440404891968, 'memory': 0.297024, 'gpu_memory': 713.03168}\n",
      "{'seed': 123, 'mean_val_acc': 0.8330196078431372, 'time': 26.27299976348877, 'memory': 0.28321, 'gpu_memory': 715.128832}\n",
      "{'seed': 456, 'mean_val_acc': 0.8287058823529412, 'time': 27.797683000564575, 'memory': 0.279138, 'gpu_memory': 715.128832}\n",
      "{'seed': 789, 'mean_val_acc': 0.8205490196078431, 'time': 27.251060962677002, 'memory': 0.280302, 'gpu_memory': 715.128832}\n",
      "{'seed': 101, 'mean_val_acc': 0.8204705882352942, 'time': 26.300060272216797, 'memory': 0.278916, 'gpu_memory': 715.128832}\n",
      "Total Mean Val Acc: 0.82\n",
      "Standard Deviation: 0.01\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import WebKB\n",
    "dataset_sparse = WebKB(root=\"/data/Zeyu/Pooling\", name='wisconsin')\n",
    "dataset_sparse = dataset_sparse.shuffle()\n",
    "print(dataset_sparse[0])\n",
    "\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(64, k=0, aggr_x='sum'))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_KMIS(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# \n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[37700, 128], edge_index=[2, 578006], y=[37700])\n",
      "Seed 42, Fold 1 Val Acc: 0.867\n",
      "Seed 42, Fold 2 Val Acc: 0.880\n",
      "Seed 42, Fold 3 Val Acc: 0.866\n",
      "Seed 42, Fold 4 Val Acc: 0.871\n",
      "Seed 42, Fold 5 Val Acc: 0.873\n",
      "Seed 42 Results: Mean Val Acc: 0.871, Time: 70.116 seconds, Memory: 1.435 MB, GPU Memory: 719.323 MB\n",
      "Seed 123, Fold 1 Val Acc: 0.871\n",
      "Seed 123, Fold 2 Val Acc: 0.867\n",
      "Seed 123, Fold 3 Val Acc: 0.869\n",
      "Seed 123, Fold 4 Val Acc: 0.873\n",
      "Seed 123, Fold 5 Val Acc: 0.874\n",
      "Seed 123 Results: Mean Val Acc: 0.870, Time: 67.111 seconds, Memory: 1.411 MB, GPU Memory: 719.323 MB\n",
      "Seed 456, Fold 1 Val Acc: 0.866\n",
      "Seed 456, Fold 2 Val Acc: 0.878\n",
      "Seed 456, Fold 3 Val Acc: 0.868\n",
      "Seed 456, Fold 4 Val Acc: 0.872\n",
      "Seed 456, Fold 5 Val Acc: 0.870\n",
      "Seed 456 Results: Mean Val Acc: 0.871, Time: 67.856 seconds, Memory: 1.413 MB, GPU Memory: 719.323 MB\n",
      "Seed 789, Fold 1 Val Acc: 0.869\n",
      "Seed 789, Fold 2 Val Acc: 0.874\n",
      "Seed 789, Fold 3 Val Acc: 0.870\n",
      "Seed 789, Fold 4 Val Acc: 0.869\n",
      "Seed 789, Fold 5 Val Acc: 0.869\n",
      "Seed 789 Results: Mean Val Acc: 0.870, Time: 67.959 seconds, Memory: 1.414 MB, GPU Memory: 719.323 MB\n",
      "Seed 101, Fold 1 Val Acc: 0.868\n",
      "Seed 101, Fold 2 Val Acc: 0.875\n",
      "Seed 101, Fold 3 Val Acc: 0.873\n",
      "Seed 101, Fold 4 Val Acc: 0.869\n",
      "Seed 101, Fold 5 Val Acc: 0.873\n",
      "Seed 101 Results: Mean Val Acc: 0.872, Time: 68.863 seconds, Memory: 1.410 MB, GPU Memory: 719.323 MB\n",
      "{'seed': 42, 'mean_val_acc': 0.8711671087533157, 'time': 70.11639523506165, 'memory': 1.434503, 'gpu_memory': 719.323136}\n",
      "{'seed': 123, 'mean_val_acc': 0.8704509283819629, 'time': 67.11127924919128, 'memory': 1.411444, 'gpu_memory': 719.323136}\n",
      "{'seed': 456, 'mean_val_acc': 0.8707957559681697, 'time': 67.85648345947266, 'memory': 1.412676, 'gpu_memory': 719.323136}\n",
      "{'seed': 789, 'mean_val_acc': 0.8703713527851459, 'time': 67.95878767967224, 'memory': 1.414175, 'gpu_memory': 719.323136}\n",
      "{'seed': 101, 'mean_val_acc': 0.8715119363395225, 'time': 68.86349749565125, 'memory': 1.41012, 'gpu_memory': 719.323136}\n",
      "Total Mean Val Acc: 0.87\n",
      "Standard Deviation: 0.00\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import GitHub\n",
    "dataset_sparse = GitHub(root=\"/data/Zeyu/Pooling/GitHub\")\n",
    "dataset_sparse = dataset_sparse.shuffle()\n",
    "print(dataset_sparse[0])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(64, k=6, aggr_x='sum'))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_KMIS(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# \n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to featurize datapoint 0, C. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[22:06:02] WARNING: not removing hydrogen atom without neighbors\n",
      "Failed to featurize datapoint 5505, [H]. Appending empty array\n",
      "Exception message: More than one atom should be present in the molecule for this featurizer to work.\n",
      "[22:06:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:06:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:06:03] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:06:03] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:06:03] WARNING: not removing hydrogen atom without neighbors\n",
      "[22:06:03] WARNING: not removing hydrogen atom without neighbors\n",
      "Exception message: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6834,) + inhomogeneous part.\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from deepchem.feat import MolGraphConvFeaturizer\n",
    "from deepchem.data import NumpyDataset\n",
    "\n",
    "# QM7\n",
    "qm7_dataset = \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/datasets/qm7.csv\"\n",
    "data = pd.read_csv(qm7_dataset)\n",
    "\n",
    "# SMILES\n",
    "smiles = data['smiles'].values\n",
    "\n",
    "# DeepChemMolGraphConvFeaturizerSMILES\n",
    "featurizer = MolGraphConvFeaturizer()\n",
    "mols = featurizer.featurize(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of nodes: 6.79\n",
      "Average number of edges: 6.44\n",
      "Average clustering coefficient: 0.06\n",
      "Average diameter: 4.21\n",
      "Average degree: 1.89\n"
     ]
    }
   ],
   "source": [
    "def graphdata_to_nx(graph_data):\n",
    "    G = nx.Graph()\n",
    "    num_nodes = graph_data.node_features.shape[0]\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    for i in range(graph_data.edge_index.shape[1]):\n",
    "        source = graph_data.edge_index[0, i]\n",
    "        target = graph_data.edge_index[1, i]\n",
    "        G.add_edge(source, target)\n",
    "    return G\n",
    "\n",
    "# \n",
    "total_nodes = 0\n",
    "total_edges = 0\n",
    "total_clustering_coefficient = 0\n",
    "total_diameter = 0\n",
    "total_degree = 0\n",
    "connected_graph_count = 0\n",
    "total_graph_count = 0\n",
    "\n",
    "# \n",
    "for i, mol in enumerate(mols):\n",
    "    if isinstance(mol, dc.feat.graph_data.GraphData):  #  mol  GraphData \n",
    "        if mol.node_features.size == 0:  # \n",
    "            continue\n",
    "        G = graphdata_to_nx(mol)\n",
    "        num_nodes = G.number_of_nodes()\n",
    "        num_edges = G.number_of_edges()\n",
    "        \n",
    "        total_nodes += num_nodes\n",
    "        total_edges += num_edges\n",
    "        total_clustering_coefficient += nx.average_clustering(G)\n",
    "        total_degree += sum(dict(G.degree()).values()) / num_nodes  # \n",
    "        \n",
    "        if nx.is_connected(G):\n",
    "            total_diameter += nx.diameter(G)\n",
    "            connected_graph_count += 1  # \n",
    "        total_graph_count += 1\n",
    "\n",
    "if total_graph_count > 0:\n",
    "    avg_nodes = total_nodes / total_graph_count\n",
    "    avg_edges = total_edges / total_graph_count\n",
    "    avg_clustering_coefficient = total_clustering_coefficient / total_graph_count\n",
    "    avg_diameter = total_diameter / connected_graph_count if connected_graph_count > 0 else float('nan')\n",
    "    avg_degree = total_degree / total_graph_count\n",
    "\n",
    "    # \n",
    "    print(f\"Average number of nodes: {avg_nodes:.2f}\")\n",
    "    print(f\"Average number of edges: {avg_edges:.2f}\")\n",
    "    print(f\"Average clustering coefficient: {avg_clustering_coefficient:.2f}\")\n",
    "    print(f\"Average diameter: {avg_diameter:.2f}\")\n",
    "    print(f\"Average degree: {avg_degree:.2f}\")\n",
    "else:\n",
    "    print(\"No valid graphs found in the dataset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CG-ODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
