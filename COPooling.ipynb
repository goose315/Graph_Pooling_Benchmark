{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "class GPR_prop(MessagePassing):\n",
    "    '''\n",
    "    propagation class for GPR_GNN\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, alpha, Init, Gamma=None, bias=True, **kwargs):\n",
    "        super(GPR_prop, self).__init__(aggr='add', **kwargs)\n",
    "        self.K = K\n",
    "        self.Init = Init\n",
    "        self.alpha = alpha\n",
    "\n",
    "        assert Init in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']\n",
    "        if Init == 'SGC':\n",
    "            # SGC-like\n",
    "            TEMP = 0.0*np.ones(K+1)\n",
    "            TEMP[alpha] = 1.0\n",
    "        elif Init == 'PPR':\n",
    "            # PPR-like\n",
    "            TEMP = alpha*(1-alpha)**np.arange(K+1)\n",
    "            TEMP[-1] = (1-alpha)**K\n",
    "        elif Init == 'NPPR':\n",
    "            # Negative PPR\n",
    "            TEMP = (alpha)**np.arange(K+1)\n",
    "            TEMP = TEMP/np.sum(np.abs(TEMP))\n",
    "        elif Init == 'Random':\n",
    "            # Random\n",
    "            bound = np.sqrt(3/(K+1))\n",
    "            TEMP = np.random.uniform(-bound, bound, K+1)\n",
    "            TEMP = TEMP/np.sum(np.abs(TEMP))\n",
    "        elif Init == 'WS':\n",
    "            # Specify Gamma\n",
    "            TEMP = Gamma\n",
    "\n",
    "        self.temp = Parameter(torch.tensor(TEMP))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.zeros_(self.temp)\n",
    "        for k in range(self.K+1):\n",
    "            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n",
    "        self.temp.data[-1] = (1-self.alpha)**self.K\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        edge_index, norm = gcn_norm(\n",
    "            edge_index, edge_weight, num_nodes=x.size(0), dtype=x.dtype)\n",
    "\n",
    "        hidden = x*(self.temp[0])\n",
    "        for k in range(self.K):\n",
    "            x = self.propagate(edge_index, x=x, norm=norm)\n",
    "            gamma = self.temp[k+1]\n",
    "            hidden = hidden + gamma*x\n",
    "        return hidden\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n",
    "                                           self.temp)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) # in case all the edges are removed\n",
    "\n",
    "        edge_index = edge_index.type(torch.long)\n",
    "        row, col = edge_index\n",
    "        # print(row, col)\n",
    "        # print(edge_weight.shape, row.shape, num_nodes)\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        # row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "class graph_attention(torch.nn.Module):\n",
    "    # reference: https://github.com/gordicaleksa/pytorch-GAT/blob/39c8f0ee634477033e8b1a6e9a6da3c7ed71bbd1/models/definitions/GAT.py#L324\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    nodes_dim = 0      # node dimension/axis\n",
    "    head_dim = 1       # attention head dimension/axis\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, dropout_prob=0.6, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Saving these as we'll need them in forward propagation in children layers (imp1/2/3)\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        # You can treat this one matrix as num_of_heads independent W matrices\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the additive scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
    "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
    "        Feel free to experiment - there may be better initializations depending on your problem.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features = x  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = scores_source_lifted + scores_target_lifted\n",
    "\n",
    "        return torch.sigmoid(scores_per_edge)\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "\n",
    "\n",
    "class CoPooling(torch.nn.Module):\n",
    "    # reference for GAT code: https://github.com/PetarV-/GAT\n",
    "    # reference for generalized pagerank code: https://github.com/jianhao2016/GPRGNN\n",
    "    def __init__(self, ratio=0.5, K=0.05, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=None):\n",
    "        super(CoPooling, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "        self.edge_ratio = edge_ratio\n",
    "\n",
    "        self.prop1 = GPR_prop(K, alpha, Init, Gamma)\n",
    "\n",
    "        score_dim = 32\n",
    "        self.G_att = graph_attention(num_in_features=nhid, num_out_features=score_dim, num_of_heads=1)\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(2*nhid, nhid))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        self.bias = Parameter(torch.Tensor(nhid))\n",
    "        nn.init.zeros_(self.bias.data)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        nn.init.zeros_(self.bias.data)\n",
    "        self.prop1.reset_parameters()\n",
    "        self.G_att.init_params()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None, nodes_index=None, node_attr=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        ori_batch = batch.clone()\n",
    "        device = x.device\n",
    "        num_nodes = x.shape[0]\n",
    "\n",
    "        # cut edges based on scores\n",
    "        x_cut = self.prop1(x, edge_index) # run generalized pagerank to update features\n",
    "\n",
    "        attention = self.G_att(x_cut, edge_index) # get the attention weights after sigmoid\n",
    "        attention = attention.sum(dim=1) #sum the weights on head dim\n",
    "        edge_index, attention = add_self_loops(edge_index, attention, 1.0, num_nodes) # add self loops in case no edges\n",
    "\n",
    "        # to get a systemitic adj matrix\n",
    "        edge_index_t, attention_t = transpose(edge_index, attention, num_nodes, num_nodes)\n",
    "        edge_tmp = torch.cat((edge_index, edge_index_t), 1)\n",
    "        att_tmp = torch.cat((attention, attention_t),0)\n",
    "        edge_index, attention = coalesce(edge_tmp, att_tmp, num_nodes, num_nodes, 'mean')\n",
    "\n",
    "        attention_np = attention.cpu().data.numpy()\n",
    "        cut_val = np.percentile(attention_np, int(100*(1-self.edge_ratio))) # this is for keep the top edge_ratio edges\n",
    "        attention = attention * (attention >= cut_val) # keep the edge_ratio higher weights of edges\n",
    "\n",
    "        kep_idx = attention > 0.0\n",
    "        cut_edge_index, cut_edge_attr = edge_index[:, kep_idx], attention[kep_idx]\n",
    "\n",
    "        # Graph Pooling based on nodes\n",
    "        x_information_score = self.calc_information_score(x, cut_edge_index, cut_edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x_topk = x[perm]\n",
    "        batch = batch[perm]\n",
    "        if nodes_index is not None:\n",
    "            nodes_index = nodes_index[perm]\n",
    "\n",
    "        if node_attr is not None:\n",
    "            node_attr = node_attr[perm]\n",
    "        if cut_edge_index is not None or cut_edge_index.nelement() != 0:\n",
    "            induced_edge_index, induced_edge_attr = filter_adj(cut_edge_index, cut_edge_attr, perm, num_nodes=num_nodes)\n",
    "        else:\n",
    "            print('All edges are cut!')\n",
    "            induced_edge_index, induced_edge_attr = cut_edge_index, cut_edge_attr\n",
    "\n",
    "        # update node features\n",
    "        attention_dense = (to_dense_adj(cut_edge_index, edge_attr=cut_edge_attr, max_num_nodes=num_nodes)).squeeze()\n",
    "        x = F.relu(torch.matmul(torch.cat((x_topk, torch.matmul(attention_dense[perm],x)), 1), self.weight) + self.bias)\n",
    "\n",
    "        return x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 162 for seed 43\n",
      "Average Time: 10.02 seconds\n",
      "Var Time: 0.98 seconds\n",
      "Average Memory: 122.00 MB\n",
      "Average Best Val Acc: 0.8571\n",
      "Std Best Test Acc: 0.0325\n",
      "Average Test Acc: 0.8391\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 150\n",
    "data_path = \"/data/XXX/Pooling/1\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"MUTAG\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_CO(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_CO, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = CoPooling(ratio=0.9, K=1, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=1.0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = CoPooling(ratio=0.9, K=1, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=1.0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, perm, _, batch, _, _, _ = self.pool1(x, edge_index, edge_attr=None, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, perm, _, batch, _, _, _ = self.pool2(x, edge_index, edge_attr=None, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 0.7131, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 002, Loss: 0.7110, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 003, Loss: 0.7093, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 004, Loss: 0.7073, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 005, Loss: 0.7052, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 006, Loss: 0.7036, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 007, Loss: 0.7020, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 008, Loss: 0.7004, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 009, Loss: 0.6986, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 010, Loss: 0.6968, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 011, Loss: 0.6949, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 012, Loss: 0.6928, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 013, Loss: 0.6914, Val Acc: 0.4505, Test Acc: 0.4414\n",
      "Seed: 42, Epoch: 014, Loss: 0.6893, Val Acc: 0.4595, Test Acc: 0.4685\n",
      "Seed: 42, Epoch: 015, Loss: 0.6874, Val Acc: 0.5586, Test Acc: 0.6216\n",
      "Seed: 42, Epoch: 016, Loss: 0.6874, Val Acc: 0.5766, Test Acc: 0.6216\n",
      "Seed: 42, Epoch: 017, Loss: 0.6858, Val Acc: 0.5946, Test Acc: 0.6396\n",
      "Seed: 42, Epoch: 018, Loss: 0.6832, Val Acc: 0.6306, Test Acc: 0.6396\n",
      "Seed: 42, Epoch: 019, Loss: 0.6803, Val Acc: 0.6126, Test Acc: 0.6216\n",
      "Seed: 42, Epoch: 020, Loss: 0.6789, Val Acc: 0.6396, Test Acc: 0.6216\n",
      "Seed: 42, Epoch: 021, Loss: 0.6765, Val Acc: 0.6577, Test Acc: 0.6577\n",
      "Seed: 42, Epoch: 022, Loss: 0.6714, Val Acc: 0.5946, Test Acc: 0.6036\n",
      "Seed: 42, Epoch: 023, Loss: 0.6708, Val Acc: 0.6036, Test Acc: 0.5586\n",
      "Seed: 42, Epoch: 024, Loss: 0.6706, Val Acc: 0.5856, Test Acc: 0.5676\n",
      "Seed: 42, Epoch: 025, Loss: 0.6670, Val Acc: 0.6036, Test Acc: 0.6306\n",
      "Seed: 42, Epoch: 026, Loss: 0.6634, Val Acc: 0.6126, Test Acc: 0.6306\n",
      "Seed: 42, Epoch: 027, Loss: 0.6609, Val Acc: 0.5856, Test Acc: 0.6216\n",
      "Seed: 42, Epoch: 028, Loss: 0.6553, Val Acc: 0.6577, Test Acc: 0.6937\n",
      "Seed: 42, Epoch: 029, Loss: 0.6514, Val Acc: 0.5946, Test Acc: 0.5676\n",
      "Seed: 42, Epoch: 030, Loss: 0.6642, Val Acc: 0.6306, Test Acc: 0.6216\n",
      "Seed: 42, Epoch: 031, Loss: 0.6451, Val Acc: 0.6577, Test Acc: 0.6306\n",
      "Seed: 42, Epoch: 032, Loss: 0.6419, Val Acc: 0.6667, Test Acc: 0.6396\n",
      "Seed: 42, Epoch: 033, Loss: 0.6423, Val Acc: 0.6667, Test Acc: 0.6577\n",
      "Seed: 42, Epoch: 034, Loss: 0.6419, Val Acc: 0.6667, Test Acc: 0.6486\n",
      "Seed: 42, Epoch: 035, Loss: 0.6389, Val Acc: 0.7027, Test Acc: 0.6396\n",
      "Seed: 42, Epoch: 036, Loss: 0.6345, Val Acc: 0.6577, Test Acc: 0.6036\n",
      "Seed: 42, Epoch: 037, Loss: 0.6308, Val Acc: 0.6847, Test Acc: 0.6036\n",
      "Seed: 42, Epoch: 038, Loss: 0.6252, Val Acc: 0.6847, Test Acc: 0.6757\n",
      "Seed: 42, Epoch: 039, Loss: 0.6223, Val Acc: 0.6937, Test Acc: 0.6396\n",
      "Seed: 42, Epoch: 040, Loss: 0.6190, Val Acc: 0.6937, Test Acc: 0.6757\n",
      "Seed: 42, Epoch: 041, Loss: 0.6153, Val Acc: 0.7027, Test Acc: 0.6577\n",
      "Seed: 42, Epoch: 042, Loss: 0.6163, Val Acc: 0.7117, Test Acc: 0.6847\n",
      "Seed: 42, Epoch: 043, Loss: 0.6159, Val Acc: 0.6577, Test Acc: 0.5946\n",
      "Seed: 42, Epoch: 044, Loss: 0.6251, Val Acc: 0.6306, Test Acc: 0.5946\n",
      "Seed: 42, Epoch: 045, Loss: 0.6285, Val Acc: 0.7477, Test Acc: 0.6757\n",
      "Seed: 42, Epoch: 046, Loss: 0.5984, Val Acc: 0.6667, Test Acc: 0.6396\n",
      "Seed: 42, Epoch: 047, Loss: 0.6180, Val Acc: 0.7027, Test Acc: 0.6577\n",
      "Seed: 42, Epoch: 048, Loss: 0.6101, Val Acc: 0.7477, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 049, Loss: 0.6021, Val Acc: 0.7117, Test Acc: 0.6757\n",
      "Seed: 42, Epoch: 050, Loss: 0.6034, Val Acc: 0.6667, Test Acc: 0.6216\n",
      "Seed: 42, Epoch: 051, Loss: 0.6197, Val Acc: 0.6667, Test Acc: 0.5946\n",
      "Seed: 42, Epoch: 052, Loss: 0.6177, Val Acc: 0.7207, Test Acc: 0.6577\n",
      "Seed: 42, Epoch: 053, Loss: 0.5990, Val Acc: 0.7297, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 054, Loss: 0.5923, Val Acc: 0.7568, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 055, Loss: 0.5884, Val Acc: 0.7748, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 056, Loss: 0.5854, Val Acc: 0.7568, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 057, Loss: 0.5827, Val Acc: 0.7297, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 058, Loss: 0.5782, Val Acc: 0.7568, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 059, Loss: 0.5796, Val Acc: 0.7387, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 060, Loss: 0.5761, Val Acc: 0.7477, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 061, Loss: 0.5723, Val Acc: 0.7297, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 062, Loss: 0.5768, Val Acc: 0.7117, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 063, Loss: 0.5710, Val Acc: 0.7658, Test Acc: 0.6937\n",
      "Seed: 42, Epoch: 064, Loss: 0.5655, Val Acc: 0.7477, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 065, Loss: 0.5642, Val Acc: 0.6847, Test Acc: 0.6937\n",
      "Seed: 42, Epoch: 066, Loss: 0.5954, Val Acc: 0.6486, Test Acc: 0.6306\n",
      "Seed: 42, Epoch: 067, Loss: 0.6193, Val Acc: 0.7297, Test Acc: 0.6937\n",
      "Seed: 42, Epoch: 068, Loss: 0.5724, Val Acc: 0.7387, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 069, Loss: 0.5627, Val Acc: 0.7477, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 070, Loss: 0.5593, Val Acc: 0.7658, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 071, Loss: 0.5627, Val Acc: 0.6847, Test Acc: 0.6667\n",
      "Seed: 42, Epoch: 072, Loss: 0.5867, Val Acc: 0.6937, Test Acc: 0.6757\n",
      "Seed: 42, Epoch: 073, Loss: 0.5762, Val Acc: 0.7568, Test Acc: 0.7568\n",
      "Seed: 42, Epoch: 074, Loss: 0.5575, Val Acc: 0.7477, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 075, Loss: 0.5634, Val Acc: 0.7207, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 076, Loss: 0.5721, Val Acc: 0.7297, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 077, Loss: 0.5596, Val Acc: 0.7658, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 078, Loss: 0.5498, Val Acc: 0.7658, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 079, Loss: 0.5549, Val Acc: 0.7568, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 080, Loss: 0.5514, Val Acc: 0.7658, Test Acc: 0.7568\n",
      "Seed: 42, Epoch: 081, Loss: 0.5463, Val Acc: 0.7838, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 082, Loss: 0.5441, Val Acc: 0.7387, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 083, Loss: 0.5521, Val Acc: 0.7387, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 084, Loss: 0.5467, Val Acc: 0.7658, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 085, Loss: 0.5438, Val Acc: 0.7568, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 086, Loss: 0.5406, Val Acc: 0.7748, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 087, Loss: 0.5400, Val Acc: 0.7568, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 088, Loss: 0.5379, Val Acc: 0.7568, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 089, Loss: 0.5403, Val Acc: 0.7568, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 090, Loss: 0.5363, Val Acc: 0.7477, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 091, Loss: 0.5345, Val Acc: 0.7387, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 092, Loss: 0.5479, Val Acc: 0.7477, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 093, Loss: 0.5383, Val Acc: 0.7658, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 094, Loss: 0.5260, Val Acc: 0.7297, Test Acc: 0.6937\n",
      "Seed: 42, Epoch: 095, Loss: 0.5457, Val Acc: 0.7387, Test Acc: 0.6937\n",
      "Seed: 42, Epoch: 096, Loss: 0.5383, Val Acc: 0.7387, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 097, Loss: 0.5294, Val Acc: 0.6937, Test Acc: 0.6757\n",
      "Seed: 42, Epoch: 098, Loss: 0.5654, Val Acc: 0.7027, Test Acc: 0.6757\n",
      "Seed: 42, Epoch: 099, Loss: 0.5483, Val Acc: 0.7568, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 100, Loss: 0.5281, Val Acc: 0.7568, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 101, Loss: 0.5264, Val Acc: 0.7748, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 102, Loss: 0.5328, Val Acc: 0.7477, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 103, Loss: 0.5324, Val Acc: 0.7748, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 104, Loss: 0.5333, Val Acc: 0.7207, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 105, Loss: 0.5377, Val Acc: 0.7658, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 106, Loss: 0.5311, Val Acc: 0.7838, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 107, Loss: 0.5289, Val Acc: 0.7838, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 108, Loss: 0.5276, Val Acc: 0.7658, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 109, Loss: 0.5272, Val Acc: 0.7568, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 110, Loss: 0.5235, Val Acc: 0.7568, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 111, Loss: 0.5374, Val Acc: 0.6847, Test Acc: 0.6847\n",
      "Seed: 42, Epoch: 112, Loss: 0.5558, Val Acc: 0.7027, Test Acc: 0.6937\n",
      "Seed: 42, Epoch: 113, Loss: 0.5478, Val Acc: 0.7568, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 114, Loss: 0.5253, Val Acc: 0.7387, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 115, Loss: 0.5412, Val Acc: 0.7027, Test Acc: 0.6667\n",
      "Seed: 42, Epoch: 116, Loss: 0.5496, Val Acc: 0.7297, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 117, Loss: 0.5294, Val Acc: 0.7748, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 118, Loss: 0.5195, Val Acc: 0.7748, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 119, Loss: 0.5177, Val Acc: 0.7568, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 120, Loss: 0.5161, Val Acc: 0.7748, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 121, Loss: 0.5136, Val Acc: 0.7748, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 122, Loss: 0.5119, Val Acc: 0.7838, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 123, Loss: 0.5103, Val Acc: 0.7477, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 124, Loss: 0.5116, Val Acc: 0.7387, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 125, Loss: 0.5119, Val Acc: 0.7477, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 126, Loss: 0.5177, Val Acc: 0.7477, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 127, Loss: 0.5100, Val Acc: 0.7838, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 128, Loss: 0.5092, Val Acc: 0.7658, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 129, Loss: 0.5102, Val Acc: 0.7748, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 130, Loss: 0.5017, Val Acc: 0.7477, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 131, Loss: 0.5179, Val Acc: 0.7477, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 132, Loss: 0.5029, Val Acc: 0.8018, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 133, Loss: 0.5045, Val Acc: 0.7477, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 134, Loss: 0.5128, Val Acc: 0.7838, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 135, Loss: 0.4986, Val Acc: 0.7387, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 136, Loss: 0.4991, Val Acc: 0.7568, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 137, Loss: 0.5000, Val Acc: 0.7658, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 138, Loss: 0.5021, Val Acc: 0.7838, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 139, Loss: 0.4959, Val Acc: 0.7477, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 140, Loss: 0.5085, Val Acc: 0.7477, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 141, Loss: 0.5093, Val Acc: 0.7568, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 142, Loss: 0.5034, Val Acc: 0.7477, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 143, Loss: 0.5066, Val Acc: 0.7387, Test Acc: 0.7568\n",
      "Seed: 42, Epoch: 144, Loss: 0.4931, Val Acc: 0.7568, Test Acc: 0.7568\n",
      "Seed: 42, Epoch: 145, Loss: 0.4975, Val Acc: 0.7658, Test Acc: 0.7748\n",
      "Seed: 42, Epoch: 146, Loss: 0.4861, Val Acc: 0.6937, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 147, Loss: 0.5660, Val Acc: 0.7117, Test Acc: 0.6847\n",
      "Seed: 42, Epoch: 148, Loss: 0.5504, Val Acc: 0.7748, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 149, Loss: 0.4901, Val Acc: 0.7027, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 150, Loss: 0.5502, Val Acc: 0.6577, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 151, Loss: 0.5386, Val Acc: 0.7658, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 152, Loss: 0.4918, Val Acc: 0.7748, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 153, Loss: 0.4955, Val Acc: 0.7748, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 154, Loss: 0.4986, Val Acc: 0.7477, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 155, Loss: 0.4994, Val Acc: 0.7748, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 156, Loss: 0.4999, Val Acc: 0.7748, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 157, Loss: 0.4975, Val Acc: 0.7838, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 158, Loss: 0.5080, Val Acc: 0.7748, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 159, Loss: 0.4985, Val Acc: 0.7568, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 160, Loss: 0.5010, Val Acc: 0.7477, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 161, Loss: 0.5228, Val Acc: 0.7477, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 162, Loss: 0.5167, Val Acc: 0.7748, Test Acc: 0.7568\n",
      "Seed: 42, Epoch: 163, Loss: 0.4988, Val Acc: 0.7838, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 164, Loss: 0.4942, Val Acc: 0.7658, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 165, Loss: 0.4923, Val Acc: 0.7748, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 166, Loss: 0.4923, Val Acc: 0.7838, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 167, Loss: 0.4993, Val Acc: 0.7838, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 168, Loss: 0.4888, Val Acc: 0.7838, Test Acc: 0.7568\n",
      "Seed: 42, Epoch: 169, Loss: 0.4839, Val Acc: 0.7748, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 170, Loss: 0.4861, Val Acc: 0.7748, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 171, Loss: 0.4837, Val Acc: 0.7658, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 172, Loss: 0.4982, Val Acc: 0.7477, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 173, Loss: 0.5125, Val Acc: 0.7477, Test Acc: 0.7568\n",
      "Seed: 42, Epoch: 174, Loss: 0.4958, Val Acc: 0.7838, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 175, Loss: 0.4851, Val Acc: 0.7568, Test Acc: 0.6937\n",
      "Seed: 42, Epoch: 176, Loss: 0.4853, Val Acc: 0.7568, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 177, Loss: 0.4846, Val Acc: 0.7568, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 178, Loss: 0.4816, Val Acc: 0.7748, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 179, Loss: 0.4839, Val Acc: 0.7748, Test Acc: 0.7658\n",
      "Seed: 42, Epoch: 180, Loss: 0.4915, Val Acc: 0.7477, Test Acc: 0.7658\n",
      "Seed: 42, Epoch: 181, Loss: 0.5020, Val Acc: 0.7568, Test Acc: 0.7658\n",
      "Seed: 42, Epoch: 182, Loss: 0.4959, Val Acc: 0.7748, Test Acc: 0.7658\n",
      "Seed: 42, Epoch: 183, Loss: 0.4836, Val Acc: 0.7568, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 184, Loss: 0.4765, Val Acc: 0.7568, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 185, Loss: 0.4747, Val Acc: 0.7658, Test Acc: 0.7297\n",
      "Seed: 42, Epoch: 186, Loss: 0.4748, Val Acc: 0.7568, Test Acc: 0.7658\n",
      "Seed: 42, Epoch: 187, Loss: 0.4786, Val Acc: 0.7477, Test Acc: 0.6937\n",
      "Seed: 42, Epoch: 188, Loss: 0.4807, Val Acc: 0.7297, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 189, Loss: 0.5110, Val Acc: 0.7297, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 190, Loss: 0.5051, Val Acc: 0.7658, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 191, Loss: 0.4694, Val Acc: 0.7568, Test Acc: 0.7658\n",
      "Seed: 42, Epoch: 192, Loss: 0.4717, Val Acc: 0.7658, Test Acc: 0.7748\n",
      "Seed: 42, Epoch: 193, Loss: 0.4688, Val Acc: 0.7748, Test Acc: 0.7117\n",
      "Seed: 42, Epoch: 194, Loss: 0.4749, Val Acc: 0.7658, Test Acc: 0.6937\n",
      "Seed: 42, Epoch: 195, Loss: 0.4713, Val Acc: 0.7748, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 196, Loss: 0.4765, Val Acc: 0.7477, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 197, Loss: 0.4994, Val Acc: 0.7477, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 198, Loss: 0.4948, Val Acc: 0.7838, Test Acc: 0.7477\n",
      "Seed: 42, Epoch: 199, Loss: 0.4676, Val Acc: 0.7658, Test Acc: 0.7207\n",
      "Seed: 42, Epoch: 200, Loss: 0.4789, Val Acc: 0.7658, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 001, Loss: 0.6972, Val Acc: 0.4595, Test Acc: 0.3874\n",
      "Seed: 43, Epoch: 002, Loss: 0.6961, Val Acc: 0.4595, Test Acc: 0.3874\n",
      "Seed: 43, Epoch: 003, Loss: 0.6950, Val Acc: 0.4595, Test Acc: 0.3874\n",
      "Seed: 43, Epoch: 004, Loss: 0.6939, Val Acc: 0.4595, Test Acc: 0.3874\n",
      "Seed: 43, Epoch: 005, Loss: 0.6923, Val Acc: 0.4595, Test Acc: 0.3874\n",
      "Seed: 43, Epoch: 006, Loss: 0.6911, Val Acc: 0.4595, Test Acc: 0.3874\n",
      "Seed: 43, Epoch: 007, Loss: 0.6902, Val Acc: 0.5225, Test Acc: 0.4685\n",
      "Seed: 43, Epoch: 008, Loss: 0.6888, Val Acc: 0.6306, Test Acc: 0.5315\n",
      "Seed: 43, Epoch: 009, Loss: 0.6868, Val Acc: 0.6216, Test Acc: 0.5225\n",
      "Seed: 43, Epoch: 010, Loss: 0.6846, Val Acc: 0.5315, Test Acc: 0.4955\n",
      "Seed: 43, Epoch: 011, Loss: 0.6828, Val Acc: 0.4955, Test Acc: 0.4955\n",
      "Seed: 43, Epoch: 012, Loss: 0.6807, Val Acc: 0.4865, Test Acc: 0.4775\n",
      "Seed: 43, Epoch: 013, Loss: 0.6783, Val Acc: 0.4685, Test Acc: 0.4595\n",
      "Seed: 43, Epoch: 014, Loss: 0.6776, Val Acc: 0.4685, Test Acc: 0.4685\n",
      "Seed: 43, Epoch: 015, Loss: 0.6748, Val Acc: 0.5315, Test Acc: 0.4955\n",
      "Seed: 43, Epoch: 016, Loss: 0.6681, Val Acc: 0.6306, Test Acc: 0.5856\n",
      "Seed: 43, Epoch: 017, Loss: 0.6645, Val Acc: 0.6486, Test Acc: 0.6126\n",
      "Seed: 43, Epoch: 018, Loss: 0.6625, Val Acc: 0.6396, Test Acc: 0.6396\n",
      "Seed: 43, Epoch: 019, Loss: 0.6609, Val Acc: 0.6486, Test Acc: 0.6396\n",
      "Seed: 43, Epoch: 020, Loss: 0.6567, Val Acc: 0.6486, Test Acc: 0.6216\n",
      "Seed: 43, Epoch: 021, Loss: 0.6518, Val Acc: 0.6667, Test Acc: 0.6396\n",
      "Seed: 43, Epoch: 022, Loss: 0.6473, Val Acc: 0.6757, Test Acc: 0.6396\n",
      "Seed: 43, Epoch: 023, Loss: 0.6424, Val Acc: 0.6667, Test Acc: 0.6306\n",
      "Seed: 43, Epoch: 024, Loss: 0.6378, Val Acc: 0.6396, Test Acc: 0.6216\n",
      "Seed: 43, Epoch: 025, Loss: 0.6368, Val Acc: 0.6036, Test Acc: 0.6577\n",
      "Seed: 43, Epoch: 026, Loss: 0.6308, Val Acc: 0.6577, Test Acc: 0.6306\n",
      "Seed: 43, Epoch: 027, Loss: 0.6293, Val Acc: 0.6667, Test Acc: 0.7568\n",
      "Seed: 43, Epoch: 028, Loss: 0.6343, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 029, Loss: 0.6269, Val Acc: 0.6577, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 030, Loss: 0.6189, Val Acc: 0.6577, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 031, Loss: 0.6154, Val Acc: 0.6577, Test Acc: 0.6667\n",
      "Seed: 43, Epoch: 032, Loss: 0.6098, Val Acc: 0.6396, Test Acc: 0.6486\n",
      "Seed: 43, Epoch: 033, Loss: 0.6085, Val Acc: 0.6486, Test Acc: 0.6306\n",
      "Seed: 43, Epoch: 034, Loss: 0.6080, Val Acc: 0.6396, Test Acc: 0.6306\n",
      "Seed: 43, Epoch: 035, Loss: 0.6099, Val Acc: 0.6036, Test Acc: 0.6577\n",
      "Seed: 43, Epoch: 036, Loss: 0.6050, Val Acc: 0.6847, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 037, Loss: 0.5975, Val Acc: 0.5946, Test Acc: 0.7297\n",
      "Seed: 43, Epoch: 038, Loss: 0.6110, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 039, Loss: 0.5859, Val Acc: 0.6036, Test Acc: 0.6306\n",
      "Seed: 43, Epoch: 040, Loss: 0.6096, Val Acc: 0.6036, Test Acc: 0.6396\n",
      "Seed: 43, Epoch: 041, Loss: 0.5945, Val Acc: 0.6757, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 042, Loss: 0.5870, Val Acc: 0.5946, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 043, Loss: 0.6086, Val Acc: 0.6036, Test Acc: 0.7297\n",
      "Seed: 43, Epoch: 044, Loss: 0.6075, Val Acc: 0.6036, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 045, Loss: 0.6035, Val Acc: 0.6216, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 046, Loss: 0.5956, Val Acc: 0.6757, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 047, Loss: 0.5783, Val Acc: 0.6577, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 048, Loss: 0.5738, Val Acc: 0.6036, Test Acc: 0.6486\n",
      "Seed: 43, Epoch: 049, Loss: 0.5960, Val Acc: 0.5946, Test Acc: 0.6306\n",
      "Seed: 43, Epoch: 050, Loss: 0.6067, Val Acc: 0.5946, Test Acc: 0.6486\n",
      "Seed: 43, Epoch: 051, Loss: 0.5910, Val Acc: 0.6486, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 052, Loss: 0.5691, Val Acc: 0.6577, Test Acc: 0.7658\n",
      "Seed: 43, Epoch: 053, Loss: 0.5831, Val Acc: 0.6036, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 054, Loss: 0.6062, Val Acc: 0.5495, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 055, Loss: 0.6204, Val Acc: 0.5766, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 056, Loss: 0.6067, Val Acc: 0.6396, Test Acc: 0.7748\n",
      "Seed: 43, Epoch: 057, Loss: 0.5799, Val Acc: 0.6937, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 058, Loss: 0.5669, Val Acc: 0.6486, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 059, Loss: 0.5692, Val Acc: 0.6306, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 060, Loss: 0.5718, Val Acc: 0.6486, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 061, Loss: 0.5639, Val Acc: 0.6937, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 062, Loss: 0.5623, Val Acc: 0.6306, Test Acc: 0.7658\n",
      "Seed: 43, Epoch: 063, Loss: 0.5707, Val Acc: 0.6306, Test Acc: 0.7568\n",
      "Seed: 43, Epoch: 064, Loss: 0.5691, Val Acc: 0.6486, Test Acc: 0.7658\n",
      "Seed: 43, Epoch: 065, Loss: 0.5597, Val Acc: 0.6847, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 066, Loss: 0.5511, Val Acc: 0.6757, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 067, Loss: 0.5507, Val Acc: 0.6306, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 068, Loss: 0.5609, Val Acc: 0.5766, Test Acc: 0.7297\n",
      "Seed: 43, Epoch: 069, Loss: 0.5789, Val Acc: 0.6577, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 070, Loss: 0.5469, Val Acc: 0.6486, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 071, Loss: 0.5451, Val Acc: 0.6396, Test Acc: 0.6757\n",
      "Seed: 43, Epoch: 072, Loss: 0.5510, Val Acc: 0.6486, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 073, Loss: 0.5444, Val Acc: 0.6577, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 074, Loss: 0.5385, Val Acc: 0.6937, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 075, Loss: 0.5460, Val Acc: 0.6216, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 076, Loss: 0.5660, Val Acc: 0.6396, Test Acc: 0.7568\n",
      "Seed: 43, Epoch: 077, Loss: 0.5513, Val Acc: 0.6667, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 078, Loss: 0.5378, Val Acc: 0.6937, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 079, Loss: 0.5348, Val Acc: 0.6847, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 080, Loss: 0.5399, Val Acc: 0.6486, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 081, Loss: 0.5522, Val Acc: 0.6486, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 082, Loss: 0.5509, Val Acc: 0.6667, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 083, Loss: 0.5384, Val Acc: 0.6847, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 084, Loss: 0.5323, Val Acc: 0.6847, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 085, Loss: 0.5350, Val Acc: 0.6577, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 086, Loss: 0.5454, Val Acc: 0.6577, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 087, Loss: 0.5488, Val Acc: 0.6486, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 088, Loss: 0.5343, Val Acc: 0.6937, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 089, Loss: 0.5244, Val Acc: 0.6937, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 090, Loss: 0.5208, Val Acc: 0.6937, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 091, Loss: 0.5232, Val Acc: 0.7117, Test Acc: 0.7297\n",
      "Seed: 43, Epoch: 092, Loss: 0.5174, Val Acc: 0.6937, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 093, Loss: 0.5124, Val Acc: 0.6847, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 094, Loss: 0.5127, Val Acc: 0.6667, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 095, Loss: 0.5211, Val Acc: 0.6306, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 096, Loss: 0.5387, Val Acc: 0.6757, Test Acc: 0.7297\n",
      "Seed: 43, Epoch: 097, Loss: 0.5111, Val Acc: 0.7027, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 098, Loss: 0.5173, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 099, Loss: 0.5093, Val Acc: 0.6757, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 100, Loss: 0.5336, Val Acc: 0.6667, Test Acc: 0.7748\n",
      "Seed: 43, Epoch: 101, Loss: 0.5343, Val Acc: 0.6847, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 102, Loss: 0.5206, Val Acc: 0.6667, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 103, Loss: 0.5246, Val Acc: 0.6847, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 104, Loss: 0.5190, Val Acc: 0.6847, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 105, Loss: 0.5171, Val Acc: 0.6847, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 106, Loss: 0.5190, Val Acc: 0.6667, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 107, Loss: 0.5213, Val Acc: 0.6937, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 108, Loss: 0.5105, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 109, Loss: 0.5001, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 110, Loss: 0.4956, Val Acc: 0.6847, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 111, Loss: 0.4947, Val Acc: 0.6847, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 112, Loss: 0.4934, Val Acc: 0.6847, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 113, Loss: 0.4943, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 114, Loss: 0.4895, Val Acc: 0.6937, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 115, Loss: 0.4874, Val Acc: 0.6937, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 116, Loss: 0.4923, Val Acc: 0.7027, Test Acc: 0.6757\n",
      "Seed: 43, Epoch: 117, Loss: 0.5005, Val Acc: 0.6937, Test Acc: 0.6757\n",
      "Seed: 43, Epoch: 118, Loss: 0.4966, Val Acc: 0.7027, Test Acc: 0.6757\n",
      "Seed: 43, Epoch: 119, Loss: 0.4992, Val Acc: 0.6847, Test Acc: 0.6757\n",
      "Seed: 43, Epoch: 120, Loss: 0.5062, Val Acc: 0.7027, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 121, Loss: 0.4921, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 122, Loss: 0.5066, Val Acc: 0.6937, Test Acc: 0.7568\n",
      "Seed: 43, Epoch: 123, Loss: 0.5264, Val Acc: 0.6667, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 124, Loss: 0.5333, Val Acc: 0.6757, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 125, Loss: 0.5145, Val Acc: 0.6577, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 126, Loss: 0.4942, Val Acc: 0.6757, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 127, Loss: 0.4986, Val Acc: 0.6937, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 128, Loss: 0.4927, Val Acc: 0.6847, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 129, Loss: 0.4879, Val Acc: 0.6847, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 130, Loss: 0.4854, Val Acc: 0.6667, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 131, Loss: 0.4902, Val Acc: 0.6667, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 132, Loss: 0.5119, Val Acc: 0.6577, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 133, Loss: 0.5065, Val Acc: 0.6667, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 134, Loss: 0.4881, Val Acc: 0.6577, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 135, Loss: 0.5008, Val Acc: 0.6486, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 136, Loss: 0.5130, Val Acc: 0.6577, Test Acc: 0.6757\n",
      "Seed: 43, Epoch: 137, Loss: 0.5034, Val Acc: 0.6667, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 138, Loss: 0.4876, Val Acc: 0.6757, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 139, Loss: 0.4805, Val Acc: 0.6757, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 140, Loss: 0.4801, Val Acc: 0.6757, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 141, Loss: 0.4732, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 142, Loss: 0.4795, Val Acc: 0.6667, Test Acc: 0.7297\n",
      "Seed: 43, Epoch: 143, Loss: 0.4952, Val Acc: 0.6757, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 144, Loss: 0.4912, Val Acc: 0.6667, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 145, Loss: 0.4794, Val Acc: 0.6667, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 146, Loss: 0.4642, Val Acc: 0.6937, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 147, Loss: 0.4752, Val Acc: 0.6667, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 148, Loss: 0.4923, Val Acc: 0.6937, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 149, Loss: 0.4721, Val Acc: 0.6667, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 150, Loss: 0.4669, Val Acc: 0.6667, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 151, Loss: 0.4786, Val Acc: 0.6667, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 152, Loss: 0.4567, Val Acc: 0.6937, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 153, Loss: 0.4782, Val Acc: 0.6757, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 154, Loss: 0.4991, Val Acc: 0.6937, Test Acc: 0.6757\n",
      "Seed: 43, Epoch: 155, Loss: 0.4644, Val Acc: 0.6667, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 156, Loss: 0.4661, Val Acc: 0.6847, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 157, Loss: 0.4675, Val Acc: 0.6577, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 158, Loss: 0.4576, Val Acc: 0.6667, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 159, Loss: 0.4559, Val Acc: 0.6847, Test Acc: 0.7568\n",
      "Seed: 43, Epoch: 160, Loss: 0.4675, Val Acc: 0.6937, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 161, Loss: 0.4781, Val Acc: 0.6937, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 162, Loss: 0.4726, Val Acc: 0.6757, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 163, Loss: 0.4538, Val Acc: 0.6757, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 164, Loss: 0.4594, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 165, Loss: 0.4577, Val Acc: 0.6757, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 166, Loss: 0.4501, Val Acc: 0.6667, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 167, Loss: 0.4644, Val Acc: 0.6667, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 168, Loss: 0.4624, Val Acc: 0.6757, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 169, Loss: 0.4469, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 170, Loss: 0.4467, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 171, Loss: 0.4475, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 172, Loss: 0.4449, Val Acc: 0.6847, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 173, Loss: 0.4553, Val Acc: 0.6937, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 174, Loss: 0.4578, Val Acc: 0.6847, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 175, Loss: 0.4537, Val Acc: 0.6757, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 176, Loss: 0.4514, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 177, Loss: 0.4439, Val Acc: 0.6937, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 178, Loss: 0.4501, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 179, Loss: 0.4484, Val Acc: 0.6937, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 180, Loss: 0.5051, Val Acc: 0.6847, Test Acc: 0.6847\n",
      "Seed: 43, Epoch: 181, Loss: 0.4870, Val Acc: 0.6757, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 182, Loss: 0.4497, Val Acc: 0.7117, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 183, Loss: 0.4747, Val Acc: 0.6757, Test Acc: 0.7477\n",
      "Seed: 43, Epoch: 184, Loss: 0.4849, Val Acc: 0.7027, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 185, Loss: 0.4447, Val Acc: 0.7117, Test Acc: 0.6757\n",
      "Seed: 43, Epoch: 186, Loss: 0.4619, Val Acc: 0.6937, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 187, Loss: 0.4638, Val Acc: 0.6667, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 188, Loss: 0.4345, Val Acc: 0.6847, Test Acc: 0.7297\n",
      "Seed: 43, Epoch: 189, Loss: 0.4457, Val Acc: 0.6847, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 190, Loss: 0.4390, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 191, Loss: 0.4325, Val Acc: 0.6757, Test Acc: 0.6937\n",
      "Seed: 43, Epoch: 192, Loss: 0.4367, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 193, Loss: 0.4244, Val Acc: 0.6757, Test Acc: 0.7207\n",
      "Seed: 43, Epoch: 194, Loss: 0.4220, Val Acc: 0.6667, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 195, Loss: 0.4208, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 196, Loss: 0.4188, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 197, Loss: 0.4301, Val Acc: 0.6937, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 198, Loss: 0.4237, Val Acc: 0.6757, Test Acc: 0.7027\n",
      "Seed: 43, Epoch: 199, Loss: 0.4165, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 43, Epoch: 200, Loss: 0.4208, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 001, Loss: 0.6855, Val Acc: 0.4955, Test Acc: 0.5766\n",
      "Seed: 44, Epoch: 002, Loss: 0.6848, Val Acc: 0.4955, Test Acc: 0.5766\n",
      "Seed: 44, Epoch: 003, Loss: 0.6845, Val Acc: 0.4955, Test Acc: 0.5766\n",
      "Seed: 44, Epoch: 004, Loss: 0.6843, Val Acc: 0.4955, Test Acc: 0.5766\n",
      "Seed: 44, Epoch: 005, Loss: 0.6838, Val Acc: 0.4955, Test Acc: 0.5766\n",
      "Seed: 44, Epoch: 006, Loss: 0.6832, Val Acc: 0.4955, Test Acc: 0.5766\n",
      "Seed: 44, Epoch: 007, Loss: 0.6827, Val Acc: 0.4955, Test Acc: 0.5766\n",
      "Seed: 44, Epoch: 008, Loss: 0.6821, Val Acc: 0.4955, Test Acc: 0.5766\n",
      "Seed: 44, Epoch: 009, Loss: 0.6810, Val Acc: 0.4955, Test Acc: 0.5766\n",
      "Seed: 44, Epoch: 010, Loss: 0.6797, Val Acc: 0.4955, Test Acc: 0.5766\n",
      "Seed: 44, Epoch: 011, Loss: 0.6778, Val Acc: 0.4865, Test Acc: 0.5676\n",
      "Seed: 44, Epoch: 012, Loss: 0.6758, Val Acc: 0.6396, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 013, Loss: 0.6735, Val Acc: 0.6667, Test Acc: 0.6757\n",
      "Seed: 44, Epoch: 014, Loss: 0.6705, Val Acc: 0.6126, Test Acc: 0.5856\n",
      "Seed: 44, Epoch: 015, Loss: 0.6713, Val Acc: 0.5586, Test Acc: 0.5315\n",
      "Seed: 44, Epoch: 016, Loss: 0.6845, Val Acc: 0.5856, Test Acc: 0.5405\n",
      "Seed: 44, Epoch: 017, Loss: 0.6758, Val Acc: 0.6216, Test Acc: 0.6036\n",
      "Seed: 44, Epoch: 018, Loss: 0.6632, Val Acc: 0.7297, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 019, Loss: 0.6614, Val Acc: 0.6036, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 020, Loss: 0.6671, Val Acc: 0.5495, Test Acc: 0.6036\n",
      "Seed: 44, Epoch: 021, Loss: 0.6682, Val Acc: 0.5495, Test Acc: 0.6216\n",
      "Seed: 44, Epoch: 022, Loss: 0.6659, Val Acc: 0.6126, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 023, Loss: 0.6625, Val Acc: 0.6577, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 024, Loss: 0.6578, Val Acc: 0.6577, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 025, Loss: 0.6546, Val Acc: 0.6667, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 026, Loss: 0.6537, Val Acc: 0.6396, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 027, Loss: 0.6516, Val Acc: 0.6486, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 028, Loss: 0.6481, Val Acc: 0.6577, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 029, Loss: 0.6447, Val Acc: 0.7117, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 030, Loss: 0.6410, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 031, Loss: 0.6382, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 032, Loss: 0.6373, Val Acc: 0.6847, Test Acc: 0.6757\n",
      "Seed: 44, Epoch: 033, Loss: 0.6400, Val Acc: 0.6577, Test Acc: 0.6306\n",
      "Seed: 44, Epoch: 034, Loss: 0.6503, Val Acc: 0.6757, Test Acc: 0.6757\n",
      "Seed: 44, Epoch: 035, Loss: 0.6345, Val Acc: 0.6847, Test Acc: 0.6757\n",
      "Seed: 44, Epoch: 036, Loss: 0.6217, Val Acc: 0.6757, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 037, Loss: 0.6275, Val Acc: 0.6036, Test Acc: 0.6396\n",
      "Seed: 44, Epoch: 038, Loss: 0.6397, Val Acc: 0.5676, Test Acc: 0.6126\n",
      "Seed: 44, Epoch: 039, Loss: 0.6418, Val Acc: 0.5856, Test Acc: 0.6216\n",
      "Seed: 44, Epoch: 040, Loss: 0.6352, Val Acc: 0.6396, Test Acc: 0.6757\n",
      "Seed: 44, Epoch: 041, Loss: 0.6192, Val Acc: 0.6757, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 042, Loss: 0.6087, Val Acc: 0.6577, Test Acc: 0.6577\n",
      "Seed: 44, Epoch: 043, Loss: 0.6446, Val Acc: 0.6486, Test Acc: 0.6396\n",
      "Seed: 44, Epoch: 044, Loss: 0.6492, Val Acc: 0.6937, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 045, Loss: 0.6047, Val Acc: 0.6486, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 046, Loss: 0.6076, Val Acc: 0.6216, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 047, Loss: 0.6135, Val Acc: 0.6216, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 048, Loss: 0.6119, Val Acc: 0.6577, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 049, Loss: 0.6045, Val Acc: 0.6667, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 050, Loss: 0.5965, Val Acc: 0.7027, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 051, Loss: 0.5938, Val Acc: 0.6757, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 052, Loss: 0.5965, Val Acc: 0.6216, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 053, Loss: 0.6056, Val Acc: 0.6216, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 054, Loss: 0.6032, Val Acc: 0.6486, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 055, Loss: 0.5930, Val Acc: 0.6577, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 056, Loss: 0.5845, Val Acc: 0.7027, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 057, Loss: 0.5841, Val Acc: 0.7117, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 058, Loss: 0.5836, Val Acc: 0.7207, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 059, Loss: 0.5785, Val Acc: 0.6577, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 060, Loss: 0.5915, Val Acc: 0.5946, Test Acc: 0.6126\n",
      "Seed: 44, Epoch: 061, Loss: 0.6118, Val Acc: 0.6036, Test Acc: 0.6396\n",
      "Seed: 44, Epoch: 062, Loss: 0.5994, Val Acc: 0.6036, Test Acc: 0.6577\n",
      "Seed: 44, Epoch: 063, Loss: 0.5954, Val Acc: 0.6126, Test Acc: 0.6577\n",
      "Seed: 44, Epoch: 064, Loss: 0.5903, Val Acc: 0.6396, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 065, Loss: 0.5805, Val Acc: 0.6577, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 066, Loss: 0.5698, Val Acc: 0.6937, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 067, Loss: 0.5719, Val Acc: 0.6937, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 068, Loss: 0.5886, Val Acc: 0.7117, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 069, Loss: 0.5746, Val Acc: 0.6577, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 070, Loss: 0.5670, Val Acc: 0.6216, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 071, Loss: 0.5861, Val Acc: 0.6126, Test Acc: 0.6306\n",
      "Seed: 44, Epoch: 072, Loss: 0.5962, Val Acc: 0.6126, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 073, Loss: 0.5795, Val Acc: 0.6757, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 074, Loss: 0.5617, Val Acc: 0.7027, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 075, Loss: 0.5789, Val Acc: 0.6757, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 076, Loss: 0.5945, Val Acc: 0.6757, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 077, Loss: 0.5790, Val Acc: 0.6937, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 078, Loss: 0.5624, Val Acc: 0.6667, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 079, Loss: 0.5617, Val Acc: 0.6667, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 080, Loss: 0.5626, Val Acc: 0.6757, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 081, Loss: 0.5591, Val Acc: 0.6667, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 082, Loss: 0.5561, Val Acc: 0.6667, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 083, Loss: 0.5524, Val Acc: 0.6667, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 084, Loss: 0.5514, Val Acc: 0.6667, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 085, Loss: 0.5469, Val Acc: 0.6577, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 086, Loss: 0.5461, Val Acc: 0.6757, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 087, Loss: 0.5467, Val Acc: 0.6847, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 088, Loss: 0.5447, Val Acc: 0.6667, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 089, Loss: 0.5642, Val Acc: 0.6396, Test Acc: 0.6486\n",
      "Seed: 44, Epoch: 090, Loss: 0.5780, Val Acc: 0.6577, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 091, Loss: 0.5561, Val Acc: 0.6937, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 092, Loss: 0.5421, Val Acc: 0.6847, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 093, Loss: 0.5441, Val Acc: 0.6306, Test Acc: 0.6396\n",
      "Seed: 44, Epoch: 094, Loss: 0.5777, Val Acc: 0.6306, Test Acc: 0.6396\n",
      "Seed: 44, Epoch: 095, Loss: 0.5929, Val Acc: 0.6396, Test Acc: 0.6396\n",
      "Seed: 44, Epoch: 096, Loss: 0.5725, Val Acc: 0.7117, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 097, Loss: 0.5396, Val Acc: 0.6667, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 098, Loss: 0.5351, Val Acc: 0.6667, Test Acc: 0.7568\n",
      "Seed: 44, Epoch: 099, Loss: 0.5326, Val Acc: 0.6847, Test Acc: 0.7568\n",
      "Seed: 44, Epoch: 100, Loss: 0.5315, Val Acc: 0.6757, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 101, Loss: 0.5289, Val Acc: 0.6937, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 102, Loss: 0.5272, Val Acc: 0.6847, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 103, Loss: 0.5314, Val Acc: 0.6757, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 104, Loss: 0.5265, Val Acc: 0.6847, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 105, Loss: 0.5238, Val Acc: 0.7027, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 106, Loss: 0.5226, Val Acc: 0.6847, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 107, Loss: 0.5204, Val Acc: 0.6757, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 108, Loss: 0.5204, Val Acc: 0.6847, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 109, Loss: 0.5153, Val Acc: 0.7117, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 110, Loss: 0.5242, Val Acc: 0.6937, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 111, Loss: 0.5324, Val Acc: 0.7027, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 112, Loss: 0.5174, Val Acc: 0.6667, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 113, Loss: 0.5312, Val Acc: 0.6937, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 114, Loss: 0.5327, Val Acc: 0.6757, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 115, Loss: 0.5211, Val Acc: 0.7027, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 116, Loss: 0.5291, Val Acc: 0.6847, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 117, Loss: 0.5232, Val Acc: 0.6577, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 118, Loss: 0.5159, Val Acc: 0.6577, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 119, Loss: 0.5201, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 120, Loss: 0.5311, Val Acc: 0.6757, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 121, Loss: 0.5288, Val Acc: 0.6757, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 122, Loss: 0.5107, Val Acc: 0.7297, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 123, Loss: 0.5259, Val Acc: 0.7297, Test Acc: 0.6757\n",
      "Seed: 44, Epoch: 124, Loss: 0.5203, Val Acc: 0.6937, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 125, Loss: 0.5071, Val Acc: 0.6667, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 126, Loss: 0.5081, Val Acc: 0.6757, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 127, Loss: 0.5062, Val Acc: 0.7027, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 128, Loss: 0.5056, Val Acc: 0.7027, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 129, Loss: 0.5051, Val Acc: 0.7117, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 130, Loss: 0.5078, Val Acc: 0.7117, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 131, Loss: 0.5110, Val Acc: 0.7207, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 132, Loss: 0.5061, Val Acc: 0.7117, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 133, Loss: 0.4994, Val Acc: 0.7027, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 134, Loss: 0.4990, Val Acc: 0.6847, Test Acc: 0.7568\n",
      "Seed: 44, Epoch: 135, Loss: 0.4992, Val Acc: 0.6757, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 136, Loss: 0.5069, Val Acc: 0.6577, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 137, Loss: 0.5119, Val Acc: 0.6757, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 138, Loss: 0.4882, Val Acc: 0.7027, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 139, Loss: 0.4876, Val Acc: 0.7027, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 140, Loss: 0.4911, Val Acc: 0.7027, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 141, Loss: 0.4907, Val Acc: 0.6757, Test Acc: 0.7658\n",
      "Seed: 44, Epoch: 142, Loss: 0.4792, Val Acc: 0.6396, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 143, Loss: 0.4918, Val Acc: 0.6396, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 144, Loss: 0.4851, Val Acc: 0.6667, Test Acc: 0.7658\n",
      "Seed: 44, Epoch: 145, Loss: 0.4722, Val Acc: 0.7117, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 146, Loss: 0.4762, Val Acc: 0.7027, Test Acc: 0.7568\n",
      "Seed: 44, Epoch: 147, Loss: 0.4681, Val Acc: 0.6757, Test Acc: 0.7568\n",
      "Seed: 44, Epoch: 148, Loss: 0.4714, Val Acc: 0.6937, Test Acc: 0.7568\n",
      "Seed: 44, Epoch: 149, Loss: 0.4668, Val Acc: 0.6757, Test Acc: 0.7658\n",
      "Seed: 44, Epoch: 150, Loss: 0.4757, Val Acc: 0.7387, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 151, Loss: 0.4703, Val Acc: 0.6306, Test Acc: 0.6306\n",
      "Seed: 44, Epoch: 152, Loss: 0.5777, Val Acc: 0.6126, Test Acc: 0.6306\n",
      "Seed: 44, Epoch: 153, Loss: 0.5486, Val Acc: 0.7027, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 154, Loss: 0.4746, Val Acc: 0.7027, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 155, Loss: 0.4671, Val Acc: 0.7207, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 156, Loss: 0.4669, Val Acc: 0.7297, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 157, Loss: 0.4653, Val Acc: 0.6757, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 158, Loss: 0.4631, Val Acc: 0.6757, Test Acc: 0.7658\n",
      "Seed: 44, Epoch: 159, Loss: 0.4602, Val Acc: 0.6757, Test Acc: 0.7568\n",
      "Seed: 44, Epoch: 160, Loss: 0.4592, Val Acc: 0.6937, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 161, Loss: 0.4716, Val Acc: 0.6667, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 162, Loss: 0.5172, Val Acc: 0.6937, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 163, Loss: 0.4849, Val Acc: 0.6937, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 164, Loss: 0.4496, Val Acc: 0.6486, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 165, Loss: 0.4912, Val Acc: 0.6396, Test Acc: 0.7027\n",
      "Seed: 44, Epoch: 166, Loss: 0.4851, Val Acc: 0.6396, Test Acc: 0.7658\n",
      "Seed: 44, Epoch: 167, Loss: 0.4508, Val Acc: 0.6937, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 168, Loss: 0.4470, Val Acc: 0.6937, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 169, Loss: 0.4465, Val Acc: 0.6667, Test Acc: 0.7568\n",
      "Seed: 44, Epoch: 170, Loss: 0.4509, Val Acc: 0.6847, Test Acc: 0.7658\n",
      "Seed: 44, Epoch: 171, Loss: 0.4449, Val Acc: 0.6757, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 172, Loss: 0.4542, Val Acc: 0.6667, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 173, Loss: 0.4954, Val Acc: 0.6847, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 174, Loss: 0.4729, Val Acc: 0.6847, Test Acc: 0.7658\n",
      "Seed: 44, Epoch: 175, Loss: 0.4775, Val Acc: 0.6757, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 176, Loss: 0.4491, Val Acc: 0.7027, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 177, Loss: 0.4943, Val Acc: 0.6396, Test Acc: 0.6577\n",
      "Seed: 44, Epoch: 178, Loss: 0.5323, Val Acc: 0.7027, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 179, Loss: 0.4717, Val Acc: 0.6847, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 180, Loss: 0.4508, Val Acc: 0.6757, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 181, Loss: 0.4509, Val Acc: 0.6847, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 182, Loss: 0.4498, Val Acc: 0.6937, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 183, Loss: 0.4636, Val Acc: 0.7117, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 184, Loss: 0.4706, Val Acc: 0.6937, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 185, Loss: 0.4658, Val Acc: 0.7117, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 186, Loss: 0.4481, Val Acc: 0.6757, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 187, Loss: 0.4337, Val Acc: 0.6667, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 188, Loss: 0.4370, Val Acc: 0.6667, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 189, Loss: 0.4322, Val Acc: 0.6847, Test Acc: 0.7117\n",
      "Seed: 44, Epoch: 190, Loss: 0.4330, Val Acc: 0.6486, Test Acc: 0.6757\n",
      "Seed: 44, Epoch: 191, Loss: 0.4583, Val Acc: 0.6396, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 192, Loss: 0.4385, Val Acc: 0.6667, Test Acc: 0.6937\n",
      "Seed: 44, Epoch: 193, Loss: 0.4172, Val Acc: 0.6757, Test Acc: 0.7297\n",
      "Seed: 44, Epoch: 194, Loss: 0.4274, Val Acc: 0.6667, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 195, Loss: 0.4370, Val Acc: 0.6577, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 196, Loss: 0.4287, Val Acc: 0.6486, Test Acc: 0.6847\n",
      "Seed: 44, Epoch: 197, Loss: 0.4623, Val Acc: 0.6757, Test Acc: 0.7207\n",
      "Seed: 44, Epoch: 198, Loss: 0.4115, Val Acc: 0.6577, Test Acc: 0.7568\n",
      "Seed: 44, Epoch: 199, Loss: 0.4310, Val Acc: 0.6577, Test Acc: 0.7477\n",
      "Seed: 44, Epoch: 200, Loss: 0.4378, Val Acc: 0.6577, Test Acc: 0.7387\n",
      "Average Time: 149.44 seconds\n",
      "Var Time: 1.03 seconds\n",
      "Average Memory: 44907.33 MB\n",
      "Average Best Val Acc: 0.7508\n",
      "Std Best Test Acc: 0.0042\n",
      "Average Test Acc: 0.7357\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/XXX/Pooling/\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"DD\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_CO(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_CO, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = CoPooling(ratio=0.9, K=1, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=1.0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = CoPooling(ratio=0.9, K=1, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=1.0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, perm, _, batch, _, _, _ = self.pool1(x, edge_index, edge_attr=None, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, perm, _, batch, _, _, _ = self.pool2(x, edge_index, edge_attr=None, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 0.7022, Val Acc: 0.5200, Test Acc: 0.5267\n",
      "Seed: 42, Epoch: 002, Loss: 0.6978, Val Acc: 0.5200, Test Acc: 0.5267\n",
      "Seed: 42, Epoch: 003, Loss: 0.6925, Val Acc: 0.5200, Test Acc: 0.5333\n",
      "Seed: 42, Epoch: 004, Loss: 0.6894, Val Acc: 0.5267, Test Acc: 0.5467\n",
      "Seed: 42, Epoch: 005, Loss: 0.6826, Val Acc: 0.5333, Test Acc: 0.6200\n",
      "Seed: 42, Epoch: 006, Loss: 0.6737, Val Acc: 0.5533, Test Acc: 0.6533\n",
      "Seed: 42, Epoch: 007, Loss: 0.6642, Val Acc: 0.5800, Test Acc: 0.6933\n",
      "Seed: 42, Epoch: 008, Loss: 0.6490, Val Acc: 0.5867, Test Acc: 0.6800\n",
      "Seed: 42, Epoch: 009, Loss: 0.6323, Val Acc: 0.6400, Test Acc: 0.6867\n",
      "Seed: 42, Epoch: 010, Loss: 0.6244, Val Acc: 0.6333, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 011, Loss: 0.5967, Val Acc: 0.6867, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 012, Loss: 0.5940, Val Acc: 0.6933, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 013, Loss: 0.5772, Val Acc: 0.7600, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 014, Loss: 0.5671, Val Acc: 0.7867, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 015, Loss: 0.5696, Val Acc: 0.8067, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 016, Loss: 0.5554, Val Acc: 0.7600, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 017, Loss: 0.5704, Val Acc: 0.7800, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 018, Loss: 0.5462, Val Acc: 0.7867, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 019, Loss: 0.5525, Val Acc: 0.7800, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 020, Loss: 0.5513, Val Acc: 0.7867, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 021, Loss: 0.5435, Val Acc: 0.7800, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 022, Loss: 0.5387, Val Acc: 0.7933, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 023, Loss: 0.5372, Val Acc: 0.7867, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 024, Loss: 0.5268, Val Acc: 0.7733, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 025, Loss: 0.5288, Val Acc: 0.7867, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 026, Loss: 0.5153, Val Acc: 0.7933, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 027, Loss: 0.5215, Val Acc: 0.8200, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 028, Loss: 0.5252, Val Acc: 0.7867, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 029, Loss: 0.5064, Val Acc: 0.7867, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 030, Loss: 0.5103, Val Acc: 0.7867, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 031, Loss: 0.5145, Val Acc: 0.7800, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 032, Loss: 0.5052, Val Acc: 0.7867, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 033, Loss: 0.5073, Val Acc: 0.7867, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 034, Loss: 0.5053, Val Acc: 0.7800, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 035, Loss: 0.5039, Val Acc: 0.7867, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 036, Loss: 0.5005, Val Acc: 0.8000, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 037, Loss: 0.4968, Val Acc: 0.7733, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 038, Loss: 0.4946, Val Acc: 0.7933, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 039, Loss: 0.5047, Val Acc: 0.7867, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 040, Loss: 0.4983, Val Acc: 0.7800, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 041, Loss: 0.5027, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 042, Loss: 0.4909, Val Acc: 0.7533, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 043, Loss: 0.5130, Val Acc: 0.7800, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 044, Loss: 0.4931, Val Acc: 0.7667, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 045, Loss: 0.5210, Val Acc: 0.7867, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 046, Loss: 0.4922, Val Acc: 0.7467, Test Acc: 0.6867\n",
      "Seed: 42, Epoch: 047, Loss: 0.5261, Val Acc: 0.8067, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 048, Loss: 0.4912, Val Acc: 0.7667, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 049, Loss: 0.5141, Val Acc: 0.7933, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 050, Loss: 0.4891, Val Acc: 0.7800, Test Acc: 0.6867\n",
      "Seed: 42, Epoch: 051, Loss: 0.5090, Val Acc: 0.7467, Test Acc: 0.6800\n",
      "Seed: 42, Epoch: 052, Loss: 0.5146, Val Acc: 0.7667, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 053, Loss: 0.4873, Val Acc: 0.8067, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 054, Loss: 0.4943, Val Acc: 0.8067, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 055, Loss: 0.4947, Val Acc: 0.7867, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 056, Loss: 0.4854, Val Acc: 0.7933, Test Acc: 0.6933\n",
      "Seed: 42, Epoch: 057, Loss: 0.4959, Val Acc: 0.7800, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 058, Loss: 0.4857, Val Acc: 0.7933, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 059, Loss: 0.4841, Val Acc: 0.7933, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 060, Loss: 0.4890, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 061, Loss: 0.4820, Val Acc: 0.7867, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 062, Loss: 0.4749, Val Acc: 0.7867, Test Acc: 0.6933\n",
      "Seed: 42, Epoch: 063, Loss: 0.4783, Val Acc: 0.7867, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 064, Loss: 0.4767, Val Acc: 0.7800, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 065, Loss: 0.4729, Val Acc: 0.8000, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 066, Loss: 0.4761, Val Acc: 0.7933, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 067, Loss: 0.4692, Val Acc: 0.7800, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 068, Loss: 0.4712, Val Acc: 0.7733, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 069, Loss: 0.4690, Val Acc: 0.7933, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 070, Loss: 0.4661, Val Acc: 0.7933, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 071, Loss: 0.4703, Val Acc: 0.7933, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 072, Loss: 0.4623, Val Acc: 0.7933, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 073, Loss: 0.4617, Val Acc: 0.8000, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 074, Loss: 0.4606, Val Acc: 0.8000, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 075, Loss: 0.4532, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 076, Loss: 0.4578, Val Acc: 0.7867, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 077, Loss: 0.4524, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 078, Loss: 0.4557, Val Acc: 0.7733, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 079, Loss: 0.4567, Val Acc: 0.7733, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 080, Loss: 0.4459, Val Acc: 0.7667, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 081, Loss: 0.4427, Val Acc: 0.7533, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 082, Loss: 0.4451, Val Acc: 0.7667, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 083, Loss: 0.4547, Val Acc: 0.7667, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 084, Loss: 0.4395, Val Acc: 0.7467, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 085, Loss: 0.4296, Val Acc: 0.7533, Test Acc: 0.6933\n",
      "Seed: 42, Epoch: 086, Loss: 0.4386, Val Acc: 0.7667, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 087, Loss: 0.4310, Val Acc: 0.7800, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 088, Loss: 0.4356, Val Acc: 0.7733, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 089, Loss: 0.4346, Val Acc: 0.7800, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 090, Loss: 0.4482, Val Acc: 0.7733, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 091, Loss: 0.4371, Val Acc: 0.7667, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 092, Loss: 0.4277, Val Acc: 0.7667, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 093, Loss: 0.4344, Val Acc: 0.7600, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 094, Loss: 0.4378, Val Acc: 0.7600, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 095, Loss: 0.4175, Val Acc: 0.7667, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 096, Loss: 0.4207, Val Acc: 0.7600, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 097, Loss: 0.4251, Val Acc: 0.7467, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 098, Loss: 0.4164, Val Acc: 0.7600, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 099, Loss: 0.4209, Val Acc: 0.7800, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 100, Loss: 0.4140, Val Acc: 0.7667, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 101, Loss: 0.4294, Val Acc: 0.8000, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 102, Loss: 0.4701, Val Acc: 0.7667, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 103, Loss: 0.4309, Val Acc: 0.7600, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 104, Loss: 0.4476, Val Acc: 0.7733, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 105, Loss: 0.4473, Val Acc: 0.7733, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 106, Loss: 0.4334, Val Acc: 0.7667, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 107, Loss: 0.4172, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 108, Loss: 0.4458, Val Acc: 0.7600, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 109, Loss: 0.4255, Val Acc: 0.7867, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 110, Loss: 0.4255, Val Acc: 0.7867, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 111, Loss: 0.4294, Val Acc: 0.7800, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 112, Loss: 0.4333, Val Acc: 0.7667, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 113, Loss: 0.4202, Val Acc: 0.7733, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 114, Loss: 0.4199, Val Acc: 0.7733, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 115, Loss: 0.4120, Val Acc: 0.7800, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 116, Loss: 0.4123, Val Acc: 0.7867, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 117, Loss: 0.4090, Val Acc: 0.7867, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 118, Loss: 0.4169, Val Acc: 0.7867, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 119, Loss: 0.4130, Val Acc: 0.7933, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 120, Loss: 0.4152, Val Acc: 0.7733, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 121, Loss: 0.4104, Val Acc: 0.7667, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 122, Loss: 0.3976, Val Acc: 0.8067, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 123, Loss: 0.4142, Val Acc: 0.8067, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 124, Loss: 0.4063, Val Acc: 0.7667, Test Acc: 0.6733\n",
      "Seed: 42, Epoch: 125, Loss: 0.4181, Val Acc: 0.7533, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 126, Loss: 0.4039, Val Acc: 0.7800, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 127, Loss: 0.4102, Val Acc: 0.7867, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 128, Loss: 0.4085, Val Acc: 0.7867, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 129, Loss: 0.3991, Val Acc: 0.7667, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 130, Loss: 0.3925, Val Acc: 0.7933, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 131, Loss: 0.3936, Val Acc: 0.7933, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 132, Loss: 0.4032, Val Acc: 0.7867, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 133, Loss: 0.3836, Val Acc: 0.7600, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 134, Loss: 0.4112, Val Acc: 0.7733, Test Acc: 0.6867\n",
      "Seed: 42, Epoch: 135, Loss: 0.4098, Val Acc: 0.7800, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 136, Loss: 0.3889, Val Acc: 0.7267, Test Acc: 0.6933\n",
      "Seed: 42, Epoch: 137, Loss: 0.4276, Val Acc: 0.7800, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 138, Loss: 0.4156, Val Acc: 0.7867, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 139, Loss: 0.4072, Val Acc: 0.7867, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 140, Loss: 0.4169, Val Acc: 0.7933, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 141, Loss: 0.3995, Val Acc: 0.8067, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 142, Loss: 0.3897, Val Acc: 0.7800, Test Acc: 0.7800\n",
      "Seed: 42, Epoch: 143, Loss: 0.3906, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 144, Loss: 0.3882, Val Acc: 0.7667, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 145, Loss: 0.3959, Val Acc: 0.8000, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 146, Loss: 0.3804, Val Acc: 0.7800, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 147, Loss: 0.3935, Val Acc: 0.7800, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 148, Loss: 0.3892, Val Acc: 0.7600, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 149, Loss: 0.3857, Val Acc: 0.7867, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 150, Loss: 0.3898, Val Acc: 0.7933, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 151, Loss: 0.3795, Val Acc: 0.7467, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 152, Loss: 0.3831, Val Acc: 0.7533, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 153, Loss: 0.3859, Val Acc: 0.7800, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 154, Loss: 0.3864, Val Acc: 0.7733, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 155, Loss: 0.3778, Val Acc: 0.7733, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 156, Loss: 0.3687, Val Acc: 0.7667, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 157, Loss: 0.3796, Val Acc: 0.7933, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 158, Loss: 0.3993, Val Acc: 0.8000, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 159, Loss: 0.3718, Val Acc: 0.7467, Test Acc: 0.6867\n",
      "Seed: 42, Epoch: 160, Loss: 0.3964, Val Acc: 0.7733, Test Acc: 0.6933\n",
      "Seed: 42, Epoch: 161, Loss: 0.3785, Val Acc: 0.7867, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 162, Loss: 0.3645, Val Acc: 0.7733, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 163, Loss: 0.3708, Val Acc: 0.7733, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 164, Loss: 0.3575, Val Acc: 0.7733, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 165, Loss: 0.3847, Val Acc: 0.7600, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 166, Loss: 0.3708, Val Acc: 0.7667, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 167, Loss: 0.3653, Val Acc: 0.7867, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 168, Loss: 0.3776, Val Acc: 0.7867, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 169, Loss: 0.3694, Val Acc: 0.7867, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 170, Loss: 0.3580, Val Acc: 0.7733, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 171, Loss: 0.3728, Val Acc: 0.7867, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 172, Loss: 0.3729, Val Acc: 0.7800, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 173, Loss: 0.3640, Val Acc: 0.7867, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 174, Loss: 0.3767, Val Acc: 0.8000, Test Acc: 0.7667\n",
      "Seed: 42, Epoch: 175, Loss: 0.3633, Val Acc: 0.7800, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 176, Loss: 0.3669, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 177, Loss: 0.3468, Val Acc: 0.7533, Test Acc: 0.6800\n",
      "Early stopping at epoch 177 for seed 42\n",
      "Seed: 43, Epoch: 001, Loss: 0.6923, Val Acc: 0.4467, Test Acc: 0.5067\n",
      "Seed: 43, Epoch: 002, Loss: 0.6879, Val Acc: 0.4867, Test Acc: 0.4933\n",
      "Seed: 43, Epoch: 003, Loss: 0.6846, Val Acc: 0.5333, Test Acc: 0.5400\n",
      "Seed: 43, Epoch: 004, Loss: 0.6784, Val Acc: 0.5667, Test Acc: 0.5533\n",
      "Seed: 43, Epoch: 005, Loss: 0.6714, Val Acc: 0.5933, Test Acc: 0.5867\n",
      "Seed: 43, Epoch: 006, Loss: 0.6607, Val Acc: 0.6200, Test Acc: 0.6000\n",
      "Seed: 43, Epoch: 007, Loss: 0.6466, Val Acc: 0.6000, Test Acc: 0.6733\n",
      "Seed: 43, Epoch: 008, Loss: 0.6399, Val Acc: 0.6267, Test Acc: 0.6733\n",
      "Seed: 43, Epoch: 009, Loss: 0.6287, Val Acc: 0.6533, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 010, Loss: 0.6047, Val Acc: 0.6600, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 011, Loss: 0.5889, Val Acc: 0.6800, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 012, Loss: 0.5854, Val Acc: 0.6867, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 013, Loss: 0.5683, Val Acc: 0.6800, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 014, Loss: 0.5516, Val Acc: 0.7000, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 015, Loss: 0.5410, Val Acc: 0.6800, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 016, Loss: 0.5306, Val Acc: 0.6933, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 017, Loss: 0.5283, Val Acc: 0.6667, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 018, Loss: 0.5076, Val Acc: 0.6667, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 019, Loss: 0.5161, Val Acc: 0.6600, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 020, Loss: 0.5154, Val Acc: 0.6733, Test Acc: 0.6800\n",
      "Seed: 43, Epoch: 021, Loss: 0.5111, Val Acc: 0.6800, Test Acc: 0.6867\n",
      "Seed: 43, Epoch: 022, Loss: 0.5066, Val Acc: 0.7000, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 023, Loss: 0.5004, Val Acc: 0.7000, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 024, Loss: 0.4980, Val Acc: 0.6800, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 025, Loss: 0.4964, Val Acc: 0.6800, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 026, Loss: 0.5121, Val Acc: 0.7000, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 027, Loss: 0.4883, Val Acc: 0.6867, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 028, Loss: 0.4892, Val Acc: 0.6733, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 029, Loss: 0.4901, Val Acc: 0.6867, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 030, Loss: 0.4827, Val Acc: 0.6733, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 031, Loss: 0.4775, Val Acc: 0.6867, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 032, Loss: 0.4831, Val Acc: 0.6667, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 033, Loss: 0.4770, Val Acc: 0.6667, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 034, Loss: 0.4711, Val Acc: 0.6667, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 035, Loss: 0.4893, Val Acc: 0.6733, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 036, Loss: 0.4725, Val Acc: 0.6800, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 037, Loss: 0.4641, Val Acc: 0.6867, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 038, Loss: 0.4638, Val Acc: 0.6800, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 039, Loss: 0.4644, Val Acc: 0.6733, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 040, Loss: 0.4690, Val Acc: 0.6800, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 041, Loss: 0.4817, Val Acc: 0.6933, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 042, Loss: 0.4663, Val Acc: 0.6933, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 043, Loss: 0.4744, Val Acc: 0.6733, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 044, Loss: 0.4421, Val Acc: 0.7133, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 045, Loss: 0.4657, Val Acc: 0.6867, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 046, Loss: 0.4561, Val Acc: 0.6867, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 047, Loss: 0.4625, Val Acc: 0.6733, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 048, Loss: 0.4556, Val Acc: 0.7000, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 049, Loss: 0.4411, Val Acc: 0.7067, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 050, Loss: 0.4443, Val Acc: 0.6800, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 051, Loss: 0.4487, Val Acc: 0.6867, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 052, Loss: 0.4347, Val Acc: 0.6800, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 053, Loss: 0.4439, Val Acc: 0.6867, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 054, Loss: 0.4391, Val Acc: 0.6733, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 055, Loss: 0.4377, Val Acc: 0.6667, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 056, Loss: 0.4263, Val Acc: 0.6933, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 057, Loss: 0.4421, Val Acc: 0.6600, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 058, Loss: 0.4338, Val Acc: 0.6600, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 059, Loss: 0.4321, Val Acc: 0.6733, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 060, Loss: 0.4317, Val Acc: 0.6733, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 061, Loss: 0.4232, Val Acc: 0.6800, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 062, Loss: 0.4201, Val Acc: 0.6667, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 063, Loss: 0.4364, Val Acc: 0.6933, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 064, Loss: 0.4125, Val Acc: 0.6800, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 065, Loss: 0.4390, Val Acc: 0.6667, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 066, Loss: 0.4350, Val Acc: 0.6800, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 067, Loss: 0.4033, Val Acc: 0.6867, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 068, Loss: 0.4182, Val Acc: 0.7000, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 069, Loss: 0.3982, Val Acc: 0.6667, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 070, Loss: 0.4162, Val Acc: 0.6600, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 071, Loss: 0.4122, Val Acc: 0.6800, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 072, Loss: 0.4131, Val Acc: 0.6933, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 073, Loss: 0.4140, Val Acc: 0.6667, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 074, Loss: 0.4069, Val Acc: 0.6333, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 075, Loss: 0.4068, Val Acc: 0.6267, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 076, Loss: 0.4130, Val Acc: 0.6600, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 077, Loss: 0.3900, Val Acc: 0.6467, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 078, Loss: 0.3913, Val Acc: 0.6867, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 079, Loss: 0.3931, Val Acc: 0.6933, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 080, Loss: 0.4001, Val Acc: 0.6933, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 081, Loss: 0.3901, Val Acc: 0.7067, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 082, Loss: 0.3929, Val Acc: 0.6933, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 083, Loss: 0.3779, Val Acc: 0.6600, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 084, Loss: 0.3966, Val Acc: 0.6867, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 085, Loss: 0.3848, Val Acc: 0.6733, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 086, Loss: 0.4005, Val Acc: 0.6600, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 087, Loss: 0.3807, Val Acc: 0.6867, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 088, Loss: 0.3947, Val Acc: 0.6933, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 089, Loss: 0.3900, Val Acc: 0.6867, Test Acc: 0.6933\n",
      "Seed: 43, Epoch: 090, Loss: 0.3770, Val Acc: 0.6933, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 091, Loss: 0.3682, Val Acc: 0.6733, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 092, Loss: 0.3905, Val Acc: 0.6400, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 093, Loss: 0.3729, Val Acc: 0.6533, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 094, Loss: 0.3752, Val Acc: 0.6533, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 095, Loss: 0.3722, Val Acc: 0.6733, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 096, Loss: 0.3841, Val Acc: 0.6600, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 097, Loss: 0.3579, Val Acc: 0.6933, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 098, Loss: 0.3808, Val Acc: 0.6733, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 099, Loss: 0.3742, Val Acc: 0.6400, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 100, Loss: 0.3642, Val Acc: 0.6733, Test Acc: 0.6733\n",
      "Seed: 43, Epoch: 101, Loss: 0.3649, Val Acc: 0.6733, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 102, Loss: 0.4031, Val Acc: 0.6400, Test Acc: 0.6867\n",
      "Seed: 43, Epoch: 103, Loss: 0.4140, Val Acc: 0.6667, Test Acc: 0.6933\n",
      "Seed: 43, Epoch: 104, Loss: 0.3833, Val Acc: 0.6867, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 105, Loss: 0.3926, Val Acc: 0.7000, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 106, Loss: 0.3716, Val Acc: 0.6800, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 107, Loss: 0.3684, Val Acc: 0.6667, Test Acc: 0.6933\n",
      "Seed: 43, Epoch: 108, Loss: 0.3546, Val Acc: 0.6600, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 109, Loss: 0.3728, Val Acc: 0.6400, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 110, Loss: 0.3555, Val Acc: 0.6400, Test Acc: 0.6933\n",
      "Seed: 43, Epoch: 111, Loss: 0.3613, Val Acc: 0.6400, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 112, Loss: 0.3540, Val Acc: 0.6200, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 113, Loss: 0.3600, Val Acc: 0.6600, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 114, Loss: 0.3850, Val Acc: 0.6267, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 115, Loss: 0.3421, Val Acc: 0.6467, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 116, Loss: 0.3897, Val Acc: 0.6533, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 117, Loss: 0.3541, Val Acc: 0.7200, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 118, Loss: 0.4319, Val Acc: 0.6667, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 119, Loss: 0.3788, Val Acc: 0.6733, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 120, Loss: 0.3987, Val Acc: 0.6733, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 121, Loss: 0.3722, Val Acc: 0.6800, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 122, Loss: 0.3822, Val Acc: 0.6800, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 123, Loss: 0.3666, Val Acc: 0.6600, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 124, Loss: 0.3588, Val Acc: 0.6400, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 125, Loss: 0.3783, Val Acc: 0.6467, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 126, Loss: 0.3595, Val Acc: 0.6333, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 127, Loss: 0.3553, Val Acc: 0.6600, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 128, Loss: 0.3656, Val Acc: 0.6533, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 129, Loss: 0.3523, Val Acc: 0.6333, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 130, Loss: 0.3719, Val Acc: 0.6333, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 131, Loss: 0.3542, Val Acc: 0.7000, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 132, Loss: 0.3608, Val Acc: 0.6533, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 133, Loss: 0.3517, Val Acc: 0.6600, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 134, Loss: 0.3453, Val Acc: 0.6333, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 135, Loss: 0.3452, Val Acc: 0.6467, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 136, Loss: 0.3441, Val Acc: 0.6533, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 137, Loss: 0.3446, Val Acc: 0.6333, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 138, Loss: 0.3421, Val Acc: 0.6533, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 139, Loss: 0.3257, Val Acc: 0.6533, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 140, Loss: 0.3412, Val Acc: 0.6333, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 141, Loss: 0.3476, Val Acc: 0.6533, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 142, Loss: 0.3325, Val Acc: 0.6600, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 143, Loss: 0.3348, Val Acc: 0.6533, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 144, Loss: 0.3497, Val Acc: 0.6533, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 145, Loss: 0.3206, Val Acc: 0.6400, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 146, Loss: 0.3300, Val Acc: 0.6267, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 147, Loss: 0.3215, Val Acc: 0.6400, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 148, Loss: 0.3358, Val Acc: 0.6600, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 149, Loss: 0.3385, Val Acc: 0.6533, Test Acc: 0.6733\n",
      "Seed: 43, Epoch: 150, Loss: 0.3256, Val Acc: 0.6400, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 151, Loss: 0.3394, Val Acc: 0.6533, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 152, Loss: 0.3599, Val Acc: 0.6467, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 153, Loss: 0.3392, Val Acc: 0.6467, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 154, Loss: 0.3330, Val Acc: 0.6600, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 155, Loss: 0.3458, Val Acc: 0.6800, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 156, Loss: 0.3406, Val Acc: 0.6733, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 157, Loss: 0.3454, Val Acc: 0.6867, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 158, Loss: 0.3265, Val Acc: 0.7200, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 159, Loss: 0.3136, Val Acc: 0.6933, Test Acc: 0.6733\n",
      "Seed: 43, Epoch: 160, Loss: 0.3367, Val Acc: 0.6800, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 161, Loss: 0.3258, Val Acc: 0.6800, Test Acc: 0.6933\n",
      "Seed: 43, Epoch: 162, Loss: 0.3089, Val Acc: 0.6867, Test Acc: 0.6867\n",
      "Seed: 43, Epoch: 163, Loss: 0.3179, Val Acc: 0.6600, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 164, Loss: 0.3242, Val Acc: 0.6400, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 165, Loss: 0.3297, Val Acc: 0.6667, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 166, Loss: 0.3233, Val Acc: 0.6667, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 167, Loss: 0.3236, Val Acc: 0.6533, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 168, Loss: 0.3258, Val Acc: 0.7133, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 169, Loss: 0.3208, Val Acc: 0.7067, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 170, Loss: 0.3150, Val Acc: 0.7000, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 171, Loss: 0.3159, Val Acc: 0.6600, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 172, Loss: 0.3256, Val Acc: 0.6733, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 173, Loss: 0.3336, Val Acc: 0.6867, Test Acc: 0.6933\n",
      "Seed: 43, Epoch: 174, Loss: 0.3319, Val Acc: 0.6667, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 175, Loss: 0.3134, Val Acc: 0.6733, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 176, Loss: 0.3117, Val Acc: 0.7000, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 177, Loss: 0.3232, Val Acc: 0.7267, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 178, Loss: 0.3120, Val Acc: 0.7133, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 179, Loss: 0.2942, Val Acc: 0.7000, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 180, Loss: 0.3093, Val Acc: 0.7133, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 181, Loss: 0.3205, Val Acc: 0.7000, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 182, Loss: 0.2843, Val Acc: 0.7067, Test Acc: 0.7267\n",
      "Seed: 43, Epoch: 183, Loss: 0.3052, Val Acc: 0.7067, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 184, Loss: 0.3020, Val Acc: 0.6867, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 185, Loss: 0.2877, Val Acc: 0.7000, Test Acc: 0.6800\n",
      "Seed: 43, Epoch: 186, Loss: 0.3014, Val Acc: 0.7067, Test Acc: 0.7200\n",
      "Seed: 43, Epoch: 187, Loss: 0.3157, Val Acc: 0.6933, Test Acc: 0.7067\n",
      "Seed: 43, Epoch: 188, Loss: 0.2943, Val Acc: 0.6933, Test Acc: 0.6800\n",
      "Seed: 43, Epoch: 189, Loss: 0.3162, Val Acc: 0.6933, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 190, Loss: 0.3206, Val Acc: 0.6800, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 191, Loss: 0.3023, Val Acc: 0.7000, Test Acc: 0.6867\n",
      "Seed: 43, Epoch: 192, Loss: 0.3050, Val Acc: 0.6933, Test Acc: 0.7000\n",
      "Seed: 43, Epoch: 193, Loss: 0.2980, Val Acc: 0.7000, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 194, Loss: 0.3212, Val Acc: 0.7067, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 195, Loss: 0.2949, Val Acc: 0.6800, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 196, Loss: 0.3002, Val Acc: 0.7067, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 197, Loss: 0.2988, Val Acc: 0.6867, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 198, Loss: 0.2809, Val Acc: 0.7067, Test Acc: 0.7133\n",
      "Seed: 43, Epoch: 199, Loss: 0.2798, Val Acc: 0.7000, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 200, Loss: 0.2874, Val Acc: 0.7067, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 001, Loss: 0.6927, Val Acc: 0.5867, Test Acc: 0.5667\n",
      "Seed: 44, Epoch: 002, Loss: 0.6856, Val Acc: 0.5933, Test Acc: 0.6533\n",
      "Seed: 44, Epoch: 003, Loss: 0.6799, Val Acc: 0.6200, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 004, Loss: 0.6688, Val Acc: 0.6400, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 005, Loss: 0.6573, Val Acc: 0.6467, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 006, Loss: 0.6476, Val Acc: 0.6667, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 007, Loss: 0.6339, Val Acc: 0.6667, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 008, Loss: 0.6160, Val Acc: 0.6667, Test Acc: 0.6533\n",
      "Seed: 44, Epoch: 009, Loss: 0.6100, Val Acc: 0.6533, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 010, Loss: 0.5908, Val Acc: 0.6600, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 011, Loss: 0.5823, Val Acc: 0.6533, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 012, Loss: 0.5632, Val Acc: 0.7000, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 013, Loss: 0.5581, Val Acc: 0.7000, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 014, Loss: 0.5443, Val Acc: 0.7333, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 015, Loss: 0.5323, Val Acc: 0.6933, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 016, Loss: 0.5400, Val Acc: 0.6667, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 017, Loss: 0.5444, Val Acc: 0.7200, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 018, Loss: 0.5185, Val Acc: 0.7200, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 019, Loss: 0.5181, Val Acc: 0.7333, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 020, Loss: 0.5099, Val Acc: 0.7467, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 021, Loss: 0.5092, Val Acc: 0.7400, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 022, Loss: 0.5005, Val Acc: 0.7533, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 023, Loss: 0.5097, Val Acc: 0.7533, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 024, Loss: 0.5022, Val Acc: 0.7733, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 025, Loss: 0.4981, Val Acc: 0.7600, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 026, Loss: 0.4914, Val Acc: 0.7667, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 027, Loss: 0.4818, Val Acc: 0.7600, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 028, Loss: 0.4835, Val Acc: 0.7533, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 029, Loss: 0.4790, Val Acc: 0.7667, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 030, Loss: 0.4729, Val Acc: 0.7600, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 031, Loss: 0.4689, Val Acc: 0.7733, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 032, Loss: 0.4612, Val Acc: 0.7933, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 033, Loss: 0.4651, Val Acc: 0.7533, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 034, Loss: 0.4685, Val Acc: 0.7800, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 035, Loss: 0.4598, Val Acc: 0.7800, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 036, Loss: 0.4650, Val Acc: 0.7400, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 037, Loss: 0.4893, Val Acc: 0.7800, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 038, Loss: 0.4578, Val Acc: 0.7467, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 039, Loss: 0.4740, Val Acc: 0.7867, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 040, Loss: 0.4516, Val Acc: 0.7533, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 041, Loss: 0.4604, Val Acc: 0.7867, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 042, Loss: 0.4527, Val Acc: 0.7600, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 043, Loss: 0.4528, Val Acc: 0.7733, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 044, Loss: 0.4477, Val Acc: 0.7733, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 045, Loss: 0.4488, Val Acc: 0.7733, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 046, Loss: 0.4442, Val Acc: 0.7733, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 047, Loss: 0.4452, Val Acc: 0.7733, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 048, Loss: 0.4373, Val Acc: 0.7800, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 049, Loss: 0.4366, Val Acc: 0.7867, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 050, Loss: 0.4356, Val Acc: 0.8000, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 051, Loss: 0.4300, Val Acc: 0.7867, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 052, Loss: 0.4304, Val Acc: 0.7867, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 053, Loss: 0.4337, Val Acc: 0.8000, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 054, Loss: 0.4250, Val Acc: 0.7933, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 055, Loss: 0.4211, Val Acc: 0.7933, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 056, Loss: 0.4184, Val Acc: 0.7933, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 057, Loss: 0.4197, Val Acc: 0.7933, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 058, Loss: 0.4162, Val Acc: 0.8000, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 059, Loss: 0.4076, Val Acc: 0.7933, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 060, Loss: 0.4105, Val Acc: 0.7600, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 061, Loss: 0.4206, Val Acc: 0.7800, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 062, Loss: 0.4047, Val Acc: 0.7667, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 063, Loss: 0.4071, Val Acc: 0.7667, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 064, Loss: 0.4169, Val Acc: 0.7800, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 065, Loss: 0.4068, Val Acc: 0.7733, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 066, Loss: 0.3955, Val Acc: 0.7600, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 067, Loss: 0.4179, Val Acc: 0.7800, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 068, Loss: 0.4059, Val Acc: 0.7733, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 069, Loss: 0.4018, Val Acc: 0.7600, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 070, Loss: 0.3923, Val Acc: 0.7600, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 071, Loss: 0.3937, Val Acc: 0.7667, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 072, Loss: 0.3929, Val Acc: 0.7733, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 073, Loss: 0.3976, Val Acc: 0.7533, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 074, Loss: 0.3912, Val Acc: 0.7667, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 075, Loss: 0.3933, Val Acc: 0.7733, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 076, Loss: 0.3849, Val Acc: 0.7467, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 077, Loss: 0.3780, Val Acc: 0.7667, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 078, Loss: 0.3865, Val Acc: 0.7467, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 079, Loss: 0.3835, Val Acc: 0.7467, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 080, Loss: 0.3727, Val Acc: 0.7667, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 081, Loss: 0.3736, Val Acc: 0.7733, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 082, Loss: 0.3756, Val Acc: 0.7533, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 083, Loss: 0.3866, Val Acc: 0.7467, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 084, Loss: 0.3930, Val Acc: 0.7667, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 085, Loss: 0.3734, Val Acc: 0.7267, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 086, Loss: 0.3898, Val Acc: 0.7667, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 087, Loss: 0.3675, Val Acc: 0.7667, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 088, Loss: 0.3762, Val Acc: 0.7533, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 089, Loss: 0.3709, Val Acc: 0.7600, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 090, Loss: 0.3635, Val Acc: 0.7533, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 091, Loss: 0.3563, Val Acc: 0.7533, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 092, Loss: 0.3546, Val Acc: 0.7533, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 093, Loss: 0.3503, Val Acc: 0.7667, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 094, Loss: 0.3605, Val Acc: 0.7533, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 095, Loss: 0.3733, Val Acc: 0.7533, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 096, Loss: 0.3820, Val Acc: 0.7400, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 097, Loss: 0.3653, Val Acc: 0.7400, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 098, Loss: 0.3624, Val Acc: 0.7400, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 099, Loss: 0.3691, Val Acc: 0.7533, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 100, Loss: 0.3721, Val Acc: 0.7400, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 101, Loss: 0.3627, Val Acc: 0.7667, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 102, Loss: 0.3708, Val Acc: 0.7333, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 103, Loss: 0.3668, Val Acc: 0.7333, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 104, Loss: 0.3475, Val Acc: 0.7467, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 105, Loss: 0.3617, Val Acc: 0.7333, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 106, Loss: 0.3473, Val Acc: 0.7400, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 107, Loss: 0.3436, Val Acc: 0.7533, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 108, Loss: 0.3510, Val Acc: 0.7267, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 109, Loss: 0.3436, Val Acc: 0.7467, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 110, Loss: 0.3475, Val Acc: 0.7533, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 111, Loss: 0.3397, Val Acc: 0.7467, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 112, Loss: 0.3388, Val Acc: 0.7200, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 113, Loss: 0.3511, Val Acc: 0.7467, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 114, Loss: 0.3453, Val Acc: 0.7267, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 115, Loss: 0.3402, Val Acc: 0.7400, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 116, Loss: 0.3295, Val Acc: 0.7400, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 117, Loss: 0.3288, Val Acc: 0.7467, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 118, Loss: 0.3272, Val Acc: 0.7267, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 119, Loss: 0.3403, Val Acc: 0.7533, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 120, Loss: 0.3444, Val Acc: 0.7067, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 121, Loss: 0.3481, Val Acc: 0.7000, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 122, Loss: 0.3327, Val Acc: 0.7200, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 123, Loss: 0.3490, Val Acc: 0.7200, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 124, Loss: 0.3535, Val Acc: 0.7267, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 125, Loss: 0.3266, Val Acc: 0.7600, Test Acc: 0.6600\n",
      "Seed: 44, Epoch: 126, Loss: 0.3501, Val Acc: 0.7200, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 127, Loss: 0.3617, Val Acc: 0.7400, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 128, Loss: 0.3407, Val Acc: 0.7733, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 129, Loss: 0.3376, Val Acc: 0.7200, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 130, Loss: 0.3473, Val Acc: 0.7133, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 131, Loss: 0.3319, Val Acc: 0.7733, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 132, Loss: 0.3508, Val Acc: 0.7333, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 133, Loss: 0.3313, Val Acc: 0.7200, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 134, Loss: 0.3525, Val Acc: 0.7200, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 135, Loss: 0.3263, Val Acc: 0.7400, Test Acc: 0.6667\n",
      "Seed: 44, Epoch: 136, Loss: 0.3408, Val Acc: 0.7067, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 137, Loss: 0.3271, Val Acc: 0.6933, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 138, Loss: 0.3421, Val Acc: 0.7267, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 139, Loss: 0.3427, Val Acc: 0.7200, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 140, Loss: 0.3547, Val Acc: 0.6800, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 141, Loss: 0.3404, Val Acc: 0.6800, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 142, Loss: 0.3187, Val Acc: 0.7533, Test Acc: 0.6867\n",
      "Seed: 44, Epoch: 143, Loss: 0.3477, Val Acc: 0.6933, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 144, Loss: 0.3236, Val Acc: 0.6867, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 145, Loss: 0.3245, Val Acc: 0.7067, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 146, Loss: 0.3162, Val Acc: 0.7067, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 147, Loss: 0.3119, Val Acc: 0.6933, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 148, Loss: 0.3089, Val Acc: 0.6867, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 149, Loss: 0.3086, Val Acc: 0.6867, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 150, Loss: 0.3285, Val Acc: 0.7267, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 151, Loss: 0.3093, Val Acc: 0.7000, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 152, Loss: 0.3217, Val Acc: 0.7000, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 153, Loss: 0.3346, Val Acc: 0.6933, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 154, Loss: 0.3272, Val Acc: 0.7133, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 155, Loss: 0.3251, Val Acc: 0.7133, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 156, Loss: 0.3133, Val Acc: 0.6933, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 157, Loss: 0.3339, Val Acc: 0.7133, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 158, Loss: 0.3190, Val Acc: 0.7133, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 159, Loss: 0.3277, Val Acc: 0.7000, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 160, Loss: 0.3189, Val Acc: 0.7133, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 161, Loss: 0.3106, Val Acc: 0.7133, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 162, Loss: 0.3206, Val Acc: 0.7400, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 163, Loss: 0.3186, Val Acc: 0.7067, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 164, Loss: 0.3329, Val Acc: 0.7133, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 165, Loss: 0.3031, Val Acc: 0.7467, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 166, Loss: 0.3350, Val Acc: 0.7200, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 167, Loss: 0.3077, Val Acc: 0.7133, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 168, Loss: 0.3297, Val Acc: 0.7267, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 169, Loss: 0.3098, Val Acc: 0.7333, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 170, Loss: 0.3287, Val Acc: 0.7133, Test Acc: 0.6933\n",
      "Seed: 44, Epoch: 171, Loss: 0.3589, Val Acc: 0.7000, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 172, Loss: 0.3591, Val Acc: 0.7267, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 173, Loss: 0.3423, Val Acc: 0.7267, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 174, Loss: 0.3146, Val Acc: 0.7000, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 175, Loss: 0.3375, Val Acc: 0.7067, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 176, Loss: 0.3330, Val Acc: 0.7267, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 177, Loss: 0.3171, Val Acc: 0.7333, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 178, Loss: 0.3183, Val Acc: 0.7067, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 179, Loss: 0.3067, Val Acc: 0.7067, Test Acc: 0.7133\n",
      "Seed: 44, Epoch: 180, Loss: 0.3054, Val Acc: 0.7067, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 181, Loss: 0.2992, Val Acc: 0.7133, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 182, Loss: 0.2985, Val Acc: 0.7067, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 183, Loss: 0.2954, Val Acc: 0.7000, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 184, Loss: 0.2963, Val Acc: 0.6933, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 185, Loss: 0.3073, Val Acc: 0.7200, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 186, Loss: 0.2964, Val Acc: 0.7200, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 187, Loss: 0.3014, Val Acc: 0.7000, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 188, Loss: 0.2975, Val Acc: 0.7000, Test Acc: 0.7333\n",
      "Seed: 44, Epoch: 189, Loss: 0.2958, Val Acc: 0.7200, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 190, Loss: 0.3036, Val Acc: 0.7200, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 191, Loss: 0.2914, Val Acc: 0.6933, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 192, Loss: 0.2903, Val Acc: 0.7267, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 193, Loss: 0.2909, Val Acc: 0.7200, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 194, Loss: 0.2996, Val Acc: 0.7133, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 195, Loss: 0.2860, Val Acc: 0.7000, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 196, Loss: 0.2926, Val Acc: 0.7000, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 197, Loss: 0.2870, Val Acc: 0.6933, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 198, Loss: 0.2922, Val Acc: 0.6867, Test Acc: 0.7667\n",
      "Seed: 44, Epoch: 199, Loss: 0.2897, Val Acc: 0.6933, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 200, Loss: 0.2831, Val Acc: 0.7200, Test Acc: 0.7333\n",
      "Early stopping at epoch 200 for seed 44\n",
      "Average Time: 35.54 seconds\n",
      "Var Time: 4.91 seconds\n",
      "Average Memory: 3022.67 MB\n",
      "Average Best Val Acc: 0.7822\n",
      "Std Best Test Acc: 0.0440\n",
      "Average Test Acc: 0.7444\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/XXX/Pooling/\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-BINARY\", transform=T.Compose([T.OneHotDegree(136)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_CO(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_CO, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = CoPooling(ratio=0.7, K=3, edge_ratio=0.8, nhid=64, alpha=0.1, Init='Random', Gamma=1.0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = CoPooling(ratio=0.7, K=3, edge_ratio=0.8, nhid=64, alpha=0.1, Init='Random', Gamma=1.0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, perm, _, batch, _, _, _ = self.pool1(x, edge_index, edge_attr=None, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, perm, _, batch, _, _, _ = self.pool2(x, edge_index, edge_attr=None, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 1.0919, Val Acc: 0.3022, Test Acc: 0.3333\n",
      "Seed: 42, Epoch: 002, Loss: 1.0851, Val Acc: 0.3911, Test Acc: 0.3333\n",
      "Seed: 42, Epoch: 003, Loss: 1.0776, Val Acc: 0.4000, Test Acc: 0.3378\n",
      "Seed: 42, Epoch: 004, Loss: 1.0758, Val Acc: 0.3733, Test Acc: 0.3644\n",
      "Seed: 42, Epoch: 005, Loss: 1.0727, Val Acc: 0.4178, Test Acc: 0.4356\n",
      "Seed: 42, Epoch: 006, Loss: 1.0665, Val Acc: 0.4267, Test Acc: 0.4489\n",
      "Seed: 42, Epoch: 007, Loss: 1.0806, Val Acc: 0.4222, Test Acc: 0.4444\n",
      "Seed: 42, Epoch: 008, Loss: 1.0609, Val Acc: 0.4222, Test Acc: 0.4133\n",
      "Seed: 42, Epoch: 009, Loss: 1.0579, Val Acc: 0.3822, Test Acc: 0.4267\n",
      "Seed: 42, Epoch: 010, Loss: 1.0651, Val Acc: 0.4133, Test Acc: 0.3867\n",
      "Seed: 42, Epoch: 011, Loss: 1.0544, Val Acc: 0.4089, Test Acc: 0.4089\n",
      "Seed: 42, Epoch: 012, Loss: 1.0486, Val Acc: 0.4000, Test Acc: 0.4089\n",
      "Seed: 42, Epoch: 013, Loss: 1.0480, Val Acc: 0.4000, Test Acc: 0.4133\n",
      "Seed: 42, Epoch: 014, Loss: 1.0427, Val Acc: 0.4089, Test Acc: 0.4178\n",
      "Seed: 42, Epoch: 015, Loss: 1.0371, Val Acc: 0.4133, Test Acc: 0.4267\n",
      "Seed: 42, Epoch: 016, Loss: 1.0337, Val Acc: 0.4089, Test Acc: 0.4311\n",
      "Seed: 42, Epoch: 017, Loss: 1.0275, Val Acc: 0.4222, Test Acc: 0.4089\n",
      "Seed: 42, Epoch: 018, Loss: 1.0175, Val Acc: 0.4222, Test Acc: 0.4356\n",
      "Seed: 42, Epoch: 019, Loss: 1.0183, Val Acc: 0.4222, Test Acc: 0.4489\n",
      "Seed: 42, Epoch: 020, Loss: 1.0030, Val Acc: 0.4356, Test Acc: 0.4444\n",
      "Seed: 42, Epoch: 021, Loss: 1.0068, Val Acc: 0.4311, Test Acc: 0.4311\n",
      "Seed: 42, Epoch: 022, Loss: 1.0025, Val Acc: 0.4222, Test Acc: 0.4356\n",
      "Seed: 42, Epoch: 023, Loss: 1.0082, Val Acc: 0.4267, Test Acc: 0.4356\n",
      "Seed: 42, Epoch: 024, Loss: 1.0004, Val Acc: 0.4267, Test Acc: 0.4533\n",
      "Seed: 42, Epoch: 025, Loss: 0.9964, Val Acc: 0.4711, Test Acc: 0.4667\n",
      "Seed: 42, Epoch: 026, Loss: 0.9970, Val Acc: 0.4667, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 027, Loss: 0.9961, Val Acc: 0.4844, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 028, Loss: 0.9860, Val Acc: 0.4889, Test Acc: 0.4667\n",
      "Seed: 42, Epoch: 029, Loss: 0.9853, Val Acc: 0.4933, Test Acc: 0.4667\n",
      "Seed: 42, Epoch: 030, Loss: 0.9880, Val Acc: 0.4844, Test Acc: 0.4533\n",
      "Seed: 42, Epoch: 031, Loss: 0.9849, Val Acc: 0.4667, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 032, Loss: 0.9862, Val Acc: 0.4844, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 033, Loss: 0.9828, Val Acc: 0.4844, Test Acc: 0.4622\n",
      "Seed: 42, Epoch: 034, Loss: 0.9818, Val Acc: 0.4800, Test Acc: 0.4533\n",
      "Seed: 42, Epoch: 035, Loss: 0.9861, Val Acc: 0.4844, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 036, Loss: 0.9762, Val Acc: 0.4756, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 037, Loss: 0.9728, Val Acc: 0.4800, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 038, Loss: 0.9819, Val Acc: 0.4578, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 039, Loss: 0.9769, Val Acc: 0.4756, Test Acc: 0.4489\n",
      "Seed: 42, Epoch: 040, Loss: 0.9802, Val Acc: 0.4889, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 041, Loss: 0.9869, Val Acc: 0.4933, Test Acc: 0.4489\n",
      "Seed: 42, Epoch: 042, Loss: 0.9832, Val Acc: 0.4844, Test Acc: 0.4444\n",
      "Seed: 42, Epoch: 043, Loss: 0.9763, Val Acc: 0.4844, Test Acc: 0.4578\n",
      "Seed: 42, Epoch: 044, Loss: 0.9741, Val Acc: 0.4844, Test Acc: 0.4622\n",
      "Seed: 42, Epoch: 045, Loss: 0.9699, Val Acc: 0.4978, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 046, Loss: 0.9683, Val Acc: 0.4800, Test Acc: 0.4578\n",
      "Seed: 42, Epoch: 047, Loss: 0.9657, Val Acc: 0.4933, Test Acc: 0.4578\n",
      "Seed: 42, Epoch: 048, Loss: 0.9648, Val Acc: 0.4933, Test Acc: 0.4400\n",
      "Seed: 42, Epoch: 049, Loss: 0.9678, Val Acc: 0.4800, Test Acc: 0.4444\n",
      "Seed: 42, Epoch: 050, Loss: 0.9607, Val Acc: 0.4622, Test Acc: 0.4489\n",
      "Seed: 42, Epoch: 051, Loss: 0.9699, Val Acc: 0.4578, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 052, Loss: 0.9746, Val Acc: 0.4578, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 053, Loss: 0.9673, Val Acc: 0.4756, Test Acc: 0.4400\n",
      "Seed: 42, Epoch: 054, Loss: 0.9770, Val Acc: 0.4844, Test Acc: 0.4400\n",
      "Seed: 42, Epoch: 055, Loss: 0.9775, Val Acc: 0.4889, Test Acc: 0.4400\n",
      "Seed: 42, Epoch: 056, Loss: 0.9731, Val Acc: 0.4844, Test Acc: 0.4400\n",
      "Seed: 42, Epoch: 057, Loss: 0.9601, Val Acc: 0.4711, Test Acc: 0.4533\n",
      "Seed: 42, Epoch: 058, Loss: 0.9677, Val Acc: 0.4622, Test Acc: 0.4622\n",
      "Seed: 42, Epoch: 059, Loss: 0.9587, Val Acc: 0.4622, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 060, Loss: 0.9618, Val Acc: 0.4933, Test Acc: 0.4622\n",
      "Seed: 42, Epoch: 061, Loss: 0.9518, Val Acc: 0.4889, Test Acc: 0.4622\n",
      "Seed: 42, Epoch: 062, Loss: 0.9515, Val Acc: 0.4844, Test Acc: 0.4667\n",
      "Seed: 42, Epoch: 063, Loss: 0.9573, Val Acc: 0.4756, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 064, Loss: 0.9479, Val Acc: 0.4800, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 065, Loss: 0.9531, Val Acc: 0.4756, Test Acc: 0.4667\n",
      "Seed: 42, Epoch: 066, Loss: 0.9577, Val Acc: 0.4622, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 067, Loss: 0.9556, Val Acc: 0.4756, Test Acc: 0.4622\n",
      "Seed: 42, Epoch: 068, Loss: 0.9538, Val Acc: 0.4711, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 069, Loss: 0.9629, Val Acc: 0.4711, Test Acc: 0.4533\n",
      "Seed: 42, Epoch: 070, Loss: 0.9525, Val Acc: 0.4667, Test Acc: 0.4578\n",
      "Seed: 42, Epoch: 071, Loss: 0.9505, Val Acc: 0.4756, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 072, Loss: 0.9515, Val Acc: 0.4711, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 073, Loss: 0.9533, Val Acc: 0.4756, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 074, Loss: 0.9518, Val Acc: 0.4667, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 075, Loss: 0.9441, Val Acc: 0.4622, Test Acc: 0.4622\n",
      "Seed: 42, Epoch: 076, Loss: 0.9476, Val Acc: 0.4622, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 077, Loss: 0.9477, Val Acc: 0.4489, Test Acc: 0.4978\n",
      "Seed: 42, Epoch: 078, Loss: 0.9460, Val Acc: 0.4533, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 079, Loss: 0.9412, Val Acc: 0.4622, Test Acc: 0.4533\n",
      "Seed: 42, Epoch: 080, Loss: 0.9486, Val Acc: 0.4667, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 081, Loss: 0.9425, Val Acc: 0.4711, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 082, Loss: 0.9389, Val Acc: 0.4578, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 083, Loss: 0.9418, Val Acc: 0.4489, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 084, Loss: 0.9435, Val Acc: 0.4711, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 085, Loss: 0.9345, Val Acc: 0.4578, Test Acc: 0.5111\n",
      "Seed: 42, Epoch: 086, Loss: 0.9374, Val Acc: 0.4622, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 087, Loss: 0.9276, Val Acc: 0.4711, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 088, Loss: 0.9303, Val Acc: 0.4711, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 089, Loss: 0.9283, Val Acc: 0.4667, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 090, Loss: 0.9268, Val Acc: 0.4844, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 091, Loss: 0.9244, Val Acc: 0.4844, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 092, Loss: 0.9289, Val Acc: 0.4756, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 093, Loss: 0.9228, Val Acc: 0.4889, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 094, Loss: 0.9228, Val Acc: 0.4844, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 095, Loss: 0.9203, Val Acc: 0.4578, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 096, Loss: 0.9261, Val Acc: 0.4711, Test Acc: 0.5111\n",
      "Seed: 42, Epoch: 097, Loss: 0.9333, Val Acc: 0.4800, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 098, Loss: 0.9325, Val Acc: 0.4622, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 099, Loss: 0.9340, Val Acc: 0.4622, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 100, Loss: 0.9351, Val Acc: 0.4667, Test Acc: 0.4622\n",
      "Seed: 42, Epoch: 101, Loss: 0.9362, Val Acc: 0.4667, Test Acc: 0.4667\n",
      "Seed: 42, Epoch: 102, Loss: 0.9308, Val Acc: 0.4711, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 103, Loss: 0.9373, Val Acc: 0.4844, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 104, Loss: 0.9315, Val Acc: 0.4933, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 105, Loss: 0.9382, Val Acc: 0.4889, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 106, Loss: 0.9343, Val Acc: 0.4844, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 107, Loss: 0.9235, Val Acc: 0.4889, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 108, Loss: 0.9216, Val Acc: 0.4844, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 109, Loss: 0.9188, Val Acc: 0.4889, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 110, Loss: 0.9125, Val Acc: 0.4800, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 111, Loss: 0.9090, Val Acc: 0.4889, Test Acc: 0.4978\n",
      "Seed: 42, Epoch: 112, Loss: 0.9128, Val Acc: 0.4978, Test Acc: 0.4978\n",
      "Seed: 42, Epoch: 113, Loss: 0.9084, Val Acc: 0.4800, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 114, Loss: 0.9015, Val Acc: 0.4711, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 115, Loss: 0.9150, Val Acc: 0.4756, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 116, Loss: 0.9168, Val Acc: 0.4756, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 117, Loss: 0.9179, Val Acc: 0.4756, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 118, Loss: 0.9229, Val Acc: 0.4889, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 119, Loss: 0.9209, Val Acc: 0.4889, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 120, Loss: 0.9131, Val Acc: 0.4889, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 121, Loss: 0.9303, Val Acc: 0.4889, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 122, Loss: 0.9162, Val Acc: 0.4756, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 123, Loss: 0.9206, Val Acc: 0.4844, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 124, Loss: 0.9464, Val Acc: 0.4578, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 125, Loss: 0.9679, Val Acc: 0.4889, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 126, Loss: 0.9462, Val Acc: 0.4978, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 127, Loss: 0.9437, Val Acc: 0.4933, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 128, Loss: 0.9465, Val Acc: 0.4889, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 129, Loss: 0.9529, Val Acc: 0.4844, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 130, Loss: 0.9393, Val Acc: 0.4756, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 131, Loss: 0.9370, Val Acc: 0.4844, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 132, Loss: 0.9448, Val Acc: 0.4622, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 133, Loss: 0.9353, Val Acc: 0.4800, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 134, Loss: 0.9302, Val Acc: 0.4756, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 135, Loss: 0.9320, Val Acc: 0.5022, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 136, Loss: 0.9169, Val Acc: 0.4800, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 137, Loss: 0.9228, Val Acc: 0.4800, Test Acc: 0.5156\n",
      "Seed: 42, Epoch: 138, Loss: 0.9357, Val Acc: 0.4711, Test Acc: 0.5156\n",
      "Seed: 42, Epoch: 139, Loss: 0.9223, Val Acc: 0.4667, Test Acc: 0.5111\n",
      "Seed: 42, Epoch: 140, Loss: 0.9163, Val Acc: 0.4800, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 141, Loss: 0.9176, Val Acc: 0.4711, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 142, Loss: 0.9168, Val Acc: 0.4844, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 143, Loss: 0.9282, Val Acc: 0.4800, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 144, Loss: 0.9199, Val Acc: 0.4711, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 145, Loss: 0.9245, Val Acc: 0.4667, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 146, Loss: 0.9257, Val Acc: 0.4667, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 147, Loss: 0.9147, Val Acc: 0.4756, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 148, Loss: 0.9163, Val Acc: 0.4711, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 149, Loss: 0.9177, Val Acc: 0.4800, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 150, Loss: 0.9162, Val Acc: 0.4800, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 151, Loss: 0.9033, Val Acc: 0.4889, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 152, Loss: 0.9057, Val Acc: 0.4800, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 153, Loss: 0.8958, Val Acc: 0.4933, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 154, Loss: 0.8935, Val Acc: 0.4711, Test Acc: 0.4756\n",
      "Seed: 42, Epoch: 155, Loss: 0.8894, Val Acc: 0.4667, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 156, Loss: 0.8994, Val Acc: 0.4844, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 157, Loss: 0.8893, Val Acc: 0.4933, Test Acc: 0.5200\n",
      "Seed: 42, Epoch: 158, Loss: 0.8972, Val Acc: 0.4844, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 159, Loss: 0.8985, Val Acc: 0.4800, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 160, Loss: 0.9079, Val Acc: 0.4667, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 161, Loss: 0.9132, Val Acc: 0.4711, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 162, Loss: 0.9087, Val Acc: 0.4667, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 163, Loss: 0.9005, Val Acc: 0.4756, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 164, Loss: 0.8942, Val Acc: 0.4800, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 165, Loss: 0.8930, Val Acc: 0.4889, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 166, Loss: 0.8968, Val Acc: 0.4844, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 167, Loss: 0.8992, Val Acc: 0.4844, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 168, Loss: 0.9076, Val Acc: 0.4800, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 169, Loss: 0.8884, Val Acc: 0.4711, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 170, Loss: 0.8934, Val Acc: 0.4800, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 171, Loss: 0.8897, Val Acc: 0.4800, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 172, Loss: 0.8896, Val Acc: 0.4844, Test Acc: 0.4978\n",
      "Seed: 42, Epoch: 173, Loss: 0.9006, Val Acc: 0.4800, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 174, Loss: 0.8891, Val Acc: 0.4400, Test Acc: 0.4978\n",
      "Seed: 42, Epoch: 175, Loss: 0.9233, Val Acc: 0.4489, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 176, Loss: 0.9265, Val Acc: 0.4756, Test Acc: 0.5111\n",
      "Seed: 42, Epoch: 177, Loss: 0.9279, Val Acc: 0.4889, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 178, Loss: 0.9218, Val Acc: 0.4844, Test Acc: 0.4667\n",
      "Seed: 42, Epoch: 179, Loss: 0.9192, Val Acc: 0.4844, Test Acc: 0.4711\n",
      "Seed: 42, Epoch: 180, Loss: 0.9230, Val Acc: 0.4978, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 181, Loss: 0.9167, Val Acc: 0.4889, Test Acc: 0.4533\n",
      "Seed: 42, Epoch: 182, Loss: 0.9136, Val Acc: 0.4889, Test Acc: 0.4667\n",
      "Seed: 42, Epoch: 183, Loss: 0.9027, Val Acc: 0.4844, Test Acc: 0.4978\n",
      "Seed: 42, Epoch: 184, Loss: 0.9100, Val Acc: 0.4756, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 185, Loss: 0.8970, Val Acc: 0.4800, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 186, Loss: 0.8943, Val Acc: 0.4844, Test Acc: 0.5111\n",
      "Seed: 42, Epoch: 187, Loss: 0.8936, Val Acc: 0.4978, Test Acc: 0.5156\n",
      "Seed: 42, Epoch: 188, Loss: 0.8875, Val Acc: 0.4756, Test Acc: 0.5067\n",
      "Seed: 42, Epoch: 189, Loss: 0.9063, Val Acc: 0.4711, Test Acc: 0.5111\n",
      "Seed: 42, Epoch: 190, Loss: 0.9055, Val Acc: 0.4800, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 191, Loss: 0.8969, Val Acc: 0.4844, Test Acc: 0.4978\n",
      "Seed: 42, Epoch: 192, Loss: 0.8929, Val Acc: 0.4889, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 193, Loss: 0.8859, Val Acc: 0.4889, Test Acc: 0.4844\n",
      "Seed: 42, Epoch: 194, Loss: 0.8912, Val Acc: 0.4711, Test Acc: 0.4933\n",
      "Seed: 42, Epoch: 195, Loss: 0.8824, Val Acc: 0.4756, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 196, Loss: 0.8752, Val Acc: 0.4889, Test Acc: 0.4800\n",
      "Seed: 42, Epoch: 197, Loss: 0.8740, Val Acc: 0.4800, Test Acc: 0.4889\n",
      "Seed: 42, Epoch: 198, Loss: 0.8708, Val Acc: 0.4933, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 199, Loss: 0.8767, Val Acc: 0.4933, Test Acc: 0.5022\n",
      "Seed: 42, Epoch: 200, Loss: 0.8768, Val Acc: 0.4800, Test Acc: 0.4933\n",
      "Seed: 43, Epoch: 001, Loss: 1.0978, Val Acc: 0.3600, Test Acc: 0.3422\n",
      "Seed: 43, Epoch: 002, Loss: 1.0930, Val Acc: 0.3644, Test Acc: 0.3422\n",
      "Seed: 43, Epoch: 003, Loss: 1.0859, Val Acc: 0.3689, Test Acc: 0.3289\n",
      "Seed: 43, Epoch: 004, Loss: 1.0762, Val Acc: 0.4267, Test Acc: 0.3689\n",
      "Seed: 43, Epoch: 005, Loss: 1.0626, Val Acc: 0.3956, Test Acc: 0.3778\n",
      "Seed: 43, Epoch: 006, Loss: 1.0580, Val Acc: 0.4089, Test Acc: 0.3422\n",
      "Seed: 43, Epoch: 007, Loss: 1.0515, Val Acc: 0.4222, Test Acc: 0.3600\n",
      "Seed: 43, Epoch: 008, Loss: 1.0449, Val Acc: 0.4222, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 009, Loss: 1.0380, Val Acc: 0.4311, Test Acc: 0.4089\n",
      "Seed: 43, Epoch: 010, Loss: 1.0322, Val Acc: 0.4133, Test Acc: 0.4089\n",
      "Seed: 43, Epoch: 011, Loss: 1.0272, Val Acc: 0.4000, Test Acc: 0.3956\n",
      "Seed: 43, Epoch: 012, Loss: 1.0296, Val Acc: 0.4222, Test Acc: 0.4089\n",
      "Seed: 43, Epoch: 013, Loss: 1.0222, Val Acc: 0.4267, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 014, Loss: 1.0193, Val Acc: 0.4311, Test Acc: 0.4044\n",
      "Seed: 43, Epoch: 015, Loss: 1.0262, Val Acc: 0.4311, Test Acc: 0.3778\n",
      "Seed: 43, Epoch: 016, Loss: 1.0290, Val Acc: 0.4489, Test Acc: 0.3778\n",
      "Seed: 43, Epoch: 017, Loss: 1.0261, Val Acc: 0.4444, Test Acc: 0.3956\n",
      "Seed: 43, Epoch: 018, Loss: 1.0215, Val Acc: 0.4400, Test Acc: 0.3911\n",
      "Seed: 43, Epoch: 019, Loss: 1.0117, Val Acc: 0.4444, Test Acc: 0.3822\n",
      "Seed: 43, Epoch: 020, Loss: 1.0100, Val Acc: 0.4444, Test Acc: 0.4089\n",
      "Seed: 43, Epoch: 021, Loss: 1.0055, Val Acc: 0.4222, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 022, Loss: 1.0049, Val Acc: 0.4178, Test Acc: 0.4133\n",
      "Seed: 43, Epoch: 023, Loss: 0.9983, Val Acc: 0.4133, Test Acc: 0.4044\n",
      "Seed: 43, Epoch: 024, Loss: 0.9966, Val Acc: 0.4444, Test Acc: 0.4000\n",
      "Seed: 43, Epoch: 025, Loss: 0.9969, Val Acc: 0.4444, Test Acc: 0.4089\n",
      "Seed: 43, Epoch: 026, Loss: 1.0063, Val Acc: 0.3911, Test Acc: 0.4044\n",
      "Seed: 43, Epoch: 027, Loss: 1.0139, Val Acc: 0.4044, Test Acc: 0.4089\n",
      "Seed: 43, Epoch: 028, Loss: 0.9927, Val Acc: 0.4667, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 029, Loss: 0.9880, Val Acc: 0.4444, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 030, Loss: 0.9893, Val Acc: 0.4711, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 031, Loss: 0.9891, Val Acc: 0.4667, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 032, Loss: 0.9847, Val Acc: 0.4978, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 033, Loss: 0.9847, Val Acc: 0.5022, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 034, Loss: 0.9829, Val Acc: 0.4622, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 035, Loss: 0.9823, Val Acc: 0.4711, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 036, Loss: 0.9748, Val Acc: 0.4800, Test Acc: 0.4133\n",
      "Seed: 43, Epoch: 037, Loss: 0.9709, Val Acc: 0.4800, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 038, Loss: 0.9666, Val Acc: 0.5111, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 039, Loss: 0.9635, Val Acc: 0.5022, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 040, Loss: 0.9637, Val Acc: 0.4978, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 041, Loss: 0.9600, Val Acc: 0.5156, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 042, Loss: 0.9570, Val Acc: 0.4800, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 043, Loss: 0.9507, Val Acc: 0.4756, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 044, Loss: 0.9466, Val Acc: 0.4578, Test Acc: 0.4044\n",
      "Seed: 43, Epoch: 045, Loss: 0.9488, Val Acc: 0.4711, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 046, Loss: 0.9514, Val Acc: 0.4889, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 047, Loss: 0.9434, Val Acc: 0.4844, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 048, Loss: 0.9507, Val Acc: 0.4933, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 049, Loss: 0.9487, Val Acc: 0.5022, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 050, Loss: 0.9531, Val Acc: 0.4578, Test Acc: 0.4044\n",
      "Seed: 43, Epoch: 051, Loss: 0.9604, Val Acc: 0.4756, Test Acc: 0.4133\n",
      "Seed: 43, Epoch: 052, Loss: 0.9600, Val Acc: 0.4800, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 053, Loss: 0.9531, Val Acc: 0.5200, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 054, Loss: 0.9593, Val Acc: 0.5111, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 055, Loss: 0.9658, Val Acc: 0.5067, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 056, Loss: 0.9693, Val Acc: 0.5067, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 057, Loss: 0.9605, Val Acc: 0.4889, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 058, Loss: 0.9566, Val Acc: 0.4933, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 059, Loss: 0.9556, Val Acc: 0.4978, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 060, Loss: 0.9480, Val Acc: 0.4844, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 061, Loss: 0.9484, Val Acc: 0.4844, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 062, Loss: 0.9458, Val Acc: 0.4800, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 063, Loss: 0.9440, Val Acc: 0.4933, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 064, Loss: 0.9419, Val Acc: 0.4889, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 065, Loss: 0.9422, Val Acc: 0.4933, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 066, Loss: 0.9394, Val Acc: 0.4978, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 067, Loss: 0.9455, Val Acc: 0.5022, Test Acc: 0.4444\n",
      "Seed: 43, Epoch: 068, Loss: 0.9363, Val Acc: 0.4756, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 069, Loss: 0.9415, Val Acc: 0.4889, Test Acc: 0.4089\n",
      "Seed: 43, Epoch: 070, Loss: 0.9424, Val Acc: 0.4844, Test Acc: 0.4089\n",
      "Seed: 43, Epoch: 071, Loss: 0.9485, Val Acc: 0.4800, Test Acc: 0.4089\n",
      "Seed: 43, Epoch: 072, Loss: 0.9510, Val Acc: 0.4800, Test Acc: 0.4133\n",
      "Seed: 43, Epoch: 073, Loss: 0.9557, Val Acc: 0.4933, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 074, Loss: 0.9449, Val Acc: 0.4933, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 075, Loss: 0.9553, Val Acc: 0.4933, Test Acc: 0.4533\n",
      "Seed: 43, Epoch: 076, Loss: 0.9659, Val Acc: 0.4889, Test Acc: 0.4578\n",
      "Seed: 43, Epoch: 077, Loss: 0.9629, Val Acc: 0.5067, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 078, Loss: 0.9638, Val Acc: 0.4978, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 079, Loss: 0.9594, Val Acc: 0.4889, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 080, Loss: 0.9538, Val Acc: 0.5022, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 081, Loss: 0.9465, Val Acc: 0.5022, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 082, Loss: 0.9406, Val Acc: 0.4978, Test Acc: 0.4578\n",
      "Seed: 43, Epoch: 083, Loss: 0.9391, Val Acc: 0.4978, Test Acc: 0.4578\n",
      "Seed: 43, Epoch: 084, Loss: 0.9318, Val Acc: 0.4978, Test Acc: 0.4622\n",
      "Seed: 43, Epoch: 085, Loss: 0.9323, Val Acc: 0.4889, Test Acc: 0.4533\n",
      "Seed: 43, Epoch: 086, Loss: 0.9333, Val Acc: 0.5067, Test Acc: 0.4622\n",
      "Seed: 43, Epoch: 087, Loss: 0.9371, Val Acc: 0.5022, Test Acc: 0.4711\n",
      "Seed: 43, Epoch: 088, Loss: 0.9410, Val Acc: 0.4889, Test Acc: 0.4578\n",
      "Seed: 43, Epoch: 089, Loss: 0.9389, Val Acc: 0.4933, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 090, Loss: 0.9327, Val Acc: 0.4978, Test Acc: 0.4533\n",
      "Seed: 43, Epoch: 091, Loss: 0.9340, Val Acc: 0.4889, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 092, Loss: 0.9407, Val Acc: 0.5022, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 093, Loss: 0.9276, Val Acc: 0.5022, Test Acc: 0.4622\n",
      "Seed: 43, Epoch: 094, Loss: 0.9277, Val Acc: 0.5067, Test Acc: 0.4444\n",
      "Seed: 43, Epoch: 095, Loss: 0.9319, Val Acc: 0.5111, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 096, Loss: 0.9391, Val Acc: 0.5067, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 097, Loss: 0.9379, Val Acc: 0.5067, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 098, Loss: 0.9316, Val Acc: 0.5111, Test Acc: 0.4444\n",
      "Seed: 43, Epoch: 099, Loss: 0.9271, Val Acc: 0.4978, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 100, Loss: 0.9220, Val Acc: 0.5156, Test Acc: 0.4444\n",
      "Seed: 43, Epoch: 101, Loss: 0.9203, Val Acc: 0.4978, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 102, Loss: 0.9164, Val Acc: 0.5067, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 103, Loss: 0.9212, Val Acc: 0.5067, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 104, Loss: 0.9213, Val Acc: 0.5067, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 105, Loss: 0.9088, Val Acc: 0.4933, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 106, Loss: 0.9192, Val Acc: 0.4978, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 107, Loss: 0.9197, Val Acc: 0.4933, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 108, Loss: 0.9261, Val Acc: 0.4800, Test Acc: 0.4578\n",
      "Seed: 43, Epoch: 109, Loss: 0.9369, Val Acc: 0.4489, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 110, Loss: 0.9336, Val Acc: 0.4667, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 111, Loss: 0.9245, Val Acc: 0.4978, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 112, Loss: 0.9223, Val Acc: 0.5067, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 113, Loss: 0.9243, Val Acc: 0.5067, Test Acc: 0.4533\n",
      "Seed: 43, Epoch: 114, Loss: 0.9212, Val Acc: 0.4889, Test Acc: 0.4578\n",
      "Seed: 43, Epoch: 115, Loss: 0.9170, Val Acc: 0.4889, Test Acc: 0.4533\n",
      "Seed: 43, Epoch: 116, Loss: 0.9113, Val Acc: 0.5022, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 117, Loss: 0.9220, Val Acc: 0.5067, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 118, Loss: 0.9171, Val Acc: 0.5111, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 119, Loss: 0.9180, Val Acc: 0.4889, Test Acc: 0.4533\n",
      "Seed: 43, Epoch: 120, Loss: 0.9123, Val Acc: 0.4844, Test Acc: 0.4622\n",
      "Seed: 43, Epoch: 121, Loss: 0.9201, Val Acc: 0.4800, Test Acc: 0.4533\n",
      "Seed: 43, Epoch: 122, Loss: 0.9212, Val Acc: 0.5022, Test Acc: 0.4622\n",
      "Seed: 43, Epoch: 123, Loss: 0.9197, Val Acc: 0.5156, Test Acc: 0.4444\n",
      "Seed: 43, Epoch: 124, Loss: 0.9211, Val Acc: 0.5289, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 125, Loss: 0.9148, Val Acc: 0.5200, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 126, Loss: 0.9146, Val Acc: 0.5111, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 127, Loss: 0.9182, Val Acc: 0.5022, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 128, Loss: 0.9091, Val Acc: 0.5111, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 129, Loss: 0.9177, Val Acc: 0.5111, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 130, Loss: 0.9147, Val Acc: 0.5156, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 131, Loss: 0.9150, Val Acc: 0.5200, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 132, Loss: 0.9149, Val Acc: 0.5156, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 133, Loss: 0.9068, Val Acc: 0.4978, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 134, Loss: 0.8975, Val Acc: 0.4933, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 135, Loss: 0.9010, Val Acc: 0.5111, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 136, Loss: 0.9036, Val Acc: 0.5067, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 137, Loss: 0.8919, Val Acc: 0.5111, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 138, Loss: 0.8914, Val Acc: 0.5022, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 139, Loss: 0.9001, Val Acc: 0.4933, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 140, Loss: 0.9049, Val Acc: 0.5022, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 141, Loss: 0.9174, Val Acc: 0.4756, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 142, Loss: 0.9119, Val Acc: 0.4889, Test Acc: 0.4044\n",
      "Seed: 43, Epoch: 143, Loss: 0.9705, Val Acc: 0.4889, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 144, Loss: 0.9166, Val Acc: 0.4889, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 145, Loss: 0.9251, Val Acc: 0.4400, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 146, Loss: 0.9294, Val Acc: 0.4889, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 147, Loss: 0.9266, Val Acc: 0.4889, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 148, Loss: 0.9159, Val Acc: 0.4889, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 149, Loss: 0.9174, Val Acc: 0.4933, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 150, Loss: 0.9145, Val Acc: 0.4978, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 151, Loss: 0.9127, Val Acc: 0.5156, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 152, Loss: 0.9116, Val Acc: 0.4933, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 153, Loss: 0.9028, Val Acc: 0.5111, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 154, Loss: 0.9065, Val Acc: 0.5022, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 155, Loss: 0.9014, Val Acc: 0.5067, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 156, Loss: 0.9026, Val Acc: 0.5111, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 157, Loss: 0.8991, Val Acc: 0.5067, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 158, Loss: 0.8960, Val Acc: 0.4933, Test Acc: 0.4089\n",
      "Seed: 43, Epoch: 159, Loss: 0.8943, Val Acc: 0.4978, Test Acc: 0.4044\n",
      "Seed: 43, Epoch: 160, Loss: 0.9064, Val Acc: 0.5111, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 161, Loss: 0.8999, Val Acc: 0.5067, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 162, Loss: 0.8952, Val Acc: 0.5067, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 163, Loss: 0.9087, Val Acc: 0.5022, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 164, Loss: 0.9096, Val Acc: 0.4978, Test Acc: 0.4444\n",
      "Seed: 43, Epoch: 165, Loss: 0.9136, Val Acc: 0.4800, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 166, Loss: 0.9134, Val Acc: 0.4800, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 167, Loss: 0.9146, Val Acc: 0.4889, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 168, Loss: 0.9117, Val Acc: 0.4844, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 169, Loss: 0.9057, Val Acc: 0.4933, Test Acc: 0.4444\n",
      "Seed: 43, Epoch: 170, Loss: 0.8992, Val Acc: 0.4978, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 171, Loss: 0.9043, Val Acc: 0.5022, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 172, Loss: 0.8918, Val Acc: 0.5022, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 173, Loss: 0.9018, Val Acc: 0.5022, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 174, Loss: 0.9010, Val Acc: 0.4444, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 175, Loss: 0.8993, Val Acc: 0.4844, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 176, Loss: 0.9081, Val Acc: 0.5022, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 177, Loss: 0.9034, Val Acc: 0.5067, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 178, Loss: 0.9048, Val Acc: 0.4889, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 179, Loss: 0.9078, Val Acc: 0.4978, Test Acc: 0.4533\n",
      "Seed: 43, Epoch: 180, Loss: 0.9109, Val Acc: 0.5022, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 181, Loss: 0.8995, Val Acc: 0.5289, Test Acc: 0.4400\n",
      "Seed: 43, Epoch: 182, Loss: 0.8954, Val Acc: 0.5111, Test Acc: 0.4533\n",
      "Seed: 43, Epoch: 183, Loss: 0.8883, Val Acc: 0.5067, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 184, Loss: 0.8888, Val Acc: 0.5022, Test Acc: 0.4489\n",
      "Seed: 43, Epoch: 185, Loss: 0.8954, Val Acc: 0.4889, Test Acc: 0.4178\n",
      "Seed: 43, Epoch: 186, Loss: 0.8951, Val Acc: 0.5022, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 187, Loss: 0.8854, Val Acc: 0.5111, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 188, Loss: 0.8852, Val Acc: 0.4578, Test Acc: 0.4267\n",
      "Seed: 43, Epoch: 189, Loss: 0.9025, Val Acc: 0.4889, Test Acc: 0.4044\n",
      "Seed: 43, Epoch: 190, Loss: 0.8963, Val Acc: 0.4844, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 191, Loss: 0.8962, Val Acc: 0.4978, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 192, Loss: 0.8937, Val Acc: 0.5022, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 193, Loss: 0.8938, Val Acc: 0.5067, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 194, Loss: 0.8991, Val Acc: 0.4978, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 195, Loss: 0.8923, Val Acc: 0.5111, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 196, Loss: 0.8902, Val Acc: 0.4933, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 197, Loss: 0.9061, Val Acc: 0.4889, Test Acc: 0.4311\n",
      "Seed: 43, Epoch: 198, Loss: 0.9000, Val Acc: 0.4978, Test Acc: 0.4222\n",
      "Seed: 43, Epoch: 199, Loss: 0.8949, Val Acc: 0.4800, Test Acc: 0.4356\n",
      "Seed: 43, Epoch: 200, Loss: 0.8878, Val Acc: 0.4889, Test Acc: 0.4356\n",
      "Seed: 44, Epoch: 001, Loss: 1.0996, Val Acc: 0.3867, Test Acc: 0.3244\n",
      "Seed: 44, Epoch: 002, Loss: 1.0870, Val Acc: 0.3911, Test Acc: 0.3244\n",
      "Seed: 44, Epoch: 003, Loss: 1.0808, Val Acc: 0.3911, Test Acc: 0.3244\n",
      "Seed: 44, Epoch: 004, Loss: 1.0779, Val Acc: 0.3911, Test Acc: 0.3200\n",
      "Seed: 44, Epoch: 005, Loss: 1.0757, Val Acc: 0.3911, Test Acc: 0.3200\n",
      "Seed: 44, Epoch: 006, Loss: 1.0732, Val Acc: 0.3911, Test Acc: 0.3200\n",
      "Seed: 44, Epoch: 007, Loss: 1.0727, Val Acc: 0.3956, Test Acc: 0.3244\n",
      "Seed: 44, Epoch: 008, Loss: 1.0728, Val Acc: 0.3956, Test Acc: 0.3156\n",
      "Seed: 44, Epoch: 009, Loss: 1.0727, Val Acc: 0.3867, Test Acc: 0.3244\n",
      "Seed: 44, Epoch: 010, Loss: 1.0709, Val Acc: 0.3911, Test Acc: 0.3244\n",
      "Seed: 44, Epoch: 011, Loss: 1.0687, Val Acc: 0.4000, Test Acc: 0.3778\n",
      "Seed: 44, Epoch: 012, Loss: 1.0658, Val Acc: 0.4044, Test Acc: 0.3956\n",
      "Seed: 44, Epoch: 013, Loss: 1.0619, Val Acc: 0.4311, Test Acc: 0.4711\n",
      "Seed: 44, Epoch: 014, Loss: 1.0579, Val Acc: 0.4622, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 015, Loss: 1.0513, Val Acc: 0.4533, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 016, Loss: 1.0471, Val Acc: 0.4578, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 017, Loss: 1.0413, Val Acc: 0.3867, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 018, Loss: 1.0644, Val Acc: 0.4756, Test Acc: 0.4578\n",
      "Seed: 44, Epoch: 019, Loss: 1.0301, Val Acc: 0.4622, Test Acc: 0.3822\n",
      "Seed: 44, Epoch: 020, Loss: 1.0259, Val Acc: 0.4667, Test Acc: 0.4089\n",
      "Seed: 44, Epoch: 021, Loss: 1.0248, Val Acc: 0.4667, Test Acc: 0.4489\n",
      "Seed: 44, Epoch: 022, Loss: 1.0144, Val Acc: 0.4533, Test Acc: 0.4756\n",
      "Seed: 44, Epoch: 023, Loss: 1.0180, Val Acc: 0.4356, Test Acc: 0.4800\n",
      "Seed: 44, Epoch: 024, Loss: 1.0035, Val Acc: 0.4489, Test Acc: 0.4578\n",
      "Seed: 44, Epoch: 025, Loss: 0.9968, Val Acc: 0.4667, Test Acc: 0.4667\n",
      "Seed: 44, Epoch: 026, Loss: 1.0030, Val Acc: 0.4356, Test Acc: 0.4667\n",
      "Seed: 44, Epoch: 027, Loss: 1.0169, Val Acc: 0.4533, Test Acc: 0.4489\n",
      "Seed: 44, Epoch: 028, Loss: 0.9999, Val Acc: 0.4756, Test Acc: 0.4578\n",
      "Seed: 44, Epoch: 029, Loss: 1.0003, Val Acc: 0.4267, Test Acc: 0.4178\n",
      "Seed: 44, Epoch: 030, Loss: 1.0039, Val Acc: 0.4356, Test Acc: 0.4133\n",
      "Seed: 44, Epoch: 031, Loss: 1.0008, Val Acc: 0.4133, Test Acc: 0.4667\n",
      "Seed: 44, Epoch: 032, Loss: 0.9977, Val Acc: 0.4311, Test Acc: 0.4667\n",
      "Seed: 44, Epoch: 033, Loss: 0.9912, Val Acc: 0.4356, Test Acc: 0.4533\n",
      "Seed: 44, Epoch: 034, Loss: 0.9870, Val Acc: 0.4400, Test Acc: 0.4622\n",
      "Seed: 44, Epoch: 035, Loss: 0.9833, Val Acc: 0.4356, Test Acc: 0.4667\n",
      "Seed: 44, Epoch: 036, Loss: 0.9818, Val Acc: 0.4267, Test Acc: 0.4711\n",
      "Seed: 44, Epoch: 037, Loss: 0.9798, Val Acc: 0.4400, Test Acc: 0.4533\n",
      "Seed: 44, Epoch: 038, Loss: 0.9785, Val Acc: 0.4444, Test Acc: 0.4533\n",
      "Seed: 44, Epoch: 039, Loss: 0.9773, Val Acc: 0.4356, Test Acc: 0.4578\n",
      "Seed: 44, Epoch: 040, Loss: 0.9765, Val Acc: 0.4178, Test Acc: 0.4578\n",
      "Seed: 44, Epoch: 041, Loss: 0.9763, Val Acc: 0.4267, Test Acc: 0.4533\n",
      "Seed: 44, Epoch: 042, Loss: 0.9751, Val Acc: 0.4222, Test Acc: 0.4533\n",
      "Seed: 44, Epoch: 043, Loss: 0.9724, Val Acc: 0.4267, Test Acc: 0.4489\n",
      "Seed: 44, Epoch: 044, Loss: 0.9708, Val Acc: 0.4356, Test Acc: 0.4267\n",
      "Seed: 44, Epoch: 045, Loss: 0.9766, Val Acc: 0.4178, Test Acc: 0.4400\n",
      "Seed: 44, Epoch: 046, Loss: 0.9754, Val Acc: 0.4311, Test Acc: 0.4356\n",
      "Seed: 44, Epoch: 047, Loss: 0.9754, Val Acc: 0.4222, Test Acc: 0.4711\n",
      "Seed: 44, Epoch: 048, Loss: 0.9802, Val Acc: 0.4311, Test Acc: 0.4578\n",
      "Seed: 44, Epoch: 049, Loss: 0.9714, Val Acc: 0.4178, Test Acc: 0.4489\n",
      "Seed: 44, Epoch: 050, Loss: 0.9679, Val Acc: 0.4089, Test Acc: 0.4356\n",
      "Seed: 44, Epoch: 051, Loss: 0.9678, Val Acc: 0.4089, Test Acc: 0.4267\n",
      "Seed: 44, Epoch: 052, Loss: 0.9687, Val Acc: 0.4133, Test Acc: 0.4222\n",
      "Seed: 44, Epoch: 053, Loss: 0.9704, Val Acc: 0.4133, Test Acc: 0.4311\n",
      "Seed: 44, Epoch: 054, Loss: 0.9618, Val Acc: 0.4444, Test Acc: 0.4356\n",
      "Seed: 44, Epoch: 055, Loss: 0.9661, Val Acc: 0.4444, Test Acc: 0.4356\n",
      "Seed: 44, Epoch: 056, Loss: 0.9682, Val Acc: 0.4533, Test Acc: 0.4400\n",
      "Seed: 44, Epoch: 057, Loss: 0.9654, Val Acc: 0.4578, Test Acc: 0.4622\n",
      "Seed: 44, Epoch: 058, Loss: 0.9635, Val Acc: 0.4533, Test Acc: 0.4622\n",
      "Seed: 44, Epoch: 059, Loss: 0.9619, Val Acc: 0.4489, Test Acc: 0.4756\n",
      "Seed: 44, Epoch: 060, Loss: 0.9577, Val Acc: 0.4356, Test Acc: 0.4667\n",
      "Seed: 44, Epoch: 061, Loss: 0.9549, Val Acc: 0.4267, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 062, Loss: 0.9544, Val Acc: 0.4756, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 063, Loss: 0.9542, Val Acc: 0.4711, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 064, Loss: 0.9566, Val Acc: 0.4711, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 065, Loss: 0.9563, Val Acc: 0.4667, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 066, Loss: 0.9581, Val Acc: 0.4844, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 067, Loss: 0.9629, Val Acc: 0.4933, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 068, Loss: 0.9732, Val Acc: 0.4222, Test Acc: 0.4800\n",
      "Seed: 44, Epoch: 069, Loss: 0.9822, Val Acc: 0.4933, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 070, Loss: 0.9774, Val Acc: 0.4933, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 071, Loss: 0.9732, Val Acc: 0.4889, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 072, Loss: 0.9671, Val Acc: 0.4622, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 073, Loss: 0.9611, Val Acc: 0.4667, Test Acc: 0.4844\n",
      "Seed: 44, Epoch: 074, Loss: 0.9564, Val Acc: 0.4622, Test Acc: 0.4978\n",
      "Seed: 44, Epoch: 075, Loss: 0.9467, Val Acc: 0.4667, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 076, Loss: 0.9490, Val Acc: 0.4978, Test Acc: 0.4756\n",
      "Seed: 44, Epoch: 077, Loss: 0.9645, Val Acc: 0.4978, Test Acc: 0.4978\n",
      "Seed: 44, Epoch: 078, Loss: 0.9481, Val Acc: 0.5022, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 079, Loss: 0.9506, Val Acc: 0.4889, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 080, Loss: 0.9479, Val Acc: 0.4756, Test Acc: 0.4978\n",
      "Seed: 44, Epoch: 081, Loss: 0.9519, Val Acc: 0.4756, Test Acc: 0.5022\n",
      "Seed: 44, Epoch: 082, Loss: 0.9534, Val Acc: 0.4756, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 083, Loss: 0.9482, Val Acc: 0.4844, Test Acc: 0.5067\n",
      "Seed: 44, Epoch: 084, Loss: 0.9519, Val Acc: 0.4756, Test Acc: 0.5067\n",
      "Seed: 44, Epoch: 085, Loss: 0.9583, Val Acc: 0.4711, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 086, Loss: 0.9570, Val Acc: 0.4844, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 087, Loss: 0.9521, Val Acc: 0.4889, Test Acc: 0.5333\n",
      "Seed: 44, Epoch: 088, Loss: 0.9463, Val Acc: 0.4933, Test Acc: 0.5289\n",
      "Seed: 44, Epoch: 089, Loss: 0.9407, Val Acc: 0.4800, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 090, Loss: 0.9406, Val Acc: 0.4844, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 091, Loss: 0.9370, Val Acc: 0.4844, Test Acc: 0.5022\n",
      "Seed: 44, Epoch: 092, Loss: 0.9358, Val Acc: 0.4711, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 093, Loss: 0.9350, Val Acc: 0.4667, Test Acc: 0.5067\n",
      "Seed: 44, Epoch: 094, Loss: 0.9343, Val Acc: 0.4622, Test Acc: 0.5067\n",
      "Seed: 44, Epoch: 095, Loss: 0.9347, Val Acc: 0.4711, Test Acc: 0.5022\n",
      "Seed: 44, Epoch: 096, Loss: 0.9371, Val Acc: 0.4978, Test Acc: 0.4844\n",
      "Seed: 44, Epoch: 097, Loss: 0.9342, Val Acc: 0.5156, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 098, Loss: 0.9435, Val Acc: 0.4978, Test Acc: 0.4844\n",
      "Seed: 44, Epoch: 099, Loss: 0.9371, Val Acc: 0.4933, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 100, Loss: 0.9370, Val Acc: 0.4889, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 101, Loss: 0.9348, Val Acc: 0.4844, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 102, Loss: 0.9365, Val Acc: 0.4933, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 103, Loss: 0.9402, Val Acc: 0.4933, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 104, Loss: 0.9399, Val Acc: 0.4933, Test Acc: 0.5378\n",
      "Seed: 44, Epoch: 105, Loss: 0.9332, Val Acc: 0.4889, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 106, Loss: 0.9311, Val Acc: 0.4756, Test Acc: 0.5333\n",
      "Seed: 44, Epoch: 107, Loss: 0.9296, Val Acc: 0.4756, Test Acc: 0.5289\n",
      "Seed: 44, Epoch: 108, Loss: 0.9307, Val Acc: 0.4756, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 109, Loss: 0.9296, Val Acc: 0.4711, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 110, Loss: 0.9360, Val Acc: 0.4667, Test Acc: 0.5022\n",
      "Seed: 44, Epoch: 111, Loss: 0.9341, Val Acc: 0.4800, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 112, Loss: 0.9302, Val Acc: 0.5022, Test Acc: 0.5422\n",
      "Seed: 44, Epoch: 113, Loss: 0.9272, Val Acc: 0.5022, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 114, Loss: 0.9265, Val Acc: 0.4844, Test Acc: 0.4756\n",
      "Seed: 44, Epoch: 115, Loss: 0.9382, Val Acc: 0.4933, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 116, Loss: 0.9283, Val Acc: 0.4889, Test Acc: 0.5422\n",
      "Seed: 44, Epoch: 117, Loss: 0.9375, Val Acc: 0.4933, Test Acc: 0.5289\n",
      "Seed: 44, Epoch: 118, Loss: 0.9570, Val Acc: 0.4889, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 119, Loss: 0.9556, Val Acc: 0.4933, Test Acc: 0.5333\n",
      "Seed: 44, Epoch: 120, Loss: 0.9452, Val Acc: 0.4978, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 121, Loss: 0.9384, Val Acc: 0.4978, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 122, Loss: 0.9330, Val Acc: 0.4978, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 123, Loss: 0.9307, Val Acc: 0.5067, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 124, Loss: 0.9312, Val Acc: 0.5111, Test Acc: 0.4889\n",
      "Seed: 44, Epoch: 125, Loss: 0.9253, Val Acc: 0.5111, Test Acc: 0.5022\n",
      "Seed: 44, Epoch: 126, Loss: 0.9240, Val Acc: 0.5067, Test Acc: 0.5067\n",
      "Seed: 44, Epoch: 127, Loss: 0.9177, Val Acc: 0.4933, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 128, Loss: 0.9173, Val Acc: 0.5022, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 129, Loss: 0.9136, Val Acc: 0.4978, Test Acc: 0.5422\n",
      "Seed: 44, Epoch: 130, Loss: 0.9166, Val Acc: 0.4800, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 131, Loss: 0.9207, Val Acc: 0.4889, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 132, Loss: 0.9205, Val Acc: 0.4933, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 133, Loss: 0.9186, Val Acc: 0.4978, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 134, Loss: 0.9160, Val Acc: 0.4889, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 135, Loss: 0.9282, Val Acc: 0.4844, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 136, Loss: 0.9222, Val Acc: 0.5156, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 137, Loss: 0.9126, Val Acc: 0.5067, Test Acc: 0.5067\n",
      "Seed: 44, Epoch: 138, Loss: 0.9114, Val Acc: 0.5067, Test Acc: 0.5289\n",
      "Seed: 44, Epoch: 139, Loss: 0.9097, Val Acc: 0.5067, Test Acc: 0.5289\n",
      "Seed: 44, Epoch: 140, Loss: 0.9066, Val Acc: 0.5156, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 141, Loss: 0.9066, Val Acc: 0.5111, Test Acc: 0.4889\n",
      "Seed: 44, Epoch: 142, Loss: 0.9131, Val Acc: 0.5022, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 143, Loss: 0.9147, Val Acc: 0.4844, Test Acc: 0.5289\n",
      "Seed: 44, Epoch: 144, Loss: 0.9103, Val Acc: 0.4844, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 145, Loss: 0.9126, Val Acc: 0.4889, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 146, Loss: 0.9127, Val Acc: 0.4889, Test Acc: 0.5333\n",
      "Seed: 44, Epoch: 147, Loss: 0.9136, Val Acc: 0.4844, Test Acc: 0.4978\n",
      "Seed: 44, Epoch: 148, Loss: 0.9063, Val Acc: 0.4889, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 149, Loss: 0.9100, Val Acc: 0.4844, Test Acc: 0.5022\n",
      "Seed: 44, Epoch: 150, Loss: 0.9047, Val Acc: 0.4800, Test Acc: 0.4978\n",
      "Seed: 44, Epoch: 151, Loss: 0.9083, Val Acc: 0.4978, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 152, Loss: 0.9108, Val Acc: 0.4889, Test Acc: 0.5022\n",
      "Seed: 44, Epoch: 153, Loss: 0.9084, Val Acc: 0.4889, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 154, Loss: 0.9014, Val Acc: 0.4844, Test Acc: 0.5333\n",
      "Seed: 44, Epoch: 155, Loss: 0.8935, Val Acc: 0.4889, Test Acc: 0.5289\n",
      "Seed: 44, Epoch: 156, Loss: 0.8953, Val Acc: 0.4756, Test Acc: 0.5422\n",
      "Seed: 44, Epoch: 157, Loss: 0.8943, Val Acc: 0.4711, Test Acc: 0.5467\n",
      "Seed: 44, Epoch: 158, Loss: 0.8953, Val Acc: 0.4844, Test Acc: 0.5422\n",
      "Seed: 44, Epoch: 159, Loss: 0.8958, Val Acc: 0.4889, Test Acc: 0.5511\n",
      "Seed: 44, Epoch: 160, Loss: 0.8952, Val Acc: 0.4844, Test Acc: 0.5467\n",
      "Seed: 44, Epoch: 161, Loss: 0.8961, Val Acc: 0.4889, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 162, Loss: 0.9016, Val Acc: 0.4933, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 163, Loss: 0.8998, Val Acc: 0.4844, Test Acc: 0.5067\n",
      "Seed: 44, Epoch: 164, Loss: 0.9131, Val Acc: 0.4933, Test Acc: 0.5067\n",
      "Seed: 44, Epoch: 165, Loss: 0.9102, Val Acc: 0.5022, Test Acc: 0.4978\n",
      "Seed: 44, Epoch: 166, Loss: 0.9051, Val Acc: 0.5200, Test Acc: 0.5333\n",
      "Seed: 44, Epoch: 167, Loss: 0.9042, Val Acc: 0.5244, Test Acc: 0.5333\n",
      "Seed: 44, Epoch: 168, Loss: 0.9077, Val Acc: 0.5200, Test Acc: 0.4889\n",
      "Seed: 44, Epoch: 169, Loss: 0.9024, Val Acc: 0.5111, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 170, Loss: 0.9029, Val Acc: 0.5111, Test Acc: 0.4844\n",
      "Seed: 44, Epoch: 171, Loss: 0.9051, Val Acc: 0.5067, Test Acc: 0.4844\n",
      "Seed: 44, Epoch: 172, Loss: 0.9073, Val Acc: 0.5022, Test Acc: 0.4800\n",
      "Seed: 44, Epoch: 173, Loss: 0.8969, Val Acc: 0.4978, Test Acc: 0.4622\n",
      "Seed: 44, Epoch: 174, Loss: 0.8987, Val Acc: 0.4933, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 175, Loss: 0.8941, Val Acc: 0.4933, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 176, Loss: 0.8854, Val Acc: 0.5200, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 177, Loss: 0.8908, Val Acc: 0.5156, Test Acc: 0.4933\n",
      "Seed: 44, Epoch: 178, Loss: 0.8909, Val Acc: 0.5067, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 179, Loss: 0.8919, Val Acc: 0.4756, Test Acc: 0.4978\n",
      "Seed: 44, Epoch: 180, Loss: 0.8904, Val Acc: 0.4756, Test Acc: 0.5022\n",
      "Seed: 44, Epoch: 181, Loss: 0.8946, Val Acc: 0.4844, Test Acc: 0.5022\n",
      "Seed: 44, Epoch: 182, Loss: 0.8974, Val Acc: 0.5022, Test Acc: 0.4978\n",
      "Seed: 44, Epoch: 183, Loss: 0.8954, Val Acc: 0.4978, Test Acc: 0.4978\n",
      "Seed: 44, Epoch: 184, Loss: 0.8984, Val Acc: 0.4978, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 185, Loss: 0.9003, Val Acc: 0.4978, Test Acc: 0.5289\n",
      "Seed: 44, Epoch: 186, Loss: 0.8979, Val Acc: 0.5022, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 187, Loss: 0.8922, Val Acc: 0.5022, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 188, Loss: 0.8872, Val Acc: 0.5022, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 189, Loss: 0.8814, Val Acc: 0.4933, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 190, Loss: 0.8802, Val Acc: 0.4933, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 191, Loss: 0.8838, Val Acc: 0.4933, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 192, Loss: 0.8801, Val Acc: 0.5022, Test Acc: 0.5022\n",
      "Seed: 44, Epoch: 193, Loss: 0.8812, Val Acc: 0.4889, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 194, Loss: 0.8813, Val Acc: 0.5022, Test Acc: 0.5067\n",
      "Seed: 44, Epoch: 195, Loss: 0.8815, Val Acc: 0.5022, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 196, Loss: 0.8831, Val Acc: 0.5111, Test Acc: 0.5156\n",
      "Seed: 44, Epoch: 197, Loss: 0.8845, Val Acc: 0.5067, Test Acc: 0.5111\n",
      "Seed: 44, Epoch: 198, Loss: 0.8868, Val Acc: 0.4933, Test Acc: 0.5200\n",
      "Seed: 44, Epoch: 199, Loss: 0.8891, Val Acc: 0.5022, Test Acc: 0.5244\n",
      "Seed: 44, Epoch: 200, Loss: 0.8917, Val Acc: 0.4933, Test Acc: 0.5600\n",
      "Average Time: 48.17 seconds\n",
      "Var Time: 0.78 seconds\n",
      "Average Memory: 1943.33 MB\n",
      "Average Best Val Acc: 0.5185\n",
      "Std Best Test Acc: 0.0382\n",
      "Average Test Acc: 0.4889\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/XXX/Pooling/\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-MULTI\", transform=T.Compose([T.OneHotDegree(88)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_CO(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_CO, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = CoPooling(ratio=0.9, K=1, edge_ratio=0.8, nhid=64, alpha=0.1, Init='Random', Gamma=1.0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = CoPooling(ratio=0.9, K=1, edge_ratio=0.8, nhid=64, alpha=0.1, Init='Random', Gamma=1.0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, perm, _, batch, _, _, _ = self.pool1(x, edge_index, edge_attr=None, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, perm, _, batch, _, _, _ = self.pool2(x, edge_index, edge_attr=None, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 0.9134, Val Acc: 0.6280, Test Acc: 0.5933\n",
      "Seed: 42, Epoch: 002, Loss: 0.8109, Val Acc: 0.6773, Test Acc: 0.6693\n",
      "Seed: 42, Epoch: 003, Loss: 0.7397, Val Acc: 0.6600, Test Acc: 0.6600\n",
      "Seed: 42, Epoch: 004, Loss: 0.6781, Val Acc: 0.7027, Test Acc: 0.6960\n",
      "Seed: 42, Epoch: 005, Loss: 0.6423, Val Acc: 0.6973, Test Acc: 0.6893\n",
      "Seed: 42, Epoch: 006, Loss: 0.6250, Val Acc: 0.7080, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 007, Loss: 0.5973, Val Acc: 0.7067, Test Acc: 0.6960\n",
      "Seed: 42, Epoch: 008, Loss: 0.5910, Val Acc: 0.7080, Test Acc: 0.6973\n",
      "Seed: 42, Epoch: 009, Loss: 0.5700, Val Acc: 0.7067, Test Acc: 0.7053\n",
      "Seed: 42, Epoch: 010, Loss: 0.5570, Val Acc: 0.6773, Test Acc: 0.6933\n",
      "Seed: 42, Epoch: 011, Loss: 0.5613, Val Acc: 0.7107, Test Acc: 0.7093\n",
      "Seed: 42, Epoch: 012, Loss: 0.5453, Val Acc: 0.7147, Test Acc: 0.7080\n",
      "Seed: 42, Epoch: 013, Loss: 0.5364, Val Acc: 0.7133, Test Acc: 0.7040\n",
      "Seed: 42, Epoch: 014, Loss: 0.5270, Val Acc: 0.6800, Test Acc: 0.6947\n",
      "Seed: 42, Epoch: 015, Loss: 0.5248, Val Acc: 0.7067, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 016, Loss: 0.5110, Val Acc: 0.7227, Test Acc: 0.7120\n",
      "Seed: 42, Epoch: 017, Loss: 0.4977, Val Acc: 0.7147, Test Acc: 0.7120\n",
      "Seed: 42, Epoch: 018, Loss: 0.4833, Val Acc: 0.7227, Test Acc: 0.7147\n",
      "Seed: 42, Epoch: 019, Loss: 0.4683, Val Acc: 0.7467, Test Acc: 0.7227\n",
      "Seed: 42, Epoch: 020, Loss: 0.4517, Val Acc: 0.7400, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 021, Loss: 0.4520, Val Acc: 0.7467, Test Acc: 0.7240\n",
      "Seed: 42, Epoch: 022, Loss: 0.4460, Val Acc: 0.7560, Test Acc: 0.7280\n",
      "Seed: 42, Epoch: 023, Loss: 0.4374, Val Acc: 0.7587, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 024, Loss: 0.4268, Val Acc: 0.7613, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 025, Loss: 0.4189, Val Acc: 0.7720, Test Acc: 0.7560\n",
      "Seed: 42, Epoch: 026, Loss: 0.4109, Val Acc: 0.7600, Test Acc: 0.7533\n",
      "Seed: 42, Epoch: 027, Loss: 0.4158, Val Acc: 0.7627, Test Acc: 0.7520\n",
      "Seed: 42, Epoch: 028, Loss: 0.4076, Val Acc: 0.7867, Test Acc: 0.7573\n",
      "Seed: 42, Epoch: 029, Loss: 0.4140, Val Acc: 0.7600, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 030, Loss: 0.4019, Val Acc: 0.7667, Test Acc: 0.7600\n",
      "Seed: 42, Epoch: 031, Loss: 0.3977, Val Acc: 0.7947, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 032, Loss: 0.4026, Val Acc: 0.7667, Test Acc: 0.7680\n",
      "Seed: 42, Epoch: 033, Loss: 0.3825, Val Acc: 0.7920, Test Acc: 0.7787\n",
      "Seed: 42, Epoch: 034, Loss: 0.3813, Val Acc: 0.7787, Test Acc: 0.7640\n",
      "Seed: 42, Epoch: 035, Loss: 0.3800, Val Acc: 0.7960, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 036, Loss: 0.3678, Val Acc: 0.7707, Test Acc: 0.7880\n",
      "Seed: 42, Epoch: 037, Loss: 0.3695, Val Acc: 0.7960, Test Acc: 0.7827\n",
      "Seed: 42, Epoch: 038, Loss: 0.3601, Val Acc: 0.7880, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 039, Loss: 0.3553, Val Acc: 0.7853, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 040, Loss: 0.3469, Val Acc: 0.7853, Test Acc: 0.7840\n",
      "Seed: 42, Epoch: 041, Loss: 0.3486, Val Acc: 0.7867, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 042, Loss: 0.3460, Val Acc: 0.7827, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 043, Loss: 0.3462, Val Acc: 0.7800, Test Acc: 0.7813\n",
      "Seed: 42, Epoch: 044, Loss: 0.3463, Val Acc: 0.8000, Test Acc: 0.7853\n",
      "Seed: 42, Epoch: 045, Loss: 0.3406, Val Acc: 0.7947, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 046, Loss: 0.3256, Val Acc: 0.7947, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 047, Loss: 0.3184, Val Acc: 0.8053, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 048, Loss: 0.3149, Val Acc: 0.8027, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 049, Loss: 0.3103, Val Acc: 0.7987, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 050, Loss: 0.3059, Val Acc: 0.7920, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 051, Loss: 0.3042, Val Acc: 0.8120, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 052, Loss: 0.3610, Val Acc: 0.7400, Test Acc: 0.7573\n",
      "Seed: 42, Epoch: 053, Loss: 0.3528, Val Acc: 0.7973, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 054, Loss: 0.3265, Val Acc: 0.7973, Test Acc: 0.7880\n",
      "Seed: 42, Epoch: 055, Loss: 0.3201, Val Acc: 0.8147, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 056, Loss: 0.3090, Val Acc: 0.8133, Test Acc: 0.7880\n",
      "Seed: 42, Epoch: 057, Loss: 0.3056, Val Acc: 0.8187, Test Acc: 0.8147\n",
      "Seed: 42, Epoch: 058, Loss: 0.2982, Val Acc: 0.8160, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 059, Loss: 0.2934, Val Acc: 0.8040, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 060, Loss: 0.2897, Val Acc: 0.7853, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 061, Loss: 0.2859, Val Acc: 0.8200, Test Acc: 0.8173\n",
      "Seed: 42, Epoch: 062, Loss: 0.2787, Val Acc: 0.8187, Test Acc: 0.8200\n",
      "Seed: 42, Epoch: 063, Loss: 0.2736, Val Acc: 0.8160, Test Acc: 0.8187\n",
      "Seed: 42, Epoch: 064, Loss: 0.2721, Val Acc: 0.8080, Test Acc: 0.8227\n",
      "Seed: 42, Epoch: 065, Loss: 0.2701, Val Acc: 0.8093, Test Acc: 0.8107\n",
      "Seed: 42, Epoch: 066, Loss: 0.2640, Val Acc: 0.8173, Test Acc: 0.8173\n",
      "Seed: 42, Epoch: 067, Loss: 0.2670, Val Acc: 0.8147, Test Acc: 0.8147\n",
      "Seed: 42, Epoch: 068, Loss: 0.2673, Val Acc: 0.8200, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 069, Loss: 0.2597, Val Acc: 0.8173, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 070, Loss: 0.2645, Val Acc: 0.8147, Test Acc: 0.8107\n",
      "Seed: 42, Epoch: 071, Loss: 0.2664, Val Acc: 0.8040, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 072, Loss: 0.2732, Val Acc: 0.8173, Test Acc: 0.8293\n",
      "Seed: 42, Epoch: 073, Loss: 0.2664, Val Acc: 0.8013, Test Acc: 0.8133\n",
      "Seed: 42, Epoch: 074, Loss: 0.2564, Val Acc: 0.8293, Test Acc: 0.8213\n",
      "Seed: 42, Epoch: 075, Loss: 0.2501, Val Acc: 0.8147, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 076, Loss: 0.2515, Val Acc: 0.8200, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 077, Loss: 0.2542, Val Acc: 0.8160, Test Acc: 0.8133\n",
      "Seed: 42, Epoch: 078, Loss: 0.2504, Val Acc: 0.8227, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 079, Loss: 0.2459, Val Acc: 0.8173, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 080, Loss: 0.2478, Val Acc: 0.7880, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 081, Loss: 0.2403, Val Acc: 0.8227, Test Acc: 0.8200\n",
      "Seed: 42, Epoch: 082, Loss: 0.2465, Val Acc: 0.8227, Test Acc: 0.8187\n",
      "Seed: 42, Epoch: 083, Loss: 0.2443, Val Acc: 0.8080, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 084, Loss: 0.2436, Val Acc: 0.8080, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 085, Loss: 0.2344, Val Acc: 0.8027, Test Acc: 0.8187\n",
      "Seed: 42, Epoch: 086, Loss: 0.2356, Val Acc: 0.8200, Test Acc: 0.8120\n",
      "Seed: 42, Epoch: 087, Loss: 0.2345, Val Acc: 0.8173, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 088, Loss: 0.2456, Val Acc: 0.8200, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 089, Loss: 0.2424, Val Acc: 0.8160, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 090, Loss: 0.2382, Val Acc: 0.8027, Test Acc: 0.8107\n",
      "Seed: 42, Epoch: 091, Loss: 0.2345, Val Acc: 0.8160, Test Acc: 0.8107\n",
      "Seed: 42, Epoch: 092, Loss: 0.2319, Val Acc: 0.8213, Test Acc: 0.8147\n",
      "Seed: 42, Epoch: 093, Loss: 0.2317, Val Acc: 0.8120, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 094, Loss: 0.2504, Val Acc: 0.8253, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 095, Loss: 0.2325, Val Acc: 0.8267, Test Acc: 0.8080\n",
      "Seed: 42, Epoch: 096, Loss: 0.2307, Val Acc: 0.8147, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 097, Loss: 0.2259, Val Acc: 0.7960, Test Acc: 0.8080\n",
      "Seed: 42, Epoch: 098, Loss: 0.2389, Val Acc: 0.8187, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 099, Loss: 0.2235, Val Acc: 0.8213, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 100, Loss: 0.2297, Val Acc: 0.8133, Test Acc: 0.8200\n",
      "Seed: 42, Epoch: 101, Loss: 0.2276, Val Acc: 0.7960, Test Acc: 0.8080\n",
      "Seed: 42, Epoch: 102, Loss: 0.2181, Val Acc: 0.8080, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 103, Loss: 0.2167, Val Acc: 0.8240, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 104, Loss: 0.2089, Val Acc: 0.8107, Test Acc: 0.8147\n",
      "Seed: 42, Epoch: 105, Loss: 0.2049, Val Acc: 0.8133, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 106, Loss: 0.2033, Val Acc: 0.8240, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 107, Loss: 0.2030, Val Acc: 0.8200, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 108, Loss: 0.2053, Val Acc: 0.7907, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 109, Loss: 0.2072, Val Acc: 0.8187, Test Acc: 0.8133\n",
      "Seed: 42, Epoch: 110, Loss: 0.2031, Val Acc: 0.8173, Test Acc: 0.8173\n",
      "Seed: 42, Epoch: 111, Loss: 0.2027, Val Acc: 0.8173, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 112, Loss: 0.2250, Val Acc: 0.7880, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 113, Loss: 0.2307, Val Acc: 0.7867, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 114, Loss: 0.2402, Val Acc: 0.7787, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 115, Loss: 0.2371, Val Acc: 0.7987, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 116, Loss: 0.2204, Val Acc: 0.8093, Test Acc: 0.8107\n",
      "Seed: 42, Epoch: 117, Loss: 0.2074, Val Acc: 0.8307, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 118, Loss: 0.2038, Val Acc: 0.8133, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 119, Loss: 0.1938, Val Acc: 0.8227, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 120, Loss: 0.1899, Val Acc: 0.8200, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 121, Loss: 0.1894, Val Acc: 0.8133, Test Acc: 0.8133\n",
      "Seed: 42, Epoch: 122, Loss: 0.1864, Val Acc: 0.8040, Test Acc: 0.8080\n",
      "Seed: 42, Epoch: 123, Loss: 0.1845, Val Acc: 0.8133, Test Acc: 0.8080\n",
      "Seed: 42, Epoch: 124, Loss: 0.1807, Val Acc: 0.7973, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 125, Loss: 0.1836, Val Acc: 0.8120, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 126, Loss: 0.1832, Val Acc: 0.8040, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 127, Loss: 0.1799, Val Acc: 0.8067, Test Acc: 0.8080\n",
      "Seed: 42, Epoch: 128, Loss: 0.1785, Val Acc: 0.8067, Test Acc: 0.8080\n",
      "Seed: 42, Epoch: 129, Loss: 0.1798, Val Acc: 0.8107, Test Acc: 0.7933\n",
      "Seed: 42, Epoch: 130, Loss: 0.1787, Val Acc: 0.8120, Test Acc: 0.8133\n",
      "Seed: 42, Epoch: 131, Loss: 0.1924, Val Acc: 0.8133, Test Acc: 0.7920\n",
      "Seed: 42, Epoch: 132, Loss: 0.2158, Val Acc: 0.7867, Test Acc: 0.7880\n",
      "Seed: 42, Epoch: 133, Loss: 0.2029, Val Acc: 0.7947, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 134, Loss: 0.1912, Val Acc: 0.8027, Test Acc: 0.8227\n",
      "Seed: 42, Epoch: 135, Loss: 0.1834, Val Acc: 0.8027, Test Acc: 0.8120\n",
      "Seed: 42, Epoch: 136, Loss: 0.1829, Val Acc: 0.8027, Test Acc: 0.8147\n",
      "Seed: 42, Epoch: 137, Loss: 0.1759, Val Acc: 0.7947, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 138, Loss: 0.1758, Val Acc: 0.8053, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 139, Loss: 0.1744, Val Acc: 0.8107, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 140, Loss: 0.1799, Val Acc: 0.8093, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 141, Loss: 0.1679, Val Acc: 0.8040, Test Acc: 0.8147\n",
      "Seed: 42, Epoch: 142, Loss: 0.1747, Val Acc: 0.8213, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 143, Loss: 0.1756, Val Acc: 0.8053, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 144, Loss: 0.1665, Val Acc: 0.7973, Test Acc: 0.8187\n",
      "Seed: 42, Epoch: 145, Loss: 0.1769, Val Acc: 0.8093, Test Acc: 0.7853\n",
      "Seed: 42, Epoch: 146, Loss: 0.3190, Val Acc: 0.7840, Test Acc: 0.7947\n",
      "Seed: 42, Epoch: 147, Loss: 0.3512, Val Acc: 0.7933, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 148, Loss: 0.2701, Val Acc: 0.8227, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 149, Loss: 0.2304, Val Acc: 0.8040, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 150, Loss: 0.2160, Val Acc: 0.8067, Test Acc: 0.8080\n",
      "Seed: 42, Epoch: 151, Loss: 0.2012, Val Acc: 0.8133, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 152, Loss: 0.1897, Val Acc: 0.8000, Test Acc: 0.8173\n",
      "Seed: 42, Epoch: 153, Loss: 0.1822, Val Acc: 0.8053, Test Acc: 0.8187\n",
      "Seed: 42, Epoch: 154, Loss: 0.1728, Val Acc: 0.8160, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 155, Loss: 0.1727, Val Acc: 0.8107, Test Acc: 0.8147\n",
      "Seed: 42, Epoch: 156, Loss: 0.1696, Val Acc: 0.8067, Test Acc: 0.8160\n",
      "Seed: 42, Epoch: 157, Loss: 0.1619, Val Acc: 0.8053, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 158, Loss: 0.1563, Val Acc: 0.8067, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 159, Loss: 0.1564, Val Acc: 0.8107, Test Acc: 0.8133\n",
      "Seed: 42, Epoch: 160, Loss: 0.1525, Val Acc: 0.8107, Test Acc: 0.8120\n",
      "Seed: 42, Epoch: 161, Loss: 0.1526, Val Acc: 0.8133, Test Acc: 0.8120\n",
      "Seed: 42, Epoch: 162, Loss: 0.1494, Val Acc: 0.8013, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 163, Loss: 0.1563, Val Acc: 0.8080, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 164, Loss: 0.1531, Val Acc: 0.8227, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 165, Loss: 0.1487, Val Acc: 0.8080, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 166, Loss: 0.1512, Val Acc: 0.8147, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 167, Loss: 0.1482, Val Acc: 0.8120, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 168, Loss: 0.1446, Val Acc: 0.8147, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 169, Loss: 0.1414, Val Acc: 0.8253, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 170, Loss: 0.1406, Val Acc: 0.8133, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 171, Loss: 0.1419, Val Acc: 0.8107, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 172, Loss: 0.1397, Val Acc: 0.8173, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 173, Loss: 0.1405, Val Acc: 0.8160, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 174, Loss: 0.1409, Val Acc: 0.8227, Test Acc: 0.7947\n",
      "Seed: 42, Epoch: 175, Loss: 0.1627, Val Acc: 0.8013, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 176, Loss: 0.1729, Val Acc: 0.8133, Test Acc: 0.7893\n",
      "Seed: 42, Epoch: 177, Loss: 0.2690, Val Acc: 0.7693, Test Acc: 0.7787\n",
      "Seed: 42, Epoch: 178, Loss: 0.2491, Val Acc: 0.7760, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 179, Loss: 0.2162, Val Acc: 0.8240, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 180, Loss: 0.1786, Val Acc: 0.8133, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 181, Loss: 0.1614, Val Acc: 0.8173, Test Acc: 0.8120\n",
      "Seed: 42, Epoch: 182, Loss: 0.1519, Val Acc: 0.8147, Test Acc: 0.8093\n",
      "Seed: 42, Epoch: 183, Loss: 0.1450, Val Acc: 0.8133, Test Acc: 0.8120\n",
      "Seed: 42, Epoch: 184, Loss: 0.1527, Val Acc: 0.7853, Test Acc: 0.7867\n",
      "Seed: 42, Epoch: 185, Loss: 0.2077, Val Acc: 0.7973, Test Acc: 0.7733\n",
      "Seed: 42, Epoch: 186, Loss: 0.1932, Val Acc: 0.8200, Test Acc: 0.7960\n",
      "Seed: 42, Epoch: 187, Loss: 0.1812, Val Acc: 0.8093, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 188, Loss: 0.1672, Val Acc: 0.8200, Test Acc: 0.8107\n",
      "Seed: 42, Epoch: 189, Loss: 0.1597, Val Acc: 0.7947, Test Acc: 0.8027\n",
      "Seed: 42, Epoch: 190, Loss: 0.1602, Val Acc: 0.7800, Test Acc: 0.7973\n",
      "Seed: 42, Epoch: 191, Loss: 0.1553, Val Acc: 0.7907, Test Acc: 0.8040\n",
      "Seed: 42, Epoch: 192, Loss: 0.1483, Val Acc: 0.7973, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 193, Loss: 0.1446, Val Acc: 0.8000, Test Acc: 0.8053\n",
      "Seed: 42, Epoch: 194, Loss: 0.1414, Val Acc: 0.8120, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 195, Loss: 0.1453, Val Acc: 0.8107, Test Acc: 0.7947\n",
      "Seed: 42, Epoch: 196, Loss: 0.1427, Val Acc: 0.8027, Test Acc: 0.8000\n",
      "Seed: 42, Epoch: 197, Loss: 0.1379, Val Acc: 0.8040, Test Acc: 0.7987\n",
      "Seed: 42, Epoch: 198, Loss: 0.1301, Val Acc: 0.7933, Test Acc: 0.8067\n",
      "Seed: 42, Epoch: 199, Loss: 0.1267, Val Acc: 0.8187, Test Acc: 0.8013\n",
      "Seed: 42, Epoch: 200, Loss: 0.1262, Val Acc: 0.8080, Test Acc: 0.8040\n",
      "Seed: 43, Epoch: 001, Loss: 1.0167, Val Acc: 0.3853, Test Acc: 0.3867\n",
      "Seed: 43, Epoch: 002, Loss: 0.9071, Val Acc: 0.5333, Test Acc: 0.5333\n",
      "Seed: 43, Epoch: 003, Loss: 0.8070, Val Acc: 0.6493, Test Acc: 0.6333\n",
      "Seed: 43, Epoch: 004, Loss: 0.7397, Val Acc: 0.6947, Test Acc: 0.6867\n",
      "Seed: 43, Epoch: 005, Loss: 0.6873, Val Acc: 0.7240, Test Acc: 0.7147\n",
      "Seed: 43, Epoch: 006, Loss: 0.6419, Val Acc: 0.7253, Test Acc: 0.7280\n",
      "Seed: 43, Epoch: 007, Loss: 0.6153, Val Acc: 0.7293, Test Acc: 0.7320\n",
      "Seed: 43, Epoch: 008, Loss: 0.5956, Val Acc: 0.7293, Test Acc: 0.7227\n",
      "Seed: 43, Epoch: 009, Loss: 0.5743, Val Acc: 0.7333, Test Acc: 0.7413\n",
      "Seed: 43, Epoch: 010, Loss: 0.5622, Val Acc: 0.7360, Test Acc: 0.7440\n",
      "Seed: 43, Epoch: 011, Loss: 0.5522, Val Acc: 0.7387, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 012, Loss: 0.5436, Val Acc: 0.7587, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 013, Loss: 0.5298, Val Acc: 0.7640, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 014, Loss: 0.5199, Val Acc: 0.7680, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 015, Loss: 0.5178, Val Acc: 0.7613, Test Acc: 0.7560\n",
      "Seed: 43, Epoch: 016, Loss: 0.5089, Val Acc: 0.7800, Test Acc: 0.7653\n",
      "Seed: 43, Epoch: 017, Loss: 0.4953, Val Acc: 0.7747, Test Acc: 0.7640\n",
      "Seed: 43, Epoch: 018, Loss: 0.4916, Val Acc: 0.7773, Test Acc: 0.7613\n",
      "Seed: 43, Epoch: 019, Loss: 0.4818, Val Acc: 0.7867, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 020, Loss: 0.4771, Val Acc: 0.7787, Test Acc: 0.7493\n",
      "Seed: 43, Epoch: 021, Loss: 0.4777, Val Acc: 0.7733, Test Acc: 0.7547\n",
      "Seed: 43, Epoch: 022, Loss: 0.4819, Val Acc: 0.7800, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 023, Loss: 0.4679, Val Acc: 0.7733, Test Acc: 0.7653\n",
      "Seed: 43, Epoch: 024, Loss: 0.4656, Val Acc: 0.7787, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 025, Loss: 0.4555, Val Acc: 0.7840, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 026, Loss: 0.4507, Val Acc: 0.7933, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 027, Loss: 0.4355, Val Acc: 0.7800, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 028, Loss: 0.4277, Val Acc: 0.7840, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 029, Loss: 0.4226, Val Acc: 0.7880, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 030, Loss: 0.4213, Val Acc: 0.7960, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 031, Loss: 0.4090, Val Acc: 0.7933, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 032, Loss: 0.4062, Val Acc: 0.8013, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 033, Loss: 0.3977, Val Acc: 0.8027, Test Acc: 0.7947\n",
      "Seed: 43, Epoch: 034, Loss: 0.3954, Val Acc: 0.8053, Test Acc: 0.8013\n",
      "Seed: 43, Epoch: 035, Loss: 0.3839, Val Acc: 0.8040, Test Acc: 0.7987\n",
      "Seed: 43, Epoch: 036, Loss: 0.3792, Val Acc: 0.8133, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 037, Loss: 0.3739, Val Acc: 0.8027, Test Acc: 0.7987\n",
      "Seed: 43, Epoch: 038, Loss: 0.3685, Val Acc: 0.8173, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 039, Loss: 0.3701, Val Acc: 0.8120, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 040, Loss: 0.3717, Val Acc: 0.8067, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 041, Loss: 0.3754, Val Acc: 0.8053, Test Acc: 0.7960\n",
      "Seed: 43, Epoch: 042, Loss: 0.3687, Val Acc: 0.8160, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 043, Loss: 0.3611, Val Acc: 0.8053, Test Acc: 0.7920\n",
      "Seed: 43, Epoch: 044, Loss: 0.3663, Val Acc: 0.8107, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 045, Loss: 0.3537, Val Acc: 0.8133, Test Acc: 0.8053\n",
      "Seed: 43, Epoch: 046, Loss: 0.3450, Val Acc: 0.8187, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 047, Loss: 0.3397, Val Acc: 0.8213, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 048, Loss: 0.3374, Val Acc: 0.8147, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 049, Loss: 0.3366, Val Acc: 0.8240, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 050, Loss: 0.3287, Val Acc: 0.8187, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 051, Loss: 0.3276, Val Acc: 0.8213, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 052, Loss: 0.3259, Val Acc: 0.8173, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 053, Loss: 0.3221, Val Acc: 0.8187, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 054, Loss: 0.3180, Val Acc: 0.8213, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 055, Loss: 0.3178, Val Acc: 0.8213, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 056, Loss: 0.3109, Val Acc: 0.8267, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 057, Loss: 0.3174, Val Acc: 0.8213, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 058, Loss: 0.3187, Val Acc: 0.8160, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 059, Loss: 0.3142, Val Acc: 0.8200, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 060, Loss: 0.3044, Val Acc: 0.8240, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 061, Loss: 0.3057, Val Acc: 0.8320, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 062, Loss: 0.3103, Val Acc: 0.8147, Test Acc: 0.8013\n",
      "Seed: 43, Epoch: 063, Loss: 0.3091, Val Acc: 0.8200, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 064, Loss: 0.3106, Val Acc: 0.8293, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 065, Loss: 0.3074, Val Acc: 0.8293, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 066, Loss: 0.2980, Val Acc: 0.8293, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 067, Loss: 0.2934, Val Acc: 0.8333, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 068, Loss: 0.2930, Val Acc: 0.8293, Test Acc: 0.8307\n",
      "Seed: 43, Epoch: 069, Loss: 0.3012, Val Acc: 0.8213, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 070, Loss: 0.3005, Val Acc: 0.8360, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 071, Loss: 0.2880, Val Acc: 0.8293, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 072, Loss: 0.2827, Val Acc: 0.8280, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 073, Loss: 0.2893, Val Acc: 0.8200, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 074, Loss: 0.3468, Val Acc: 0.8093, Test Acc: 0.7920\n",
      "Seed: 43, Epoch: 075, Loss: 0.3343, Val Acc: 0.8227, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 076, Loss: 0.3184, Val Acc: 0.8133, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 077, Loss: 0.3113, Val Acc: 0.8147, Test Acc: 0.8053\n",
      "Seed: 43, Epoch: 078, Loss: 0.2995, Val Acc: 0.8107, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 079, Loss: 0.3000, Val Acc: 0.8293, Test Acc: 0.8053\n",
      "Seed: 43, Epoch: 080, Loss: 0.2957, Val Acc: 0.8227, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 081, Loss: 0.2906, Val Acc: 0.8267, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 082, Loss: 0.2933, Val Acc: 0.8267, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 083, Loss: 0.2878, Val Acc: 0.8333, Test Acc: 0.8293\n",
      "Seed: 43, Epoch: 084, Loss: 0.2822, Val Acc: 0.8213, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 085, Loss: 0.2816, Val Acc: 0.8240, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 086, Loss: 0.2731, Val Acc: 0.8347, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 087, Loss: 0.2685, Val Acc: 0.8227, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 088, Loss: 0.2718, Val Acc: 0.8347, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 089, Loss: 0.2632, Val Acc: 0.8227, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 090, Loss: 0.2658, Val Acc: 0.8267, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 091, Loss: 0.2676, Val Acc: 0.8280, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 092, Loss: 0.2647, Val Acc: 0.8333, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 093, Loss: 0.2622, Val Acc: 0.8347, Test Acc: 0.8280\n",
      "Seed: 43, Epoch: 094, Loss: 0.2600, Val Acc: 0.8333, Test Acc: 0.8293\n",
      "Seed: 43, Epoch: 095, Loss: 0.2513, Val Acc: 0.8307, Test Acc: 0.8293\n",
      "Seed: 43, Epoch: 096, Loss: 0.2538, Val Acc: 0.8280, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 097, Loss: 0.2510, Val Acc: 0.8373, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 098, Loss: 0.2521, Val Acc: 0.8240, Test Acc: 0.8293\n",
      "Seed: 43, Epoch: 099, Loss: 0.2621, Val Acc: 0.8213, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 100, Loss: 0.2849, Val Acc: 0.8280, Test Acc: 0.8373\n",
      "Seed: 43, Epoch: 101, Loss: 0.2701, Val Acc: 0.8333, Test Acc: 0.8360\n",
      "Seed: 43, Epoch: 102, Loss: 0.2547, Val Acc: 0.8307, Test Acc: 0.8333\n",
      "Seed: 43, Epoch: 103, Loss: 0.2534, Val Acc: 0.8307, Test Acc: 0.8280\n",
      "Seed: 43, Epoch: 104, Loss: 0.2429, Val Acc: 0.8347, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 105, Loss: 0.2451, Val Acc: 0.8293, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 106, Loss: 0.2391, Val Acc: 0.8333, Test Acc: 0.8280\n",
      "Seed: 43, Epoch: 107, Loss: 0.2358, Val Acc: 0.8373, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 108, Loss: 0.2353, Val Acc: 0.8387, Test Acc: 0.8280\n",
      "Seed: 43, Epoch: 109, Loss: 0.2304, Val Acc: 0.8253, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 110, Loss: 0.2314, Val Acc: 0.8320, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 111, Loss: 0.2287, Val Acc: 0.8333, Test Acc: 0.8267\n",
      "Seed: 43, Epoch: 112, Loss: 0.2252, Val Acc: 0.8360, Test Acc: 0.8347\n",
      "Seed: 43, Epoch: 113, Loss: 0.2363, Val Acc: 0.8307, Test Acc: 0.8040\n",
      "Seed: 43, Epoch: 114, Loss: 0.2373, Val Acc: 0.8307, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 115, Loss: 0.2293, Val Acc: 0.8267, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 116, Loss: 0.2271, Val Acc: 0.8240, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 117, Loss: 0.2254, Val Acc: 0.8240, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 118, Loss: 0.2293, Val Acc: 0.8320, Test Acc: 0.8333\n",
      "Seed: 43, Epoch: 119, Loss: 0.2251, Val Acc: 0.8307, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 120, Loss: 0.2236, Val Acc: 0.8293, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 121, Loss: 0.2335, Val Acc: 0.8307, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 122, Loss: 0.2214, Val Acc: 0.8347, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 123, Loss: 0.2166, Val Acc: 0.8293, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 124, Loss: 0.2120, Val Acc: 0.8280, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 125, Loss: 0.2073, Val Acc: 0.8347, Test Acc: 0.8240\n",
      "Seed: 43, Epoch: 126, Loss: 0.2091, Val Acc: 0.8373, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 127, Loss: 0.2147, Val Acc: 0.8267, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 128, Loss: 0.2193, Val Acc: 0.8373, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 129, Loss: 0.2265, Val Acc: 0.8333, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 130, Loss: 0.2209, Val Acc: 0.8333, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 131, Loss: 0.2158, Val Acc: 0.8293, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 132, Loss: 0.2156, Val Acc: 0.8360, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 133, Loss: 0.2244, Val Acc: 0.8293, Test Acc: 0.8267\n",
      "Seed: 43, Epoch: 134, Loss: 0.2228, Val Acc: 0.8347, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 135, Loss: 0.2083, Val Acc: 0.8227, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 136, Loss: 0.2050, Val Acc: 0.8307, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 137, Loss: 0.1993, Val Acc: 0.8320, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 138, Loss: 0.1962, Val Acc: 0.8307, Test Acc: 0.8293\n",
      "Seed: 43, Epoch: 139, Loss: 0.1965, Val Acc: 0.8360, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 140, Loss: 0.2035, Val Acc: 0.8253, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 141, Loss: 0.2079, Val Acc: 0.8267, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 142, Loss: 0.2000, Val Acc: 0.8280, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 143, Loss: 0.1903, Val Acc: 0.8387, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 144, Loss: 0.1876, Val Acc: 0.8307, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 145, Loss: 0.1861, Val Acc: 0.8253, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 146, Loss: 0.2029, Val Acc: 0.8280, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 147, Loss: 0.2104, Val Acc: 0.8227, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 148, Loss: 0.1985, Val Acc: 0.8347, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 149, Loss: 0.1914, Val Acc: 0.8307, Test Acc: 0.8253\n",
      "Seed: 43, Epoch: 150, Loss: 0.1966, Val Acc: 0.8360, Test Acc: 0.8320\n",
      "Seed: 43, Epoch: 151, Loss: 0.1901, Val Acc: 0.8360, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 152, Loss: 0.1846, Val Acc: 0.8373, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 153, Loss: 0.1789, Val Acc: 0.8293, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 154, Loss: 0.1827, Val Acc: 0.8227, Test Acc: 0.8200\n",
      "Seed: 43, Epoch: 155, Loss: 0.1793, Val Acc: 0.8333, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 156, Loss: 0.1765, Val Acc: 0.8280, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 157, Loss: 0.1764, Val Acc: 0.8240, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 158, Loss: 0.1820, Val Acc: 0.8240, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 159, Loss: 0.1743, Val Acc: 0.8333, Test Acc: 0.8227\n",
      "Seed: 43, Epoch: 160, Loss: 0.1737, Val Acc: 0.8307, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 161, Loss: 0.1743, Val Acc: 0.8360, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 162, Loss: 0.1781, Val Acc: 0.8093, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 163, Loss: 0.1783, Val Acc: 0.8360, Test Acc: 0.8053\n",
      "Seed: 43, Epoch: 164, Loss: 0.1771, Val Acc: 0.8267, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 165, Loss: 0.1961, Val Acc: 0.8200, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 166, Loss: 0.1908, Val Acc: 0.8187, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 167, Loss: 0.1916, Val Acc: 0.8147, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 168, Loss: 0.1973, Val Acc: 0.8280, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 169, Loss: 0.1880, Val Acc: 0.8133, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 170, Loss: 0.1805, Val Acc: 0.8173, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 171, Loss: 0.1743, Val Acc: 0.8267, Test Acc: 0.8040\n",
      "Seed: 43, Epoch: 172, Loss: 0.1748, Val Acc: 0.8333, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 173, Loss: 0.1705, Val Acc: 0.8227, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 174, Loss: 0.1675, Val Acc: 0.8240, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 175, Loss: 0.1623, Val Acc: 0.8320, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 176, Loss: 0.1635, Val Acc: 0.8240, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 177, Loss: 0.1586, Val Acc: 0.8267, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 178, Loss: 0.1572, Val Acc: 0.8253, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 179, Loss: 0.1555, Val Acc: 0.8307, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 180, Loss: 0.1505, Val Acc: 0.8240, Test Acc: 0.8133\n",
      "Seed: 43, Epoch: 181, Loss: 0.1523, Val Acc: 0.8253, Test Acc: 0.8213\n",
      "Seed: 43, Epoch: 182, Loss: 0.1599, Val Acc: 0.8280, Test Acc: 0.8187\n",
      "Seed: 43, Epoch: 183, Loss: 0.1672, Val Acc: 0.8280, Test Acc: 0.8173\n",
      "Seed: 43, Epoch: 184, Loss: 0.1584, Val Acc: 0.8280, Test Acc: 0.8040\n",
      "Seed: 43, Epoch: 185, Loss: 0.1575, Val Acc: 0.8227, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 186, Loss: 0.1568, Val Acc: 0.8173, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 187, Loss: 0.1503, Val Acc: 0.8133, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 188, Loss: 0.1538, Val Acc: 0.8253, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 189, Loss: 0.1490, Val Acc: 0.8227, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 190, Loss: 0.1459, Val Acc: 0.8227, Test Acc: 0.8120\n",
      "Seed: 43, Epoch: 191, Loss: 0.1427, Val Acc: 0.8187, Test Acc: 0.8080\n",
      "Seed: 43, Epoch: 192, Loss: 0.1405, Val Acc: 0.8213, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 193, Loss: 0.1375, Val Acc: 0.8253, Test Acc: 0.8107\n",
      "Seed: 43, Epoch: 194, Loss: 0.1350, Val Acc: 0.8200, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 195, Loss: 0.1356, Val Acc: 0.8227, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 196, Loss: 0.1421, Val Acc: 0.8187, Test Acc: 0.8160\n",
      "Seed: 43, Epoch: 197, Loss: 0.1760, Val Acc: 0.8173, Test Acc: 0.8000\n",
      "Seed: 43, Epoch: 198, Loss: 0.1623, Val Acc: 0.8120, Test Acc: 0.8040\n",
      "Seed: 43, Epoch: 199, Loss: 0.1478, Val Acc: 0.8227, Test Acc: 0.8147\n",
      "Seed: 43, Epoch: 200, Loss: 0.1412, Val Acc: 0.8253, Test Acc: 0.8173\n",
      "Seed: 44, Epoch: 001, Loss: 1.0245, Val Acc: 0.5800, Test Acc: 0.6040\n",
      "Seed: 44, Epoch: 002, Loss: 0.9341, Val Acc: 0.6107, Test Acc: 0.6373\n",
      "Seed: 44, Epoch: 003, Loss: 0.8074, Val Acc: 0.6560, Test Acc: 0.6800\n",
      "Seed: 44, Epoch: 004, Loss: 0.7012, Val Acc: 0.6787, Test Acc: 0.6840\n",
      "Seed: 44, Epoch: 005, Loss: 0.6574, Val Acc: 0.6960, Test Acc: 0.6987\n",
      "Seed: 44, Epoch: 006, Loss: 0.6235, Val Acc: 0.7253, Test Acc: 0.7147\n",
      "Seed: 44, Epoch: 007, Loss: 0.5983, Val Acc: 0.7320, Test Acc: 0.7227\n",
      "Seed: 44, Epoch: 008, Loss: 0.5815, Val Acc: 0.7280, Test Acc: 0.7173\n",
      "Seed: 44, Epoch: 009, Loss: 0.5660, Val Acc: 0.7347, Test Acc: 0.7160\n",
      "Seed: 44, Epoch: 010, Loss: 0.5504, Val Acc: 0.7333, Test Acc: 0.7160\n",
      "Seed: 44, Epoch: 011, Loss: 0.5448, Val Acc: 0.7347, Test Acc: 0.7107\n",
      "Seed: 44, Epoch: 012, Loss: 0.5400, Val Acc: 0.7333, Test Acc: 0.7147\n",
      "Seed: 44, Epoch: 013, Loss: 0.5255, Val Acc: 0.7347, Test Acc: 0.7200\n",
      "Seed: 44, Epoch: 014, Loss: 0.5116, Val Acc: 0.7333, Test Acc: 0.7227\n",
      "Seed: 44, Epoch: 015, Loss: 0.5023, Val Acc: 0.7320, Test Acc: 0.7240\n",
      "Seed: 44, Epoch: 016, Loss: 0.4929, Val Acc: 0.7373, Test Acc: 0.7293\n",
      "Seed: 44, Epoch: 017, Loss: 0.4851, Val Acc: 0.7373, Test Acc: 0.7293\n",
      "Seed: 44, Epoch: 018, Loss: 0.4767, Val Acc: 0.7400, Test Acc: 0.7293\n",
      "Seed: 44, Epoch: 019, Loss: 0.4656, Val Acc: 0.7373, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 020, Loss: 0.4603, Val Acc: 0.7413, Test Acc: 0.7267\n",
      "Seed: 44, Epoch: 021, Loss: 0.4571, Val Acc: 0.7440, Test Acc: 0.7293\n",
      "Seed: 44, Epoch: 022, Loss: 0.4503, Val Acc: 0.7280, Test Acc: 0.7173\n",
      "Seed: 44, Epoch: 023, Loss: 0.4434, Val Acc: 0.7320, Test Acc: 0.7120\n",
      "Seed: 44, Epoch: 024, Loss: 0.4342, Val Acc: 0.7427, Test Acc: 0.7280\n",
      "Seed: 44, Epoch: 025, Loss: 0.4307, Val Acc: 0.7333, Test Acc: 0.7307\n",
      "Seed: 44, Epoch: 026, Loss: 0.4165, Val Acc: 0.7373, Test Acc: 0.7227\n",
      "Seed: 44, Epoch: 027, Loss: 0.4062, Val Acc: 0.7360, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 028, Loss: 0.4124, Val Acc: 0.7480, Test Acc: 0.7387\n",
      "Seed: 44, Epoch: 029, Loss: 0.3995, Val Acc: 0.7373, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 030, Loss: 0.3931, Val Acc: 0.7440, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 031, Loss: 0.3863, Val Acc: 0.7467, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 032, Loss: 0.3896, Val Acc: 0.7467, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 033, Loss: 0.3782, Val Acc: 0.7520, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 034, Loss: 0.3744, Val Acc: 0.7680, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 035, Loss: 0.3687, Val Acc: 0.7693, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 036, Loss: 0.3657, Val Acc: 0.7653, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 037, Loss: 0.3644, Val Acc: 0.7680, Test Acc: 0.7880\n",
      "Seed: 44, Epoch: 038, Loss: 0.3788, Val Acc: 0.7627, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 039, Loss: 0.4174, Val Acc: 0.7600, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 040, Loss: 0.3978, Val Acc: 0.7613, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 041, Loss: 0.3778, Val Acc: 0.7787, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 042, Loss: 0.3645, Val Acc: 0.7853, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 043, Loss: 0.3534, Val Acc: 0.7853, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 044, Loss: 0.3568, Val Acc: 0.7853, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 045, Loss: 0.3511, Val Acc: 0.7747, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 046, Loss: 0.3419, Val Acc: 0.7933, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 047, Loss: 0.3465, Val Acc: 0.7907, Test Acc: 0.8213\n",
      "Seed: 44, Epoch: 048, Loss: 0.3303, Val Acc: 0.7853, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 049, Loss: 0.3299, Val Acc: 0.7947, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 050, Loss: 0.3281, Val Acc: 0.7907, Test Acc: 0.8347\n",
      "Seed: 44, Epoch: 051, Loss: 0.3228, Val Acc: 0.7853, Test Acc: 0.8213\n",
      "Seed: 44, Epoch: 052, Loss: 0.3406, Val Acc: 0.7867, Test Acc: 0.8240\n",
      "Seed: 44, Epoch: 053, Loss: 0.3447, Val Acc: 0.7920, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 054, Loss: 0.3398, Val Acc: 0.7880, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 055, Loss: 0.3355, Val Acc: 0.7827, Test Acc: 0.8187\n",
      "Seed: 44, Epoch: 056, Loss: 0.3267, Val Acc: 0.7907, Test Acc: 0.8307\n",
      "Seed: 44, Epoch: 057, Loss: 0.3174, Val Acc: 0.8000, Test Acc: 0.8213\n",
      "Seed: 44, Epoch: 058, Loss: 0.3068, Val Acc: 0.7960, Test Acc: 0.8293\n",
      "Seed: 44, Epoch: 059, Loss: 0.3076, Val Acc: 0.7947, Test Acc: 0.8253\n",
      "Seed: 44, Epoch: 060, Loss: 0.3018, Val Acc: 0.8000, Test Acc: 0.8320\n",
      "Seed: 44, Epoch: 061, Loss: 0.2963, Val Acc: 0.8053, Test Acc: 0.8307\n",
      "Seed: 44, Epoch: 062, Loss: 0.2917, Val Acc: 0.8053, Test Acc: 0.8333\n",
      "Seed: 44, Epoch: 063, Loss: 0.2842, Val Acc: 0.7933, Test Acc: 0.8360\n",
      "Seed: 44, Epoch: 064, Loss: 0.2857, Val Acc: 0.8013, Test Acc: 0.8267\n",
      "Seed: 44, Epoch: 065, Loss: 0.2809, Val Acc: 0.8013, Test Acc: 0.8293\n",
      "Seed: 44, Epoch: 066, Loss: 0.2747, Val Acc: 0.8013, Test Acc: 0.8280\n",
      "Seed: 44, Epoch: 067, Loss: 0.2715, Val Acc: 0.8053, Test Acc: 0.8280\n",
      "Seed: 44, Epoch: 068, Loss: 0.2708, Val Acc: 0.7920, Test Acc: 0.8267\n",
      "Seed: 44, Epoch: 069, Loss: 0.2663, Val Acc: 0.8040, Test Acc: 0.8227\n",
      "Seed: 44, Epoch: 070, Loss: 0.2667, Val Acc: 0.7960, Test Acc: 0.8200\n",
      "Seed: 44, Epoch: 071, Loss: 0.2591, Val Acc: 0.8013, Test Acc: 0.8253\n",
      "Seed: 44, Epoch: 072, Loss: 0.2609, Val Acc: 0.7947, Test Acc: 0.8200\n",
      "Seed: 44, Epoch: 073, Loss: 0.2601, Val Acc: 0.8107, Test Acc: 0.8240\n",
      "Seed: 44, Epoch: 074, Loss: 0.2756, Val Acc: 0.7973, Test Acc: 0.8173\n",
      "Seed: 44, Epoch: 075, Loss: 0.2812, Val Acc: 0.8053, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 076, Loss: 0.2647, Val Acc: 0.8053, Test Acc: 0.8200\n",
      "Seed: 44, Epoch: 077, Loss: 0.2648, Val Acc: 0.7880, Test Acc: 0.8253\n",
      "Seed: 44, Epoch: 078, Loss: 0.2732, Val Acc: 0.8027, Test Acc: 0.8333\n",
      "Seed: 44, Epoch: 079, Loss: 0.2591, Val Acc: 0.7933, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 080, Loss: 0.2486, Val Acc: 0.8027, Test Acc: 0.8280\n",
      "Seed: 44, Epoch: 081, Loss: 0.2443, Val Acc: 0.8040, Test Acc: 0.8200\n",
      "Seed: 44, Epoch: 082, Loss: 0.2490, Val Acc: 0.8027, Test Acc: 0.8360\n",
      "Seed: 44, Epoch: 083, Loss: 0.2425, Val Acc: 0.8000, Test Acc: 0.8280\n",
      "Seed: 44, Epoch: 084, Loss: 0.2377, Val Acc: 0.8027, Test Acc: 0.8280\n",
      "Seed: 44, Epoch: 085, Loss: 0.2387, Val Acc: 0.8067, Test Acc: 0.8267\n",
      "Seed: 44, Epoch: 086, Loss: 0.2336, Val Acc: 0.8013, Test Acc: 0.8320\n",
      "Seed: 44, Epoch: 087, Loss: 0.2426, Val Acc: 0.8120, Test Acc: 0.8173\n",
      "Seed: 44, Epoch: 088, Loss: 0.2660, Val Acc: 0.8040, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 089, Loss: 0.2574, Val Acc: 0.7893, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 090, Loss: 0.2622, Val Acc: 0.7893, Test Acc: 0.8240\n",
      "Seed: 44, Epoch: 091, Loss: 0.2471, Val Acc: 0.8120, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 092, Loss: 0.2454, Val Acc: 0.8027, Test Acc: 0.8333\n",
      "Seed: 44, Epoch: 093, Loss: 0.2354, Val Acc: 0.8000, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 094, Loss: 0.2540, Val Acc: 0.7973, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 095, Loss: 0.2384, Val Acc: 0.8053, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 096, Loss: 0.2346, Val Acc: 0.8080, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 097, Loss: 0.2263, Val Acc: 0.8080, Test Acc: 0.8360\n",
      "Seed: 44, Epoch: 098, Loss: 0.2196, Val Acc: 0.8053, Test Acc: 0.8320\n",
      "Seed: 44, Epoch: 099, Loss: 0.2200, Val Acc: 0.7933, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 100, Loss: 0.2328, Val Acc: 0.7987, Test Acc: 0.8293\n",
      "Seed: 44, Epoch: 101, Loss: 0.2231, Val Acc: 0.7987, Test Acc: 0.8200\n",
      "Seed: 44, Epoch: 102, Loss: 0.2208, Val Acc: 0.8120, Test Acc: 0.8280\n",
      "Seed: 44, Epoch: 103, Loss: 0.2202, Val Acc: 0.8120, Test Acc: 0.8267\n",
      "Seed: 44, Epoch: 104, Loss: 0.2231, Val Acc: 0.8027, Test Acc: 0.8267\n",
      "Seed: 44, Epoch: 105, Loss: 0.2220, Val Acc: 0.8040, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 106, Loss: 0.2520, Val Acc: 0.8120, Test Acc: 0.8227\n",
      "Seed: 44, Epoch: 107, Loss: 0.2239, Val Acc: 0.8000, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 108, Loss: 0.2295, Val Acc: 0.8147, Test Acc: 0.8240\n",
      "Seed: 44, Epoch: 109, Loss: 0.2215, Val Acc: 0.8093, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 110, Loss: 0.2191, Val Acc: 0.8080, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 111, Loss: 0.2060, Val Acc: 0.8067, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 112, Loss: 0.2044, Val Acc: 0.7960, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 113, Loss: 0.1984, Val Acc: 0.8160, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 114, Loss: 0.2041, Val Acc: 0.8093, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 115, Loss: 0.1978, Val Acc: 0.8067, Test Acc: 0.8173\n",
      "Seed: 44, Epoch: 116, Loss: 0.1942, Val Acc: 0.8173, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 117, Loss: 0.2021, Val Acc: 0.8107, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 118, Loss: 0.1916, Val Acc: 0.8040, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 119, Loss: 0.1940, Val Acc: 0.8093, Test Acc: 0.8173\n",
      "Seed: 44, Epoch: 120, Loss: 0.2054, Val Acc: 0.8027, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 121, Loss: 0.1941, Val Acc: 0.7987, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 122, Loss: 0.1997, Val Acc: 0.8200, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 123, Loss: 0.2004, Val Acc: 0.8107, Test Acc: 0.8213\n",
      "Seed: 44, Epoch: 124, Loss: 0.1952, Val Acc: 0.7973, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 125, Loss: 0.1955, Val Acc: 0.8160, Test Acc: 0.8213\n",
      "Seed: 44, Epoch: 126, Loss: 0.1855, Val Acc: 0.8053, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 127, Loss: 0.1901, Val Acc: 0.8107, Test Acc: 0.8187\n",
      "Seed: 44, Epoch: 128, Loss: 0.1922, Val Acc: 0.7960, Test Acc: 0.7933\n",
      "Seed: 44, Epoch: 129, Loss: 0.2141, Val Acc: 0.8067, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 130, Loss: 0.2977, Val Acc: 0.7853, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 131, Loss: 0.2760, Val Acc: 0.7707, Test Acc: 0.7680\n",
      "Seed: 44, Epoch: 132, Loss: 0.2684, Val Acc: 0.7947, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 133, Loss: 0.2453, Val Acc: 0.7987, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 134, Loss: 0.2230, Val Acc: 0.8107, Test Acc: 0.8187\n",
      "Seed: 44, Epoch: 135, Loss: 0.2098, Val Acc: 0.8107, Test Acc: 0.8173\n",
      "Seed: 44, Epoch: 136, Loss: 0.1958, Val Acc: 0.8120, Test Acc: 0.8187\n",
      "Seed: 44, Epoch: 137, Loss: 0.1880, Val Acc: 0.8133, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 138, Loss: 0.1855, Val Acc: 0.7973, Test Acc: 0.8147\n",
      "Seed: 44, Epoch: 139, Loss: 0.1855, Val Acc: 0.8067, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 140, Loss: 0.1784, Val Acc: 0.8080, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 141, Loss: 0.1789, Val Acc: 0.8053, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 142, Loss: 0.1730, Val Acc: 0.7973, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 143, Loss: 0.1745, Val Acc: 0.8000, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 144, Loss: 0.1736, Val Acc: 0.8013, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 145, Loss: 0.1745, Val Acc: 0.7987, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 146, Loss: 0.1734, Val Acc: 0.8027, Test Acc: 0.8133\n",
      "Seed: 44, Epoch: 147, Loss: 0.1703, Val Acc: 0.8120, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 148, Loss: 0.1754, Val Acc: 0.8027, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 149, Loss: 0.1742, Val Acc: 0.8067, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 150, Loss: 0.1699, Val Acc: 0.8053, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 151, Loss: 0.1670, Val Acc: 0.7987, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 152, Loss: 0.1631, Val Acc: 0.8067, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 153, Loss: 0.1626, Val Acc: 0.7947, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 154, Loss: 0.1651, Val Acc: 0.8013, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 155, Loss: 0.1617, Val Acc: 0.8053, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 156, Loss: 0.1652, Val Acc: 0.8067, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 157, Loss: 0.1554, Val Acc: 0.8027, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 158, Loss: 0.1557, Val Acc: 0.8080, Test Acc: 0.8120\n",
      "Seed: 44, Epoch: 159, Loss: 0.1552, Val Acc: 0.8027, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 160, Loss: 0.1624, Val Acc: 0.8107, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 161, Loss: 0.1572, Val Acc: 0.7987, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 162, Loss: 0.1516, Val Acc: 0.8013, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 163, Loss: 0.1490, Val Acc: 0.8080, Test Acc: 0.8053\n",
      "Seed: 44, Epoch: 164, Loss: 0.1501, Val Acc: 0.8027, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 165, Loss: 0.1643, Val Acc: 0.8053, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 166, Loss: 0.1576, Val Acc: 0.8080, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 167, Loss: 0.1579, Val Acc: 0.8160, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 168, Loss: 0.1653, Val Acc: 0.8093, Test Acc: 0.8107\n",
      "Seed: 44, Epoch: 169, Loss: 0.1525, Val Acc: 0.7987, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 170, Loss: 0.1809, Val Acc: 0.7893, Test Acc: 0.7920\n",
      "Seed: 44, Epoch: 171, Loss: 0.2272, Val Acc: 0.8080, Test Acc: 0.8160\n",
      "Seed: 44, Epoch: 172, Loss: 0.2003, Val Acc: 0.8080, Test Acc: 0.8267\n",
      "Seed: 44, Epoch: 173, Loss: 0.1758, Val Acc: 0.8067, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 174, Loss: 0.1705, Val Acc: 0.8147, Test Acc: 0.8080\n",
      "Seed: 44, Epoch: 175, Loss: 0.1639, Val Acc: 0.8053, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 176, Loss: 0.1563, Val Acc: 0.8067, Test Acc: 0.8093\n",
      "Seed: 44, Epoch: 177, Loss: 0.1541, Val Acc: 0.7920, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 178, Loss: 0.1556, Val Acc: 0.8013, Test Acc: 0.8013\n",
      "Seed: 44, Epoch: 179, Loss: 0.1517, Val Acc: 0.8013, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 180, Loss: 0.1569, Val Acc: 0.8067, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 181, Loss: 0.1491, Val Acc: 0.8053, Test Acc: 0.7867\n",
      "Seed: 44, Epoch: 182, Loss: 0.1620, Val Acc: 0.8120, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 183, Loss: 0.1523, Val Acc: 0.8147, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 184, Loss: 0.1405, Val Acc: 0.8040, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 185, Loss: 0.1348, Val Acc: 0.7947, Test Acc: 0.7947\n",
      "Seed: 44, Epoch: 186, Loss: 0.1399, Val Acc: 0.8053, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 187, Loss: 0.1445, Val Acc: 0.8067, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 188, Loss: 0.1665, Val Acc: 0.7960, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 189, Loss: 0.1692, Val Acc: 0.8067, Test Acc: 0.8067\n",
      "Seed: 44, Epoch: 190, Loss: 0.1545, Val Acc: 0.8040, Test Acc: 0.7933\n",
      "Seed: 44, Epoch: 191, Loss: 0.1681, Val Acc: 0.8080, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 192, Loss: 0.1575, Val Acc: 0.7987, Test Acc: 0.7880\n",
      "Seed: 44, Epoch: 193, Loss: 0.1540, Val Acc: 0.8133, Test Acc: 0.8040\n",
      "Seed: 44, Epoch: 194, Loss: 0.1459, Val Acc: 0.8133, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 195, Loss: 0.1405, Val Acc: 0.8187, Test Acc: 0.8027\n",
      "Seed: 44, Epoch: 196, Loss: 0.1543, Val Acc: 0.8160, Test Acc: 0.7987\n",
      "Seed: 44, Epoch: 197, Loss: 0.1558, Val Acc: 0.8133, Test Acc: 0.7973\n",
      "Seed: 44, Epoch: 198, Loss: 0.1409, Val Acc: 0.8053, Test Acc: 0.7893\n",
      "Seed: 44, Epoch: 199, Loss: 0.1361, Val Acc: 0.8120, Test Acc: 0.8000\n",
      "Seed: 44, Epoch: 200, Loss: 0.1344, Val Acc: 0.8040, Test Acc: 0.8027\n",
      "Average Time: 692.68 seconds\n",
      "Var Time: 43.22 seconds\n",
      "Average Memory: 49931.33 MB\n",
      "Average Best Val Acc: 0.8298\n",
      "Std Best Test Acc: 0.0115\n",
      "Average Test Acc: 0.8133\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "data_path = \"/data1/Pooling/\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"COLLAB\", transform=T.Compose([T.OneHotDegree(491)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_CO(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_CO, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = CoPooling(ratio=0.9, K=1, edge_ratio=0.9, nhid=64, alpha=0.1, Init='Random', Gamma=1.0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = CoPooling(ratio=0.9, K=1, edge_ratio=0.9, nhid=64, alpha=0.1, Init='Random', Gamma=1.0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, perm, _, batch, _, _, _ = self.pool1(x, edge_index, edge_attr=None, batch=batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, perm, _, batch, _, _, _ = self.pool2(x, edge_index, edge_attr=None, batch=batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_CO(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1549.9253 MAE=1547.9928 MAE=1545.0317 MAE=1543.1550 MAE=1540.5305 MAE=1537.8622 MAE=1534.8763 MAE=1531.3993 MAE=1527.0344 Epoch: 10/500MAE=1524.3928 MAE=1518.8035 MAE=1513.8445 MAE=1508.5342 MAE=1503.7075 MAE=1497.8966 MAE=1491.4194 MAE=1485.8691 MAE=1478.4685 MAE=1471.7300 Epoch: 20/500MAE=1463.4873 MAE=1455.6171 MAE=1444.8009 MAE=1442.0656 MAE=1435.0598 MAE=1424.4014 MAE=1414.2493 MAE=1404.2482 MAE=1391.5200 MAE=1382.3337 Epoch: 30/500MAE=1375.0381 MAE=1362.1423 MAE=1352.3616 MAE=1346.1841 MAE=1324.6183 MAE=1317.3501 MAE=1303.8706 MAE=1287.2766 MAE=1272.1208 MAE=1264.2551 Epoch: 40/500MAE=1248.8540 MAE=1238.7097 MAE=1230.0125 MAE=1207.6420 MAE=1195.9270 MAE=1171.2872 MAE=1155.2688 MAE=1145.0004 MAE=1124.8623 MAE=1105.5095 Epoch: 50/500MAE=1086.6848 MAE=1073.1459 MAE=1054.5303 MAE=1028.1768 MAE=1010.2982 MAE=990.4036 MAE=968.2885 MAE=958.4547 MAE=927.0352 MAE=913.2657 Epoch: 60/500MAE=891.1541 MAE=861.3942 MAE=850.5145 MAE=820.2670 MAE=797.3368 MAE=784.0908 MAE=761.6205 MAE=746.9660 MAE=728.0054 MAE=725.6623 Epoch: 70/500MAE=668.4966 MAE=660.5314 MAE=644.8918 MAE=607.9857 MAE=563.0980 MAE=628.5529 MAE=541.6077 MAE=528.6232 MAE=509.3218 MAE=501.5646 Epoch: 80/500MAE=475.7186 MAE=438.9364 MAE=412.4971 MAE=411.3354 MAE=355.5224 MAE=366.4672 MAE=333.1085 MAE=333.3259 MAE=296.0154 MAE=280.4976 Epoch: 90/500MAE=260.2507 MAE=263.5638 MAE=228.1117 MAE=225.7872 MAE=284.9824 MAE=244.2646 MAE=207.7633 MAE=156.3075 MAE=170.8250 MAE=150.6137 Epoch: 100/500MAE=165.7605 MAE=142.9391 MAE=142.1932 MAE=147.4279 MAE=140.4871 MAE=146.5113 MAE=139.5165 MAE=140.6332 MAE=121.2975 MAE=122.7842 Epoch: 110/500MAE=129.9923 MAE=122.4396 MAE=136.8713 MAE=119.6243 MAE=118.5309 MAE=116.1088 MAE=119.8238 MAE=114.5953 MAE=115.8907 MAE=116.7877 Epoch: 120/500MAE=120.6094 MAE=113.6711 MAE=113.7363 MAE=117.1915 MAE=114.0264 MAE=115.8776 MAE=111.9361 MAE=114.8620 MAE=111.4917 MAE=111.2500 Epoch: 130/500MAE=115.8200 MAE=111.1235 MAE=112.6165 MAE=114.2235 MAE=110.3400 MAE=109.9148 MAE=113.0979 MAE=110.8091 MAE=110.6603 MAE=114.8045 Epoch: 140/500MAE=111.6155 MAE=111.6200 MAE=110.6309 MAE=110.3098 MAE=111.6968 MAE=111.8718 MAE=111.1029 MAE=111.5733 MAE=111.2830 MAE=110.4820 Epoch: 150/500MAE=110.9470 MAE=110.7859 MAE=110.5211 MAE=110.8280 MAE=111.0187 MAE=110.5937 MAE=110.8986 MAE=110.6402 MAE=110.2685 MAE=110.5897 Epoch: 160/500MAE=110.5842 MAE=110.5292 MAE=110.4521 MAE=110.7251 MAE=111.0575 MAE=110.8025 MAE=110.7800 MAE=110.6955 MAE=110.8979 MAE=110.7952 Epoch: 170/500MAE=110.8159 MAE=110.5572 MAE=110.6017 MAE=110.6203 MAE=111.1317 MAE=110.6567 MAE=110.8186 MAE=110.8001 MAE=110.6968 MAE=110.7094 Epoch: 180/500MAE=110.5852 MAE=110.9609 MAE=110.8690 MAE=110.4152 MAE=110.7528 MAE=110.6251 MAE=110.7717 MAE=110.7911 MAE=110.6901 MAE=110.9602 Epoch: 190/500MAE=110.7820 MAE=110.7751 MAE=111.0791 MAE=110.8388 MAE=110.7573 MAE=110.6925 MAE=110.6910 MAE=110.8007 MAE=110.8618 MAE=110.9574 Epoch: 200/500MAE=110.7516 MAE=111.0089 MAE=111.0056 MAE=111.0706 MAE=110.5853 MAE=110.4517 MAE=110.9527 MAE=110.5794 MAE=110.3141 MAE=110.7444 Epoch: 210/500MAE=110.8602 MAE=110.8160 MAE=111.0237 MAE=110.7480 MAE=110.7192 MAE=110.4862 MAE=110.5694 MAE=110.7617 MAE=110.8502 MAE=110.6848 Epoch: 220/500MAE=110.6145 MAE=110.8745 MAE=110.5875 MAE=110.6069 MAE=110.5713 MAE=110.6684 MAE=110.4997 MAE=110.7523 MAE=110.7587 MAE=110.6969 Epoch: 230/500MAE=110.7292 MAE=110.9176 MAE=110.7045 MAE=110.5105 MAE=110.2114 MAE=110.8647 MAE=110.7152 MAE=111.0165 MAE=110.5275 MAE=110.6234 Epoch: 240/500MAE=110.4130 MAE=110.7614 MAE=110.3991 MAE=110.8407 MAE=110.5161 MAE=110.2906 MAE=110.5902 MAE=110.8176 MAE=110.7546 MAE=110.7593 Epoch: 250/500MAE=110.5483 MAE=110.9146 MAE=110.6117 MAE=110.4806 MAE=110.4049 MAE=110.8256 MAE=110.5526 MAE=110.6088 MAE=110.7999 MAE=110.5925 Epoch: 260/500MAE=110.6996 MAE=110.6546 MAE=110.6241 MAE=110.7623 MAE=110.8344 MAE=110.8338 MAE=110.3312 MAE=110.7660 MAE=110.9654 MAE=110.7307 Epoch: 270/500MAE=110.7059 MAE=110.9894 MAE=110.5885 MAE=110.5656 MAE=110.7489 MAE=110.6094 MAE=110.8417 MAE=110.2087 MAE=110.3225 MAE=110.6918 Epoch: 280/500MAE=110.7350 MAE=110.7771 MAE=110.7844 MAE=110.5000 MAE=110.6693 MAE=110.7207 MAE=118.6553 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 118.655 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1549.7585 MAE=1547.4669 MAE=1544.5573 MAE=1542.0769 MAE=1540.1190 MAE=1536.8848 MAE=1533.6486 MAE=1530.2561 MAE=1526.1935 Epoch: 10/500MAE=1523.2334 MAE=1517.7522 MAE=1512.1960 MAE=1507.7633 MAE=1502.3535 MAE=1495.6084 MAE=1488.8451 MAE=1481.9766 MAE=1474.0703 MAE=1468.4277 Epoch: 20/500MAE=1458.2865 MAE=1450.6536 MAE=1440.4393 MAE=1434.0394 MAE=1423.8066 MAE=1413.6196 MAE=1403.1353 MAE=1393.1787 MAE=1382.8508 MAE=1373.6392 Epoch: 30/500MAE=1364.6335 MAE=1344.6265 MAE=1327.9139 MAE=1322.1956 MAE=1311.1482 MAE=1294.7545 MAE=1281.5444 MAE=1268.5488 MAE=1253.3336 MAE=1236.1195 Epoch: 40/500MAE=1221.6404 MAE=1212.3453 MAE=1190.2651 MAE=1174.5970 MAE=1165.5396 MAE=1144.5475 MAE=1129.1128 MAE=1108.4003 MAE=1090.4918 MAE=1074.7852 Epoch: 50/500MAE=1056.6055 MAE=967.7555 MAE=1038.3020 MAE=1001.3423 MAE=966.7277 MAE=966.4647 MAE=949.8693 MAE=925.9418 MAE=904.1726 MAE=893.7679 Epoch: 60/500MAE=857.2123 MAE=840.8923 MAE=814.9576 MAE=807.0811 MAE=782.5859 MAE=749.1799 MAE=737.8534 MAE=703.3772 MAE=690.6040 MAE=666.6240 Epoch: 70/500MAE=638.3942 MAE=609.6616 MAE=576.7878 MAE=559.5972 MAE=544.3575 MAE=511.4935 MAE=451.8535 MAE=445.1046 MAE=425.0480 MAE=407.8578 Epoch: 80/500MAE=367.8672 MAE=385.8963 MAE=328.8631 MAE=325.0322 MAE=309.3454 MAE=284.5946 MAE=261.9731 MAE=305.8081 MAE=247.6072 MAE=257.8300 Epoch: 90/500MAE=230.3032 MAE=192.3334 MAE=210.3864 MAE=175.0194 MAE=167.2334 MAE=149.9415 MAE=156.8449 MAE=140.2131 MAE=139.2518 MAE=138.6236 Epoch: 100/500MAE=128.0768 MAE=128.5686 MAE=136.1232 MAE=125.3341 MAE=122.1150 MAE=121.4705 MAE=126.7052 MAE=117.6453 MAE=116.1024 MAE=118.4780 Epoch: 110/500MAE=116.1822 MAE=116.0247 MAE=115.4353 MAE=125.5535 MAE=117.1749 MAE=124.7320 MAE=117.5177 MAE=115.1211 MAE=111.8270 MAE=114.3629 Epoch: 120/500MAE=112.7933 MAE=111.7982 MAE=111.9923 MAE=114.1518 MAE=113.8478 MAE=113.8884 MAE=111.9560 MAE=112.1841 MAE=112.5941 MAE=111.3553 Epoch: 130/500MAE=111.2067 MAE=111.2561 MAE=110.5010 MAE=111.3614 MAE=111.5757 MAE=110.7446 MAE=110.7244 MAE=110.8215 MAE=110.9106 MAE=111.1844 Epoch: 140/500MAE=110.9066 MAE=110.8966 MAE=110.0709 MAE=110.0497 MAE=110.4576 MAE=110.9433 MAE=110.1844 MAE=110.2638 MAE=110.3378 MAE=110.3312 Epoch: 150/500MAE=109.8888 MAE=109.7566 MAE=110.5484 MAE=110.1879 MAE=110.0519 MAE=110.6133 MAE=110.3737 MAE=110.4219 MAE=110.7112 MAE=110.6453 Epoch: 160/500MAE=110.4677 MAE=110.3053 MAE=110.3580 MAE=110.0172 MAE=110.1242 MAE=110.3980 MAE=110.6054 MAE=110.4559 MAE=110.3105 MAE=110.4139 Epoch: 170/500MAE=110.4769 MAE=110.1088 MAE=110.2213 MAE=110.2759 MAE=110.2794 MAE=110.4056 MAE=110.4172 MAE=110.3010 MAE=110.2127 MAE=110.2246 Epoch: 180/500MAE=110.0639 MAE=110.2328 MAE=110.0837 MAE=109.9521 MAE=110.3039 MAE=110.0966 MAE=110.4008 MAE=110.3867 MAE=110.1356 MAE=110.3771 Epoch: 190/500MAE=110.1952 MAE=110.1333 MAE=110.1944 MAE=109.8572 MAE=110.1414 MAE=110.1160 MAE=110.1336 MAE=109.9633 MAE=110.0078 MAE=109.8609 Epoch: 200/500MAE=110.0308 MAE=110.1455 MAE=109.9494 MAE=109.9702 MAE=110.1180 MAE=109.9762 MAE=109.7865 MAE=110.0056 MAE=110.0520 MAE=110.0298 Epoch: 210/500MAE=110.1048 MAE=109.8866 MAE=110.1149 MAE=109.9293 MAE=109.8089 MAE=110.2506 MAE=109.6915 MAE=109.8500 MAE=109.7343 MAE=110.2667 Epoch: 220/500MAE=109.9099 MAE=110.0235 MAE=110.0634 MAE=109.8825 MAE=110.0198 MAE=110.2878 MAE=109.7300 MAE=110.0779 MAE=109.7295 MAE=109.9711 Epoch: 230/500MAE=109.9745 MAE=110.0333 MAE=109.8760 MAE=110.1903 MAE=109.7878 MAE=109.8988 MAE=109.8008 MAE=109.8981 MAE=110.1871 MAE=109.9615 Epoch: 240/500MAE=110.0006 MAE=110.0174 MAE=109.8739 MAE=110.1439 MAE=110.3492 MAE=109.8271 MAE=110.1581 MAE=110.3939 MAE=110.0977 MAE=110.1816 Epoch: 250/500MAE=110.3872 MAE=109.9029 MAE=109.7258 MAE=109.7947 MAE=110.2434 MAE=109.8507 MAE=109.9507 MAE=110.2698 MAE=110.1014 MAE=109.9044 Epoch: 260/500MAE=110.1184 MAE=110.1406 MAE=110.2031 MAE=109.9664 MAE=110.2103 MAE=109.8674 MAE=110.2148 MAE=109.8092 MAE=110.1539 MAE=110.0619 Epoch: 270/500MAE=110.0090 MAE=110.0272 MAE=109.9951 MAE=110.0548 MAE=109.9506 MAE=110.0431 MAE=110.0553 MAE=109.9403 MAE=110.0417 MAE=109.8000 Epoch: 280/500MAE=109.9900 MAE=110.3237 MAE=110.2493 MAE=110.2636 MAE=110.3843 MAE=110.2150 MAE=109.8895 MAE=110.1296 MAE=109.9070 MAE=110.1604 Epoch: 290/500MAE=109.9017 MAE=110.0848 MAE=110.1693 MAE=109.8448 MAE=110.2488 MAE=110.0376 MAE=110.1317 MAE=110.1861 MAE=110.0240 MAE=110.1592 Epoch: 300/500MAE=109.7716 MAE=110.1484 MAE=110.1574 MAE=110.4942 MAE=109.7551 MAE=110.4062 MAE=110.2102 MAE=110.0136 MAE=110.0690 MAE=110.0849 Epoch: 310/500MAE=110.1540 MAE=109.7997 MAE=109.9946 MAE=110.0205 MAE=110.0729 MAE=109.9741 MAE=109.8561 MAE=110.1279 MAE=109.9258 MAE=109.8124 Epoch: 320/500MAE=109.9312 MAE=110.1395 MAE=110.1491 MAE=109.8778 MAE=109.9492 MAE=109.8865 MAE=110.1049 MAE=109.9143 MAE=109.7774 MAE=110.2163 Epoch: 330/500MAE=110.3117 MAE=109.7915 MAE=110.1121 MAE=110.1541 MAE=110.0412 MAE=110.2667 MAE=110.1961 MAE=110.2358 MAE=109.9843 MAE=109.9303 Epoch: 340/500MAE=109.9877 MAE=110.0147 MAE=110.1238 MAE=109.9953 MAE=109.9260 MAE=110.2853 MAE=110.0285 MAE=109.8848 MAE=110.2204 MAE=110.1962 Epoch: 350/500MAE=109.9539 MAE=110.1447 MAE=110.0099 MAE=110.1633 MAE=109.9501 MAE=109.9834 MAE=109.8450 MAE=110.0556 MAE=110.1674 MAE=110.0872 Epoch: 360/500MAE=109.9793 MAE=110.0208 MAE=109.7861 MAE=109.9755 MAE=110.1936 MAE=110.0653 MAE=109.9078 MAE=116.7114 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 117.683 +/- 0.972\n",
      "\n",
      "Epoch: 1/500MAE=1549.6530 MAE=1547.0890 MAE=1544.6890 MAE=1542.4165 MAE=1539.5563 MAE=1536.9601 MAE=1534.2153 MAE=1531.0552 MAE=1527.0803 Epoch: 10/500MAE=1523.5886 MAE=1518.9650 MAE=1513.6782 MAE=1507.0022 MAE=1501.8625 MAE=1496.2244 MAE=1491.3531 MAE=1482.7341 MAE=1477.2010 MAE=1470.0599 Epoch: 20/500MAE=1458.7305 MAE=1450.7974 MAE=1440.9238 MAE=1430.8407 MAE=1425.9009 MAE=1418.7489 MAE=1405.0087 MAE=1395.8879 MAE=1385.2188 MAE=1369.1636 Epoch: 30/500MAE=1361.0098 MAE=1353.4359 MAE=1338.8990 MAE=1329.5273 MAE=1311.4426 MAE=1300.9009 MAE=1287.4279 MAE=1272.3806 MAE=1262.1748 MAE=1253.8719 Epoch: 40/500MAE=1231.7795 MAE=1229.1389 MAE=1212.7993 MAE=1178.5660 MAE=1165.0610 MAE=1156.1355 MAE=1147.8047 MAE=1118.6617 MAE=1105.8752 MAE=1094.7102 Epoch: 50/500MAE=1063.9263 MAE=1050.8550 MAE=1047.9336 MAE=1022.3672 MAE=978.8600 MAE=986.1738 MAE=952.7272 MAE=959.7328 MAE=932.7382 MAE=913.2565 Epoch: 60/500MAE=905.1621 MAE=881.3451 MAE=839.8690 MAE=808.0906 MAE=799.2625 MAE=801.7472 MAE=773.6553 MAE=721.9617 MAE=706.3046 MAE=701.1466 Epoch: 70/500MAE=662.6495 MAE=657.3065 MAE=618.2314 MAE=577.8605 MAE=569.5841 MAE=549.7362 MAE=558.1359 MAE=485.2314 MAE=462.4212 MAE=440.4751 Epoch: 80/500MAE=437.0960 MAE=402.0870 MAE=390.5531 MAE=361.2504 MAE=353.0989 MAE=316.5387 MAE=304.5370 MAE=298.4979 MAE=240.0074 MAE=221.3935 Epoch: 90/500MAE=277.9133 MAE=200.0278 MAE=239.3068 MAE=212.1363 MAE=173.0941 MAE=162.4696 MAE=166.0741 MAE=136.2429 MAE=143.9691 MAE=139.5585 Epoch: 100/500MAE=139.6867 MAE=123.1100 MAE=139.9113 MAE=139.7083 MAE=142.9690 MAE=126.8952 MAE=127.9092 MAE=127.6982 MAE=119.0745 MAE=110.5092 Epoch: 110/500MAE=113.5617 MAE=112.4530 MAE=111.7992 MAE=110.2815 MAE=110.2788 MAE=110.4248 MAE=110.3417 MAE=107.8506 MAE=111.3815 MAE=111.4548 Epoch: 120/500MAE=109.6787 MAE=108.8806 MAE=107.7956 MAE=108.4367 MAE=108.6779 MAE=108.3450 MAE=107.5738 MAE=107.6355 MAE=107.5779 MAE=107.2544 Epoch: 130/500MAE=108.7784 MAE=108.3811 MAE=107.2279 MAE=107.0938 MAE=106.7235 MAE=107.0858 MAE=107.1449 MAE=105.8332 MAE=107.0441 MAE=106.7109 Epoch: 140/500MAE=106.5594 MAE=106.2771 MAE=107.0711 MAE=105.3765 MAE=105.2828 MAE=106.2462 MAE=107.0136 MAE=105.9157 MAE=106.3444 MAE=105.3256 Epoch: 150/500MAE=105.2221 MAE=104.8291 MAE=104.5925 MAE=105.0846 MAE=105.5535 MAE=105.1947 MAE=105.1494 MAE=105.5937 MAE=105.3625 MAE=105.1425 Epoch: 160/500MAE=105.4772 MAE=105.5247 MAE=104.9660 MAE=105.1174 MAE=104.8550 MAE=105.2367 MAE=104.7882 MAE=104.9515 MAE=105.7086 MAE=104.8778 Epoch: 170/500MAE=104.6520 MAE=105.3242 MAE=104.8878 MAE=105.5575 MAE=105.3267 MAE=104.8826 MAE=105.1595 MAE=105.0304 MAE=105.1965 MAE=105.2141 Epoch: 180/500MAE=104.9628 MAE=104.9086 MAE=105.0906 MAE=105.6566 MAE=104.9303 MAE=105.1805 MAE=104.9218 MAE=105.3164 MAE=104.9713 MAE=105.3151 Epoch: 190/500MAE=104.9238 MAE=105.3560 MAE=104.9722 MAE=104.8992 MAE=105.1556 MAE=104.7509 MAE=105.1977 MAE=105.1740 MAE=105.1734 MAE=105.0073 Epoch: 200/500MAE=105.3805 MAE=104.7648 MAE=105.1338 MAE=105.5043 MAE=104.9285 MAE=105.5158 MAE=105.1113 MAE=105.0461 MAE=105.0182 MAE=105.2007 Epoch: 210/500MAE=104.9016 MAE=104.9503 MAE=105.3622 MAE=105.0455 MAE=104.9828 MAE=104.8699 MAE=104.6558 MAE=105.2991 MAE=105.0479 MAE=104.8558 Epoch: 220/500MAE=104.9137 MAE=105.1318 MAE=104.9450 MAE=105.3589 MAE=105.5238 MAE=105.5253 MAE=105.2140 MAE=105.1181 MAE=105.2328 MAE=104.5705 Epoch: 230/500MAE=104.8893 MAE=104.9585 MAE=104.8043 MAE=105.0827 MAE=105.0448 MAE=105.2102 MAE=104.9993 MAE=105.2845 MAE=105.1510 MAE=105.2727 Epoch: 240/500MAE=104.7480 MAE=105.0243 MAE=104.6291 MAE=104.8742 MAE=105.0840 MAE=105.0732 MAE=105.4578 MAE=104.6771 MAE=105.2165 MAE=104.9708 Epoch: 250/500MAE=105.2012 MAE=105.1763 MAE=105.0733 MAE=104.7742 MAE=105.0075 MAE=105.1084 MAE=104.8911 MAE=105.1041 MAE=104.6980 MAE=105.2810 Epoch: 260/500MAE=104.7507 MAE=104.9070 MAE=104.9743 MAE=104.6928 MAE=105.3066 MAE=105.6184 MAE=105.5595 MAE=105.0742 MAE=105.0020 MAE=104.8197 Epoch: 270/500MAE=105.2834 MAE=104.9003 MAE=105.1150 MAE=105.7584 MAE=104.9715 MAE=105.1640 MAE=105.0994 MAE=105.2557 MAE=104.9702 MAE=105.2289 Epoch: 280/500MAE=105.3117 MAE=104.6755 MAE=104.9184 MAE=104.8352 MAE=104.5972 MAE=104.8315 MAE=105.0701 MAE=105.4004 MAE=105.1257 MAE=105.1571 Epoch: 290/500MAE=104.9301 MAE=105.2072 MAE=105.3083 MAE=105.5071 MAE=105.1903 MAE=105.4134 MAE=104.8569 MAE=105.4076 MAE=105.4921 MAE=104.8191 Epoch: 300/500MAE=104.6237 MAE=104.7418 MAE=105.2135 MAE=105.0153 MAE=105.1464 MAE=104.9403 MAE=105.6362 MAE=105.1588 MAE=105.1156 MAE=105.0408 Epoch: 310/500MAE=105.2304 MAE=104.8677 MAE=104.8786 MAE=105.0508 MAE=104.9701 MAE=104.6095 MAE=105.2323 MAE=104.8558 MAE=105.3920 MAE=104.8658 Epoch: 320/500MAE=105.5276 MAE=104.7769 MAE=105.2417 MAE=105.4441 MAE=105.2623 MAE=105.3759 MAE=104.7608 MAE=105.0583 MAE=105.2154 MAE=104.7584 Epoch: 330/500MAE=105.2866 MAE=105.0664 MAE=105.2815 MAE=105.3964 MAE=105.3917 MAE=105.3500 MAE=105.1126 MAE=104.9509 MAE=104.7935 MAE=105.1770 Epoch: 340/500MAE=105.1870 MAE=104.6245 MAE=104.9080 MAE=105.2538 MAE=104.9314 MAE=105.1925 MAE=105.1684 MAE=104.8994 MAE=104.9222 MAE=105.0167 Epoch: 350/500MAE=105.5346 MAE=105.0428 MAE=105.6801 MAE=105.3812 MAE=105.1556 MAE=105.1581 MAE=105.1815 MAE=105.1644 MAE=104.8631 MAE=105.0826 Epoch: 360/500MAE=105.4001 MAE=105.1822 MAE=105.1073 MAE=104.6304 MAE=104.9705 MAE=104.9960 MAE=104.9081 MAE=104.9365 MAE=105.1043 MAE=105.2695 Epoch: 370/500MAE=105.1618 MAE=105.6063 MAE=105.2144 MAE=104.9023 MAE=105.1464 MAE=105.2209 MAE=104.8967 MAE=105.2894 MAE=104.8283 MAE=105.2523 MAE=115.3763 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 116.914 +/- 1.346\n",
      "\n",
      "Epoch: 1/500MAE=1549.6498 MAE=1547.1864 MAE=1544.5898 MAE=1542.4937 MAE=1539.9888 MAE=1537.5031 MAE=1534.6199 MAE=1531.4354 MAE=1527.7571 Epoch: 10/500MAE=1523.7263 MAE=1519.8646 MAE=1513.6941 MAE=1509.0764 MAE=1503.8374 MAE=1497.0046 MAE=1491.7344 MAE=1482.7401 MAE=1474.3187 MAE=1470.4352 Epoch: 20/500MAE=1460.0635 MAE=1453.5209 MAE=1450.0032 MAE=1439.9971 MAE=1424.6211 MAE=1421.0404 MAE=1409.6262 MAE=1401.7908 MAE=1391.7747 MAE=1382.4551 Epoch: 30/500MAE=1369.3962 MAE=1356.5579 MAE=1345.1074 MAE=1333.9268 MAE=1321.5385 MAE=1305.9072 MAE=1300.4606 MAE=1278.3969 MAE=1266.8884 MAE=1256.2314 Epoch: 40/500MAE=1230.5209 MAE=1231.1311 MAE=1219.5264 MAE=1191.8719 MAE=1174.4832 MAE=1158.2830 MAE=1141.3651 MAE=1113.4241 MAE=1098.9606 MAE=1091.3496 Epoch: 50/500MAE=1058.1265 MAE=1064.7112 MAE=1030.6604 MAE=1010.9640 MAE=992.3665 MAE=974.4362 MAE=949.1178 MAE=946.2136 MAE=906.4572 MAE=900.2057 Epoch: 60/500MAE=914.9427 MAE=876.2130 MAE=827.3943 MAE=846.8297 MAE=812.9489 MAE=693.2944 MAE=731.9550 MAE=794.2568 MAE=665.9626 MAE=714.7225 Epoch: 70/500MAE=666.6993 MAE=671.0814 MAE=598.6917 MAE=603.2778 MAE=568.1552 MAE=590.4196 MAE=482.1266 MAE=528.8707 MAE=449.9561 MAE=428.0094 Epoch: 80/500MAE=407.9179 MAE=415.5256 MAE=589.7664 MAE=424.8706 MAE=578.8099 MAE=335.6401 MAE=290.6359 MAE=284.9597 MAE=256.0537 MAE=266.3415 Epoch: 90/500MAE=256.6925 MAE=242.7791 MAE=232.0406 MAE=246.7963 MAE=219.5194 MAE=236.6851 MAE=215.4752 MAE=220.7126 MAE=221.9458 MAE=234.7366 Epoch: 100/500MAE=197.7586 MAE=190.4280 MAE=198.4837 MAE=210.5085 MAE=169.4695 MAE=195.1760 MAE=212.6295 MAE=200.7121 MAE=195.9700 MAE=168.3399 Epoch: 110/500MAE=166.8241 MAE=167.8065 MAE=170.2102 MAE=165.5905 MAE=171.4435 MAE=159.0589 MAE=158.9139 MAE=152.9723 MAE=144.6914 MAE=157.8248 Epoch: 120/500MAE=149.7174 MAE=150.4856 MAE=138.0045 MAE=137.5278 MAE=146.0212 MAE=141.7019 MAE=139.5460 MAE=140.6192 MAE=141.1914 MAE=134.2890 Epoch: 130/500MAE=135.6329 MAE=137.7964 MAE=136.3926 MAE=135.4736 MAE=133.6767 MAE=133.6119 MAE=134.7642 MAE=135.5015 MAE=132.8544 MAE=134.1070 Epoch: 140/500MAE=132.4835 MAE=134.6177 MAE=133.5835 MAE=132.4036 MAE=132.6618 MAE=132.4878 MAE=131.1486 MAE=130.1590 MAE=132.6761 MAE=132.0965 Epoch: 150/500MAE=130.5468 MAE=130.9713 MAE=131.7455 MAE=130.9132 MAE=131.2191 MAE=131.1902 MAE=130.5341 MAE=130.5839 MAE=130.3601 MAE=129.5174 Epoch: 160/500MAE=130.1077 MAE=130.5282 MAE=129.8414 MAE=131.0532 MAE=131.1098 MAE=130.8896 MAE=130.1209 MAE=128.7834 MAE=129.3703 MAE=129.4851 Epoch: 170/500MAE=129.3660 MAE=130.3817 MAE=129.5952 MAE=129.8430 MAE=130.1798 MAE=129.9113 MAE=130.2547 MAE=129.9522 MAE=130.0641 MAE=129.9327 Epoch: 180/500MAE=130.3137 MAE=129.9257 MAE=130.4371 MAE=130.0752 MAE=130.0426 MAE=129.7269 MAE=129.8936 MAE=129.5169 MAE=130.2354 MAE=129.8331 Epoch: 190/500MAE=130.1514 MAE=129.7056 MAE=129.8558 MAE=129.7365 MAE=129.8512 MAE=130.0211 MAE=129.5060 MAE=129.4217 MAE=129.7790 MAE=129.7115 Epoch: 200/500MAE=130.2297 MAE=129.7508 MAE=129.6348 MAE=129.4949 MAE=129.7810 MAE=130.0572 MAE=129.4913 MAE=129.4293 MAE=129.9525 MAE=129.8387 Epoch: 210/500MAE=129.4685 MAE=129.1400 MAE=129.9668 MAE=129.8990 MAE=129.8291 MAE=129.6172 MAE=129.7977 MAE=129.3956 MAE=129.3884 MAE=130.4226 Epoch: 220/500MAE=129.4680 MAE=129.6657 MAE=129.7876 MAE=129.8546 MAE=129.6464 MAE=129.9214 MAE=129.5232 MAE=130.0710 MAE=129.3414 MAE=129.5390 Epoch: 230/500MAE=129.9514 MAE=129.4521 MAE=128.4933 MAE=128.9248 MAE=129.6537 MAE=129.7650 MAE=129.5159 MAE=129.8860 MAE=129.3707 MAE=129.9917 Epoch: 240/500MAE=129.7161 MAE=129.9248 MAE=129.7534 MAE=129.1713 MAE=129.6508 MAE=128.9426 MAE=129.4236 MAE=129.8459 MAE=129.1853 MAE=129.7623 Epoch: 250/500MAE=130.0169 MAE=129.3240 MAE=129.7548 MAE=129.7912 MAE=129.4479 MAE=129.8811 MAE=129.5379 MAE=129.4921 MAE=129.2921 MAE=129.3351 Epoch: 260/500MAE=129.4980 MAE=129.4862 MAE=129.3184 MAE=129.6469 MAE=129.1313 MAE=129.2490 MAE=129.2877 MAE=129.6015 MAE=129.1179 MAE=128.8558 Epoch: 270/500MAE=129.3100 MAE=129.5698 MAE=129.2554 MAE=128.8289 MAE=128.9351 MAE=128.7612 MAE=128.8875 MAE=129.6232 MAE=129.3884 MAE=128.7429 Epoch: 280/500MAE=129.4962 MAE=129.0955 MAE=129.3415 MAE=129.7701 MAE=129.3278 MAE=128.8804 MAE=128.6194 MAE=129.1004 MAE=128.7494 MAE=129.2477 Epoch: 290/500MAE=129.2213 MAE=129.6382 MAE=129.5602 MAE=129.4724 MAE=129.0601 MAE=128.6515 MAE=129.3633 MAE=129.2850 MAE=128.9720 MAE=129.4974 Epoch: 300/500MAE=129.6822 MAE=128.9121 MAE=128.8967 MAE=129.1830 MAE=129.4376 MAE=129.8255 MAE=130.0527 MAE=129.5505 MAE=129.2906 MAE=129.7625 Epoch: 310/500MAE=129.2467 MAE=129.8196 MAE=129.7109 MAE=129.7159 MAE=129.5803 MAE=129.4911 MAE=128.4829 MAE=128.8124 MAE=128.4377 MAE=128.8654 Epoch: 320/500MAE=129.4749 MAE=129.4440 MAE=129.1178 MAE=129.1179 MAE=129.2352 MAE=128.8752 MAE=129.2544 MAE=129.9382 MAE=128.9273 MAE=128.3598 Epoch: 330/500MAE=129.6400 MAE=129.8811 MAE=128.6855 MAE=129.1692 MAE=129.4221 MAE=129.3294 MAE=129.1334 MAE=129.4785 MAE=129.5858 MAE=129.0259 Epoch: 340/500MAE=128.7479 MAE=129.8423 MAE=129.2990 MAE=129.2640 MAE=128.7598 MAE=129.0944 MAE=129.3920 MAE=129.1554 MAE=129.7978 MAE=129.0845 Epoch: 350/500MAE=128.8442 MAE=129.1800 MAE=129.5280 MAE=129.5733 MAE=128.8951 MAE=128.9151 MAE=129.0043 MAE=129.0300 MAE=128.6852 MAE=129.2658 Epoch: 360/500MAE=129.3615 MAE=128.9661 MAE=129.4234 MAE=128.6249 MAE=128.6400 MAE=128.7505 MAE=129.4302 MAE=129.0249 MAE=129.5781 MAE=128.8972 Epoch: 370/500MAE=128.9065 MAE=128.8163 MAE=128.8715 MAE=128.8523 MAE=129.1680 MAE=129.4251 MAE=128.4111 MAE=128.9701 MAE=129.1638 MAE=128.7372 Epoch: 380/500MAE=128.5186 MAE=129.2274 MAE=129.0736 MAE=128.8269 MAE=128.7895 MAE=129.0479 MAE=129.0307 MAE=129.3972 MAE=128.5877 MAE=128.3249 Epoch: 390/500MAE=128.5220 MAE=128.8682 MAE=128.4153 MAE=128.9009 MAE=128.8547 MAE=128.5358 MAE=128.5393 MAE=128.8093 MAE=128.9418 MAE=129.1848 Epoch: 400/500MAE=129.1875 MAE=129.3675 MAE=129.0362 MAE=129.0326 MAE=129.3629 MAE=128.9949 MAE=129.4875 MAE=129.0340 MAE=128.5702 MAE=129.2490 Epoch: 410/500MAE=129.1655 MAE=129.0336 MAE=128.7816 MAE=129.5495 MAE=128.8302 MAE=128.9347 MAE=128.5880 MAE=128.5961 MAE=128.7204 MAE=129.5495 Epoch: 420/500MAE=129.2023 MAE=128.4704 MAE=128.5920 MAE=128.5237 MAE=128.2834 MAE=128.3310 MAE=128.8656 MAE=128.8682 MAE=128.4265 MAE=128.4937 Epoch: 430/500MAE=128.3226 MAE=128.8220 MAE=128.3536 MAE=128.3062 MAE=129.0622 MAE=128.8215 MAE=128.7178 MAE=128.3414 MAE=128.4886 MAE=129.1312 Epoch: 440/500MAE=128.7982 MAE=128.4287 MAE=129.3951 MAE=128.5920 MAE=128.8349 MAE=128.7945 MAE=129.1914 MAE=128.9730 MAE=128.4782 MAE=128.5554 Epoch: 450/500MAE=128.9092 MAE=128.6198 MAE=128.6639 MAE=129.0657 MAE=128.6187 MAE=128.1440 MAE=128.8011 MAE=128.8083 MAE=128.3890 MAE=128.8264 Epoch: 460/500MAE=128.6666 MAE=129.0558 MAE=127.9958 MAE=128.7342 MAE=128.5158 MAE=128.1059 MAE=128.0885 MAE=128.6890 MAE=129.1761 MAE=128.0873 Epoch: 470/500MAE=128.0611 MAE=128.1477 MAE=128.2603 MAE=128.2144 MAE=128.4336 MAE=128.0091 MAE=128.6831 MAE=128.3087 MAE=127.9113 MAE=128.2993 Epoch: 480/500MAE=128.6498 MAE=128.4151 MAE=127.9770 MAE=128.3708 MAE=128.3065 MAE=128.2286 MAE=128.1535 MAE=128.1461 MAE=127.5744 MAE=128.2460 Epoch: 490/500MAE=128.5238 MAE=128.0260 MAE=128.4702 MAE=128.3508 MAE=128.2128 MAE=128.7536 MAE=128.8707 MAE=128.2467 MAE=128.5231 MAE=127.9080 Epoch: 500/500MAE=128.7374 MAE=131.0863 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 120.457 +/- 6.246\n",
      "\n",
      "Epoch: 1/500MAE=1549.5569 MAE=1547.3683 MAE=1544.8884 MAE=1542.2441 MAE=1539.8263 MAE=1537.3596 MAE=1534.4290 MAE=1530.9678 MAE=1527.8402 Epoch: 10/500MAE=1523.7266 MAE=1519.2932 MAE=1514.8207 MAE=1509.5437 MAE=1505.8850 MAE=1500.2487 MAE=1494.3770 MAE=1487.6636 MAE=1482.1763 MAE=1473.7920 Epoch: 20/500MAE=1468.3687 MAE=1461.1304 MAE=1452.6307 MAE=1437.3696 MAE=1429.5400 MAE=1422.8367 MAE=1416.9636 MAE=1403.0775 MAE=1393.9988 MAE=1381.6095 Epoch: 30/500MAE=1371.0465 MAE=1364.6570 MAE=1350.2802 MAE=1335.8092 MAE=1324.7340 MAE=1313.1400 MAE=1302.4033 MAE=1287.5576 MAE=1269.4869 MAE=1256.9929 Epoch: 40/500MAE=1245.0302 MAE=1226.4048 MAE=1220.4324 MAE=1207.3130 MAE=1183.9286 MAE=1174.4641 MAE=1156.1312 MAE=1144.2854 MAE=1127.8518 MAE=1108.3828 Epoch: 50/500MAE=1088.9586 MAE=1082.2102 MAE=1054.2703 MAE=1034.3074 MAE=1029.3955 MAE=1002.1832 MAE=986.5631 MAE=967.3156 MAE=938.5372 MAE=934.5927 Epoch: 60/500MAE=927.4972 MAE=791.7449 MAE=887.9306 MAE=836.1115 MAE=814.8942 MAE=801.2812 MAE=793.1953 MAE=785.8229 MAE=771.9984 MAE=754.5526 Epoch: 70/500MAE=746.8926 MAE=735.1846 MAE=724.2441 MAE=715.4551 MAE=700.8304 MAE=689.5265 MAE=689.9232 MAE=676.1002 MAE=653.4588 MAE=637.0618 Epoch: 80/500MAE=628.1924 MAE=635.2082 MAE=629.5704 MAE=588.7253 MAE=575.9374 MAE=569.7647 MAE=564.0698 MAE=559.1698 MAE=545.8580 MAE=522.9629 Epoch: 90/500MAE=547.5415 MAE=511.7816 MAE=502.5681 MAE=496.0270 MAE=487.5625 MAE=482.7363 MAE=481.4038 MAE=483.4703 MAE=484.1679 MAE=459.3985 Epoch: 100/500MAE=458.4695 MAE=422.5107 MAE=438.9165 MAE=425.4481 MAE=435.5888 MAE=406.4431 MAE=406.8084 MAE=390.8969 MAE=391.5027 MAE=355.6948 Epoch: 110/500MAE=379.6574 MAE=343.5449 MAE=353.4252 MAE=380.0723 MAE=289.7618 MAE=282.6147 MAE=274.7144 MAE=267.1976 MAE=274.9499 MAE=263.3045 Epoch: 120/500MAE=231.4088 MAE=222.5079 MAE=228.9810 MAE=220.1315 MAE=244.4791 MAE=208.4878 MAE=229.4987 MAE=231.0290 MAE=220.2040 MAE=208.1751 Epoch: 130/500MAE=204.6460 MAE=208.1933 MAE=192.1870 MAE=189.8510 MAE=165.1695 MAE=158.4011 MAE=267.5172 MAE=171.4366 MAE=223.1716 MAE=200.1703 Epoch: 140/500MAE=203.5648 MAE=199.6523 MAE=202.9094 MAE=202.6702 MAE=203.3469 MAE=197.7610 MAE=200.1580 MAE=201.6153 MAE=198.2420 MAE=199.8737 Epoch: 150/500MAE=200.9930 MAE=197.8245 MAE=194.8442 MAE=196.2915 MAE=198.1860 MAE=195.6596 MAE=195.6361 MAE=195.9555 MAE=194.9789 MAE=193.6520 Epoch: 160/500MAE=193.8496 MAE=195.4811 MAE=195.0281 MAE=194.8929 MAE=194.2191 MAE=195.8430 MAE=194.3211 MAE=194.6417 MAE=194.2647 MAE=194.4396 Epoch: 170/500MAE=194.4124 MAE=195.2575 MAE=195.0345 MAE=194.7120 MAE=194.3468 MAE=194.9330 MAE=194.9604 MAE=194.2973 MAE=193.3525 MAE=193.5379 Epoch: 180/500MAE=193.8929 MAE=195.0271 MAE=194.6325 MAE=194.2864 MAE=192.7725 MAE=193.8164 MAE=194.3793 MAE=194.3014 MAE=194.0349 MAE=194.5255 Epoch: 190/500MAE=194.8716 MAE=194.5197 MAE=194.1745 MAE=194.1110 MAE=195.5542 MAE=194.7594 MAE=194.5951 MAE=195.3447 MAE=194.6370 MAE=195.6352 Epoch: 200/500MAE=195.2821 MAE=194.7624 MAE=194.8293 MAE=194.2845 MAE=193.4652 MAE=194.3412 MAE=195.3219 MAE=195.0018 MAE=193.4672 MAE=194.6783 Epoch: 210/500MAE=195.4576 MAE=194.3056 MAE=194.1173 MAE=194.2124 MAE=195.3446 MAE=194.2893 MAE=194.8682 MAE=194.1582 MAE=193.7226 MAE=193.8182 Epoch: 220/500MAE=194.5826 MAE=194.1711 MAE=194.0263 MAE=194.1437 MAE=193.5732 MAE=194.3856 MAE=194.1883 MAE=193.3491 MAE=195.4157 MAE=193.4724 Epoch: 230/500MAE=194.2695 MAE=194.6979 MAE=194.4644 MAE=194.1301 MAE=193.5498 MAE=193.7590 MAE=194.1208 MAE=193.6519 MAE=193.6366 MAE=193.7977 Epoch: 240/500MAE=194.6886 MAE=193.5194 MAE=193.9896 MAE=194.7796 MAE=193.3806 MAE=193.3088 MAE=192.9436 MAE=194.0794 MAE=194.2832 MAE=194.0882 Epoch: 250/500MAE=194.2094 MAE=193.9900 MAE=193.6224 MAE=194.2141 MAE=193.7315 MAE=192.8719 MAE=192.7638 MAE=193.8948 MAE=193.8563 MAE=193.2201 Epoch: 260/500MAE=193.6625 MAE=194.3543 MAE=193.7046 MAE=194.6649 MAE=194.1700 MAE=194.0092 MAE=193.9486 MAE=193.4970 MAE=193.5954 MAE=192.1663 Epoch: 270/500MAE=192.7773 MAE=194.1692 MAE=193.7268 MAE=193.6555 MAE=193.7737 MAE=192.1380 MAE=193.5979 MAE=193.7056 MAE=192.9382 MAE=193.1490 Epoch: 280/500MAE=192.9308 MAE=193.1402 MAE=193.1345 MAE=194.7361 MAE=193.0814 MAE=193.8390 MAE=166.3282 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 129.632 +/- 19.180\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1549.3555 MAE=1547.4713 MAE=1545.0947 MAE=1542.4241 MAE=1539.3667 MAE=1536.6655 MAE=1533.9187 MAE=1530.7489 MAE=1527.0657 Epoch: 10/500MAE=1522.9285 MAE=1518.2097 MAE=1512.4990 MAE=1508.3586 MAE=1504.5078 MAE=1497.9617 MAE=1489.9827 MAE=1483.0059 MAE=1477.7185 MAE=1470.6490 Epoch: 20/500MAE=1462.0315 MAE=1452.8352 MAE=1439.2311 MAE=1437.0677 MAE=1418.8658 MAE=1413.3293 MAE=1412.5819 MAE=1399.2216 MAE=1384.0537 MAE=1366.4735 Epoch: 30/500MAE=1358.2457 MAE=1345.8267 MAE=1336.0841 MAE=1331.3549 MAE=1318.7080 MAE=1291.2031 MAE=1283.9531 MAE=1269.7935 MAE=1256.2383 MAE=1246.2429 Epoch: 40/500MAE=1241.4496 MAE=1199.3079 MAE=1201.7976 MAE=1183.5317 MAE=1159.7026 MAE=1167.9347 MAE=1107.5786 MAE=1110.1875 MAE=1104.6653 MAE=1046.8970 Epoch: 50/500MAE=1068.3337 MAE=1042.0906 MAE=1026.9282 MAE=1025.0635 MAE=970.6872 MAE=955.6863 MAE=956.6085 MAE=950.3192 MAE=904.1693 MAE=875.6382 Epoch: 60/500MAE=873.1423 MAE=831.7307 MAE=853.2490 MAE=767.5388 MAE=780.9415 MAE=723.9322 MAE=772.9041 MAE=719.7050 MAE=687.8241 MAE=626.8058 Epoch: 70/500MAE=644.4857 MAE=629.3657 MAE=554.6568 MAE=575.4097 MAE=542.7513 MAE=541.3424 MAE=510.1809 MAE=471.8625 MAE=472.1998 MAE=414.0546 Epoch: 80/500MAE=402.6516 MAE=366.4146 MAE=362.3333 MAE=331.8065 MAE=281.9509 MAE=270.4166 MAE=287.2363 MAE=281.7363 MAE=217.3385 MAE=216.6310 Epoch: 90/500MAE=225.5320 MAE=166.2589 MAE=209.0614 MAE=166.2319 MAE=145.9537 MAE=159.5114 MAE=140.0974 MAE=144.1323 MAE=129.2631 MAE=126.4947 Epoch: 100/500MAE=119.0065 MAE=105.7389 MAE=123.4135 MAE=113.2395 MAE=112.0956 MAE=122.8434 MAE=108.1476 MAE=101.2730 MAE=103.1379 MAE=103.6373 Epoch: 110/500MAE=101.4359 MAE=101.9546 MAE=101.1869 MAE=99.0891 MAE=100.8119 MAE=98.7850 MAE=100.0342 MAE=98.4949 MAE=98.0007 MAE=98.4255 Epoch: 120/500MAE=99.1171 MAE=98.7945 MAE=97.9816 MAE=98.4530 MAE=96.2673 MAE=98.5551 MAE=97.1866 MAE=96.2743 MAE=97.0280 MAE=97.9359 Epoch: 130/500MAE=95.9941 MAE=96.0197 MAE=95.3149 MAE=96.7990 MAE=95.9956 MAE=96.0843 MAE=95.4202 MAE=95.1363 MAE=94.8787 MAE=95.3980 Epoch: 140/500MAE=94.8897 MAE=93.3882 MAE=94.6652 MAE=95.7926 MAE=95.0360 MAE=95.0480 MAE=95.0791 MAE=95.7480 MAE=94.9530 MAE=94.7333 Epoch: 150/500MAE=96.0227 MAE=95.6921 MAE=95.5300 MAE=94.9801 MAE=95.4757 MAE=94.4604 MAE=94.7125 MAE=94.8835 MAE=95.0237 MAE=94.5768 Epoch: 160/500MAE=95.1806 MAE=94.4622 MAE=94.7971 MAE=95.7352 MAE=95.2902 MAE=94.7222 MAE=95.0583 MAE=95.7196 MAE=94.5944 MAE=95.2234 Epoch: 170/500MAE=95.0755 MAE=95.3766 MAE=95.5002 MAE=95.1771 MAE=95.4378 MAE=95.1400 MAE=95.3887 MAE=95.2030 MAE=95.1053 MAE=94.9304 Epoch: 180/500MAE=95.2268 MAE=95.3883 MAE=95.2555 MAE=95.0838 MAE=95.8410 MAE=95.2238 MAE=94.8421 MAE=94.8003 MAE=95.0299 MAE=95.2843 Epoch: 190/500MAE=94.7014 MAE=95.4048 MAE=95.2249 MAE=94.9367 MAE=95.1390 MAE=95.2293 MAE=94.8884 MAE=95.4585 MAE=93.7469 MAE=94.4518 Epoch: 200/500MAE=95.7469 MAE=95.2245 MAE=95.0117 MAE=95.8041 MAE=94.7438 MAE=94.5137 MAE=96.0764 MAE=95.2682 MAE=95.1764 MAE=95.6374 Epoch: 210/500MAE=94.6351 MAE=95.5859 MAE=94.6509 MAE=94.8989 MAE=95.0231 MAE=94.9115 MAE=95.4370 MAE=95.1724 MAE=94.8595 MAE=94.5094 Epoch: 220/500MAE=94.9330 MAE=95.8221 MAE=95.3331 MAE=94.8883 MAE=94.7009 MAE=95.0595 MAE=94.9788 MAE=94.4559 MAE=95.5657 MAE=94.9915 Epoch: 230/500MAE=94.4339 MAE=95.1773 MAE=95.2434 MAE=94.6202 MAE=94.7354 MAE=95.0010 MAE=94.3583 MAE=94.9610 MAE=94.7625 MAE=94.6046 Epoch: 240/500MAE=95.3878 MAE=95.5853 MAE=95.5826 MAE=95.0718 MAE=94.9234 MAE=94.9111 MAE=95.1356 MAE=95.9875 MAE=95.5529 MAE=95.0005 Epoch: 250/500MAE=94.8669 MAE=95.2324 MAE=94.8696 MAE=94.4142 MAE=95.4433 MAE=95.7129 MAE=95.1945 MAE=95.1427 MAE=94.5519 MAE=94.7896 Epoch: 260/500MAE=94.9583 MAE=94.5146 MAE=95.4009 MAE=94.7235 MAE=95.1804 MAE=95.3299 MAE=94.8265 MAE=95.1615 MAE=94.5320 MAE=95.5412 Epoch: 270/500MAE=95.0916 MAE=94.5977 MAE=94.7875 MAE=94.8813 MAE=94.6253 MAE=95.0456 MAE=95.7726 MAE=94.9642 MAE=95.8134 MAE=94.8319 Epoch: 280/500MAE=95.5151 MAE=95.1951 MAE=95.3171 MAE=95.0521 MAE=94.5249 MAE=95.5639 MAE=95.6865 MAE=94.9463 MAE=94.9698 MAE=95.0420 Epoch: 290/500MAE=94.9219 MAE=95.5887 MAE=100.2701 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 100.270 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1549.4852 MAE=1547.0222 MAE=1544.1493 MAE=1542.1083 MAE=1539.9006 MAE=1537.1953 MAE=1534.0679 MAE=1531.9360 MAE=1528.2000 Epoch: 10/500MAE=1524.8954 MAE=1518.2152 MAE=1513.3674 MAE=1510.0994 MAE=1498.9751 MAE=1494.4329 MAE=1498.1322 MAE=1479.5225 MAE=1487.5071 MAE=1466.6660 Epoch: 20/500MAE=1464.3752 MAE=1455.9446 MAE=1449.2728 MAE=1444.1462 MAE=1428.6379 MAE=1418.5588 MAE=1406.2848 MAE=1372.9800 MAE=1419.7157 MAE=1368.4014 Epoch: 30/500MAE=1364.4443 MAE=1359.2922 MAE=1346.0718 MAE=1329.1812 MAE=1316.2106 MAE=1302.3556 MAE=1299.8977 MAE=1271.4812 MAE=1258.4359 MAE=1275.0730 Epoch: 40/500MAE=1224.9174 MAE=1231.7334 MAE=1199.1948 MAE=1194.4673 MAE=1179.5374 MAE=1198.7705 MAE=1131.1877 MAE=1120.0745 MAE=1127.2571 MAE=1085.1316 Epoch: 50/500MAE=1073.0718 MAE=1044.8651 MAE=1106.1366 MAE=977.9937 MAE=1016.6536 MAE=973.0946 MAE=949.0628 MAE=941.5944 MAE=925.5341 MAE=901.6451 Epoch: 60/500MAE=888.6641 MAE=850.5593 MAE=853.8360 MAE=812.7482 MAE=793.5168 MAE=801.6412 MAE=771.0980 MAE=740.4337 MAE=703.9457 MAE=684.6230 Epoch: 70/500MAE=630.2269 MAE=633.9702 MAE=656.8357 MAE=589.8062 MAE=506.7600 MAE=520.6519 MAE=500.6063 MAE=540.7104 MAE=400.8000 MAE=556.3602 Epoch: 80/500MAE=440.0359 MAE=394.1793 MAE=392.9661 MAE=390.1003 MAE=364.5219 MAE=335.9495 MAE=292.5942 MAE=319.5806 MAE=277.1487 MAE=224.7683 Epoch: 90/500MAE=229.7933 MAE=220.1955 MAE=232.5562 MAE=192.4106 MAE=155.9694 MAE=194.4857 MAE=198.0507 MAE=130.2383 MAE=129.2721 MAE=131.9693 Epoch: 100/500MAE=168.1435 MAE=128.6248 MAE=137.2136 MAE=113.3822 MAE=111.7236 MAE=112.7423 MAE=112.0893 MAE=100.2325 MAE=117.5767 MAE=107.3866 Epoch: 110/500MAE=100.9780 MAE=106.3874 MAE=105.5849 MAE=96.5737 MAE=96.9271 MAE=100.7003 MAE=100.4926 MAE=94.7977 MAE=96.0878 MAE=93.6677 Epoch: 120/500MAE=98.5430 MAE=93.8213 MAE=94.3979 MAE=94.7365 MAE=93.7838 MAE=92.0918 MAE=94.2001 MAE=92.6289 MAE=92.6228 MAE=94.1186 Epoch: 130/500MAE=92.7659 MAE=92.4515 MAE=92.9203 MAE=91.9322 MAE=91.5413 MAE=91.8596 MAE=91.4951 MAE=91.9711 MAE=91.4900 MAE=90.6807 Epoch: 140/500MAE=91.9119 MAE=91.4802 MAE=91.0541 MAE=91.8194 MAE=90.8756 MAE=90.5509 MAE=91.8288 MAE=90.6471 MAE=91.2680 MAE=92.7201 Epoch: 150/500MAE=89.7483 MAE=91.2410 MAE=90.8914 MAE=90.2433 MAE=90.0181 MAE=91.2198 MAE=91.1214 MAE=91.0222 MAE=91.3182 MAE=91.3992 Epoch: 160/500MAE=90.3032 MAE=91.2036 MAE=90.9105 MAE=90.6193 MAE=89.9094 MAE=90.1861 MAE=91.1463 MAE=90.7431 MAE=90.8668 MAE=90.6683 Epoch: 170/500MAE=91.7401 MAE=91.3762 MAE=91.2759 MAE=91.3359 MAE=90.8090 MAE=91.6321 MAE=90.9274 MAE=90.6495 MAE=90.1717 MAE=90.3758 Epoch: 180/500MAE=90.8974 MAE=90.7272 MAE=90.2472 MAE=89.9172 MAE=90.4532 MAE=90.2968 MAE=90.4860 MAE=90.1041 MAE=90.4653 MAE=90.4614 Epoch: 190/500MAE=91.2068 MAE=90.9934 MAE=90.4871 MAE=90.9570 MAE=91.1873 MAE=90.9320 MAE=90.5383 MAE=90.0616 MAE=90.1718 MAE=90.6355 Epoch: 200/500MAE=90.8025 MAE=90.3713 MAE=90.1358 MAE=91.5098 MAE=90.3254 MAE=90.8983 MAE=90.2796 MAE=90.5328 MAE=89.7666 MAE=89.8413 Epoch: 210/500MAE=90.6644 MAE=90.3877 MAE=90.0299 MAE=90.3644 MAE=90.9800 MAE=89.8991 MAE=90.6940 MAE=90.5992 MAE=89.7288 MAE=90.7058 Epoch: 220/500MAE=90.2513 MAE=89.9764 MAE=90.9377 MAE=90.4987 MAE=90.1721 MAE=90.7854 MAE=89.9972 MAE=90.6161 MAE=90.2711 MAE=90.0939 Epoch: 230/500MAE=90.6776 MAE=90.6513 MAE=90.7568 MAE=89.8274 MAE=89.7221 MAE=90.0466 MAE=89.7679 MAE=90.2153 MAE=91.2661 MAE=89.7568 Epoch: 240/500MAE=90.3025 MAE=89.4819 MAE=90.5738 MAE=90.4546 MAE=90.5706 MAE=89.7125 MAE=90.3601 MAE=90.7562 MAE=89.9254 MAE=90.4686 Epoch: 250/500MAE=90.6986 MAE=90.4388 MAE=90.5640 MAE=90.5877 MAE=90.7952 MAE=91.3007 MAE=89.3120 MAE=90.1471 MAE=91.0450 MAE=90.6033 Epoch: 260/500MAE=90.0445 MAE=90.4517 MAE=89.8562 MAE=89.8199 MAE=89.4895 MAE=90.6612 MAE=90.2775 MAE=90.2191 MAE=90.4437 MAE=90.0114 Epoch: 270/500MAE=89.7062 MAE=89.9266 MAE=90.3311 MAE=91.3349 MAE=90.2440 MAE=90.6196 MAE=89.0676 MAE=89.4994 MAE=90.0557 MAE=90.3802 Epoch: 280/500MAE=90.5742 MAE=89.5955 MAE=90.9313 MAE=89.8153 MAE=90.1761 MAE=90.4856 MAE=90.1898 MAE=90.2214 MAE=89.8835 MAE=90.7690 Epoch: 290/500MAE=89.4342 MAE=90.6793 MAE=90.3588 MAE=89.7042 MAE=90.9669 MAE=90.7091 MAE=90.1157 MAE=90.5687 MAE=90.4236 MAE=90.2420 Epoch: 300/500MAE=91.0691 MAE=90.5613 MAE=89.8747 MAE=90.7766 MAE=89.6765 MAE=90.4981 MAE=90.1278 MAE=90.5543 MAE=90.8346 MAE=90.5192 Epoch: 310/500MAE=90.6613 MAE=90.9220 MAE=91.1997 MAE=91.1602 MAE=91.1895 MAE=91.6181 MAE=90.8496 MAE=90.5508 MAE=90.5241 MAE=90.4920 Epoch: 320/500MAE=89.7788 MAE=89.5774 MAE=89.8407 MAE=90.6274 MAE=89.8046 MAE=91.0998 MAE=90.3443 MAE=90.4209 MAE=90.2087 MAE=91.1190 Epoch: 330/500MAE=89.9632 MAE=90.7479 MAE=90.3171 MAE=90.9177 MAE=90.5414 MAE=90.5186 MAE=89.5228 MAE=89.8510 MAE=90.2605 MAE=90.4232 Epoch: 340/500MAE=90.7382 MAE=90.5972 MAE=90.0285 MAE=89.9197 MAE=90.9770 MAE=90.2080 MAE=90.0288 MAE=91.4458 MAE=90.4406 MAE=90.5844 Epoch: 350/500MAE=90.4849 MAE=90.2407 MAE=90.2118 MAE=90.4229 MAE=91.1022 MAE=90.4779 MAE=89.9808 MAE=90.0990 MAE=90.3913 MAE=89.9962 Epoch: 360/500MAE=90.9599 MAE=90.0322 MAE=90.2465 MAE=90.5263 MAE=90.1000 MAE=90.3835 MAE=90.8614 MAE=90.4447 MAE=90.6445 MAE=90.9707 Epoch: 370/500MAE=90.4807 MAE=90.8116 MAE=89.8207 MAE=91.0325 MAE=90.2483 MAE=90.9891 MAE=90.5697 MAE=90.9956 MAE=90.1521 MAE=89.7464 Epoch: 380/500MAE=90.3729 MAE=89.8955 MAE=89.2798 MAE=90.7523 MAE=90.2320 MAE=90.6070 MAE=89.6086 MAE=90.5156 MAE=90.8195 MAE=90.0293 Epoch: 390/500MAE=90.0178 MAE=90.1108 MAE=89.5253 MAE=90.2123 MAE=90.1308 MAE=88.9021 MAE=90.5353 MAE=89.9412 MAE=90.6010 MAE=89.8347 Epoch: 400/500MAE=90.9846 MAE=89.2043 MAE=89.9351 MAE=90.1465 MAE=90.7170 MAE=90.0374 MAE=90.1442 MAE=90.2524 MAE=89.7935 MAE=90.5583 Epoch: 410/500MAE=91.0690 MAE=90.6525 MAE=90.0634 MAE=88.4304 MAE=88.9071 MAE=89.5250 MAE=89.8504 MAE=90.0988 MAE=89.3617 MAE=90.2004 Epoch: 420/500MAE=90.4615 MAE=90.3002 MAE=90.1223 MAE=90.1103 MAE=89.3307 MAE=90.7689 MAE=90.9012 MAE=90.8346 MAE=90.1955 MAE=89.9922 Epoch: 430/500MAE=90.3541 MAE=90.4944 MAE=90.9486 MAE=90.4239 MAE=90.5097 MAE=89.8581 MAE=89.9705 MAE=91.0909 MAE=89.5243 MAE=89.6463 Epoch: 440/500MAE=90.1005 MAE=89.8646 MAE=89.9860 MAE=90.5683 MAE=90.3555 MAE=90.0112 MAE=89.8590 MAE=91.2499 MAE=90.1353 MAE=89.4934 Epoch: 450/500MAE=90.1996 MAE=89.6742 MAE=89.3880 MAE=90.4348 MAE=90.7871 MAE=90.6079 MAE=90.2772 MAE=90.7155 MAE=89.0010 MAE=90.2912 Epoch: 460/500MAE=89.5034 MAE=90.1380 MAE=90.0911 MAE=90.3479 MAE=89.5957 MAE=90.1368 MAE=90.4489 MAE=89.4483 MAE=89.7917 MAE=90.2839 Epoch: 470/500MAE=90.7906 MAE=89.4599 MAE=89.9771 MAE=90.1396 MAE=89.8773 MAE=89.7724 MAE=90.4202 MAE=90.8837 MAE=89.7672 MAE=90.1567 Epoch: 480/500MAE=90.3377 MAE=89.4734 MAE=88.8531 MAE=90.3233 MAE=90.6486 MAE=89.9060 MAE=89.7507 MAE=90.0985 MAE=90.2678 MAE=89.4652 Epoch: 490/500MAE=88.9257 MAE=89.8877 MAE=90.4546 MAE=90.1056 MAE=90.9171 MAE=90.3197 MAE=89.1360 MAE=89.8097 MAE=90.1703 MAE=89.6190 Epoch: 500/500MAE=89.8091 MAE=89.0117 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 94.641 +/- 5.629\n",
      "\n",
      "Epoch: 1/500MAE=1549.4243 MAE=1546.7520 MAE=1544.6886 MAE=1542.5696 MAE=1539.9785 MAE=1537.1753 MAE=1534.1743 MAE=1530.4805 MAE=1525.8777 Epoch: 10/500MAE=1522.4905 MAE=1516.7454 MAE=1512.6978 MAE=1505.3715 MAE=1502.8839 MAE=1497.5574 MAE=1489.9552 MAE=1484.6500 MAE=1475.3416 MAE=1469.1250 Epoch: 20/500MAE=1461.6899 MAE=1453.0956 MAE=1446.3622 MAE=1436.2474 MAE=1426.1284 MAE=1419.7156 MAE=1406.5408 MAE=1399.4348 MAE=1390.5402 MAE=1375.9814 Epoch: 30/500MAE=1367.7487 MAE=1354.1611 MAE=1344.0850 MAE=1332.2040 MAE=1321.6119 MAE=1309.6392 MAE=1293.1835 MAE=1281.7278 MAE=1269.4185 MAE=1258.7104 Epoch: 40/500MAE=1243.3162 MAE=1229.2368 MAE=1213.7324 MAE=1199.3553 MAE=1174.7561 MAE=1186.3457 MAE=1153.8540 MAE=1133.4734 MAE=1124.1338 MAE=1104.1912 Epoch: 50/500MAE=1084.6520 MAE=1068.0153 MAE=1048.4858 MAE=1033.3572 MAE=1010.0641 MAE=1010.3875 MAE=982.4568 MAE=949.3880 MAE=941.5072 MAE=911.1942 Epoch: 60/500MAE=892.6724 MAE=873.8941 MAE=848.7315 MAE=829.9817 MAE=815.3773 MAE=759.7818 MAE=738.1485 MAE=714.1080 MAE=708.0754 MAE=692.5127 Epoch: 70/500MAE=623.6240 MAE=591.3574 MAE=652.9004 MAE=653.4639 MAE=576.4361 MAE=573.7618 MAE=525.4713 MAE=471.4914 MAE=499.0010 MAE=458.9783 Epoch: 80/500MAE=489.1168 MAE=408.2344 MAE=414.6172 MAE=371.6989 MAE=332.5039 MAE=364.6729 MAE=252.7856 MAE=271.2411 MAE=286.8719 MAE=245.5008 Epoch: 90/500MAE=253.5128 MAE=230.6331 MAE=270.3626 MAE=186.5786 MAE=251.8609 MAE=174.5704 MAE=137.3402 MAE=141.8568 MAE=156.5979 MAE=151.1800 Epoch: 100/500MAE=125.5248 MAE=130.1893 MAE=111.0118 MAE=122.9237 MAE=134.7531 MAE=109.4589 MAE=105.3394 MAE=106.3774 MAE=102.0457 MAE=105.7768 Epoch: 110/500MAE=109.4000 MAE=103.9663 MAE=107.1301 MAE=94.2550 MAE=95.8206 MAE=95.6273 MAE=95.7055 MAE=94.1571 MAE=96.2349 MAE=92.4083 Epoch: 120/500MAE=94.6259 MAE=92.5558 MAE=92.4635 MAE=92.4710 MAE=93.2594 MAE=91.2117 MAE=91.1881 MAE=91.3938 MAE=91.3502 MAE=89.1918 Epoch: 130/500MAE=89.4806 MAE=90.0691 MAE=88.3597 MAE=90.3842 MAE=88.2742 MAE=87.9595 MAE=87.9698 MAE=89.7560 MAE=89.0553 MAE=91.1397 Epoch: 140/500MAE=88.3180 MAE=89.2268 MAE=88.3913 MAE=89.9482 MAE=88.0871 MAE=87.8802 MAE=88.5175 MAE=88.2679 MAE=88.2794 MAE=87.4791 Epoch: 150/500MAE=88.0557 MAE=88.2314 MAE=87.2846 MAE=88.1583 MAE=87.8819 MAE=87.0742 MAE=88.0902 MAE=86.3527 MAE=88.1069 MAE=88.3554 Epoch: 160/500MAE=87.6315 MAE=88.2972 MAE=87.8824 MAE=87.7854 MAE=87.8754 MAE=86.7929 MAE=87.4340 MAE=87.4549 MAE=86.9778 MAE=88.0471 Epoch: 170/500MAE=87.6928 MAE=87.6161 MAE=87.9285 MAE=87.7958 MAE=87.9804 MAE=87.8578 MAE=87.5738 MAE=87.7207 MAE=88.4489 MAE=88.1483 Epoch: 180/500MAE=87.8878 MAE=88.0204 MAE=87.3685 MAE=87.1551 MAE=87.8939 MAE=87.4514 MAE=87.9206 MAE=86.8893 MAE=87.9573 MAE=88.0168 Epoch: 190/500MAE=87.4138 MAE=87.3592 MAE=88.0818 MAE=87.5215 MAE=87.3174 MAE=87.5112 MAE=88.5584 MAE=88.4297 MAE=87.7637 MAE=87.6297 Epoch: 200/500MAE=88.0366 MAE=87.3775 MAE=87.5909 MAE=87.4030 MAE=86.7416 MAE=87.7719 MAE=87.0524 MAE=87.2609 MAE=87.3869 MAE=87.4807 Epoch: 210/500MAE=86.8944 MAE=87.9165 MAE=87.1522 MAE=87.6777 MAE=87.2419 MAE=88.1102 MAE=88.1891 MAE=87.4067 MAE=87.5087 MAE=87.4910 Epoch: 220/500MAE=87.2782 MAE=87.4415 MAE=87.8663 MAE=87.6353 MAE=87.8568 MAE=87.8337 MAE=87.4381 MAE=87.6465 MAE=87.3875 MAE=87.3851 Epoch: 230/500MAE=87.4134 MAE=87.2278 MAE=86.4599 MAE=87.8295 MAE=86.9628 MAE=87.9920 MAE=87.3261 MAE=87.2580 MAE=87.4100 MAE=87.3984 Epoch: 240/500MAE=87.5933 MAE=87.3284 MAE=87.1670 MAE=86.7475 MAE=86.9586 MAE=87.3498 MAE=87.4837 MAE=87.0513 MAE=87.8809 MAE=87.4935 Epoch: 250/500MAE=87.4706 MAE=87.4535 MAE=87.2998 MAE=87.1627 MAE=87.3060 MAE=87.6904 MAE=87.9381 MAE=88.1541 MAE=87.3613 MAE=86.7908 Epoch: 260/500MAE=88.2296 MAE=86.9433 MAE=87.1615 MAE=87.4272 MAE=87.4731 MAE=87.9305 MAE=87.3961 MAE=88.3071 MAE=87.0918 MAE=87.2955 Epoch: 270/500MAE=88.2632 MAE=87.4526 MAE=88.8002 MAE=87.7707 MAE=87.3735 MAE=87.1334 MAE=88.0064 MAE=87.9865 MAE=88.1977 MAE=87.7010 Epoch: 280/500MAE=87.9160 MAE=87.3542 MAE=87.0763 MAE=88.0236 MAE=87.4431 MAE=87.7791 MAE=87.3062 MAE=87.9923 MAE=87.6286 MAE=87.3548 Epoch: 290/500MAE=87.6083 MAE=87.0251 MAE=87.8204 MAE=87.6487 MAE=87.0856 MAE=87.6404 MAE=87.1841 MAE=87.3544 MAE=87.8309 MAE=86.9641 Epoch: 300/500MAE=87.9708 MAE=87.6357 MAE=87.7626 MAE=87.3810 MAE=87.3887 MAE=87.0136 MAE=87.9163 MAE=87.0116 MAE=91.1900 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 93.491 +/- 4.876\n",
      "\n",
      "Epoch: 1/500MAE=1549.7368 MAE=1547.7327 MAE=1544.9436 MAE=1542.3899 MAE=1540.1992 MAE=1537.4531 MAE=1533.9214 MAE=1531.4561 MAE=1526.4568 Epoch: 10/500MAE=1525.1505 MAE=1521.2163 MAE=1515.8955 MAE=1510.4048 MAE=1504.1204 MAE=1499.0413 MAE=1495.4348 MAE=1488.3302 MAE=1482.2083 MAE=1469.6753 Epoch: 20/500MAE=1464.8795 MAE=1466.7137 MAE=1453.4086 MAE=1445.4028 MAE=1420.5850 MAE=1423.7518 MAE=1420.2036 MAE=1406.6428 MAE=1390.3625 MAE=1373.1980 Epoch: 30/500MAE=1362.5056 MAE=1380.4724 MAE=1348.9730 MAE=1343.6498 MAE=1334.1365 MAE=1324.6881 MAE=1312.2482 MAE=1297.5872 MAE=1271.5326 MAE=1269.7990 Epoch: 40/500MAE=1248.4380 MAE=1249.9458 MAE=1221.7905 MAE=1213.3208 MAE=1203.3374 MAE=1183.2533 MAE=1140.5925 MAE=1152.3071 MAE=1143.5085 MAE=1100.3167 Epoch: 50/500MAE=1092.6677 MAE=1095.1367 MAE=1083.1423 MAE=1054.4816 MAE=1041.8781 MAE=1039.2253 MAE=993.9001 MAE=1005.9250 MAE=943.9793 MAE=940.8296 Epoch: 60/500MAE=932.3171 MAE=913.5121 MAE=874.9741 MAE=868.1565 MAE=870.6961 MAE=793.7919 MAE=803.9109 MAE=784.6859 MAE=749.5253 MAE=774.5121 Epoch: 70/500MAE=702.9564 MAE=692.4517 MAE=637.8791 MAE=679.7437 MAE=605.8104 MAE=578.8146 MAE=571.4939 MAE=546.7737 MAE=535.1518 MAE=485.0331 Epoch: 80/500MAE=461.2218 MAE=469.8411 MAE=436.5119 MAE=456.1827 MAE=376.2538 MAE=367.6713 MAE=358.2984 MAE=332.3490 MAE=295.5361 MAE=295.6011 Epoch: 90/500MAE=255.7878 MAE=251.4989 MAE=222.2617 MAE=239.9565 MAE=176.1384 MAE=220.4010 MAE=230.7155 MAE=188.0299 MAE=191.8547 MAE=142.2084 Epoch: 100/500MAE=152.7312 MAE=129.4060 MAE=148.2987 MAE=131.3296 MAE=155.4950 MAE=137.6774 MAE=127.3998 MAE=121.6963 MAE=118.5425 MAE=117.5326 Epoch: 110/500MAE=119.7517 MAE=115.1612 MAE=117.0435 MAE=116.9671 MAE=118.5764 MAE=119.4725 MAE=113.8297 MAE=114.3214 MAE=111.7857 MAE=111.6714 Epoch: 120/500MAE=111.9123 MAE=112.8906 MAE=111.2103 MAE=111.0404 MAE=110.5482 MAE=109.4031 MAE=109.8625 MAE=109.8467 MAE=109.3473 MAE=109.2415 Epoch: 130/500MAE=108.6035 MAE=108.4181 MAE=106.4662 MAE=107.4900 MAE=107.1100 MAE=107.7401 MAE=106.5473 MAE=107.3675 MAE=107.1737 MAE=107.3577 Epoch: 140/500MAE=105.6145 MAE=107.1485 MAE=105.3095 MAE=104.5648 MAE=104.9172 MAE=105.7935 MAE=104.5308 MAE=104.2926 MAE=103.7514 MAE=103.8492 Epoch: 150/500MAE=104.9102 MAE=103.1954 MAE=103.7460 MAE=104.9182 MAE=104.6312 MAE=102.9678 MAE=106.0737 MAE=104.6456 MAE=104.3905 MAE=104.5166 Epoch: 160/500MAE=103.9023 MAE=103.8592 MAE=103.9718 MAE=104.2297 MAE=104.5305 MAE=103.7145 MAE=103.8441 MAE=102.0317 MAE=103.3647 MAE=103.6255 Epoch: 170/500MAE=102.4899 MAE=102.1741 MAE=103.6814 MAE=103.3062 MAE=103.7418 MAE=103.8968 MAE=102.4023 MAE=103.9575 MAE=102.1461 MAE=102.4143 Epoch: 180/500MAE=102.8005 MAE=103.8781 MAE=103.3207 MAE=103.3050 MAE=103.8306 MAE=101.9915 MAE=102.7529 MAE=102.8408 MAE=103.6983 MAE=101.7746 Epoch: 190/500MAE=103.2810 MAE=103.1182 MAE=102.1443 MAE=102.2966 MAE=102.4910 MAE=102.3495 MAE=103.7887 MAE=102.5478 MAE=103.7186 MAE=102.6631 Epoch: 200/500MAE=104.5018 MAE=103.2296 MAE=103.0768 MAE=102.7565 MAE=102.5701 MAE=103.8370 MAE=103.0484 MAE=102.7331 MAE=103.9797 MAE=102.4753 Epoch: 210/500MAE=103.0069 MAE=102.4518 MAE=102.3622 MAE=102.3474 MAE=103.0302 MAE=103.3342 MAE=103.3482 MAE=103.1794 MAE=103.6107 MAE=103.6235 Epoch: 220/500MAE=103.8129 MAE=102.7769 MAE=102.8849 MAE=102.7449 MAE=102.9967 MAE=103.7138 MAE=103.8009 MAE=102.8270 MAE=103.3248 MAE=104.2114 Epoch: 230/500MAE=104.2463 MAE=102.8641 MAE=102.9512 MAE=102.4693 MAE=103.0599 MAE=102.5576 MAE=102.3760 MAE=103.2925 MAE=103.2555 MAE=103.6970 Epoch: 240/500MAE=103.1557 MAE=103.2301 MAE=102.6517 MAE=102.3245 MAE=102.7293 MAE=102.9164 MAE=103.3140 MAE=103.5034 MAE=103.3979 MAE=103.0023 Epoch: 250/500MAE=103.2443 MAE=102.8401 MAE=103.5080 MAE=102.7212 MAE=102.8447 MAE=102.9440 MAE=102.5809 MAE=102.6382 MAE=104.2641 MAE=103.4971 Epoch: 260/500MAE=103.3052 MAE=101.9719 MAE=103.9887 MAE=102.8098 MAE=102.6117 MAE=102.9502 MAE=103.3063 MAE=103.1350 MAE=103.3292 MAE=102.1678 Epoch: 270/500MAE=103.6447 MAE=102.5136 MAE=103.4082 MAE=102.8488 MAE=103.2083 MAE=102.4247 MAE=103.0790 MAE=102.7036 MAE=103.0441 MAE=103.1414 Epoch: 280/500MAE=102.2912 MAE=102.9714 MAE=102.3964 MAE=102.7714 MAE=103.1905 MAE=103.3250 MAE=103.2620 MAE=103.2936 MAE=102.6632 MAE=103.2444 Epoch: 290/500MAE=102.2594 MAE=103.6138 MAE=103.2193 MAE=102.4349 MAE=102.5747 MAE=103.5659 MAE=102.9958 MAE=102.8097 MAE=103.6512 MAE=103.0844 Epoch: 300/500MAE=103.4443 MAE=103.4473 MAE=103.2630 MAE=104.0318 MAE=103.2169 MAE=103.7579 MAE=102.4026 MAE=103.6918 MAE=102.5978 MAE=102.8210 Epoch: 310/500MAE=103.0218 MAE=102.8756 MAE=103.6870 MAE=103.3912 MAE=103.9559 MAE=103.3844 MAE=104.3996 MAE=103.7047 MAE=102.9383 MAE=103.9862 Epoch: 320/500MAE=103.1147 MAE=103.2271 MAE=102.3222 MAE=103.2182 MAE=103.5541 MAE=103.7737 MAE=103.6009 MAE=103.6144 MAE=101.9899 MAE=102.5571 Epoch: 330/500MAE=102.4714 MAE=101.8765 MAE=102.0232 MAE=102.8779 MAE=103.1300 MAE=102.8858 MAE=103.2484 MAE=103.4570 MAE=102.6896 MAE=102.8200 MAE=103.7440 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 96.054 +/- 6.127\n",
      "\n",
      "Epoch: 1/500MAE=1549.3306 MAE=1546.9043 MAE=1544.1425 MAE=1541.7522 MAE=1539.1335 MAE=1536.4329 MAE=1533.4153 MAE=1530.5139 MAE=1526.4030 Epoch: 10/500MAE=1523.1617 MAE=1519.6659 MAE=1512.9652 MAE=1508.3643 MAE=1500.0891 MAE=1495.2606 MAE=1488.8669 MAE=1486.5851 MAE=1477.1595 MAE=1469.4031 Epoch: 20/500MAE=1461.7156 MAE=1454.0005 MAE=1448.2285 MAE=1439.9307 MAE=1429.0415 MAE=1422.0012 MAE=1411.7278 MAE=1400.7605 MAE=1389.3777 MAE=1380.6324 Epoch: 30/500MAE=1370.4753 MAE=1352.2469 MAE=1340.5109 MAE=1334.9366 MAE=1314.8639 MAE=1305.3600 MAE=1293.1252 MAE=1277.4607 MAE=1268.0022 MAE=1253.1720 Epoch: 40/500MAE=1241.3745 MAE=1229.4509 MAE=1211.7915 MAE=1182.7946 MAE=1179.8826 MAE=1161.6870 MAE=1138.4922 MAE=1124.6323 MAE=1107.5997 MAE=1091.2075 Epoch: 50/500MAE=1070.7153 MAE=1063.9258 MAE=1025.7434 MAE=1022.5594 MAE=999.6731 MAE=984.0931 MAE=963.4729 MAE=940.5036 MAE=937.9315 MAE=888.2662 Epoch: 60/500MAE=893.8825 MAE=869.5277 MAE=861.7458 MAE=839.4319 MAE=827.7402 MAE=795.5809 MAE=761.1868 MAE=745.9644 MAE=703.6216 MAE=699.8680 Epoch: 70/500MAE=692.5383 MAE=662.4525 MAE=655.5023 MAE=617.4227 MAE=614.6829 MAE=596.8316 MAE=511.1594 MAE=533.7554 MAE=455.6905 MAE=468.0612 Epoch: 80/500MAE=452.6194 MAE=394.4897 MAE=366.5347 MAE=382.8704 MAE=323.5713 MAE=323.2434 MAE=274.5446 MAE=249.5911 MAE=280.4102 MAE=221.4172 Epoch: 90/500MAE=193.6582 MAE=194.2847 MAE=179.7899 MAE=193.4346 MAE=145.9109 MAE=134.1017 MAE=146.9437 MAE=134.2311 MAE=113.0171 MAE=113.9019 Epoch: 100/500MAE=106.6795 MAE=104.5773 MAE=121.8099 MAE=109.3827 MAE=100.0169 MAE=96.6144 MAE=95.3949 MAE=98.3057 MAE=91.4371 MAE=96.4397 Epoch: 110/500MAE=100.9723 MAE=96.6744 MAE=90.0844 MAE=93.0823 MAE=95.6545 MAE=98.5641 MAE=96.4325 MAE=86.5177 MAE=85.0574 MAE=90.1206 Epoch: 120/500MAE=91.3750 MAE=87.4387 MAE=87.8340 MAE=83.4398 MAE=86.5658 MAE=84.4340 MAE=82.1780 MAE=83.8925 MAE=81.2040 MAE=83.2078 Epoch: 130/500MAE=81.0686 MAE=83.8079 MAE=83.0494 MAE=84.0927 MAE=83.3940 MAE=82.4013 MAE=82.4734 MAE=82.4835 MAE=81.9521 MAE=81.4551 Epoch: 140/500MAE=82.2667 MAE=82.7891 MAE=82.4540 MAE=81.9887 MAE=81.5446 MAE=82.3078 MAE=82.1403 MAE=82.3723 MAE=81.8433 MAE=81.4540 Epoch: 150/500MAE=82.6499 MAE=81.8378 MAE=82.2679 MAE=81.1762 MAE=82.0060 MAE=82.0695 MAE=81.7187 MAE=81.9736 MAE=81.7111 MAE=81.8574 Epoch: 160/500MAE=81.5813 MAE=81.5314 MAE=81.4966 MAE=82.1311 MAE=81.4031 MAE=81.7374 MAE=82.0062 MAE=82.1495 MAE=81.3426 MAE=81.6502 Epoch: 170/500MAE=81.3495 MAE=81.6064 MAE=81.7149 MAE=81.4906 MAE=81.7234 MAE=82.0177 MAE=81.9077 MAE=81.5708 MAE=81.6183 MAE=81.4515 Epoch: 180/500MAE=81.2846 MAE=81.5912 MAE=81.5171 MAE=82.1591 MAE=81.5635 MAE=81.5011 MAE=81.4726 MAE=81.7302 MAE=81.4649 MAE=81.6565 Epoch: 190/500MAE=81.4092 MAE=81.7027 MAE=82.0719 MAE=81.6252 MAE=81.7398 MAE=81.0895 MAE=82.1573 MAE=81.8350 MAE=82.3467 MAE=82.0908 Epoch: 200/500MAE=81.4035 MAE=81.8655 MAE=81.4603 MAE=81.7896 MAE=81.7584 MAE=81.8619 MAE=81.7402 MAE=81.6520 MAE=80.7208 MAE=81.0506 Epoch: 210/500MAE=81.7443 MAE=81.6840 MAE=81.7551 MAE=82.1313 MAE=82.2123 MAE=81.3591 MAE=81.5320 MAE=81.8618 MAE=81.2685 MAE=82.0870 Epoch: 220/500MAE=81.8477 MAE=81.6922 MAE=81.3633 MAE=82.2255 MAE=81.9177 MAE=81.4480 MAE=81.2554 MAE=81.4093 MAE=81.9768 MAE=81.4781 Epoch: 230/500MAE=81.9952 MAE=81.5933 MAE=81.7885 MAE=81.2203 MAE=81.0191 MAE=81.1232 MAE=81.5500 MAE=81.3764 MAE=81.7092 MAE=82.2674 Epoch: 240/500MAE=81.8723 MAE=81.5233 MAE=81.3908 MAE=81.6597 MAE=81.6248 MAE=81.2114 MAE=81.7900 MAE=82.1433 MAE=82.0941 MAE=81.8503 Epoch: 250/500MAE=81.8266 MAE=81.6609 MAE=81.9459 MAE=81.6714 MAE=81.5049 MAE=81.8043 MAE=81.2230 MAE=81.3471 MAE=81.7307 MAE=80.9960 Epoch: 260/500MAE=81.9311 MAE=81.5726 MAE=81.2552 MAE=82.1600 MAE=81.1581 MAE=81.7246 MAE=81.3976 MAE=81.0160 MAE=81.5377 MAE=81.9576 Epoch: 270/500MAE=81.9799 MAE=81.5530 MAE=82.0716 MAE=82.0045 MAE=81.4824 MAE=81.2412 MAE=81.6272 MAE=81.7793 MAE=81.5530 MAE=81.4503 Epoch: 280/500MAE=81.7566 MAE=81.8795 MAE=81.5426 MAE=81.2786 MAE=81.6421 MAE=81.3969 MAE=81.6647 MAE=81.3333 MAE=81.6155 MAE=81.0340 Epoch: 290/500MAE=81.7364 MAE=81.4172 MAE=81.6775 MAE=82.1341 MAE=81.6920 MAE=81.8816 MAE=81.5997 MAE=82.0373 MAE=81.5655 MAE=81.5823 Epoch: 300/500MAE=81.5940 MAE=81.4440 MAE=81.0366 MAE=82.2699 MAE=81.3212 MAE=81.5507 MAE=81.3954 MAE=81.4209 MAE=81.6882 MAE=81.7168 Epoch: 310/500MAE=82.1013 MAE=81.4393 MAE=80.9327 MAE=82.4305 MAE=81.8679 MAE=81.7664 MAE=81.6378 MAE=81.5821 MAE=81.2224 MAE=82.2694 Epoch: 320/500MAE=81.7928 MAE=81.5405 MAE=81.5377 MAE=81.7718 MAE=81.6383 MAE=81.7563 MAE=81.8236 MAE=81.4991 MAE=82.0161 MAE=81.8881 Epoch: 330/500MAE=81.8633 MAE=81.4688 MAE=81.7051 MAE=82.4601 MAE=81.5817 MAE=81.7647 MAE=81.3340 MAE=81.8008 MAE=81.7084 MAE=82.0962 Epoch: 340/500MAE=82.0004 MAE=81.8601 MAE=80.9333 MAE=82.4548 MAE=81.6737 MAE=81.8135 MAE=82.3976 MAE=82.2405 MAE=81.2119 MAE=81.8071 Epoch: 350/500MAE=82.0162 MAE=81.9007 MAE=82.3505 MAE=81.9029 MAE=82.2711 MAE=81.9486 MAE=82.1044 MAE=81.6280 MAE=81.4491 MAE=85.9772 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 94.039 +/- 6.803\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1549.7780 MAE=1547.5637 MAE=1544.8230 MAE=1541.7694 MAE=1538.8083 MAE=1536.5138 MAE=1532.7061 MAE=1529.4370 MAE=1525.4062 Epoch: 10/500MAE=1521.5020 MAE=1516.4232 MAE=1512.5139 MAE=1507.7358 MAE=1494.4015 MAE=1489.3649 MAE=1487.2812 MAE=1475.0920 MAE=1468.4185 MAE=1465.1910 Epoch: 20/500MAE=1455.1545 MAE=1445.6692 MAE=1435.0018 MAE=1434.7439 MAE=1418.6743 MAE=1412.4598 MAE=1412.2468 MAE=1395.2777 MAE=1384.7291 MAE=1364.3062 Epoch: 30/500MAE=1359.5585 MAE=1355.9634 MAE=1339.3698 MAE=1329.3380 MAE=1316.7539 MAE=1299.6943 MAE=1293.4780 MAE=1281.4072 MAE=1263.8102 MAE=1240.1709 Epoch: 40/500MAE=1235.5854 MAE=1213.2593 MAE=1204.9167 MAE=1187.4913 MAE=1172.5374 MAE=1148.0380 MAE=1125.6946 MAE=1133.2485 MAE=1103.6697 MAE=1093.4658 Epoch: 50/500MAE=1069.5032 MAE=1059.6991 MAE=1028.4606 MAE=1027.8918 MAE=967.8971 MAE=968.0966 MAE=958.6692 MAE=930.2975 MAE=908.7043 MAE=894.6591 Epoch: 60/500MAE=877.7576 MAE=868.5898 MAE=722.5051 MAE=757.3302 MAE=788.4491 MAE=745.7789 MAE=744.5597 MAE=725.1570 MAE=727.7253 MAE=712.6664 Epoch: 70/500MAE=682.1363 MAE=683.0510 MAE=686.1266 MAE=662.0452 MAE=647.0169 MAE=638.6141 MAE=631.2791 MAE=651.9901 MAE=601.5715 MAE=585.8412 Epoch: 80/500MAE=579.1719 MAE=600.4625 MAE=538.9803 MAE=534.4862 MAE=557.1566 MAE=512.6893 MAE=515.0865 MAE=489.2325 MAE=495.5982 MAE=479.9875 Epoch: 90/500MAE=439.5875 MAE=439.9294 MAE=462.9418 MAE=417.3649 MAE=433.9397 MAE=448.5042 MAE=405.5129 MAE=412.3867 MAE=375.3909 MAE=390.6293 Epoch: 100/500MAE=382.6043 MAE=347.5540 MAE=333.8089 MAE=339.3508 MAE=329.0685 MAE=341.2726 MAE=304.4137 MAE=305.9243 MAE=286.5045 MAE=305.5356 Epoch: 110/500MAE=276.4020 MAE=262.7623 MAE=237.3854 MAE=273.2185 MAE=261.8251 MAE=232.9104 MAE=216.9843 MAE=197.1453 MAE=207.0253 MAE=198.8288 Epoch: 120/500MAE=227.0479 MAE=161.4016 MAE=182.7185 MAE=156.8409 MAE=154.7768 MAE=176.0342 MAE=150.7042 MAE=139.8458 MAE=154.8307 MAE=139.2817 Epoch: 130/500MAE=118.8584 MAE=149.5656 MAE=134.8889 MAE=125.0114 MAE=143.8459 MAE=118.3662 MAE=115.2709 MAE=111.1959 MAE=106.0458 MAE=102.3475 Epoch: 140/500MAE=106.1607 MAE=101.3514 MAE=102.0022 MAE=101.6131 MAE=98.5608 MAE=98.2438 MAE=100.9682 MAE=97.4774 MAE=96.8332 MAE=90.2118 Epoch: 150/500MAE=93.4218 MAE=100.4421 MAE=92.3975 MAE=93.2709 MAE=91.4205 MAE=89.0894 MAE=91.3367 MAE=92.3199 MAE=88.7796 MAE=87.0036 Epoch: 160/500MAE=87.7465 MAE=89.6552 MAE=87.5614 MAE=85.9645 MAE=86.6973 MAE=87.7930 MAE=84.4234 MAE=87.8898 MAE=86.4354 MAE=84.8175 Epoch: 170/500MAE=87.5777 MAE=85.8171 MAE=85.5476 MAE=85.6671 MAE=84.2180 MAE=84.8549 MAE=84.1773 MAE=85.1765 MAE=86.9555 MAE=85.8545 Epoch: 180/500MAE=85.4073 MAE=85.1328 MAE=84.5576 MAE=84.6032 MAE=85.1718 MAE=84.2433 MAE=85.1655 MAE=85.4369 MAE=85.0448 MAE=84.3581 Epoch: 190/500MAE=84.6663 MAE=86.1000 MAE=84.4131 MAE=84.4656 MAE=83.9086 MAE=84.1080 MAE=84.9318 MAE=84.2828 MAE=84.9930 MAE=85.6318 Epoch: 200/500MAE=84.5634 MAE=84.5976 MAE=83.9812 MAE=84.6976 MAE=84.3532 MAE=84.4234 MAE=85.2864 MAE=84.3626 MAE=83.7099 MAE=83.7885 Epoch: 210/500MAE=85.0215 MAE=85.3541 MAE=83.8116 MAE=84.3908 MAE=83.9136 MAE=84.8281 MAE=83.9634 MAE=84.8607 MAE=84.9348 MAE=84.2786 Epoch: 220/500MAE=84.3846 MAE=85.1134 MAE=84.6105 MAE=84.3215 MAE=84.8359 MAE=84.0989 MAE=85.2531 MAE=84.5225 MAE=84.0476 MAE=85.2671 Epoch: 230/500MAE=85.3699 MAE=85.0602 MAE=83.8588 MAE=84.5617 MAE=84.4700 MAE=84.4636 MAE=84.4335 MAE=84.4409 MAE=84.0563 MAE=84.0436 Epoch: 240/500MAE=84.2313 MAE=84.1366 MAE=84.6879 MAE=84.2964 MAE=84.5841 MAE=84.3491 MAE=85.1837 MAE=84.2872 MAE=84.6237 MAE=84.8558 Epoch: 250/500MAE=84.1644 MAE=85.6178 MAE=84.9571 MAE=84.1154 MAE=84.1724 MAE=85.4152 MAE=83.9295 MAE=85.0126 MAE=84.4927 MAE=85.3636 Epoch: 260/500MAE=84.4738 MAE=84.5697 MAE=85.0750 MAE=83.9313 MAE=84.3766 MAE=85.0091 MAE=85.4676 MAE=84.8564 MAE=84.7801 MAE=84.8493 Epoch: 270/500MAE=84.1040 MAE=84.9990 MAE=84.8755 MAE=84.9777 MAE=84.6595 MAE=84.4902 MAE=84.6554 MAE=84.7545 MAE=84.8453 MAE=84.8157 Epoch: 280/500MAE=85.1416 MAE=84.9322 MAE=84.7429 MAE=85.3495 MAE=83.9094 MAE=84.1408 MAE=85.0199 MAE=84.3312 MAE=85.1257 MAE=84.5027 Epoch: 290/500MAE=84.4626 MAE=84.5710 MAE=84.3415 MAE=84.6824 MAE=84.7756 MAE=84.4273 MAE=84.3289 MAE=84.1983 MAE=84.4415 MAE=84.9015 Epoch: 300/500MAE=85.3448 MAE=85.5878 MAE=85.1323 MAE=84.2950 MAE=84.6687 MAE=84.8876 MAE=85.0694 MAE=85.1538 MAE=85.0153 MAE=84.9226 Epoch: 310/500MAE=84.9055 MAE=85.2448 MAE=83.5236 MAE=85.1017 MAE=84.8672 MAE=84.3378 MAE=84.4115 MAE=84.5940 MAE=84.7191 MAE=85.1188 Epoch: 320/500MAE=84.5317 MAE=84.3227 MAE=84.6829 MAE=84.7160 MAE=85.0923 MAE=84.9021 MAE=84.2466 MAE=85.7915 MAE=84.5488 MAE=85.4382 Epoch: 330/500MAE=85.9086 MAE=85.5041 MAE=84.6206 MAE=84.1407 MAE=84.9242 MAE=85.7073 MAE=84.6073 MAE=84.5754 MAE=84.9211 MAE=84.4146 Epoch: 340/500MAE=85.6281 MAE=84.3061 MAE=84.8932 MAE=84.7216 MAE=84.4308 MAE=84.6827 MAE=84.9430 MAE=83.6559 MAE=83.8038 MAE=85.8966 Epoch: 350/500MAE=84.5043 MAE=84.5402 MAE=84.4979 MAE=84.7106 MAE=84.7082 MAE=85.0550 MAE=84.6538 MAE=84.8119 MAE=84.9629 MAE=84.4917 Epoch: 360/500MAE=84.4310 MAE=83.7869 MAE=84.5825 MAE=84.2488 MAE=84.5861 MAE=84.3777 MAE=85.2043 MAE=84.5491 MAE=84.2410 MAE=83.9722 Epoch: 370/500MAE=84.4246 MAE=84.5851 MAE=83.9920 MAE=84.1535 MAE=83.9982 MAE=84.8355 MAE=83.8755 MAE=84.7673 MAE=84.4286 MAE=83.6105 Epoch: 380/500MAE=83.2495 MAE=84.2165 MAE=84.1261 MAE=84.0787 MAE=84.6817 MAE=84.2001 MAE=84.4415 MAE=85.1223 MAE=84.0741 MAE=84.4587 Epoch: 390/500MAE=84.4056 MAE=84.4193 MAE=84.3585 MAE=84.7406 MAE=84.2152 MAE=84.2659 MAE=84.2472 MAE=85.0249 MAE=84.8492 MAE=84.0630 Epoch: 400/500MAE=84.5530 MAE=83.9366 MAE=84.4287 MAE=84.5534 MAE=83.9308 MAE=84.8562 MAE=83.9733 MAE=84.3371 MAE=83.1462 MAE=85.0897 Epoch: 410/500MAE=84.7505 MAE=84.6955 MAE=84.0135 MAE=85.3742 MAE=84.7007 MAE=84.2225 MAE=84.4717 MAE=84.7209 MAE=83.6203 MAE=84.2689 Epoch: 420/500MAE=85.0826 MAE=84.5457 MAE=84.4188 MAE=84.0638 MAE=84.2439 MAE=84.3515 MAE=83.2865 MAE=84.2764 MAE=84.4426 MAE=83.8300 Epoch: 430/500MAE=84.5088 MAE=84.0620 MAE=84.1031 MAE=84.4942 MAE=84.3606 MAE=83.8138 MAE=84.8408 MAE=84.5822 MAE=83.9477 MAE=85.2267 Epoch: 440/500MAE=84.1053 MAE=84.0458 MAE=83.9860 MAE=83.6520 MAE=84.1513 MAE=84.8991 MAE=83.7574 MAE=84.3857 MAE=84.1561 MAE=85.0066 Epoch: 450/500MAE=83.6854 MAE=84.3977 MAE=84.6904 MAE=84.4373 MAE=84.7215 MAE=84.8620 MAE=84.3610 MAE=83.5317 MAE=83.8724 MAE=84.4229 Epoch: 460/500MAE=84.1845 MAE=84.2759 MAE=84.1324 MAE=84.9472 MAE=84.9164 MAE=84.1456 MAE=83.4275 MAE=84.4378 MAE=83.6658 MAE=84.1035 Epoch: 470/500MAE=84.3787 MAE=83.6968 MAE=84.2352 MAE=84.1122 MAE=84.6228 MAE=84.4289 MAE=83.6073 MAE=84.8836 MAE=84.2329 MAE=84.0612 Epoch: 480/500MAE=83.0259 MAE=85.1323 MAE=84.2017 MAE=84.0974 MAE=84.3497 MAE=84.0772 MAE=83.8928 MAE=84.2215 MAE=83.6764 MAE=83.7495 Epoch: 490/500MAE=84.5242 MAE=83.7543 MAE=83.9524 MAE=84.2618 MAE=84.3198 MAE=83.6249 MAE=84.7966 MAE=84.0739 MAE=84.1592 MAE=83.6314 Epoch: 500/500MAE=83.8039 MAE=90.2215 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 90.222 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1549.2551 MAE=1547.3132 MAE=1545.2415 MAE=1543.9116 MAE=1540.4915 MAE=1538.5126 MAE=1534.9858 MAE=1531.0624 MAE=1528.1838 Epoch: 10/500MAE=1523.7217 MAE=1519.7009 MAE=1514.5178 MAE=1509.9272 MAE=1501.3003 MAE=1500.0538 MAE=1494.5858 MAE=1482.8441 MAE=1480.5350 MAE=1472.8315 Epoch: 20/500MAE=1466.4766 MAE=1453.7593 MAE=1451.0837 MAE=1433.6348 MAE=1426.7942 MAE=1414.8397 MAE=1410.0737 MAE=1407.1001 MAE=1393.5210 MAE=1383.3674 Epoch: 30/500MAE=1371.9163 MAE=1368.0640 MAE=1350.4471 MAE=1340.9153 MAE=1332.6268 MAE=1320.2659 MAE=1302.4500 MAE=1291.5793 MAE=1280.8889 MAE=1266.1223 Epoch: 40/500MAE=1254.3821 MAE=1233.5933 MAE=1220.0542 MAE=1216.9902 MAE=1185.4750 MAE=1178.7131 MAE=1151.9807 MAE=1147.5918 MAE=1118.7992 MAE=1110.4968 Epoch: 50/500MAE=1093.0167 MAE=1079.2839 MAE=1059.6860 MAE=1051.0068 MAE=1019.2537 MAE=1002.7065 MAE=986.9487 MAE=985.8279 MAE=966.9442 MAE=942.5499 Epoch: 60/500MAE=912.6764 MAE=900.0712 MAE=870.9146 MAE=846.6340 MAE=862.0968 MAE=809.6897 MAE=771.9646 MAE=709.3127 MAE=689.8581 MAE=724.5300 Epoch: 70/500MAE=682.1484 MAE=669.1112 MAE=635.5294 MAE=631.4091 MAE=600.3618 MAE=577.1801 MAE=595.1025 MAE=522.8828 MAE=509.8786 MAE=487.0782 Epoch: 80/500MAE=419.4278 MAE=413.0997 MAE=413.0546 MAE=364.4285 MAE=306.0923 MAE=302.5072 MAE=310.7350 MAE=279.8286 MAE=282.9055 MAE=231.6447 Epoch: 90/500MAE=242.2862 MAE=208.7292 MAE=197.6095 MAE=174.9541 MAE=181.3547 MAE=183.6010 MAE=153.4277 MAE=172.4389 MAE=119.1386 MAE=116.2262 Epoch: 100/500MAE=111.0056 MAE=102.3168 MAE=101.3868 MAE=98.9129 MAE=98.3300 MAE=99.1764 MAE=101.8952 MAE=104.9249 MAE=105.6984 MAE=94.9782 Epoch: 110/500MAE=97.3944 MAE=97.2628 MAE=94.6286 MAE=91.5578 MAE=89.7018 MAE=86.6810 MAE=89.8937 MAE=88.9477 MAE=92.7762 MAE=89.7723 Epoch: 120/500MAE=83.8236 MAE=82.6027 MAE=84.5674 MAE=82.4972 MAE=87.0939 MAE=82.1880 MAE=82.1303 MAE=83.2138 MAE=82.3108 MAE=82.9421 Epoch: 130/500MAE=82.8011 MAE=83.0423 MAE=81.8660 MAE=82.1979 MAE=81.2053 MAE=81.2517 MAE=82.3343 MAE=81.3376 MAE=81.9512 MAE=81.1030 Epoch: 140/500MAE=81.6477 MAE=80.4049 MAE=80.6677 MAE=81.8444 MAE=80.7794 MAE=80.9779 MAE=80.0425 MAE=80.3766 MAE=80.4358 MAE=80.3590 Epoch: 150/500MAE=80.8491 MAE=79.8728 MAE=80.8462 MAE=80.2730 MAE=81.0867 MAE=79.7932 MAE=80.9241 MAE=80.0537 MAE=80.9608 MAE=80.3238 Epoch: 160/500MAE=80.2562 MAE=80.5687 MAE=80.2142 MAE=80.9719 MAE=80.4043 MAE=80.6451 MAE=80.8678 MAE=80.6178 MAE=79.4797 MAE=80.1539 Epoch: 170/500MAE=80.4741 MAE=80.5251 MAE=80.6226 MAE=80.4083 MAE=80.5100 MAE=80.7364 MAE=80.4022 MAE=80.0676 MAE=80.5215 MAE=80.0770 Epoch: 180/500MAE=80.1293 MAE=79.8892 MAE=79.7984 MAE=80.4694 MAE=80.1175 MAE=79.6927 MAE=79.8843 MAE=80.4195 MAE=80.1073 MAE=80.0279 Epoch: 190/500MAE=80.0763 MAE=80.9366 MAE=80.0985 MAE=80.4756 MAE=80.4658 MAE=80.5855 MAE=80.7277 MAE=80.4285 MAE=80.0372 MAE=80.0043 Epoch: 200/500MAE=79.6865 MAE=80.9998 MAE=80.4811 MAE=79.7541 MAE=80.2200 MAE=80.5259 MAE=80.4269 MAE=80.7017 MAE=80.6097 MAE=80.5036 Epoch: 210/500MAE=80.9274 MAE=80.2184 MAE=80.0984 MAE=79.2308 MAE=81.0293 MAE=80.6789 MAE=80.7132 MAE=80.4705 MAE=80.2845 MAE=80.4317 Epoch: 220/500MAE=80.9010 MAE=80.7494 MAE=80.0303 MAE=79.8436 MAE=79.8280 MAE=79.8673 MAE=80.1941 MAE=79.8911 MAE=80.6204 MAE=80.5156 Epoch: 230/500MAE=80.4921 MAE=80.1707 MAE=80.5892 MAE=80.8283 MAE=80.2934 MAE=80.4592 MAE=80.9060 MAE=80.9278 MAE=80.0456 MAE=81.6779 Epoch: 240/500MAE=80.3146 MAE=80.4674 MAE=80.9421 MAE=80.4306 MAE=79.9895 MAE=80.3608 MAE=80.2953 MAE=80.9818 MAE=80.1246 MAE=79.8387 Epoch: 250/500MAE=81.1969 MAE=80.3244 MAE=80.3749 MAE=80.6261 MAE=81.0128 MAE=80.5092 MAE=80.2509 MAE=80.7667 MAE=80.8174 MAE=80.4200 Epoch: 260/500MAE=79.3824 MAE=80.1397 MAE=79.8961 MAE=80.8418 MAE=80.1044 MAE=80.5460 MAE=80.0883 MAE=80.0153 MAE=79.6611 MAE=80.2196 Epoch: 270/500MAE=80.4457 MAE=79.9527 MAE=80.5112 MAE=79.6230 MAE=80.8000 MAE=80.5316 MAE=80.0008 MAE=81.3344 MAE=80.0166 MAE=80.5899 Epoch: 280/500MAE=80.1325 MAE=80.3432 MAE=80.6148 MAE=80.6425 MAE=80.5501 MAE=81.4067 MAE=80.6056 MAE=80.7548 MAE=81.7506 MAE=80.7078 Epoch: 290/500MAE=80.3035 MAE=79.9830 MAE=81.6402 MAE=80.2900 MAE=80.3490 MAE=81.7422 MAE=80.8975 MAE=80.5969 MAE=81.9143 MAE=81.1155 Epoch: 300/500MAE=79.8598 MAE=80.7795 MAE=80.2563 MAE=80.7997 MAE=80.8225 MAE=79.9427 MAE=80.4466 MAE=81.1745 MAE=80.3453 MAE=80.3825 Epoch: 310/500MAE=80.7438 MAE=80.7856 MAE=81.0246 MAE=80.8477 MAE=80.5623 MAE=79.9454 MAE=80.2644 MAE=80.1953 MAE=80.6280 MAE=80.1763 Epoch: 320/500MAE=80.5370 MAE=81.1318 MAE=80.6330 MAE=80.9774 MAE=80.1532 MAE=80.3370 MAE=80.7272 MAE=80.6450 MAE=80.6721 MAE=80.6115 Epoch: 330/500MAE=80.1989 MAE=80.6846 MAE=80.3647 MAE=80.3338 MAE=80.3845 MAE=81.7268 MAE=81.3826 MAE=80.8318 MAE=81.1279 MAE=80.4577 Epoch: 340/500MAE=80.7014 MAE=80.4998 MAE=80.5831 MAE=80.4767 MAE=80.1101 MAE=80.4778 MAE=81.4770 MAE=80.2617 MAE=80.6977 MAE=80.4642 Epoch: 350/500MAE=79.9911 MAE=80.3344 MAE=80.8212 MAE=80.5011 MAE=80.9106 MAE=80.2180 MAE=80.6692 MAE=81.3385 MAE=80.5802 MAE=80.0654 Epoch: 360/500MAE=79.7874 MAE=80.3959 MAE=80.6500 MAE=80.1523 MAE=83.4294 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 86.825 +/- 3.396\n",
      "\n",
      "Epoch: 1/500MAE=1549.4456 MAE=1547.2119 MAE=1544.5809 MAE=1542.2688 MAE=1540.0093 MAE=1537.6628 MAE=1535.5536 MAE=1531.8521 MAE=1527.9841 Epoch: 10/500MAE=1524.2430 MAE=1519.4863 MAE=1515.4199 MAE=1508.2623 MAE=1502.3882 MAE=1497.1206 MAE=1488.9517 MAE=1482.1946 MAE=1476.0181 MAE=1468.5527 Epoch: 20/500MAE=1460.7100 MAE=1455.8562 MAE=1443.8324 MAE=1438.3965 MAE=1428.5231 MAE=1416.4084 MAE=1411.6257 MAE=1399.3220 MAE=1394.4248 MAE=1377.5626 Epoch: 30/500MAE=1361.5410 MAE=1356.7012 MAE=1348.4038 MAE=1335.6477 MAE=1333.6360 MAE=1306.9377 MAE=1308.6594 MAE=1265.3962 MAE=1274.7983 MAE=1268.9009 Epoch: 40/500MAE=1245.9740 MAE=1211.5166 MAE=1233.1202 MAE=1193.6544 MAE=1170.6653 MAE=1130.1630 MAE=1131.5581 MAE=1128.2539 MAE=1096.8342 MAE=1103.6897 Epoch: 50/500MAE=1083.8282 MAE=1071.9497 MAE=1049.3107 MAE=1019.0061 MAE=1004.5168 MAE=1041.5889 MAE=965.0660 MAE=963.1597 MAE=927.8339 MAE=914.4545 Epoch: 60/500MAE=823.1718 MAE=919.3309 MAE=876.9617 MAE=823.5342 MAE=821.2282 MAE=797.4022 MAE=746.9967 MAE=795.4077 MAE=685.6045 MAE=709.7745 Epoch: 70/500MAE=600.2794 MAE=684.6998 MAE=645.6614 MAE=600.4457 MAE=579.5433 MAE=553.3596 MAE=570.5829 MAE=486.0186 MAE=488.8872 MAE=428.2936 Epoch: 80/500MAE=459.4122 MAE=435.4859 MAE=450.2885 MAE=364.9712 MAE=370.7366 MAE=378.3316 MAE=315.8680 MAE=293.6270 MAE=243.9069 MAE=187.2872 Epoch: 90/500MAE=258.9501 MAE=259.2576 MAE=191.4531 MAE=149.0604 MAE=154.8179 MAE=124.6171 MAE=161.3238 MAE=186.7953 MAE=116.0892 MAE=114.7614 Epoch: 100/500MAE=103.0802 MAE=113.3410 MAE=105.8877 MAE=93.9903 MAE=110.6690 MAE=100.3644 MAE=89.6144 MAE=90.1794 MAE=92.6703 MAE=89.1737 Epoch: 110/500MAE=88.7613 MAE=88.3505 MAE=89.7682 MAE=85.5205 MAE=87.2574 MAE=84.9343 MAE=84.9001 MAE=82.4025 MAE=87.4009 MAE=83.4863 Epoch: 120/500MAE=81.8516 MAE=78.0205 MAE=78.4763 MAE=83.8826 MAE=83.3408 MAE=79.8612 MAE=79.2907 MAE=79.4953 MAE=79.5029 MAE=78.6302 Epoch: 130/500MAE=79.0980 MAE=77.5223 MAE=77.9781 MAE=76.9965 MAE=79.2817 MAE=77.9668 MAE=78.0863 MAE=78.5899 MAE=78.8439 MAE=76.5163 Epoch: 140/500MAE=77.7989 MAE=77.9306 MAE=77.0048 MAE=77.3910 MAE=77.0068 MAE=76.9257 MAE=76.3210 MAE=76.2601 MAE=76.4974 MAE=77.1461 Epoch: 150/500MAE=76.6550 MAE=76.8845 MAE=76.5633 MAE=76.7940 MAE=77.0780 MAE=76.0484 MAE=76.5401 MAE=75.9771 MAE=76.0957 MAE=77.0975 Epoch: 160/500MAE=76.2328 MAE=76.4172 MAE=76.6239 MAE=76.4348 MAE=77.0046 MAE=76.5637 MAE=76.4228 MAE=76.5047 MAE=76.7059 MAE=76.0082 Epoch: 170/500MAE=75.8366 MAE=76.3043 MAE=76.3404 MAE=76.4061 MAE=76.6364 MAE=76.2175 MAE=76.0578 MAE=76.6206 MAE=76.7277 MAE=76.3320 Epoch: 180/500MAE=76.4703 MAE=76.4340 MAE=76.3788 MAE=76.3839 MAE=76.6565 MAE=76.0974 MAE=76.6377 MAE=76.7301 MAE=75.7307 MAE=76.4647 Epoch: 190/500MAE=76.5237 MAE=76.8111 MAE=76.4458 MAE=76.4232 MAE=76.2190 MAE=76.4288 MAE=76.3954 MAE=76.5621 MAE=76.6066 MAE=76.6759 Epoch: 200/500MAE=76.4203 MAE=76.2764 MAE=76.4103 MAE=76.0159 MAE=76.3098 MAE=76.3991 MAE=76.6500 MAE=76.7452 MAE=76.2531 MAE=76.3192 Epoch: 210/500MAE=76.7686 MAE=76.5783 MAE=76.4814 MAE=76.5901 MAE=76.3041 MAE=76.2718 MAE=77.0507 MAE=76.5569 MAE=76.4777 MAE=76.8839 Epoch: 220/500MAE=76.3555 MAE=76.3254 MAE=76.0389 MAE=76.4442 MAE=76.5334 MAE=76.3051 MAE=76.0350 MAE=76.6514 MAE=76.2848 MAE=76.6247 Epoch: 230/500MAE=76.8887 MAE=76.1030 MAE=76.7742 MAE=75.9291 MAE=76.1469 MAE=76.3614 MAE=76.3597 MAE=76.8797 MAE=76.0877 MAE=76.6207 Epoch: 240/500MAE=76.2182 MAE=76.8733 MAE=76.5271 MAE=76.0693 MAE=76.2154 MAE=76.2102 MAE=76.1572 MAE=76.3643 MAE=75.9830 MAE=76.3307 Epoch: 250/500MAE=76.4516 MAE=76.5551 MAE=76.5834 MAE=76.9176 MAE=76.9244 MAE=76.5966 MAE=76.0470 MAE=76.1906 MAE=76.4953 MAE=76.5570 Epoch: 260/500MAE=75.9080 MAE=76.2668 MAE=75.9483 MAE=76.3839 MAE=76.3758 MAE=76.5456 MAE=76.5594 MAE=75.7323 MAE=76.7348 MAE=76.7335 Epoch: 270/500MAE=76.5415 MAE=75.9587 MAE=76.3540 MAE=76.1779 MAE=76.0795 MAE=76.2465 MAE=77.0388 MAE=76.6990 MAE=76.7087 MAE=76.4720 Epoch: 280/500MAE=76.1169 MAE=76.3286 MAE=76.1661 MAE=76.6069 MAE=76.2901 MAE=76.5868 MAE=76.3655 MAE=76.4561 MAE=76.9954 MAE=76.2437 Epoch: 290/500MAE=76.7620 MAE=76.5063 MAE=76.2152 MAE=76.3361 MAE=76.5012 MAE=76.2593 MAE=76.7213 MAE=76.6942 MAE=76.3369 MAE=76.4562 Epoch: 300/500MAE=76.4902 MAE=76.4450 MAE=76.6527 MAE=76.1680 MAE=76.7179 MAE=76.4237 MAE=76.4380 MAE=76.7027 MAE=76.7962 MAE=76.1452 Epoch: 310/500MAE=76.3807 MAE=76.6873 MAE=76.2267 MAE=75.9039 MAE=76.1786 MAE=76.5017 MAE=76.6989 MAE=76.1715 MAE=76.5593 MAE=76.4785 Epoch: 320/500MAE=76.6955 MAE=76.1745 MAE=76.4667 MAE=76.2260 MAE=76.6519 MAE=77.0586 MAE=76.2114 MAE=76.7166 MAE=76.1725 MAE=76.4852 Epoch: 330/500MAE=76.0845 MAE=76.3968 MAE=76.1796 MAE=76.9590 MAE=76.3867 MAE=76.7665 MAE=76.4641 MAE=76.3320 MAE=76.2398 MAE=81.2605 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 84.970 +/- 3.817\n",
      "\n",
      "Epoch: 1/500MAE=1549.4738 MAE=1546.8354 MAE=1544.0476 MAE=1541.0529 MAE=1538.9395 MAE=1536.3966 MAE=1531.8618 MAE=1528.8518 MAE=1523.6233 Epoch: 10/500MAE=1518.4092 MAE=1516.8030 MAE=1511.1606 MAE=1506.2231 MAE=1500.9180 MAE=1492.3964 MAE=1488.2870 MAE=1483.6674 MAE=1471.3810 MAE=1468.7695 Epoch: 20/500MAE=1455.8176 MAE=1452.7848 MAE=1443.6667 MAE=1432.5225 MAE=1426.2855 MAE=1408.2186 MAE=1402.7687 MAE=1390.9215 MAE=1379.9858 MAE=1373.5310 Epoch: 30/500MAE=1362.7682 MAE=1352.5831 MAE=1331.8750 MAE=1324.0161 MAE=1301.9460 MAE=1291.8204 MAE=1288.7928 MAE=1268.1938 MAE=1272.7626 MAE=1248.0839 Epoch: 40/500MAE=1215.3481 MAE=1215.2108 MAE=1180.5095 MAE=1170.6978 MAE=1184.0020 MAE=1156.5280 MAE=1121.0967 MAE=1114.1628 MAE=1100.7152 MAE=1088.3312 Epoch: 50/500MAE=1042.8037 MAE=1046.3684 MAE=1006.3484 MAE=1002.6993 MAE=990.0107 MAE=952.1404 MAE=978.1702 MAE=921.3115 MAE=923.0221 MAE=870.4884 Epoch: 60/500MAE=837.9505 MAE=846.7838 MAE=808.5382 MAE=796.3248 MAE=771.1348 MAE=719.9840 MAE=692.8981 MAE=711.1526 MAE=627.0735 MAE=621.1485 Epoch: 70/500MAE=637.3889 MAE=610.1261 MAE=534.4181 MAE=580.1635 MAE=511.0198 MAE=467.3456 MAE=521.3268 MAE=437.1019 MAE=440.6374 MAE=404.8213 Epoch: 80/500MAE=369.7576 MAE=359.6970 MAE=355.0865 MAE=263.3645 MAE=328.4121 MAE=327.5594 MAE=223.4798 MAE=208.4583 MAE=220.6847 MAE=195.6654 Epoch: 90/500MAE=172.1899 MAE=141.8273 MAE=146.8567 MAE=122.0871 MAE=119.9189 MAE=123.7199 MAE=106.3936 MAE=102.5333 MAE=114.1688 MAE=97.9307 Epoch: 100/500MAE=100.3161 MAE=106.5112 MAE=96.2147 MAE=94.4123 MAE=98.6030 MAE=89.9673 MAE=93.3559 MAE=92.8974 MAE=92.0275 MAE=91.5166 Epoch: 110/500MAE=86.6791 MAE=86.2092 MAE=84.0755 MAE=84.9493 MAE=82.5151 MAE=84.8980 MAE=85.3150 MAE=86.1565 MAE=85.2194 MAE=83.2282 Epoch: 120/500MAE=83.2833 MAE=81.5276 MAE=82.2570 MAE=81.2961 MAE=80.0758 MAE=80.5040 MAE=81.2333 MAE=82.2019 MAE=81.0398 MAE=80.3142 Epoch: 130/500MAE=79.8625 MAE=79.8385 MAE=79.6663 MAE=79.8541 MAE=80.3999 MAE=80.3955 MAE=80.8543 MAE=80.5953 MAE=80.0039 MAE=78.7769 Epoch: 140/500MAE=79.2908 MAE=79.5143 MAE=79.5388 MAE=79.6206 MAE=79.5564 MAE=79.5207 MAE=79.7994 MAE=79.7473 MAE=79.1580 MAE=79.9476 Epoch: 150/500MAE=79.3177 MAE=79.4256 MAE=79.5707 MAE=80.3808 MAE=79.3109 MAE=79.4660 MAE=79.4429 MAE=79.7559 MAE=79.5073 MAE=79.4032 Epoch: 160/500MAE=78.9039 MAE=79.1388 MAE=78.2897 MAE=79.5141 MAE=79.4783 MAE=79.4315 MAE=79.0107 MAE=79.1265 MAE=79.6308 MAE=79.3576 Epoch: 170/500MAE=79.2179 MAE=79.8085 MAE=79.2960 MAE=79.3019 MAE=79.9844 MAE=78.9696 MAE=78.7910 MAE=79.5081 MAE=79.5417 MAE=79.1233 Epoch: 180/500MAE=79.4364 MAE=79.3561 MAE=79.5383 MAE=79.5358 MAE=79.2968 MAE=79.5875 MAE=79.4868 MAE=80.0386 MAE=79.8762 MAE=79.3718 Epoch: 190/500MAE=79.0869 MAE=79.3745 MAE=78.9340 MAE=79.1954 MAE=79.9156 MAE=78.7091 MAE=80.0872 MAE=79.6951 MAE=78.9616 MAE=79.8718 Epoch: 200/500MAE=79.3568 MAE=79.5904 MAE=79.2104 MAE=79.4920 MAE=80.0538 MAE=79.2066 MAE=79.3726 MAE=80.1703 MAE=79.2881 MAE=79.5610 Epoch: 210/500MAE=79.1968 MAE=79.4093 MAE=79.6690 MAE=80.0673 MAE=79.7427 MAE=78.8714 MAE=79.3813 MAE=79.3866 MAE=79.4613 MAE=79.0858 Epoch: 220/500MAE=79.3091 MAE=79.6507 MAE=79.2542 MAE=79.6655 MAE=79.4713 MAE=79.0229 MAE=79.4281 MAE=79.5488 MAE=79.0687 MAE=79.6966 Epoch: 230/500MAE=79.2285 MAE=79.3696 MAE=79.7815 MAE=80.1023 MAE=79.1682 MAE=79.3657 MAE=79.6077 MAE=79.4064 MAE=80.1062 MAE=79.1280 Epoch: 240/500MAE=79.0509 MAE=79.6490 MAE=79.4540 MAE=79.5206 MAE=79.2397 MAE=78.7686 MAE=79.6867 MAE=79.9604 MAE=79.1250 MAE=79.0671 Epoch: 250/500MAE=79.3707 MAE=78.4489 MAE=78.9328 MAE=78.8789 MAE=79.5122 MAE=78.8733 MAE=79.4615 MAE=79.2833 MAE=79.5424 MAE=79.3230 Epoch: 260/500MAE=79.4705 MAE=79.5165 MAE=79.8798 MAE=79.6710 MAE=79.3412 MAE=79.7856 MAE=79.6943 MAE=79.6403 MAE=79.9020 MAE=78.7568 Epoch: 270/500MAE=79.4362 MAE=79.4438 MAE=79.3285 MAE=79.4497 MAE=79.8546 MAE=79.0635 MAE=79.5583 MAE=79.5735 MAE=79.1898 MAE=78.9641 Epoch: 280/500MAE=79.8177 MAE=80.0553 MAE=79.3850 MAE=79.3135 MAE=79.5461 MAE=79.4772 MAE=79.0143 MAE=79.4967 MAE=79.2576 MAE=79.4705 Epoch: 290/500MAE=79.1939 MAE=79.2670 MAE=79.9076 MAE=79.1538 MAE=79.2101 MAE=79.4418 MAE=78.7756 MAE=79.4168 MAE=78.8663 MAE=79.2447 Epoch: 300/500MAE=79.6716 MAE=79.2719 MAE=79.3409 MAE=78.9724 MAE=78.5915 MAE=79.5789 MAE=78.8034 MAE=79.7960 MAE=79.2713 MAE=78.8230 Epoch: 310/500MAE=79.3283 MAE=78.4554 MAE=79.9490 MAE=84.8272 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 84.935 +/- 3.306\n",
      "\n",
      "Epoch: 1/500MAE=1549.4238 MAE=1547.1306 MAE=1544.0715 MAE=1542.1166 MAE=1539.4193 MAE=1536.9598 MAE=1534.0190 MAE=1530.3297 MAE=1524.5750 Epoch: 10/500MAE=1521.2832 MAE=1517.6829 MAE=1512.3439 MAE=1506.6656 MAE=1501.9570 MAE=1495.0872 MAE=1486.1772 MAE=1482.9734 MAE=1468.8372 MAE=1465.7500 Epoch: 20/500MAE=1461.8484 MAE=1446.6584 MAE=1442.6857 MAE=1433.5057 MAE=1427.8970 MAE=1415.3995 MAE=1407.9602 MAE=1397.6384 MAE=1379.9761 MAE=1382.9412 Epoch: 30/500MAE=1368.1060 MAE=1351.8961 MAE=1332.4613 MAE=1333.4240 MAE=1327.1569 MAE=1302.8950 MAE=1298.6389 MAE=1289.8601 MAE=1266.1201 MAE=1243.6060 Epoch: 40/500MAE=1241.1755 MAE=1222.7959 MAE=1213.9329 MAE=1202.3933 MAE=1193.0150 MAE=1158.9181 MAE=1122.0593 MAE=1146.1506 MAE=1112.4457 MAE=1080.6663 Epoch: 50/500MAE=1098.7927 MAE=1067.4025 MAE=1060.3203 MAE=1015.3774 MAE=1015.4625 MAE=987.3909 MAE=975.1200 MAE=944.2746 MAE=929.0358 MAE=907.2324 Epoch: 60/500MAE=899.5487 MAE=850.8461 MAE=864.2852 MAE=824.2859 MAE=804.9009 MAE=780.2510 MAE=758.8986 MAE=720.6953 MAE=709.4615 MAE=717.1780 Epoch: 70/500MAE=646.8597 MAE=657.1018 MAE=609.9801 MAE=532.9055 MAE=549.7468 MAE=569.3175 MAE=518.3607 MAE=528.6325 MAE=435.4138 MAE=457.3542 Epoch: 80/500MAE=519.4459 MAE=396.0894 MAE=390.7235 MAE=362.9767 MAE=383.3729 MAE=285.5029 MAE=260.3675 MAE=265.9463 MAE=313.0807 MAE=271.8412 Epoch: 90/500MAE=245.8959 MAE=215.5804 MAE=158.4706 MAE=164.5117 MAE=173.5613 MAE=152.3836 MAE=147.0156 MAE=143.9687 MAE=122.4746 MAE=118.1047 Epoch: 100/500MAE=126.9691 MAE=119.9367 MAE=112.4727 MAE=116.9879 MAE=104.3294 MAE=93.0768 MAE=90.5233 MAE=99.5269 MAE=103.8147 MAE=106.3916 Epoch: 110/500MAE=85.2814 MAE=87.1876 MAE=84.5013 MAE=91.3751 MAE=84.0546 MAE=85.3858 MAE=92.1750 MAE=82.9561 MAE=84.6408 MAE=88.3707 Epoch: 120/500MAE=85.9420 MAE=87.6959 MAE=85.4000 MAE=83.7208 MAE=84.4668 MAE=81.2626 MAE=84.2992 MAE=91.5536 MAE=79.2281 MAE=84.6239 Epoch: 130/500MAE=78.8788 MAE=78.6980 MAE=83.1097 MAE=86.2236 MAE=81.6076 MAE=79.5735 MAE=78.0712 MAE=77.2982 MAE=76.0165 MAE=77.2154 Epoch: 140/500MAE=77.7152 MAE=80.3285 MAE=77.0982 MAE=76.5842 MAE=76.6486 MAE=75.1336 MAE=75.3982 MAE=75.3384 MAE=76.2835 MAE=75.5192 Epoch: 150/500MAE=76.0399 MAE=75.9458 MAE=75.9325 MAE=76.6886 MAE=75.1139 MAE=75.6707 MAE=75.4341 MAE=75.3263 MAE=75.2242 MAE=75.6766 Epoch: 160/500MAE=75.6165 MAE=75.6894 MAE=75.2482 MAE=75.5984 MAE=75.9541 MAE=75.7937 MAE=75.3085 MAE=75.8129 MAE=75.7206 MAE=75.1293 Epoch: 170/500MAE=75.3849 MAE=75.7526 MAE=76.2500 MAE=75.4602 MAE=75.7557 MAE=75.5874 MAE=75.2068 MAE=75.7168 MAE=75.5232 MAE=75.8758 Epoch: 180/500MAE=75.8145 MAE=75.4557 MAE=75.5033 MAE=75.2902 MAE=75.9474 MAE=75.8372 MAE=75.9547 MAE=75.2110 MAE=76.0495 MAE=75.9792 Epoch: 190/500MAE=75.5902 MAE=76.1410 MAE=75.6181 MAE=75.8760 MAE=75.3228 MAE=75.7017 MAE=75.3341 MAE=75.5647 MAE=75.5728 MAE=75.9779 Epoch: 200/500MAE=75.6121 MAE=75.3490 MAE=75.6719 MAE=75.9660 MAE=75.4344 MAE=75.8650 MAE=75.9577 MAE=75.8594 MAE=75.6688 MAE=76.3416 Epoch: 210/500MAE=75.7589 MAE=75.6189 MAE=75.8811 MAE=76.1648 MAE=75.5708 MAE=75.6298 MAE=75.5699 MAE=75.5211 MAE=75.9223 MAE=76.3378 Epoch: 220/500MAE=76.6407 MAE=75.9283 MAE=75.8373 MAE=75.4063 MAE=75.7764 MAE=75.7031 MAE=75.5876 MAE=75.5466 MAE=76.3985 MAE=75.4303 Epoch: 230/500MAE=75.4177 MAE=75.6484 MAE=75.9930 MAE=76.0800 MAE=74.9779 MAE=75.4181 MAE=75.6672 MAE=75.7416 MAE=75.5619 MAE=75.6353 Epoch: 240/500MAE=75.9238 MAE=75.7054 MAE=75.4999 MAE=75.8355 MAE=75.2806 MAE=75.8223 MAE=75.1881 MAE=75.6772 MAE=75.8951 MAE=75.4141 Epoch: 250/500MAE=75.5076 MAE=75.6115 MAE=75.5866 MAE=75.6528 MAE=76.0403 MAE=75.6978 MAE=75.9115 MAE=75.6303 MAE=75.5244 MAE=76.1211 Epoch: 260/500MAE=75.5872 MAE=76.0235 MAE=76.3140 MAE=76.2851 MAE=75.8857 MAE=75.5214 MAE=75.5135 MAE=75.5933 MAE=75.4723 MAE=75.5993 Epoch: 270/500MAE=75.7855 MAE=75.8086 MAE=75.4343 MAE=75.8006 MAE=75.3828 MAE=75.9461 MAE=75.6832 MAE=75.8155 MAE=75.7934 MAE=75.6042 Epoch: 280/500MAE=75.9369 MAE=75.9893 MAE=75.7449 MAE=75.2649 MAE=75.3340 MAE=75.2389 MAE=75.2435 MAE=76.0124 MAE=75.9067 MAE=75.1936 Epoch: 290/500MAE=75.7313 MAE=75.6243 MAE=75.8052 MAE=75.6340 MAE=75.4014 MAE=75.9305 MAE=76.2992 MAE=75.0956 MAE=75.9077 MAE=75.2695 Epoch: 300/500MAE=76.4852 MAE=75.6604 MAE=75.6556 MAE=75.5312 MAE=75.6893 MAE=74.9472 MAE=76.4153 MAE=75.6413 MAE=75.9041 MAE=75.6760 Epoch: 310/500MAE=75.5436 MAE=75.5886 MAE=75.5981 MAE=75.7364 MAE=75.5672 MAE=75.8582 MAE=75.3715 MAE=76.0904 MAE=75.4935 MAE=75.5979 Epoch: 320/500MAE=75.0823 MAE=75.6839 MAE=75.7773 MAE=75.6697 MAE=75.6418 MAE=75.9199 MAE=75.7982 MAE=75.7268 MAE=75.4307 MAE=74.9637 Epoch: 330/500MAE=75.3984 MAE=75.7222 MAE=75.9124 MAE=75.4830 MAE=75.6564 MAE=75.3574 MAE=75.4492 MAE=76.3003 MAE=75.8927 MAE=75.3275 Epoch: 340/500MAE=76.1127 MAE=75.9414 MAE=75.8269 MAE=75.9172 MAE=75.8387 MAE=75.3580 MAE=76.0185 MAE=75.5120 MAE=75.6674 MAE=75.7938 Epoch: 350/500MAE=75.2372 MAE=75.6969 MAE=76.0170 MAE=75.4418 MAE=75.6315 MAE=75.2758 MAE=76.4546 MAE=75.6121 MAE=75.5860 MAE=75.5492 Epoch: 360/500MAE=76.1753 MAE=75.5168 MAE=76.1122 MAE=75.4485 MAE=75.5915 MAE=76.2392 MAE=75.9485 MAE=75.9047 MAE=75.5805 MAE=75.0943 Epoch: 370/500MAE=75.3419 MAE=75.5773 MAE=75.9453 MAE=75.6295 MAE=75.8916 MAE=75.3265 MAE=75.0217 MAE=75.8780 MAE=75.6942 MAE=75.3435 Epoch: 380/500MAE=75.5348 MAE=75.7737 MAE=75.3607 MAE=75.6533 MAE=75.8735 MAE=75.3010 MAE=75.6196 MAE=75.2950 MAE=75.7543 MAE=75.6428 Epoch: 390/500MAE=76.0475 MAE=75.4875 MAE=75.8917 MAE=75.0841 MAE=75.7494 MAE=75.2676 MAE=75.3999 MAE=75.9099 MAE=75.6496 MAE=74.9003 Epoch: 400/500MAE=75.3314 MAE=75.6701 MAE=75.5576 MAE=75.3106 MAE=75.4902 MAE=75.7611 MAE=75.8800 MAE=75.3523 MAE=75.2739 MAE=75.5505 Epoch: 410/500MAE=75.3455 MAE=75.7488 MAE=75.3773 MAE=75.8185 MAE=75.2969 MAE=75.3010 MAE=75.8485 MAE=75.5761 MAE=75.6266 MAE=75.4336 Epoch: 420/500MAE=75.5178 MAE=75.7176 MAE=75.2305 MAE=75.7294 MAE=75.1238 MAE=75.2683 MAE=75.1321 MAE=75.5114 MAE=75.7603 MAE=75.2008 Epoch: 430/500MAE=75.5777 MAE=75.7631 MAE=75.4389 MAE=75.7435 MAE=75.1879 MAE=75.3506 MAE=75.4532 MAE=75.2447 MAE=75.7029 MAE=75.7811 Epoch: 440/500MAE=75.2887 MAE=75.8428 MAE=75.1294 MAE=75.5435 MAE=75.5018 MAE=75.4606 MAE=76.1289 MAE=75.4882 MAE=75.5604 MAE=75.5179 Epoch: 450/500MAE=75.6379 MAE=75.6329 MAE=76.0535 MAE=75.6826 MAE=75.3802 MAE=74.9951 MAE=75.8586 MAE=75.3048 MAE=75.6930 MAE=74.9121 Epoch: 460/500MAE=75.4529 MAE=75.8614 MAE=75.3123 MAE=75.6506 MAE=75.5658 MAE=75.8032 MAE=75.5668 MAE=75.1190 MAE=75.9156 MAE=75.9791 Epoch: 470/500MAE=75.7575 MAE=75.7681 MAE=74.4102 MAE=75.4944 MAE=75.7976 MAE=76.2113 MAE=76.0192 MAE=75.5401 MAE=76.0394 MAE=75.3927 Epoch: 480/500MAE=75.4183 MAE=75.8772 MAE=75.7297 MAE=75.8725 MAE=75.6626 MAE=75.4781 MAE=75.7242 MAE=75.9778 MAE=75.8745 MAE=75.2859 Epoch: 490/500MAE=75.6379 MAE=75.7146 MAE=75.6835 MAE=75.2345 MAE=75.6942 MAE=75.7038 MAE=75.6386 MAE=75.3224 MAE=75.5637 MAE=75.4921 Epoch: 500/500MAE=75.4114 MAE=81.3793 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 84.224 +/- 3.281\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1548.8251 MAE=1546.9801 MAE=1543.9517 MAE=1541.0996 MAE=1538.7668 MAE=1535.6200 MAE=1533.0560 MAE=1530.4658 MAE=1525.4510 Epoch: 10/500MAE=1523.4530 MAE=1515.2810 MAE=1509.9587 MAE=1506.6423 MAE=1503.0332 MAE=1494.2914 MAE=1491.4229 MAE=1481.7966 MAE=1477.8727 MAE=1468.8977 Epoch: 20/500MAE=1461.6144 MAE=1450.5349 MAE=1445.7855 MAE=1433.6799 MAE=1420.2073 MAE=1417.2322 MAE=1407.0574 MAE=1398.3486 MAE=1383.2183 MAE=1375.4561 Epoch: 30/500MAE=1371.8633 MAE=1354.0261 MAE=1344.1932 MAE=1324.8939 MAE=1325.7310 MAE=1304.9482 MAE=1295.2554 MAE=1285.3037 MAE=1271.2493 MAE=1262.3920 Epoch: 40/500MAE=1238.1118 MAE=1229.9121 MAE=1210.6050 MAE=1202.5731 MAE=1194.0828 MAE=1163.3079 MAE=1142.4685 MAE=1140.6516 MAE=1129.0100 MAE=1116.7717 Epoch: 50/500MAE=1093.0920 MAE=1077.8782 MAE=1042.4446 MAE=1044.7456 MAE=1019.5048 MAE=1013.2266 MAE=988.1489 MAE=962.6838 MAE=940.6708 MAE=930.1146 Epoch: 60/500MAE=919.2107 MAE=881.9562 MAE=866.9476 MAE=835.3178 MAE=827.7430 MAE=800.2468 MAE=802.8557 MAE=768.6199 MAE=740.6836 MAE=680.5837 Epoch: 70/500MAE=702.3325 MAE=680.6084 MAE=660.6503 MAE=650.1736 MAE=639.7700 MAE=605.5134 MAE=573.7278 MAE=552.5045 MAE=496.3617 MAE=477.7962 Epoch: 80/500MAE=501.0898 MAE=449.5256 MAE=491.7758 MAE=384.8835 MAE=403.8147 MAE=348.4152 MAE=303.4692 MAE=341.1602 MAE=288.0837 MAE=309.2251 Epoch: 90/500MAE=275.2531 MAE=214.0706 MAE=252.7537 MAE=209.5998 MAE=187.9000 MAE=216.6622 MAE=215.7128 MAE=171.1021 MAE=171.9810 MAE=147.1653 Epoch: 100/500MAE=133.0511 MAE=136.5588 MAE=128.5867 MAE=140.1582 MAE=118.2291 MAE=115.0258 MAE=110.2728 MAE=110.6412 MAE=108.4447 MAE=105.1449 Epoch: 110/500MAE=107.6527 MAE=115.8359 MAE=112.3436 MAE=106.7117 MAE=99.9773 MAE=108.9614 MAE=103.4670 MAE=114.3828 MAE=106.0009 MAE=101.0617 Epoch: 120/500MAE=98.1489 MAE=98.7764 MAE=97.2928 MAE=97.2809 MAE=97.4341 MAE=97.9506 MAE=97.3092 MAE=98.2015 MAE=97.5106 MAE=95.0343 Epoch: 130/500MAE=96.4457 MAE=96.8214 MAE=97.0149 MAE=95.0000 MAE=95.4426 MAE=94.9851 MAE=95.2680 MAE=96.2558 MAE=95.2381 MAE=96.2891 Epoch: 140/500MAE=95.7732 MAE=95.4898 MAE=94.5104 MAE=96.5924 MAE=96.0005 MAE=95.2630 MAE=95.6507 MAE=94.3245 MAE=95.6613 MAE=95.3783 Epoch: 150/500MAE=95.6041 MAE=94.6200 MAE=94.2946 MAE=94.8523 MAE=94.6891 MAE=95.2773 MAE=95.1534 MAE=95.5281 MAE=95.2469 MAE=95.5050 Epoch: 160/500MAE=95.5119 MAE=95.3627 MAE=94.9136 MAE=94.7493 MAE=95.2244 MAE=94.8789 MAE=94.7029 MAE=94.1081 MAE=95.3755 MAE=95.1759 Epoch: 170/500MAE=94.3260 MAE=94.6636 MAE=95.0914 MAE=94.9389 MAE=94.8907 MAE=95.3559 MAE=94.9248 MAE=95.0951 MAE=94.4448 MAE=95.2152 Epoch: 180/500MAE=94.9829 MAE=95.1678 MAE=94.9044 MAE=94.4806 MAE=94.5057 MAE=94.4143 MAE=94.8121 MAE=94.2985 MAE=94.9647 MAE=94.9692 Epoch: 190/500MAE=95.0304 MAE=95.1094 MAE=95.1407 MAE=95.1121 MAE=94.6666 MAE=94.3030 MAE=94.7775 MAE=94.2551 MAE=94.4382 MAE=95.7095 Epoch: 200/500MAE=94.1410 MAE=95.3136 MAE=94.7690 MAE=94.8446 MAE=95.2133 MAE=95.3637 MAE=94.1757 MAE=94.7967 MAE=95.0259 MAE=94.8460 Epoch: 210/500MAE=95.2989 MAE=95.0254 MAE=94.6522 MAE=94.3330 MAE=94.9872 MAE=95.2833 MAE=94.6645 MAE=94.8493 MAE=95.4901 MAE=95.0909 Epoch: 220/500MAE=95.2710 MAE=95.0054 MAE=94.9185 MAE=94.7700 MAE=94.9407 MAE=95.1671 MAE=94.9903 MAE=95.2883 MAE=94.0926 MAE=94.8101 Epoch: 230/500MAE=95.3057 MAE=95.4241 MAE=94.6947 MAE=94.5501 MAE=94.6094 MAE=95.2566 MAE=95.3496 MAE=95.9198 MAE=95.4945 MAE=94.9154 Epoch: 240/500MAE=95.2267 MAE=94.9536 MAE=95.1059 MAE=95.7809 MAE=94.8099 MAE=95.5477 MAE=95.2139 MAE=95.0169 MAE=95.1268 MAE=95.1942 Epoch: 250/500MAE=95.0300 MAE=95.0687 MAE=95.1486 MAE=94.7209 MAE=95.0106 MAE=95.0542 MAE=94.5838 MAE=95.4289 MAE=94.9125 MAE=95.4335 Epoch: 260/500MAE=94.8213 MAE=94.8138 MAE=95.4363 MAE=94.6674 MAE=95.4449 MAE=93.8765 MAE=94.4082 MAE=95.1243 MAE=95.3042 MAE=94.8575 Epoch: 270/500MAE=94.8078 MAE=94.7087 MAE=94.9812 MAE=95.2824 MAE=95.1502 MAE=95.0866 MAE=94.9620 MAE=94.8211 MAE=95.3651 MAE=95.6273 Epoch: 280/500MAE=94.8052 MAE=94.6033 MAE=94.8301 MAE=94.9509 MAE=94.6619 MAE=94.6114 MAE=95.9307 MAE=94.6365 MAE=94.6704 MAE=95.8772 Epoch: 290/500MAE=94.9593 MAE=95.4595 MAE=94.7412 MAE=95.7017 MAE=94.8105 MAE=94.7121 MAE=94.6387 MAE=94.1785 MAE=95.4421 MAE=95.1295 Epoch: 300/500MAE=95.1956 MAE=95.4715 MAE=94.7584 MAE=94.8196 MAE=95.0179 MAE=94.6793 MAE=95.1435 MAE=94.9548 MAE=94.8301 MAE=94.9292 Epoch: 310/500MAE=95.4425 MAE=95.2421 MAE=94.8501 MAE=95.6760 MAE=94.9092 MAE=95.4076 MAE=94.9619 MAE=95.5390 MAE=95.6603 MAE=94.8355 Epoch: 320/500MAE=95.4965 MAE=94.7155 MAE=95.6270 MAE=95.0533 MAE=94.5793 MAE=95.0012 MAE=94.6936 MAE=95.3365 MAE=95.1786 MAE=94.5497 Epoch: 330/500MAE=95.0924 MAE=95.2179 MAE=94.9011 MAE=94.4272 MAE=95.4810 MAE=95.5734 MAE=95.4384 MAE=94.9075 MAE=95.1116 MAE=94.7407 Epoch: 340/500MAE=95.3315 MAE=95.1986 MAE=94.5328 MAE=94.9082 MAE=94.7653 MAE=95.0141 MAE=95.1281 MAE=95.3371 MAE=94.6427 MAE=95.0585 Epoch: 350/500MAE=95.0174 MAE=94.6287 MAE=95.1740 MAE=94.7056 MAE=95.4877 MAE=95.4850 MAE=95.7447 MAE=94.7433 MAE=94.9868 MAE=95.0617 Epoch: 360/500MAE=94.6945 MAE=95.5549 MAE=95.2748 MAE=94.8338 MAE=95.1975 MAE=94.6987 MAE=94.1085 MAE=94.8559 MAE=94.9709 MAE=95.4022 Epoch: 370/500MAE=95.0289 MAE=95.6570 MAE=94.5099 MAE=95.2565 MAE=95.1607 MAE=95.1849 MAE=95.2235 MAE=94.9846 MAE=95.3291 MAE=94.6678 Epoch: 380/500MAE=95.7965 MAE=94.7151 MAE=95.5726 MAE=95.1017 MAE=94.6219 MAE=95.2302 MAE=95.8623 MAE=95.0909 MAE=94.8107 MAE=94.7765 Epoch: 390/500MAE=95.0197 MAE=95.4637 MAE=95.6965 MAE=94.7235 MAE=95.3038 MAE=94.3969 MAE=95.0113 MAE=94.7714 MAE=93.9511 MAE=94.8895 Epoch: 400/500MAE=94.8080 MAE=95.3648 MAE=94.7741 MAE=95.2922 MAE=94.6856 MAE=95.4267 MAE=95.0392 MAE=95.1269 MAE=94.7917 MAE=95.5365 Epoch: 410/500MAE=95.1732 MAE=94.2837 MAE=95.2694 MAE=95.2527 MAE=94.7268 MAE=95.1278 MAE=96.1972 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 96.197 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1548.6748 MAE=1546.9299 MAE=1544.3774 MAE=1541.2991 MAE=1539.4150 MAE=1536.1482 MAE=1533.0027 MAE=1530.3923 MAE=1525.7119 Epoch: 10/500MAE=1522.3552 MAE=1518.7344 MAE=1511.2338 MAE=1506.6227 MAE=1500.0656 MAE=1494.9446 MAE=1487.8135 MAE=1482.3765 MAE=1477.5662 MAE=1463.6880 Epoch: 20/500MAE=1458.6682 MAE=1451.8615 MAE=1442.1130 MAE=1430.4221 MAE=1423.3325 MAE=1411.3125 MAE=1407.0754 MAE=1393.1406 MAE=1381.3262 MAE=1369.6886 Epoch: 30/500MAE=1364.7876 MAE=1348.9535 MAE=1336.2751 MAE=1309.0564 MAE=1313.0603 MAE=1294.3181 MAE=1281.7341 MAE=1270.4885 MAE=1259.2402 MAE=1236.2823 Epoch: 40/500MAE=1235.6548 MAE=1216.6982 MAE=1198.1451 MAE=1186.8271 MAE=1161.5068 MAE=1168.0247 MAE=1137.5525 MAE=1128.9252 MAE=1105.3389 MAE=1069.1892 Epoch: 50/500MAE=1060.0938 MAE=1052.7317 MAE=1032.5795 MAE=1004.5963 MAE=996.9852 MAE=965.7293 MAE=955.4745 MAE=914.4242 MAE=921.1412 MAE=903.2397 Epoch: 60/500MAE=880.3710 MAE=843.7070 MAE=844.6119 MAE=813.3268 MAE=761.1503 MAE=740.4026 MAE=740.9590 MAE=726.1971 MAE=677.3241 MAE=691.1610 Epoch: 70/500MAE=668.6311 MAE=636.1269 MAE=597.8986 MAE=592.0314 MAE=587.0941 MAE=509.8729 MAE=507.8986 MAE=472.3193 MAE=463.9060 MAE=415.8229 Epoch: 80/500MAE=416.4871 MAE=406.7025 MAE=386.7594 MAE=340.1689 MAE=272.4202 MAE=289.7813 MAE=262.1091 MAE=293.9822 MAE=233.8044 MAE=279.3190 Epoch: 90/500MAE=211.9602 MAE=249.4887 MAE=164.6018 MAE=196.1523 MAE=188.6667 MAE=162.6760 MAE=140.1997 MAE=146.8056 MAE=128.0455 MAE=142.3453 Epoch: 100/500MAE=121.9755 MAE=106.4017 MAE=115.2814 MAE=114.1924 MAE=125.5598 MAE=105.9168 MAE=106.9462 MAE=118.1753 MAE=108.1705 MAE=116.6264 Epoch: 110/500MAE=107.8589 MAE=106.9105 MAE=103.7397 MAE=101.3328 MAE=107.0441 MAE=104.5663 MAE=101.7596 MAE=101.5850 MAE=96.1845 MAE=95.4677 Epoch: 120/500MAE=97.1414 MAE=94.0907 MAE=97.9412 MAE=96.1714 MAE=96.7582 MAE=93.7499 MAE=95.0449 MAE=97.8105 MAE=95.9383 MAE=96.3546 Epoch: 130/500MAE=95.4931 MAE=93.0704 MAE=93.7617 MAE=95.0881 MAE=95.6916 MAE=94.0640 MAE=94.4580 MAE=93.8872 MAE=95.1106 MAE=93.5667 Epoch: 140/500MAE=92.2092 MAE=93.0788 MAE=94.1682 MAE=94.5385 MAE=93.4042 MAE=93.8495 MAE=92.8955 MAE=93.3555 MAE=93.6997 MAE=92.7472 Epoch: 150/500MAE=93.4714 MAE=94.0676 MAE=93.5165 MAE=94.7367 MAE=94.1777 MAE=93.4045 MAE=94.4976 MAE=93.8230 MAE=93.8354 MAE=94.3002 Epoch: 160/500MAE=93.2026 MAE=93.7832 MAE=94.2726 MAE=93.8673 MAE=94.2221 MAE=93.9341 MAE=93.7240 MAE=93.5125 MAE=93.2086 MAE=93.8805 Epoch: 170/500MAE=94.5048 MAE=93.8653 MAE=94.3616 MAE=93.7257 MAE=93.9437 MAE=93.8843 MAE=93.8535 MAE=94.3507 MAE=93.9958 MAE=93.8770 Epoch: 180/500MAE=92.9736 MAE=93.7493 MAE=93.7135 MAE=93.9089 MAE=94.2420 MAE=94.6030 MAE=94.2776 MAE=94.2743 MAE=93.7470 MAE=94.0963 Epoch: 190/500MAE=94.2962 MAE=93.3826 MAE=94.2733 MAE=94.0859 MAE=94.1572 MAE=93.6105 MAE=93.7885 MAE=94.2514 MAE=94.3141 MAE=93.4998 Epoch: 200/500MAE=94.4070 MAE=93.7294 MAE=93.8701 MAE=93.5912 MAE=93.8000 MAE=93.7857 MAE=93.9422 MAE=94.6886 MAE=94.0010 MAE=93.7428 Epoch: 210/500MAE=94.2182 MAE=93.7965 MAE=93.8571 MAE=93.8557 MAE=93.8420 MAE=94.4270 MAE=94.6984 MAE=94.0839 MAE=93.9467 MAE=94.8442 Epoch: 220/500MAE=93.2961 MAE=94.4933 MAE=94.1831 MAE=93.5183 MAE=93.8362 MAE=93.2701 MAE=93.7828 MAE=95.0702 MAE=93.9491 MAE=93.9849 Epoch: 230/500MAE=94.7911 MAE=93.8593 MAE=93.7585 MAE=94.3080 MAE=94.5164 MAE=93.5918 MAE=94.1995 MAE=94.0459 MAE=93.9746 MAE=93.7877 Epoch: 240/500MAE=94.7427 MAE=93.7155 MAE=94.6216 MAE=94.6268 MAE=94.2314 MAE=95.0093 MAE=94.0845 MAE=94.3551 MAE=94.1283 MAE=94.9015 Epoch: 250/500MAE=94.1278 MAE=93.6672 MAE=93.7265 MAE=94.4066 MAE=94.4219 MAE=94.8269 MAE=93.8321 MAE=95.1412 MAE=94.2163 MAE=94.3532 Epoch: 260/500MAE=94.1571 MAE=93.7337 MAE=94.2618 MAE=95.0980 MAE=94.9307 MAE=94.6445 MAE=94.6151 MAE=95.2028 MAE=95.0727 MAE=93.9822 Epoch: 270/500MAE=93.5350 MAE=93.6923 MAE=93.5510 MAE=93.4329 MAE=95.1949 MAE=94.7005 MAE=93.7985 MAE=94.3153 MAE=94.7093 MAE=94.0825 Epoch: 280/500MAE=93.2373 MAE=94.2648 MAE=93.5629 MAE=94.3743 MAE=93.8940 MAE=93.6885 MAE=94.9344 MAE=94.8324 MAE=94.4259 MAE=94.1871 Epoch: 290/500MAE=93.5485 MAE=94.6438 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 95.421 +/- 0.777\n",
      "\n",
      "Epoch: 1/500MAE=1549.5747 MAE=1546.9270 MAE=1544.2567 MAE=1542.3845 MAE=1539.2561 MAE=1535.9478 MAE=1532.4095 MAE=1529.2532 MAE=1525.9269 Epoch: 10/500MAE=1521.6602 MAE=1515.9297 MAE=1514.2407 MAE=1507.7869 MAE=1501.9608 MAE=1497.6138 MAE=1489.3633 MAE=1484.2781 MAE=1478.7255 MAE=1470.8179 Epoch: 20/500MAE=1466.9441 MAE=1454.0001 MAE=1445.3152 MAE=1440.8345 MAE=1433.5618 MAE=1425.5887 MAE=1419.3875 MAE=1412.9216 MAE=1400.3254 MAE=1382.5596 Epoch: 30/500MAE=1378.5416 MAE=1363.5256 MAE=1345.6655 MAE=1347.4785 MAE=1333.3850 MAE=1323.4500 MAE=1294.4478 MAE=1279.0934 MAE=1260.3152 MAE=1254.9690 Epoch: 40/500MAE=1255.7324 MAE=1223.8999 MAE=1212.6357 MAE=1210.1648 MAE=1174.2819 MAE=1184.3519 MAE=1131.5120 MAE=1139.8933 MAE=1114.2693 MAE=1097.2491 Epoch: 50/500MAE=1086.2791 MAE=1055.4114 MAE=1036.3008 MAE=1041.0132 MAE=1011.5356 MAE=999.8166 MAE=985.5998 MAE=948.0368 MAE=951.8650 MAE=913.1496 Epoch: 60/500MAE=894.8278 MAE=864.2148 MAE=872.7919 MAE=845.2816 MAE=808.9155 MAE=789.1017 MAE=783.0102 MAE=751.5120 MAE=741.7123 MAE=698.3331 Epoch: 70/500MAE=685.8511 MAE=667.4949 MAE=696.0477 MAE=626.6406 MAE=408.1860 MAE=521.3737 MAE=527.3632 MAE=502.1295 MAE=424.6370 MAE=482.0217 Epoch: 80/500MAE=477.8628 MAE=482.7213 MAE=475.1554 MAE=442.1364 MAE=434.9250 MAE=455.8083 MAE=418.8118 MAE=430.1675 MAE=412.4096 MAE=420.5485 Epoch: 90/500MAE=410.7361 MAE=413.7049 MAE=404.3220 MAE=415.0132 MAE=410.2989 MAE=405.1080 MAE=402.7496 MAE=400.8178 MAE=401.4197 MAE=402.5840 Epoch: 100/500MAE=398.6475 MAE=398.1630 MAE=402.1031 MAE=399.9247 MAE=392.2300 MAE=394.1565 MAE=392.3011 MAE=391.7921 MAE=390.4348 MAE=388.6912 Epoch: 110/500MAE=388.1656 MAE=384.4482 MAE=384.3550 MAE=383.9413 MAE=382.8271 MAE=385.8798 MAE=379.6811 MAE=382.9856 MAE=379.0975 MAE=373.5616 Epoch: 120/500MAE=378.0238 MAE=375.0673 MAE=369.2177 MAE=379.8422 MAE=372.3349 MAE=368.3951 MAE=373.5343 MAE=368.9442 MAE=368.8867 MAE=362.6984 Epoch: 130/500MAE=365.3142 MAE=362.3597 MAE=358.4754 MAE=364.2526 MAE=363.2172 MAE=356.1859 MAE=361.6649 MAE=356.1869 MAE=355.6652 MAE=361.4633 Epoch: 140/500MAE=354.3058 MAE=352.8312 MAE=347.5988 MAE=350.3626 MAE=353.2374 MAE=347.3859 MAE=352.4798 MAE=344.0507 MAE=346.7990 MAE=348.5760 Epoch: 150/500MAE=344.1978 MAE=337.8662 MAE=344.2461 MAE=340.3308 MAE=340.0612 MAE=335.5505 MAE=334.7972 MAE=333.6943 MAE=338.6229 MAE=337.2188 Epoch: 160/500MAE=334.0652 MAE=329.6127 MAE=329.7685 MAE=321.1698 MAE=324.4311 MAE=328.8369 MAE=318.6150 MAE=328.8163 MAE=322.9155 MAE=318.3287 Epoch: 170/500MAE=317.1130 MAE=318.0336 MAE=321.6165 MAE=319.0025 MAE=317.6065 MAE=313.5516 MAE=319.3053 MAE=313.7497 MAE=311.2164 MAE=314.2219 Epoch: 180/500MAE=312.7595 MAE=311.9843 MAE=314.2487 MAE=310.2443 MAE=310.2805 MAE=309.2805 MAE=310.1371 MAE=310.5553 MAE=308.9848 MAE=310.9008 Epoch: 190/500MAE=309.2336 MAE=309.8835 MAE=308.6599 MAE=308.5106 MAE=308.0719 MAE=308.4152 MAE=307.6778 MAE=309.8025 MAE=306.6058 MAE=306.1016 Epoch: 200/500MAE=308.8847 MAE=307.4156 MAE=305.3383 MAE=304.4651 MAE=306.5936 MAE=307.2553 MAE=306.6424 MAE=304.1354 MAE=303.1776 MAE=302.5264 Epoch: 210/500MAE=303.0392 MAE=302.5653 MAE=303.1935 MAE=304.0708 MAE=302.7832 MAE=302.8820 MAE=303.0435 MAE=302.5470 MAE=302.2609 MAE=302.2143 Epoch: 220/500MAE=302.0314 MAE=301.6432 MAE=303.3944 MAE=302.5300 MAE=301.3730 MAE=301.3188 MAE=301.8436 MAE=302.0273 MAE=302.4834 MAE=302.2364 Epoch: 230/500MAE=300.5418 MAE=301.1017 MAE=302.2695 MAE=302.2117 MAE=301.0709 MAE=301.1706 MAE=302.1165 MAE=302.0092 MAE=300.5364 MAE=301.6853 Epoch: 240/500MAE=300.7182 MAE=301.9316 MAE=300.7842 MAE=301.5709 MAE=301.5293 MAE=301.2303 MAE=301.8498 MAE=301.5462 MAE=301.6368 MAE=301.0699 Epoch: 250/500MAE=301.0327 MAE=302.0313 MAE=301.6866 MAE=300.2950 MAE=302.1453 MAE=300.2771 MAE=301.4842 MAE=301.2362 MAE=300.7150 MAE=301.6404 Epoch: 260/500MAE=300.9770 MAE=300.3977 MAE=301.1512 MAE=300.4675 MAE=300.8975 MAE=300.4666 MAE=301.7006 MAE=301.4306 MAE=300.7818 MAE=300.0712 Epoch: 270/500MAE=301.4134 MAE=301.5588 MAE=301.0228 MAE=300.1496 MAE=302.0253 MAE=300.7797 MAE=301.1986 MAE=300.5333 MAE=302.3024 MAE=301.4503 Epoch: 280/500MAE=301.0824 MAE=301.0081 MAE=301.0163 MAE=300.5978 MAE=300.6829 MAE=299.9795 MAE=301.6645 MAE=299.7609 MAE=300.1901 MAE=300.9473 Epoch: 290/500MAE=301.4366 MAE=300.6253 MAE=300.3104 MAE=301.2064 MAE=300.5494 MAE=299.8038 MAE=299.6728 MAE=300.0566 MAE=300.4986 MAE=300.6008 Epoch: 300/500MAE=299.7618 MAE=299.1542 MAE=300.5695 MAE=300.5511 MAE=300.7609 MAE=300.2052 MAE=299.3669 MAE=299.4800 MAE=300.8866 MAE=299.4415 Epoch: 310/500MAE=300.4567 MAE=299.8246 MAE=300.2537 MAE=301.0219 MAE=300.9515 MAE=299.8325 MAE=300.3810 MAE=299.4257 MAE=300.1608 MAE=300.4882 Epoch: 320/500MAE=300.0908 MAE=300.2079 MAE=301.1056 MAE=300.4998 MAE=299.9638 MAE=300.4276 MAE=299.7947 MAE=300.3385 MAE=299.9253 MAE=299.8435 Epoch: 330/500MAE=299.4223 MAE=299.9597 MAE=299.3307 MAE=299.2943 MAE=300.2960 MAE=299.6252 MAE=298.8167 MAE=300.2990 MAE=300.0013 MAE=300.8315 Epoch: 340/500MAE=299.6576 MAE=299.2910 MAE=299.5943 MAE=298.8872 MAE=299.4358 MAE=299.0506 MAE=299.4061 MAE=299.7420 MAE=299.4781 MAE=299.5242 Epoch: 350/500MAE=298.8984 MAE=299.4392 MAE=300.3280 MAE=300.1471 MAE=299.7166 MAE=300.5192 MAE=298.5620 MAE=300.9214 MAE=299.1421 MAE=300.2089 Epoch: 360/500MAE=299.2156 MAE=298.8843 MAE=299.1214 MAE=300.3300 MAE=301.2032 MAE=299.1286 MAE=299.6687 MAE=299.0701 MAE=299.2614 MAE=299.3695 Epoch: 370/500MAE=299.8080 MAE=299.4058 MAE=299.8504 MAE=299.5582 MAE=298.7684 MAE=299.8564 MAE=299.3632 MAE=299.3955 MAE=299.6752 MAE=299.4186 Epoch: 380/500MAE=299.6166 MAE=300.2398 MAE=299.5284 MAE=298.5250 MAE=299.1930 MAE=298.4688 MAE=299.1863 MAE=300.2007 MAE=299.0684 MAE=299.3345 Epoch: 390/500MAE=299.6450 MAE=299.3781 MAE=299.3678 MAE=300.2754 MAE=299.3911 MAE=300.1481 MAE=299.2715 MAE=298.9850 MAE=297.9625 MAE=298.5462 Epoch: 400/500MAE=298.7160 MAE=299.4407 MAE=299.3366 MAE=299.3263 MAE=298.1776 MAE=298.6718 MAE=298.8339 MAE=300.1791 MAE=298.7472 MAE=298.1903 Epoch: 410/500MAE=298.6340 MAE=299.5904 MAE=298.5968 MAE=298.7072 MAE=298.4184 MAE=299.0261 MAE=299.1172 MAE=299.8649 MAE=299.2704 MAE=299.7844 Epoch: 420/500MAE=299.8102 MAE=299.0928 MAE=298.9370 MAE=298.2791 MAE=299.0881 MAE=298.5152 MAE=298.1682 MAE=297.9577 MAE=299.5141 MAE=298.7058 Epoch: 430/500MAE=297.3827 MAE=298.4290 MAE=298.7844 MAE=297.4094 MAE=299.0577 MAE=299.2860 MAE=298.3251 MAE=298.3205 MAE=298.7678 MAE=298.7968 Epoch: 440/500MAE=298.5745 MAE=298.5912 MAE=298.3082 MAE=298.5419 MAE=298.0601 MAE=298.0833 MAE=298.4296 MAE=298.2523 MAE=298.2720 MAE=298.8708 Epoch: 450/500MAE=298.2632 MAE=297.8356 MAE=298.3401 MAE=299.0713 MAE=298.8410 MAE=297.3531 MAE=298.1114 MAE=298.6395 MAE=297.4056 MAE=297.4700 Epoch: 460/500MAE=298.7264 MAE=298.2658 MAE=297.5854 MAE=298.5100 MAE=298.5670 MAE=298.3251 MAE=297.7508 MAE=298.2340 MAE=298.3583 MAE=298.1035 Epoch: 470/500MAE=297.9616 MAE=298.0800 MAE=297.9030 MAE=298.3961 MAE=297.3676 MAE=298.3055 MAE=297.8433 MAE=296.9565 MAE=297.3285 MAE=297.0277 Epoch: 480/500MAE=297.4993 MAE=296.9167 MAE=297.4637 MAE=297.7231 MAE=297.3988 MAE=297.8019 MAE=297.8049 MAE=297.9617 MAE=297.2921 MAE=297.3719 Epoch: 490/500MAE=298.0150 MAE=297.1971 MAE=298.7551 MAE=298.0675 MAE=298.2143 MAE=297.7473 MAE=298.4641 MAE=297.6493 MAE=298.6898 MAE=297.8593 Epoch: 500/500MAE=297.1622 MAE=307.4386 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 166.093 +/- 99.948\n",
      "\n",
      "Epoch: 1/500MAE=1549.5709 MAE=1547.3191 MAE=1544.2101 MAE=1541.5133 MAE=1538.7743 MAE=1536.1465 MAE=1533.3318 MAE=1529.1963 MAE=1525.1151 Epoch: 10/500MAE=1523.0713 MAE=1518.4789 MAE=1514.6085 MAE=1509.6401 MAE=1500.8236 MAE=1497.3401 MAE=1492.3472 MAE=1486.0732 MAE=1479.2672 MAE=1472.4849 Epoch: 20/500MAE=1463.5715 MAE=1457.3889 MAE=1445.8274 MAE=1441.4578 MAE=1429.9307 MAE=1424.2295 MAE=1413.9321 MAE=1400.6710 MAE=1396.7295 MAE=1384.2834 Epoch: 30/500MAE=1378.6224 MAE=1362.1685 MAE=1350.6453 MAE=1343.4094 MAE=1324.7036 MAE=1312.6964 MAE=1302.3325 MAE=1287.0186 MAE=1265.4673 MAE=1256.1343 Epoch: 40/500MAE=1253.1797 MAE=1233.3230 MAE=1226.8796 MAE=1205.7938 MAE=1194.5710 MAE=1169.5012 MAE=1166.1100 MAE=1156.8438 MAE=1141.4023 MAE=1119.1936 Epoch: 50/500MAE=1102.0172 MAE=1085.1333 MAE=1055.3850 MAE=1044.2938 MAE=1005.1178 MAE=1008.9443 MAE=952.3057 MAE=939.2991 MAE=964.2731 MAE=918.4734 Epoch: 60/500MAE=929.3600 MAE=871.1248 MAE=903.6311 MAE=797.4808 MAE=851.6636 MAE=797.5609 MAE=752.8367 MAE=738.8380 MAE=764.0023 MAE=690.4852 Epoch: 70/500MAE=695.8152 MAE=674.9236 MAE=639.9489 MAE=590.0648 MAE=650.6763 MAE=571.5555 MAE=587.7820 MAE=502.2803 MAE=547.6705 MAE=468.3903 Epoch: 80/500MAE=451.3535 MAE=408.8699 MAE=382.1831 MAE=388.3252 MAE=408.1978 MAE=366.3151 MAE=348.5703 MAE=343.3112 MAE=344.2814 MAE=297.8158 Epoch: 90/500MAE=255.2903 MAE=226.7233 MAE=204.4326 MAE=203.0455 MAE=206.7710 MAE=158.5064 MAE=159.5828 MAE=157.6161 MAE=138.4056 MAE=185.9657 Epoch: 100/500MAE=131.2678 MAE=144.0766 MAE=118.8686 MAE=129.5569 MAE=117.5211 MAE=129.3963 MAE=115.8683 MAE=112.9805 MAE=118.3732 MAE=112.7090 Epoch: 110/500MAE=104.1082 MAE=105.1411 MAE=104.8644 MAE=101.0565 MAE=120.3661 MAE=105.0893 MAE=108.5711 MAE=107.7828 MAE=105.1857 MAE=103.6291 Epoch: 120/500MAE=106.0452 MAE=104.2901 MAE=99.2698 MAE=98.1235 MAE=97.1848 MAE=99.0819 MAE=98.0880 MAE=97.0428 MAE=97.6516 MAE=96.8382 Epoch: 130/500MAE=96.9192 MAE=96.5907 MAE=95.6664 MAE=96.6531 MAE=96.4910 MAE=94.6464 MAE=96.6741 MAE=95.1505 MAE=95.5205 MAE=96.3976 Epoch: 140/500MAE=96.5403 MAE=95.6274 MAE=95.1288 MAE=95.1204 MAE=94.6631 MAE=94.8136 MAE=95.0337 MAE=94.7645 MAE=95.4920 MAE=94.8772 Epoch: 150/500MAE=94.6034 MAE=95.0482 MAE=95.0214 MAE=94.8276 MAE=95.0729 MAE=94.4500 MAE=95.3843 MAE=95.5791 MAE=95.4012 MAE=95.5157 Epoch: 160/500MAE=95.1427 MAE=95.3687 MAE=94.8802 MAE=95.5107 MAE=94.8751 MAE=95.3304 MAE=95.2891 MAE=95.0063 MAE=95.0389 MAE=94.9432 Epoch: 170/500MAE=94.8193 MAE=94.5176 MAE=95.0105 MAE=95.3833 MAE=95.3064 MAE=95.0365 MAE=94.9946 MAE=95.2134 MAE=95.2445 MAE=95.0920 Epoch: 180/500MAE=94.4922 MAE=94.5729 MAE=95.6108 MAE=94.9474 MAE=94.6751 MAE=95.0795 MAE=95.3554 MAE=95.4933 MAE=95.1165 MAE=94.6505 Epoch: 190/500MAE=95.0989 MAE=95.8493 MAE=95.2222 MAE=95.0674 MAE=94.8882 MAE=94.7248 MAE=95.0397 MAE=95.0531 MAE=95.2123 MAE=95.2534 Epoch: 200/500MAE=95.4667 MAE=95.1625 MAE=94.6373 MAE=94.8535 MAE=95.0763 MAE=95.1435 MAE=95.3462 MAE=95.0109 MAE=95.1782 MAE=95.2724 Epoch: 210/500MAE=95.4767 MAE=95.0674 MAE=94.6530 MAE=94.9438 MAE=94.7732 MAE=94.5760 MAE=95.1296 MAE=95.2517 MAE=94.8591 MAE=95.2753 Epoch: 220/500MAE=95.1840 MAE=95.5232 MAE=94.9465 MAE=95.2249 MAE=94.3793 MAE=95.5690 MAE=95.2654 MAE=94.9063 MAE=95.1521 MAE=94.9792 Epoch: 230/500MAE=95.1634 MAE=94.9413 MAE=95.0965 MAE=94.8912 MAE=94.2973 MAE=94.7939 MAE=95.6338 MAE=95.3529 MAE=95.1521 MAE=94.9329 Epoch: 240/500MAE=94.1290 MAE=95.3398 MAE=95.0513 MAE=94.6937 MAE=95.4671 MAE=95.7667 MAE=95.1172 MAE=95.0841 MAE=95.6752 MAE=95.2734 Epoch: 250/500MAE=94.6045 MAE=95.6071 MAE=95.5562 MAE=95.7091 MAE=95.0059 MAE=94.9278 MAE=95.2537 MAE=95.0125 MAE=95.6022 MAE=95.1857 Epoch: 260/500MAE=95.5072 MAE=95.2546 MAE=94.1773 MAE=94.8677 MAE=94.9357 MAE=95.3638 MAE=95.4348 MAE=94.5208 MAE=94.9442 MAE=94.9985 Epoch: 270/500MAE=94.7351 MAE=94.8086 MAE=95.4137 MAE=94.8126 MAE=95.0027 MAE=94.5423 MAE=96.0460 MAE=95.0721 MAE=94.8520 MAE=94.9417 Epoch: 280/500MAE=95.2245 MAE=94.3410 MAE=94.5888 MAE=95.4407 MAE=94.8877 MAE=95.9780 MAE=95.1791 MAE=95.4996 MAE=94.7929 MAE=95.0380 Epoch: 290/500MAE=95.5048 MAE=95.9147 MAE=94.8440 MAE=95.2086 MAE=95.4904 MAE=95.0047 MAE=94.9378 MAE=95.4545 MAE=94.8341 MAE=95.0258 Epoch: 300/500MAE=95.2386 MAE=94.9592 MAE=94.8171 MAE=95.1546 MAE=95.8095 MAE=94.8095 MAE=94.8922 MAE=94.6844 MAE=95.8510 MAE=95.3915 Epoch: 310/500MAE=94.5281 MAE=95.1793 MAE=96.3250 MAE=94.7642 MAE=96.0461 MAE=95.7629 MAE=94.8041 MAE=94.5546 MAE=95.9023 MAE=94.8676 Epoch: 320/500MAE=94.7937 MAE=94.4423 MAE=94.9558 MAE=95.0936 MAE=94.9357 MAE=94.2990 MAE=94.7065 MAE=94.5995 MAE=95.0946 MAE=95.8344 Epoch: 330/500MAE=95.3419 MAE=95.0641 MAE=95.3997 MAE=94.6156 MAE=95.1113 MAE=94.8918 MAE=95.2607 MAE=95.2159 MAE=95.1917 MAE=95.4059 Epoch: 340/500MAE=95.1065 MAE=95.3714 MAE=94.1989 MAE=95.0585 MAE=94.3461 MAE=95.2518 MAE=95.3878 MAE=95.0782 MAE=94.6624 MAE=95.2146 Epoch: 350/500MAE=94.7277 MAE=95.2317 MAE=94.9459 MAE=94.8578 MAE=94.8783 MAE=95.1544 MAE=94.9384 MAE=95.1895 MAE=94.7228 MAE=94.9553 Epoch: 360/500MAE=95.1085 MAE=94.8908 MAE=94.2005 MAE=94.4215 MAE=94.7888 MAE=94.9140 MAE=94.4722 MAE=95.4188 MAE=94.9139 MAE=95.1134 Epoch: 370/500MAE=95.1201 MAE=95.0776 MAE=95.2032 MAE=95.5648 MAE=95.3701 MAE=95.3133 MAE=95.2688 MAE=94.8830 MAE=94.9851 MAE=95.0215 Epoch: 380/500MAE=95.0332 MAE=95.6561 MAE=95.5940 MAE=95.0428 MAE=94.7486 MAE=94.8577 MAE=94.6278 MAE=95.1664 MAE=94.8744 MAE=95.2259 Epoch: 390/500MAE=94.8652 MAE=92.4723 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 147.688 +/- 92.242\n",
      "\n",
      "Epoch: 1/500MAE=1549.1992 MAE=1547.1411 MAE=1544.4666 MAE=1541.6204 MAE=1539.6602 MAE=1537.5541 MAE=1535.0128 MAE=1531.3794 MAE=1529.8260 Epoch: 10/500MAE=1524.9153 MAE=1517.9349 MAE=1515.7692 MAE=1512.5844 MAE=1507.4111 MAE=1495.8768 MAE=1494.2039 MAE=1487.8557 MAE=1480.4271 MAE=1477.3812 Epoch: 20/500MAE=1474.6833 MAE=1464.2360 MAE=1456.1643 MAE=1446.1649 MAE=1434.0723 MAE=1428.3978 MAE=1418.6311 MAE=1409.8768 MAE=1400.2729 MAE=1385.8743 Epoch: 30/500MAE=1382.5258 MAE=1369.5724 MAE=1361.1472 MAE=1350.8147 MAE=1334.1924 MAE=1330.4226 MAE=1311.6379 MAE=1312.9741 MAE=1289.1412 MAE=1279.0295 Epoch: 40/500MAE=1256.3306 MAE=1256.8000 MAE=1224.6372 MAE=1222.2085 MAE=1199.8756 MAE=1182.0032 MAE=1223.1924 MAE=1129.7610 MAE=1148.0908 MAE=1134.4014 Epoch: 50/500MAE=1103.1277 MAE=1102.8396 MAE=1056.3237 MAE=1085.3713 MAE=986.7635 MAE=1046.9092 MAE=1021.0033 MAE=994.8217 MAE=966.4828 MAE=922.8608 Epoch: 60/500MAE=960.7944 MAE=935.8948 MAE=933.2462 MAE=853.3829 MAE=876.5520 MAE=791.7065 MAE=801.2009 MAE=738.9835 MAE=798.2025 MAE=817.2543 Epoch: 70/500MAE=737.3207 MAE=715.2185 MAE=728.7024 MAE=743.2798 MAE=639.6807 MAE=641.1047 MAE=632.6313 MAE=547.0391 MAE=582.7849 MAE=478.2712 Epoch: 80/500MAE=576.6512 MAE=480.7347 MAE=438.4339 MAE=522.9907 MAE=441.3143 MAE=428.5312 MAE=343.5359 MAE=404.8276 MAE=374.8851 MAE=324.8901 Epoch: 90/500MAE=334.5663 MAE=331.6670 MAE=283.1292 MAE=291.0132 MAE=222.0446 MAE=242.6400 MAE=206.8335 MAE=171.7341 MAE=223.3362 MAE=161.4375 Epoch: 100/500MAE=136.3771 MAE=177.2581 MAE=150.0459 MAE=145.7317 MAE=116.1491 MAE=130.5628 MAE=115.4151 MAE=147.8457 MAE=133.6686 MAE=108.8425 Epoch: 110/500MAE=114.8500 MAE=101.2311 MAE=111.3209 MAE=104.7249 MAE=101.9668 MAE=108.6976 MAE=100.1015 MAE=100.2785 MAE=102.4098 MAE=98.5198 Epoch: 120/500MAE=109.8982 MAE=95.6238 MAE=103.5258 MAE=100.3135 MAE=95.5249 MAE=106.8133 MAE=94.1072 MAE=96.5543 MAE=95.2562 MAE=96.7313 Epoch: 130/500MAE=97.4786 MAE=97.0994 MAE=96.9292 MAE=95.4238 MAE=94.5618 MAE=94.4560 MAE=92.6952 MAE=93.7628 MAE=94.8010 MAE=93.7324 Epoch: 140/500MAE=93.0773 MAE=92.3944 MAE=92.9675 MAE=93.0717 MAE=93.1771 MAE=92.5242 MAE=92.8822 MAE=92.7619 MAE=93.3759 MAE=93.5831 Epoch: 150/500MAE=93.0189 MAE=92.9075 MAE=92.7082 MAE=93.5041 MAE=93.3737 MAE=93.1932 MAE=93.6419 MAE=93.2180 MAE=93.8866 MAE=93.0801 Epoch: 160/500MAE=93.0472 MAE=92.5667 MAE=93.3854 MAE=93.6968 MAE=93.6062 MAE=93.2471 MAE=92.9825 MAE=93.1102 MAE=93.1826 MAE=92.9020 Epoch: 170/500MAE=93.5378 MAE=93.0680 MAE=92.9184 MAE=92.7562 MAE=93.5972 MAE=93.2792 MAE=93.0529 MAE=92.5521 MAE=92.8838 MAE=93.2709 Epoch: 180/500MAE=93.2578 MAE=92.9562 MAE=93.1148 MAE=92.4034 MAE=93.2971 MAE=92.0744 MAE=93.4773 MAE=93.0336 MAE=93.5186 MAE=92.5980 Epoch: 190/500MAE=93.8054 MAE=93.4296 MAE=92.5081 MAE=93.6263 MAE=92.1979 MAE=92.7223 MAE=93.5550 MAE=93.4202 MAE=93.7660 MAE=92.3151 Epoch: 200/500MAE=93.6162 MAE=93.3662 MAE=93.1169 MAE=92.2924 MAE=93.2511 MAE=93.7513 MAE=92.8746 MAE=93.2783 MAE=92.5629 MAE=93.2243 Epoch: 210/500MAE=92.1245 MAE=93.1533 MAE=93.5783 MAE=92.3899 MAE=92.8742 MAE=93.1266 MAE=93.4892 MAE=93.3491 MAE=93.7627 MAE=93.0444 Epoch: 220/500MAE=92.9629 MAE=93.6735 MAE=93.4221 MAE=91.8231 MAE=93.2254 MAE=92.5122 MAE=93.2288 MAE=92.6024 MAE=92.8628 MAE=92.9222 Epoch: 230/500MAE=93.6528 MAE=93.2791 MAE=92.7791 MAE=93.7645 MAE=93.0244 MAE=92.9250 MAE=93.3218 MAE=93.2585 MAE=92.5212 MAE=93.3748 Epoch: 240/500MAE=93.1639 MAE=94.5093 MAE=92.0958 MAE=93.0835 MAE=93.1363 MAE=93.2048 MAE=93.4371 MAE=93.3650 MAE=93.3333 MAE=94.2552 Epoch: 250/500MAE=93.9031 MAE=92.6579 MAE=92.3692 MAE=93.4208 MAE=92.7414 MAE=93.0398 MAE=93.1220 MAE=92.9297 MAE=93.4378 MAE=92.8337 Epoch: 260/500MAE=93.2697 MAE=92.7779 MAE=92.9258 MAE=92.7971 MAE=92.8443 MAE=93.4493 MAE=92.6815 MAE=93.4241 MAE=92.8577 MAE=92.3391 Epoch: 270/500MAE=93.2421 MAE=92.1474 MAE=93.6351 MAE=92.9906 MAE=93.0087 MAE=92.9161 MAE=93.3733 MAE=93.4923 MAE=93.1461 MAE=93.0135 Epoch: 280/500MAE=92.8064 MAE=92.8064 MAE=93.0360 MAE=92.3039 MAE=92.7913 MAE=92.7097 MAE=93.3718 MAE=92.5845 MAE=93.0194 MAE=92.5888 Epoch: 290/500MAE=92.6030 MAE=92.5341 MAE=93.4915 MAE=93.0768 MAE=92.5925 MAE=93.0072 MAE=92.3411 MAE=92.8980 MAE=92.5847 MAE=92.7077 Epoch: 300/500MAE=91.6928 MAE=92.7446 MAE=92.8149 MAE=92.9872 MAE=92.4120 MAE=93.4145 MAE=93.1377 MAE=92.5270 MAE=93.1425 MAE=92.8855 Epoch: 310/500MAE=92.8125 MAE=92.2720 MAE=92.4340 MAE=93.0446 MAE=92.5223 MAE=93.5852 MAE=93.2980 MAE=92.4406 MAE=93.3427 MAE=93.0967 Epoch: 320/500MAE=92.5480 MAE=92.7681 MAE=92.9067 MAE=93.1292 MAE=92.6309 MAE=92.7152 MAE=93.2213 MAE=93.3686 MAE=92.7060 MAE=92.9840 Epoch: 330/500MAE=92.8383 MAE=93.3291 MAE=93.5643 MAE=93.4645 MAE=93.7589 MAE=92.7618 MAE=93.5909 MAE=93.0281 MAE=93.4866 MAE=92.8236 Epoch: 340/500MAE=92.5619 MAE=92.5764 MAE=92.7788 MAE=93.5339 MAE=93.0363 MAE=93.1046 MAE=92.6119 MAE=93.3779 MAE=92.3783 MAE=93.9830 Epoch: 350/500MAE=93.2840 MAE=93.2690 MAE=93.3827 MAE=91.8839 MAE=92.4841 MAE=93.2576 MAE=92.3667 MAE=91.9414 MAE=93.3943 MAE=93.7375 Epoch: 360/500MAE=92.3131 MAE=92.5697 MAE=93.4910 MAE=93.1098 MAE=92.7492 MAE=92.9702 MAE=93.1521 MAE=92.7992 MAE=93.5576 MAE=92.6377 Epoch: 370/500MAE=93.7085 MAE=92.5157 MAE=93.0260 MAE=92.8132 MAE=92.5661 MAE=92.7735 MAE=92.7241 MAE=92.1405 MAE=92.7800 MAE=93.0235 Epoch: 380/500MAE=93.3987 MAE=92.6917 MAE=92.6321 MAE=92.9653 MAE=92.8768 MAE=93.1520 MAE=91.4826 MAE=92.1491 MAE=91.8064 MAE=92.5179 Epoch: 390/500MAE=92.8638 MAE=92.6117 MAE=92.7920 MAE=92.3314 MAE=92.8047 MAE=92.6050 MAE=93.1493 MAE=92.6172 MAE=93.3065 MAE=93.3707 Epoch: 400/500MAE=93.4158 MAE=93.3662 MAE=93.8431 MAE=92.5692 MAE=93.7146 MAE=92.7502 MAE=92.4384 MAE=91.9661 MAE=93.9619 MAE=92.8258 Epoch: 410/500MAE=93.2111 MAE=92.9035 MAE=92.4898 MAE=92.9720 MAE=92.6844 MAE=92.9340 MAE=93.0107 MAE=93.3044 MAE=92.7746 MAE=92.4241 Epoch: 420/500MAE=93.3149 MAE=92.8870 MAE=92.5525 MAE=92.3931 MAE=92.7507 MAE=93.0179 MAE=92.5798 MAE=92.6226 MAE=93.6053 MAE=93.0480 Epoch: 430/500MAE=93.4602 MAE=92.6448 MAE=92.9672 MAE=92.6586 MAE=92.0854 MAE=93.2891 MAE=92.5714 MAE=92.8389 MAE=92.8496 MAE=92.4922 Epoch: 440/500MAE=93.1295 MAE=92.6935 MAE=92.5361 MAE=92.7214 MAE=92.4296 MAE=92.6753 MAE=92.2163 MAE=92.4611 MAE=92.4329 MAE=91.6162 Epoch: 450/500MAE=92.5474 MAE=92.5448 MAE=92.5707 MAE=93.4635 MAE=92.5799 MAE=92.7411 MAE=92.6865 MAE=92.7729 MAE=92.2719 MAE=93.0914 Epoch: 460/500MAE=91.8862 MAE=93.3341 MAE=92.7791 MAE=92.7941 MAE=92.1667 MAE=93.0688 MAE=92.3974 MAE=93.1880 MAE=92.7730 MAE=92.8342 Epoch: 470/500MAE=91.8304 MAE=92.3684 MAE=92.6040 MAE=92.6672 MAE=92.1241 MAE=92.3869 MAE=92.4975 MAE=92.9301 MAE=92.2471 MAE=93.5956 Epoch: 480/500MAE=92.2752 MAE=93.0350 MAE=92.1003 MAE=93.4143 MAE=93.4566 MAE=92.8537 MAE=92.6945 MAE=93.0419 MAE=92.6229 MAE=93.0061 Epoch: 490/500MAE=92.1830 MAE=92.4583 MAE=93.6377 MAE=92.0764 MAE=93.6776 MAE=92.4573 MAE=93.3249 MAE=92.0944 MAE=92.4447 MAE=91.8672 Epoch: 500/500MAE=92.2318 MAE=95.8892 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 137.328 +/- 85.065\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/500MAE=1548.9727 MAE=1547.1862 MAE=1543.6399 MAE=1540.8961 MAE=1538.2799 MAE=1534.5806 MAE=1533.0752 MAE=1529.7959 MAE=1522.1990 Epoch: 10/500MAE=1519.5652 MAE=1515.9711 MAE=1507.0695 MAE=1501.6825 MAE=1497.7197 MAE=1491.8582 MAE=1486.1685 MAE=1479.9423 MAE=1477.1411 MAE=1462.8394 Epoch: 20/500MAE=1459.1602 MAE=1450.0426 MAE=1440.6611 MAE=1438.1692 MAE=1420.7515 MAE=1404.4014 MAE=1395.0664 MAE=1396.3130 MAE=1381.1372 MAE=1369.3606 Epoch: 30/500MAE=1352.5286 MAE=1347.2396 MAE=1341.7520 MAE=1304.0546 MAE=1306.8535 MAE=1286.8043 MAE=1280.5883 MAE=1271.7034 MAE=1250.8737 MAE=1247.6499 Epoch: 40/500MAE=1213.0444 MAE=1205.5632 MAE=1178.7913 MAE=1174.0654 MAE=1163.1547 MAE=1141.7308 MAE=1141.1497 MAE=1123.5363 MAE=1104.2822 MAE=1079.9463 Epoch: 50/500MAE=1039.4050 MAE=1029.6809 MAE=1008.8517 MAE=984.8026 MAE=1011.7930 MAE=909.3760 MAE=927.4724 MAE=909.2264 MAE=883.6918 MAE=867.2876 Epoch: 60/500MAE=840.0183 MAE=815.8081 MAE=830.9658 MAE=786.2628 MAE=774.9474 MAE=707.7489 MAE=763.8215 MAE=720.2610 MAE=665.5366 MAE=641.0891 Epoch: 70/500MAE=628.0367 MAE=605.4022 MAE=559.3157 MAE=545.8184 MAE=566.5684 MAE=457.7439 MAE=533.6718 MAE=468.7845 MAE=452.6600 MAE=395.4473 Epoch: 80/500MAE=398.4624 MAE=415.7382 MAE=354.0265 MAE=359.5497 MAE=321.5098 MAE=241.0694 MAE=253.5252 MAE=256.1213 MAE=197.6168 MAE=155.1320 Epoch: 90/500MAE=152.8848 MAE=182.4882 MAE=140.0188 MAE=145.1490 MAE=161.5871 MAE=150.0527 MAE=116.7947 MAE=124.1999 MAE=131.4189 MAE=98.3006 Epoch: 100/500MAE=100.5598 MAE=119.2816 MAE=90.1964 MAE=83.3408 MAE=105.5554 MAE=89.8103 MAE=76.1887 MAE=87.7421 MAE=75.0308 MAE=76.2514 Epoch: 110/500MAE=74.0200 MAE=83.9731 MAE=74.1864 MAE=79.9466 MAE=82.6669 MAE=70.9005 MAE=65.9616 MAE=69.9628 MAE=70.2657 MAE=67.5736 Epoch: 120/500MAE=66.8386 MAE=63.8094 MAE=65.1069 MAE=67.7583 MAE=62.3983 MAE=64.1871 MAE=66.3629 MAE=63.7532 MAE=64.6857 MAE=63.6218 Epoch: 130/500MAE=63.9014 MAE=62.4167 MAE=61.8000 MAE=65.2090 MAE=62.8529 MAE=62.3593 MAE=62.0812 MAE=61.6550 MAE=61.5213 MAE=61.9215 Epoch: 140/500MAE=61.6718 MAE=61.9699 MAE=61.6456 MAE=61.8725 MAE=61.4859 MAE=61.7401 MAE=61.3579 MAE=61.9882 MAE=61.5274 MAE=61.6725 Epoch: 150/500MAE=61.8005 MAE=61.5301 MAE=61.5314 MAE=61.2715 MAE=61.0223 MAE=60.9395 MAE=61.2776 MAE=61.4904 MAE=61.6430 MAE=61.5132 Epoch: 160/500MAE=61.5349 MAE=61.5479 MAE=61.6011 MAE=61.5043 MAE=61.4893 MAE=61.5397 MAE=61.4298 MAE=61.4362 MAE=61.3925 MAE=61.4182 Epoch: 170/500MAE=61.4446 MAE=61.4546 MAE=61.4633 MAE=61.4949 MAE=61.4930 MAE=61.5095 MAE=61.5113 MAE=61.4941 MAE=61.5074 MAE=61.5011 Epoch: 180/500MAE=61.4931 MAE=61.4919 MAE=61.5051 MAE=61.4964 MAE=61.4594 MAE=61.4961 MAE=61.4970 MAE=61.4655 MAE=61.4746 MAE=61.5150 Epoch: 190/500MAE=61.5184 MAE=61.5247 MAE=61.5282 MAE=61.5388 MAE=61.5660 MAE=61.5652 MAE=61.5391 MAE=61.5278 MAE=61.5133 MAE=61.5034 Epoch: 200/500MAE=61.5041 MAE=61.5031 MAE=61.5060 MAE=61.5050 MAE=61.4918 MAE=61.5104 MAE=61.4944 MAE=61.4962 MAE=61.4904 MAE=61.5070 Epoch: 210/500MAE=61.4762 MAE=61.4740 MAE=61.4856 MAE=61.4866 MAE=61.4804 MAE=61.4444 MAE=61.4827 MAE=61.4601 MAE=61.4856 MAE=61.5030 Epoch: 220/500MAE=61.4905 MAE=61.4849 MAE=61.4983 MAE=61.5199 MAE=61.4813 MAE=61.4923 MAE=61.4846 MAE=61.4902 MAE=61.4987 MAE=61.4877 Epoch: 230/500MAE=61.5029 MAE=61.5133 MAE=61.5131 MAE=61.5151 MAE=61.5354 MAE=61.5311 MAE=61.5275 MAE=61.5238 MAE=61.5210 MAE=61.5230 Epoch: 240/500MAE=61.5343 MAE=61.5270 MAE=61.5180 MAE=61.5246 MAE=61.5447 MAE=61.4819 MAE=61.5115 MAE=61.5159 MAE=61.5088 MAE=61.5465 Epoch: 250/500MAE=61.5573 MAE=61.5542 MAE=61.5121 MAE=61.5242 MAE=61.5320 MAE=61.5323 MAE=61.5394 MAE=61.5421 MAE=61.5307 MAE=61.5316 Epoch: 260/500MAE=61.5356 MAE=61.5439 MAE=61.5226 MAE=61.5209 MAE=61.5262 MAE=61.5259 MAE=61.5152 MAE=61.5078 MAE=61.5144 MAE=61.5115 Epoch: 270/500MAE=61.5162 MAE=61.5274 MAE=61.4967 MAE=61.4994 MAE=61.4943 MAE=61.4934 MAE=61.4868 MAE=61.4995 MAE=61.4895 MAE=61.4764 Epoch: 280/500MAE=61.4395 MAE=61.4795 MAE=61.4667 MAE=61.4266 MAE=61.4704 MAE=61.4355 MAE=61.4345 MAE=61.4794 MAE=61.4771 MAE=61.4789 Epoch: 290/500MAE=61.4784 MAE=61.4529 MAE=61.4488 MAE=61.4392 MAE=61.4259 MAE=61.4370 MAE=61.4351 MAE=61.4164 MAE=61.4321 MAE=61.4648 Epoch: 300/500MAE=61.4608 MAE=61.4418 MAE=61.4611 MAE=61.4688 MAE=61.4660 MAE=61.4398 MAE=64.6040 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 64.604 +/- 0.000\n",
      "\n",
      "Epoch: 1/500MAE=1549.4099 MAE=1547.3988 MAE=1544.8834 MAE=1541.9303 MAE=1539.0323 MAE=1536.0825 MAE=1531.6965 MAE=1528.3882 MAE=1525.2716 Epoch: 10/500MAE=1520.9271 MAE=1517.3524 MAE=1513.2211 MAE=1507.3489 MAE=1504.1692 MAE=1495.5156 MAE=1488.1508 MAE=1485.2444 MAE=1476.6827 MAE=1474.8484 Epoch: 20/500MAE=1465.8660 MAE=1451.2134 MAE=1449.9238 MAE=1434.5963 MAE=1432.4192 MAE=1429.2212 MAE=1414.6080 MAE=1410.8690 MAE=1396.7905 MAE=1381.5237 Epoch: 30/500MAE=1373.6450 MAE=1358.6487 MAE=1361.2747 MAE=1338.9160 MAE=1341.1907 MAE=1316.4493 MAE=1294.2390 MAE=1298.6586 MAE=1258.0298 MAE=1261.2191 Epoch: 40/500MAE=1244.4397 MAE=1235.4091 MAE=1216.8365 MAE=1209.2234 MAE=1196.9781 MAE=1150.3569 MAE=1155.5620 MAE=1132.5708 MAE=1127.8557 MAE=1117.3772 Epoch: 50/500MAE=1090.7866 MAE=1082.4163 MAE=1076.1807 MAE=1046.2688 MAE=1030.0486 MAE=978.9464 MAE=1009.4269 MAE=969.8530 MAE=960.7662 MAE=952.1094 Epoch: 60/500MAE=917.5594 MAE=893.8264 MAE=912.7378 MAE=868.9277 MAE=827.5358 MAE=807.7374 MAE=774.0703 MAE=658.4624 MAE=657.2232 MAE=629.2644 Epoch: 70/500MAE=675.9058 MAE=637.1342 MAE=555.3900 MAE=548.8332 MAE=607.5573 MAE=601.2851 MAE=533.9646 MAE=435.8873 MAE=440.7195 MAE=424.2152 Epoch: 80/500MAE=430.9290 MAE=391.2394 MAE=407.9330 MAE=325.1170 MAE=381.5551 MAE=346.7168 MAE=339.2166 MAE=307.0844 MAE=317.7651 MAE=238.6209 Epoch: 90/500MAE=258.1185 MAE=225.0039 MAE=203.8991 MAE=198.1035 MAE=180.5908 MAE=170.1840 MAE=129.6271 MAE=140.6098 MAE=113.9319 MAE=129.9081 Epoch: 100/500MAE=129.6786 MAE=96.6205 MAE=95.7254 MAE=86.5273 MAE=78.0422 MAE=115.9400 MAE=86.4411 MAE=85.7889 MAE=88.3068 MAE=86.7495 Epoch: 110/500MAE=67.4600 MAE=67.1587 MAE=67.8875 MAE=67.5819 MAE=71.0243 MAE=65.0866 MAE=62.3478 MAE=62.0598 MAE=63.0271 MAE=70.0290 Epoch: 120/500MAE=76.1383 MAE=62.8918 MAE=67.7881 MAE=66.8516 MAE=75.3994 MAE=63.1601 MAE=62.3986 MAE=61.8244 MAE=61.0481 MAE=61.8861 Epoch: 130/500MAE=59.7839 MAE=62.0585 MAE=61.0381 MAE=61.3955 MAE=62.0256 MAE=59.9637 MAE=60.0668 MAE=61.6109 MAE=59.8092 MAE=59.9176 Epoch: 140/500MAE=60.0597 MAE=59.8262 MAE=59.5856 MAE=59.6635 MAE=59.1377 MAE=59.5854 MAE=60.0673 MAE=59.6947 MAE=59.3157 MAE=59.4418 Epoch: 150/500MAE=59.2523 MAE=59.6652 MAE=59.7376 MAE=59.6462 MAE=59.7178 MAE=59.5532 MAE=59.5174 MAE=59.4560 MAE=59.5035 MAE=59.6057 Epoch: 160/500MAE=59.5807 MAE=59.5429 MAE=59.5719 MAE=59.5358 MAE=59.5358 MAE=59.5274 MAE=59.5160 MAE=59.4978 MAE=59.5179 MAE=59.4760 Epoch: 170/500MAE=59.4598 MAE=59.4393 MAE=59.4424 MAE=59.4960 MAE=59.4574 MAE=59.4394 MAE=59.4736 MAE=59.4604 MAE=59.4399 MAE=59.4599 Epoch: 180/500MAE=59.5088 MAE=59.5247 MAE=59.5356 MAE=59.4767 MAE=59.4974 MAE=59.4478 MAE=59.4553 MAE=59.5264 MAE=59.5221 MAE=59.5231 Epoch: 190/500MAE=59.4883 MAE=59.4405 MAE=59.4935 MAE=59.5092 MAE=59.4313 MAE=59.4987 MAE=59.5024 MAE=59.4435 MAE=59.4882 MAE=59.5132 Epoch: 200/500MAE=59.4956 MAE=59.5036 MAE=59.4725 MAE=59.3877 MAE=59.3948 MAE=59.4010 MAE=59.4029 MAE=59.4559 MAE=59.4191 MAE=59.4099 Epoch: 210/500MAE=59.4429 MAE=59.4397 MAE=59.4528 MAE=59.4215 MAE=59.4446 MAE=59.4459 MAE=59.4330 MAE=59.4375 MAE=59.4574 MAE=59.4359 Epoch: 220/500MAE=59.4306 MAE=59.4505 MAE=59.4298 MAE=59.4402 MAE=59.4463 MAE=59.4112 MAE=59.4225 MAE=59.4443 MAE=59.4286 MAE=59.4084 Epoch: 230/500MAE=59.4304 MAE=59.4168 MAE=59.3749 MAE=59.3797 MAE=59.3462 MAE=59.2511 MAE=59.3360 MAE=59.3699 MAE=59.4079 MAE=59.3507 Epoch: 240/500MAE=59.2724 MAE=59.2708 MAE=59.3398 MAE=59.3428 MAE=59.2832 MAE=59.2720 MAE=59.2668 MAE=59.2970 MAE=59.2837 MAE=59.3058 Epoch: 250/500MAE=59.2935 MAE=59.3077 MAE=59.2963 MAE=59.3167 MAE=59.2870 MAE=59.3055 MAE=59.3353 MAE=59.3220 MAE=59.2802 MAE=59.3046 Epoch: 260/500MAE=59.2574 MAE=59.2924 MAE=59.2645 MAE=59.2926 MAE=59.2591 MAE=59.2415 MAE=59.2463 MAE=59.2258 MAE=59.2729 MAE=59.2665 Epoch: 270/500MAE=59.3153 MAE=59.3324 MAE=59.2607 MAE=59.2683 MAE=59.2557 MAE=59.3331 MAE=59.3036 MAE=59.3264 MAE=59.3142 MAE=59.3077 Epoch: 280/500MAE=59.3280 MAE=59.3314 MAE=59.3190 MAE=59.2979 MAE=59.3014 MAE=59.3229 MAE=59.2864 MAE=59.2889 MAE=59.2904 MAE=59.3217 Epoch: 290/500MAE=59.2687 MAE=59.2623 MAE=59.2795 MAE=59.2303 MAE=59.2639 MAE=59.9674 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 62.286 +/- 2.318\n",
      "\n",
      "Epoch: 1/500MAE=1549.7324 MAE=1547.2695 MAE=1544.2681 MAE=1542.1785 MAE=1538.1672 MAE=1537.2051 MAE=1534.4272 MAE=1530.4385 MAE=1526.3011 Epoch: 10/500MAE=1523.8119 MAE=1517.2690 MAE=1510.2952 MAE=1502.0764 MAE=1504.1423 MAE=1491.3500 MAE=1490.2954 MAE=1481.3944 MAE=1475.4702 MAE=1466.3022 Epoch: 20/500MAE=1459.0586 MAE=1451.7841 MAE=1438.2391 MAE=1441.0449 MAE=1423.9495 MAE=1407.5771 MAE=1413.0088 MAE=1390.2253 MAE=1390.4314 MAE=1376.4812 Epoch: 30/500MAE=1359.3383 MAE=1359.0527 MAE=1321.8440 MAE=1323.3544 MAE=1318.1934 MAE=1291.2629 MAE=1258.9097 MAE=1279.5919 MAE=1261.5662 MAE=1218.0836 Epoch: 40/500MAE=1232.9568 MAE=1212.0334 MAE=1183.8954 MAE=1178.3241 MAE=1187.0447 MAE=1144.9683 MAE=1141.5952 MAE=1125.1849 MAE=1097.5007 MAE=1060.1982 Epoch: 50/500MAE=1065.8477 MAE=1039.2031 MAE=972.3374 MAE=982.0289 MAE=1016.3838 MAE=965.3491 MAE=966.7306 MAE=923.8194 MAE=928.6391 MAE=897.5928 Epoch: 60/500MAE=833.6307 MAE=790.6011 MAE=810.9177 MAE=759.1572 MAE=675.7307 MAE=715.8024 MAE=710.1739 MAE=688.2468 MAE=698.3912 MAE=616.1873 Epoch: 70/500MAE=625.1658 MAE=638.1857 MAE=595.6635 MAE=581.8387 MAE=561.9248 MAE=592.4541 MAE=554.2283 MAE=557.1729 MAE=558.9163 MAE=550.6358 Epoch: 80/500MAE=556.5306 MAE=477.3765 MAE=528.3209 MAE=518.6566 MAE=495.4286 MAE=467.8497 MAE=441.2250 MAE=453.7488 MAE=438.7130 MAE=404.7111 Epoch: 90/500MAE=428.1718 MAE=407.5004 MAE=361.6968 MAE=354.0388 MAE=339.0643 MAE=331.0211 MAE=366.8961 MAE=301.2908 MAE=304.2951 MAE=342.9022 Epoch: 100/500MAE=348.8737 MAE=306.1631 MAE=297.1992 MAE=286.3187 MAE=299.8519 MAE=310.7940 MAE=279.2733 MAE=280.7101 MAE=271.8185 MAE=270.7489 Epoch: 110/500MAE=270.5250 MAE=264.2114 MAE=267.8715 MAE=260.8240 MAE=238.1281 MAE=238.4890 MAE=234.5630 MAE=235.7432 MAE=207.6865 MAE=229.1637 Epoch: 120/500MAE=218.5595 MAE=217.9102 MAE=204.1540 MAE=208.0365 MAE=224.8231 MAE=175.6853 MAE=202.4018 MAE=192.6847 MAE=195.8072 MAE=168.5006 Epoch: 130/500MAE=184.6295 MAE=194.9265 MAE=161.0002 MAE=159.1055 MAE=166.6968 MAE=162.3234 MAE=155.0476 MAE=139.1274 MAE=160.0264 MAE=157.3371 Epoch: 140/500MAE=149.6090 MAE=131.2732 MAE=131.5185 MAE=130.1830 MAE=121.7772 MAE=132.3482 MAE=131.5039 MAE=124.2492 MAE=105.5953 MAE=118.3338 Epoch: 150/500MAE=107.1318 MAE=131.5888 MAE=104.2148 MAE=105.2020 MAE=106.8466 MAE=95.9961 MAE=98.1660 MAE=101.1498 MAE=93.5949 MAE=97.3860 Epoch: 160/500MAE=97.5521 MAE=88.4438 MAE=87.9636 MAE=91.5374 MAE=92.8268 MAE=86.4391 MAE=89.3225 MAE=90.0927 MAE=92.3621 MAE=85.7235 Epoch: 170/500MAE=82.8030 MAE=86.7536 MAE=86.1729 MAE=88.8433 MAE=84.1284 MAE=82.9214 MAE=83.1282 MAE=81.9194 MAE=79.3848 MAE=82.4734 Epoch: 180/500MAE=80.9435 MAE=83.0012 MAE=78.7599 MAE=80.0324 MAE=81.3767 MAE=76.4763 MAE=80.6528 MAE=78.4632 MAE=78.0226 MAE=77.3540 Epoch: 190/500MAE=79.0551 MAE=74.6798 MAE=77.4932 MAE=75.2074 MAE=76.3039 MAE=75.5636 MAE=75.4109 MAE=76.5433 MAE=75.7365 MAE=75.5836 Epoch: 200/500MAE=75.2891 MAE=74.8989 MAE=75.2059 MAE=75.2068 MAE=75.2977 MAE=75.2232 MAE=75.3890 MAE=75.2534 MAE=75.1148 MAE=75.0267 Epoch: 210/500MAE=75.1056 MAE=75.1464 MAE=75.0872 MAE=75.0635 MAE=75.0759 MAE=75.1250 MAE=75.1377 MAE=75.1288 MAE=75.1543 MAE=75.1530 Epoch: 220/500MAE=75.1315 MAE=75.1785 MAE=75.0675 MAE=75.1901 MAE=75.1298 MAE=75.0851 MAE=75.1935 MAE=75.1174 MAE=75.0785 MAE=75.0326 Epoch: 230/500MAE=75.0365 MAE=75.0494 MAE=75.0398 MAE=75.0205 MAE=75.0400 MAE=75.0046 MAE=75.1138 MAE=75.1816 MAE=75.1422 MAE=75.0847 Epoch: 240/500MAE=75.0663 MAE=75.0611 MAE=75.0695 MAE=75.1365 MAE=75.0219 MAE=75.1040 MAE=75.1057 MAE=75.1003 MAE=75.0490 MAE=75.0611 Epoch: 250/500MAE=75.0822 MAE=75.0530 MAE=75.0076 MAE=75.0484 MAE=75.0462 MAE=75.0221 MAE=75.1346 MAE=75.0344 MAE=75.0021 MAE=75.0375 Epoch: 260/500MAE=75.1021 MAE=75.0541 MAE=75.0156 MAE=75.0335 MAE=75.1161 MAE=75.1251 MAE=75.0181 MAE=75.0581 MAE=75.0858 MAE=75.0257 Epoch: 270/500MAE=75.0252 MAE=75.0257 MAE=75.0725 MAE=75.0230 MAE=74.9845 MAE=74.9754 MAE=75.0046 MAE=74.9976 MAE=74.9289 MAE=75.0123 Epoch: 280/500MAE=75.0870 MAE=74.9880 MAE=75.0148 MAE=75.0251 MAE=75.0145 MAE=75.0502 MAE=75.0108 MAE=74.9838 MAE=75.0204 MAE=74.9490 Epoch: 290/500MAE=74.9877 MAE=74.9781 MAE=74.9395 MAE=74.9287 MAE=74.9953 MAE=74.9764 MAE=74.9405 MAE=74.9347 MAE=74.9676 MAE=74.9645 Epoch: 300/500MAE=74.9758 MAE=75.0136 MAE=74.9655 MAE=74.9762 MAE=74.9571 MAE=74.9919 MAE=74.9751 MAE=74.9482 MAE=75.0300 MAE=74.9374 Epoch: 310/500MAE=74.9417 MAE=74.9752 MAE=74.9426 MAE=74.9561 MAE=74.9651 MAE=75.0046 MAE=74.8827 MAE=74.9104 MAE=74.9487 MAE=74.9235 Epoch: 320/500MAE=74.9332 MAE=74.9900 MAE=74.9148 MAE=74.9173 MAE=74.9013 MAE=74.9305 MAE=74.9124 MAE=74.9084 MAE=74.8884 MAE=74.9585 Epoch: 330/500MAE=74.9254 MAE=74.9134 MAE=74.9590 MAE=74.9108 MAE=74.9224 MAE=74.9951 MAE=74.9424 MAE=74.9461 MAE=74.9400 MAE=74.9269 Epoch: 340/500MAE=74.9168 MAE=74.8357 MAE=82.2890 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 68.953 +/- 9.618\n",
      "\n",
      "Epoch: 1/500MAE=1548.8213 MAE=1547.2610 MAE=1544.8236 MAE=1542.2305 MAE=1538.8026 MAE=1536.2095 MAE=1531.7874 MAE=1529.0010 MAE=1524.8632 Epoch: 10/500MAE=1519.1179 MAE=1518.2080 MAE=1511.6676 MAE=1506.0608 MAE=1500.8259 MAE=1494.4774 MAE=1487.7926 MAE=1481.8955 MAE=1477.6370 MAE=1470.1945 Epoch: 20/500MAE=1464.5718 MAE=1449.8562 MAE=1449.1920 MAE=1435.3159 MAE=1425.6670 MAE=1416.8246 MAE=1410.5211 MAE=1394.5991 MAE=1384.0292 MAE=1359.2007 Epoch: 30/500MAE=1365.1104 MAE=1353.1782 MAE=1340.8416 MAE=1327.2891 MAE=1315.2197 MAE=1310.6519 MAE=1272.8579 MAE=1267.5175 MAE=1259.1813 MAE=1230.4863 Epoch: 40/500MAE=1196.0719 MAE=1202.5570 MAE=1192.3955 MAE=1156.3267 MAE=1172.5266 MAE=1157.6094 MAE=1150.4934 MAE=1104.0293 MAE=1100.1981 MAE=1102.9395 Epoch: 50/500MAE=1066.9093 MAE=1056.5620 MAE=1039.5427 MAE=996.4534 MAE=1002.7566 MAE=974.1986 MAE=930.0890 MAE=940.9792 MAE=899.3295 MAE=879.7179 Epoch: 60/500MAE=876.7701 MAE=833.0320 MAE=788.1106 MAE=841.1221 MAE=794.7154 MAE=724.3939 MAE=794.3939 MAE=684.7714 MAE=698.6227 MAE=624.8904 Epoch: 70/500MAE=672.3251 MAE=636.0625 MAE=605.3311 MAE=587.1517 MAE=583.6235 MAE=544.7239 MAE=500.3857 MAE=519.4871 MAE=387.4753 MAE=478.7960 Epoch: 80/500MAE=501.1942 MAE=452.7696 MAE=427.4954 MAE=400.8975 MAE=328.2538 MAE=364.7571 MAE=331.2130 MAE=324.1484 MAE=339.9344 MAE=315.2785 Epoch: 90/500MAE=310.1771 MAE=278.0567 MAE=292.3209 MAE=290.1218 MAE=277.0990 MAE=243.3715 MAE=253.6850 MAE=259.4972 MAE=242.7749 MAE=248.4515 Epoch: 100/500MAE=246.8939 MAE=186.5355 MAE=213.2137 MAE=220.4959 MAE=253.5313 MAE=208.6852 MAE=192.0189 MAE=165.9484 MAE=173.4331 MAE=176.1971 Epoch: 110/500MAE=149.8711 MAE=148.3213 MAE=175.8366 MAE=166.8826 MAE=131.4075 MAE=145.2869 MAE=129.6620 MAE=131.7777 MAE=133.4047 MAE=130.2614 Epoch: 120/500MAE=149.8854 MAE=125.1304 MAE=119.3761 MAE=125.1280 MAE=111.9391 MAE=134.3732 MAE=108.1762 MAE=110.9634 MAE=113.3391 MAE=122.8703 Epoch: 130/500MAE=111.1001 MAE=114.8353 MAE=109.8846 MAE=106.6675 MAE=108.4298 MAE=110.2553 MAE=107.2513 MAE=108.3058 MAE=106.7483 MAE=106.1328 Epoch: 140/500MAE=105.4988 MAE=108.0586 MAE=104.3986 MAE=105.2684 MAE=102.4198 MAE=104.2135 MAE=104.1876 MAE=103.7507 MAE=101.3953 MAE=102.5256 Epoch: 150/500MAE=106.1003 MAE=102.7020 MAE=104.1229 MAE=102.1278 MAE=101.3444 MAE=100.4754 MAE=100.4527 MAE=99.3632 MAE=101.3148 MAE=100.6713 Epoch: 160/500MAE=100.4295 MAE=99.9371 MAE=100.3484 MAE=100.4965 MAE=100.6626 MAE=100.1174 MAE=99.7955 MAE=99.9282 MAE=100.0035 MAE=99.6131 Epoch: 170/500MAE=99.5534 MAE=99.6169 MAE=99.3939 MAE=99.5137 MAE=99.5601 MAE=99.6439 MAE=99.6988 MAE=99.6025 MAE=99.6042 MAE=99.5716 Epoch: 180/500MAE=99.4481 MAE=99.5048 MAE=99.4116 MAE=99.5227 MAE=99.5926 MAE=99.5634 MAE=99.5708 MAE=99.5933 MAE=99.5682 MAE=99.4217 Epoch: 190/500MAE=99.4233 MAE=99.4008 MAE=99.5307 MAE=99.4557 MAE=99.4015 MAE=99.4085 MAE=99.3208 MAE=99.3514 MAE=99.4025 MAE=99.4338 Epoch: 200/500MAE=99.3617 MAE=99.3849 MAE=99.3917 MAE=99.2763 MAE=99.4165 MAE=99.3861 MAE=99.2563 MAE=99.3084 MAE=99.3501 MAE=99.3952 Epoch: 210/500MAE=99.3948 MAE=99.3482 MAE=99.4848 MAE=99.4823 MAE=99.4770 MAE=99.5096 MAE=99.3687 MAE=99.4273 MAE=99.3386 MAE=99.3084 Epoch: 220/500MAE=99.3463 MAE=99.1829 MAE=99.2295 MAE=99.2462 MAE=99.1923 MAE=99.2629 MAE=99.2713 MAE=99.2635 MAE=99.1908 MAE=99.1797 Epoch: 230/500MAE=99.1661 MAE=99.2038 MAE=99.2627 MAE=99.1886 MAE=99.2415 MAE=99.2552 MAE=99.2261 MAE=99.1710 MAE=99.0627 MAE=99.0267 Epoch: 240/500MAE=99.0645 MAE=99.0257 MAE=99.0443 MAE=99.0954 MAE=99.0705 MAE=99.0755 MAE=99.0949 MAE=99.0723 MAE=99.1103 MAE=99.0562 Epoch: 250/500MAE=99.0882 MAE=99.0677 MAE=98.9995 MAE=99.0359 MAE=99.0932 MAE=99.0348 MAE=99.0896 MAE=99.1503 MAE=99.0136 MAE=99.0202 Epoch: 260/500MAE=98.9812 MAE=98.9886 MAE=98.8981 MAE=98.8852 MAE=98.9976 MAE=98.9345 MAE=98.9363 MAE=99.0421 MAE=98.9648 MAE=98.9456 Epoch: 270/500MAE=98.8236 MAE=98.8001 MAE=98.8261 MAE=98.7241 MAE=98.7409 MAE=98.8376 MAE=98.8096 MAE=98.7600 MAE=98.7068 MAE=98.7290 Epoch: 280/500MAE=98.8137 MAE=98.7844 MAE=98.7946 MAE=98.9291 MAE=98.9470 MAE=98.9253 MAE=98.8680 MAE=98.8424 MAE=98.7652 MAE=98.6937 Epoch: 290/500MAE=98.7791 MAE=98.8437 MAE=98.7829 MAE=98.8186 MAE=98.7312 MAE=98.7554 MAE=98.7957 MAE=98.7259 MAE=98.7155 MAE=98.8167 Epoch: 300/500MAE=98.8160 MAE=98.8580 MAE=98.8223 MAE=98.8051 MAE=98.8197 MAE=98.7733 MAE=98.7787 MAE=98.5729 MAE=98.6151 MAE=98.5571 Epoch: 310/500MAE=98.6068 MAE=98.6606 MAE=98.6562 MAE=98.4850 MAE=98.5224 MAE=98.5310 MAE=98.4694 MAE=98.3911 MAE=98.3869 MAE=98.3583 Epoch: 320/500MAE=98.3620 MAE=98.3326 MAE=98.3357 MAE=98.2774 MAE=98.2241 MAE=98.2167 MAE=98.2639 MAE=98.1837 MAE=98.1320 MAE=98.1074 Epoch: 330/500MAE=98.1833 MAE=98.1455 MAE=98.1461 MAE=98.1578 MAE=98.0862 MAE=98.1867 MAE=98.3220 MAE=98.2091 MAE=98.1251 MAE=98.0818 Epoch: 340/500MAE=98.1435 MAE=98.1386 MAE=98.0710 MAE=98.0237 MAE=97.9414 MAE=98.0169 MAE=98.1063 MAE=98.0556 MAE=98.2209 MAE=98.1023 Epoch: 350/500MAE=98.0215 MAE=97.9001 MAE=97.9082 MAE=97.9253 MAE=97.9951 MAE=97.9096 MAE=97.8910 MAE=97.9253 MAE=98.0949 MAE=98.0500 Epoch: 360/500MAE=97.9477 MAE=97.8649 MAE=97.9975 MAE=97.9986 MAE=98.0305 MAE=98.0146 MAE=97.8921 MAE=97.8433 MAE=97.9029 MAE=97.9300 Epoch: 370/500MAE=97.8469 MAE=97.8503 MAE=97.9173 MAE=97.9281 MAE=97.9355 MAE=97.9528 MAE=97.9544 MAE=97.8465 MAE=98.0106 MAE=98.0038 Epoch: 380/500MAE=98.0261 MAE=97.9825 MAE=97.9311 MAE=97.6023 MAE=97.7469 MAE=97.7825 MAE=97.7426 MAE=97.7828 MAE=97.7536 MAE=97.7478 Epoch: 390/500MAE=97.7598 MAE=97.8423 MAE=97.7226 MAE=97.6141 MAE=97.6716 MAE=97.5506 MAE=97.7626 MAE=97.7506 MAE=97.6920 MAE=97.4156 Epoch: 400/500MAE=97.5289 MAE=97.4140 MAE=97.5368 MAE=97.4009 MAE=97.4959 MAE=97.5202 MAE=97.4458 MAE=97.5183 MAE=97.3913 MAE=97.3464 Epoch: 410/500MAE=97.4171 MAE=97.3391 MAE=97.4675 MAE=97.3335 MAE=97.4218 MAE=97.3844 MAE=97.3797 MAE=97.3591 MAE=97.3244 MAE=97.2979 Epoch: 420/500MAE=97.3409 MAE=97.1997 MAE=97.2236 MAE=97.2890 MAE=97.1742 MAE=97.2494 MAE=97.2636 MAE=97.2162 MAE=97.1606 MAE=97.3078 Epoch: 430/500MAE=97.1181 MAE=97.1317 MAE=97.0888 MAE=97.3355 MAE=97.2941 MAE=97.1156 MAE=97.1755 MAE=97.1716 MAE=97.3040 MAE=97.2770 Epoch: 440/500MAE=97.3068 MAE=97.2495 MAE=97.1339 MAE=97.1890 MAE=97.2654 MAE=97.2140 MAE=97.2772 MAE=97.3065 MAE=97.2337 MAE=97.1993 Epoch: 450/500MAE=97.2039 MAE=97.1881 MAE=97.2609 MAE=97.1509 MAE=97.1213 MAE=97.1770 MAE=97.0668 MAE=97.0943 MAE=97.0139 MAE=97.0102 Epoch: 460/500MAE=97.0412 MAE=97.0575 MAE=96.9904 MAE=97.1012 MAE=97.1338 MAE=97.0256 MAE=97.0343 MAE=96.9201 MAE=96.8816 MAE=96.9696 Epoch: 470/500MAE=96.9858 MAE=97.0166 MAE=97.0049 MAE=97.0083 MAE=96.9573 MAE=96.9945 MAE=96.9595 MAE=97.2144 MAE=97.1573 MAE=97.1415 Epoch: 480/500MAE=97.0736 MAE=96.8589 MAE=96.8610 MAE=96.7884 MAE=96.8916 MAE=96.7952 MAE=96.8297 MAE=96.6653 MAE=96.6705 MAE=96.7276 Epoch: 490/500MAE=96.7996 MAE=96.7956 MAE=96.7383 MAE=96.7067 MAE=96.7101 MAE=96.7688 MAE=96.9362 MAE=96.6995 MAE=96.7055 MAE=96.7826 Epoch: 500/500MAE=96.6816 MAE=105.1858 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 78.012 +/- 17.763\n",
      "\n",
      "Epoch: 1/500MAE=1549.6169 MAE=1547.4451 MAE=1544.9678 MAE=1542.4736 MAE=1538.5259 MAE=1537.1140 MAE=1534.2104 MAE=1528.7576 MAE=1527.2195 Epoch: 10/500MAE=1520.8131 MAE=1518.3683 MAE=1507.6846 MAE=1504.5676 MAE=1499.1060 MAE=1491.0435 MAE=1492.5669 MAE=1486.9148 MAE=1472.6831 MAE=1471.2815 Epoch: 20/500MAE=1461.3755 MAE=1452.0498 MAE=1447.3340 MAE=1436.4143 MAE=1426.8887 MAE=1413.4871 MAE=1401.3027 MAE=1398.3832 MAE=1382.4570 MAE=1380.3130 Epoch: 30/500MAE=1356.3438 MAE=1348.3026 MAE=1333.9316 MAE=1326.5488 MAE=1317.5645 MAE=1309.5413 MAE=1284.3196 MAE=1284.3342 MAE=1261.6934 MAE=1239.6145 Epoch: 40/500MAE=1234.7445 MAE=1243.5200 MAE=1208.0723 MAE=1183.7632 MAE=1165.0973 MAE=1164.0061 MAE=1150.0557 MAE=1058.4691 MAE=1129.6074 MAE=1087.1038 Epoch: 50/500MAE=1069.8967 MAE=1072.9351 MAE=1049.6848 MAE=1049.7037 MAE=1017.4135 MAE=1021.5515 MAE=992.1175 MAE=992.7755 MAE=988.4891 MAE=1003.1621 Epoch: 60/500MAE=967.4658 MAE=958.2309 MAE=966.1287 MAE=947.7632 MAE=928.0083 MAE=887.5421 MAE=928.1584 MAE=906.4781 MAE=895.4258 MAE=843.8835 Epoch: 70/500MAE=873.6401 MAE=867.9504 MAE=832.9104 MAE=868.3687 MAE=815.6154 MAE=816.2810 MAE=843.4999 MAE=775.6891 MAE=803.6329 MAE=789.0409 Epoch: 80/500MAE=785.1055 MAE=739.0457 MAE=783.3550 MAE=755.5004 MAE=726.1382 MAE=705.2484 MAE=746.2568 MAE=711.0836 MAE=657.7771 MAE=699.5901 Epoch: 90/500MAE=658.0096 MAE=653.8773 MAE=671.8755 MAE=664.0713 MAE=645.2562 MAE=642.2316 MAE=614.1871 MAE=603.1107 MAE=583.1553 MAE=590.1048 Epoch: 100/500MAE=585.3687 MAE=527.9816 MAE=555.9142 MAE=567.1616 MAE=527.1072 MAE=504.6626 MAE=498.3791 MAE=582.2330 MAE=466.7618 MAE=511.7797 Epoch: 110/500MAE=535.5712 MAE=471.3324 MAE=437.1194 MAE=437.4009 MAE=477.8656 MAE=449.3368 MAE=450.3880 MAE=427.0157 MAE=434.1989 MAE=420.4702 Epoch: 120/500MAE=411.9077 MAE=421.5881 MAE=397.5511 MAE=413.6313 MAE=424.0706 MAE=404.9422 MAE=399.7023 MAE=401.7609 MAE=389.5225 MAE=384.1968 Epoch: 130/500MAE=382.8289 MAE=387.4041 MAE=377.2685 MAE=383.4212 MAE=379.2264 MAE=354.0219 MAE=384.3063 MAE=357.8676 MAE=376.7904 MAE=373.4010 Epoch: 140/500MAE=359.2126 MAE=363.1258 MAE=366.0400 MAE=367.7357 MAE=361.5619 MAE=360.3411 MAE=356.1619 MAE=363.1841 MAE=361.5348 MAE=359.7126 Epoch: 150/500MAE=357.2486 MAE=356.3776 MAE=357.7392 MAE=358.8081 MAE=358.6714 MAE=357.9548 MAE=357.6022 MAE=357.5999 MAE=357.9507 MAE=357.9907 Epoch: 160/500MAE=357.6377 MAE=357.2606 MAE=357.5013 MAE=357.4224 MAE=357.3157 MAE=357.2509 MAE=357.3827 MAE=357.1583 MAE=357.1771 MAE=357.3471 Epoch: 170/500MAE=357.4224 MAE=357.4315 MAE=357.2472 MAE=357.3772 MAE=357.3552 MAE=357.2131 MAE=357.2077 MAE=357.2773 MAE=357.2495 MAE=357.0211 Epoch: 180/500MAE=357.0903 MAE=357.1056 MAE=357.0052 MAE=357.0471 MAE=357.1836 MAE=357.1658 MAE=356.9371 MAE=357.0424 MAE=357.1153 MAE=356.9636 Epoch: 190/500MAE=357.1213 MAE=357.2032 MAE=357.1794 MAE=357.1596 MAE=357.0148 MAE=357.0949 MAE=356.9771 MAE=356.6860 MAE=356.9437 MAE=356.9822 Epoch: 200/500MAE=356.8286 MAE=356.7365 MAE=356.8633 MAE=356.8682 MAE=356.8676 MAE=356.7363 MAE=356.5919 MAE=356.7504 MAE=356.7102 MAE=356.6791 Epoch: 210/500MAE=356.7285 MAE=356.7095 MAE=356.8440 MAE=356.6426 MAE=356.7041 MAE=356.6170 MAE=356.5827 MAE=356.4380 MAE=356.6443 MAE=356.5998 Epoch: 220/500MAE=356.6119 MAE=356.6542 MAE=356.5417 MAE=356.5349 MAE=356.4061 MAE=356.5052 MAE=356.6056 MAE=356.4822 MAE=356.5624 MAE=356.5412 Epoch: 230/500MAE=356.2460 MAE=356.2519 MAE=356.3627 MAE=356.5208 MAE=356.4301 MAE=356.2515 MAE=356.3224 MAE=356.1781 MAE=356.3111 MAE=356.3120 Epoch: 240/500MAE=356.2692 MAE=356.2814 MAE=356.1264 MAE=356.1067 MAE=355.9686 MAE=356.0055 MAE=356.1616 MAE=356.1683 MAE=356.1147 MAE=356.0549 Epoch: 250/500MAE=356.0316 MAE=356.1086 MAE=355.9891 MAE=355.9912 MAE=356.0031 MAE=355.9379 MAE=355.8913 MAE=355.8214 MAE=356.0287 MAE=356.0036 Epoch: 260/500MAE=355.9308 MAE=355.9465 MAE=355.9309 MAE=355.8892 MAE=355.7692 MAE=355.7755 MAE=355.7527 MAE=355.7917 MAE=355.7452 MAE=355.8376 Epoch: 270/500MAE=355.7067 MAE=355.7406 MAE=355.6813 MAE=355.6922 MAE=355.6286 MAE=355.5620 MAE=355.5585 MAE=355.5117 MAE=355.6500 MAE=355.6798 Epoch: 280/500MAE=355.5971 MAE=355.5872 MAE=355.5316 MAE=355.5432 MAE=355.6641 MAE=355.4849 MAE=370.3539 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 136.480 +/- 118.011\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --co_ratio=0.1 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --co_ratio=0.3 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --co_ratio=0.5 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --co_ratio=0.7 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --co_ratio=0.9 --pooling='CO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.1438 MAE=0.1175 MAE=0.0771 MAE=0.0544 MAE=0.0463 MAE=0.0445 MAE=0.0432 MAE=0.0386 MAE=0.0355 Epoch: 10/150MAE=0.0351 MAE=0.0316 MAE=0.0323 MAE=0.0304 MAE=0.0304 MAE=0.0299 MAE=0.0305 MAE=0.0293 MAE=0.0286 MAE=0.0299 Epoch: 20/150MAE=0.0295 MAE=0.0291 MAE=0.0288 MAE=0.0276 MAE=0.0278 MAE=0.0277 MAE=0.0281 MAE=0.0277 MAE=0.0270 MAE=0.0266 Epoch: 30/150MAE=0.0271 MAE=0.0268 MAE=0.0271 MAE=0.0271 MAE=0.0267 MAE=0.0263 MAE=0.0264 MAE=0.0263 MAE=0.0263 MAE=0.0268 Epoch: 40/150MAE=0.0262 MAE=0.0262 MAE=0.0261 MAE=0.0261 MAE=0.0260 MAE=0.0263 MAE=0.0262 MAE=0.0261 MAE=0.0261 MAE=0.0259 Epoch: 50/150MAE=0.0259 MAE=0.0258 MAE=0.0258 MAE=0.0258 MAE=0.0258 MAE=0.0258 MAE=0.0257 MAE=0.0256 MAE=0.0258 MAE=0.0258 Epoch: 60/150MAE=0.0257 MAE=0.0254 MAE=0.0253 MAE=0.0255 MAE=0.0257 MAE=0.0256 MAE=0.0254 MAE=0.0254 MAE=0.0254 MAE=0.0253 Epoch: 70/150MAE=0.0254 MAE=0.0254 MAE=0.0254 MAE=0.0254 MAE=0.0254 MAE=0.0254 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0253 Epoch: 80/150MAE=0.0253 MAE=0.0254 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0253 Epoch: 90/150MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 Epoch: 100/150MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 Epoch: 110/150MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0251 MAE=0.0252 Epoch: 120/150MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0258 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.026 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.1400 MAE=0.1250 MAE=0.0892 MAE=0.0625 MAE=0.0520 MAE=0.0459 MAE=0.0430 MAE=0.0396 MAE=0.0385 Epoch: 10/150MAE=0.0369 MAE=0.0337 MAE=0.0325 MAE=0.0307 MAE=0.0324 MAE=0.0307 MAE=0.0302 MAE=0.0298 MAE=0.0292 MAE=0.0298 Epoch: 20/150MAE=0.0297 MAE=0.0288 MAE=0.0284 MAE=0.0292 MAE=0.0288 MAE=0.0284 MAE=0.0278 MAE=0.0283 MAE=0.0277 MAE=0.0278 Epoch: 30/150MAE=0.0270 MAE=0.0273 MAE=0.0271 MAE=0.0266 MAE=0.0276 MAE=0.0279 MAE=0.0269 MAE=0.0283 MAE=0.0272 MAE=0.0264 Epoch: 40/150MAE=0.0264 MAE=0.0267 MAE=0.0262 MAE=0.0263 MAE=0.0258 MAE=0.0263 MAE=0.0261 MAE=0.0256 MAE=0.0256 MAE=0.0256 Epoch: 50/150MAE=0.0257 MAE=0.0257 MAE=0.0258 MAE=0.0251 MAE=0.0251 MAE=0.0253 MAE=0.0249 MAE=0.0254 MAE=0.0252 MAE=0.0252 Epoch: 60/150MAE=0.0250 MAE=0.0249 MAE=0.0250 MAE=0.0248 MAE=0.0250 MAE=0.0245 MAE=0.0247 MAE=0.0246 MAE=0.0245 MAE=0.0248 Epoch: 70/150MAE=0.0246 MAE=0.0247 MAE=0.0245 MAE=0.0244 MAE=0.0244 MAE=0.0245 MAE=0.0243 MAE=0.0245 MAE=0.0244 MAE=0.0244 Epoch: 80/150MAE=0.0245 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0244 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0244 Epoch: 90/150MAE=0.0244 MAE=0.0242 MAE=0.0242 MAE=0.0242 MAE=0.0243 MAE=0.0243 MAE=0.0244 MAE=0.0243 MAE=0.0243 MAE=0.0243 Epoch: 100/150MAE=0.0242 MAE=0.0241 MAE=0.0241 MAE=0.0242 MAE=0.0242 MAE=0.0242 MAE=0.0242 MAE=0.0242 MAE=0.0242 MAE=0.0242 Epoch: 110/150MAE=0.0242 MAE=0.0242 MAE=0.0242 MAE=0.0244 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.025 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1355 MAE=0.1136 MAE=0.0816 MAE=0.0577 MAE=0.0465 MAE=0.0412 MAE=0.0375 MAE=0.0411 MAE=0.0367 Epoch: 10/150MAE=0.0337 MAE=0.0321 MAE=0.0322 MAE=0.0306 MAE=0.0299 MAE=0.0291 MAE=0.0277 MAE=0.0279 MAE=0.0278 MAE=0.0273 Epoch: 20/150MAE=0.0274 MAE=0.0265 MAE=0.0262 MAE=0.0265 MAE=0.0273 MAE=0.0257 MAE=0.0264 MAE=0.0272 MAE=0.0260 MAE=0.0266 Epoch: 30/150MAE=0.0243 MAE=0.0245 MAE=0.0257 MAE=0.0248 MAE=0.0247 MAE=0.0245 MAE=0.0251 MAE=0.0248 MAE=0.0246 MAE=0.0239 Epoch: 40/150MAE=0.0238 MAE=0.0241 MAE=0.0239 MAE=0.0238 MAE=0.0240 MAE=0.0237 MAE=0.0235 MAE=0.0238 MAE=0.0239 MAE=0.0238 Epoch: 50/150MAE=0.0236 MAE=0.0236 MAE=0.0237 MAE=0.0236 MAE=0.0236 MAE=0.0236 MAE=0.0237 MAE=0.0241 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.025 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1371 MAE=0.1118 MAE=0.0756 MAE=0.0579 MAE=0.0537 MAE=0.0467 MAE=0.0423 MAE=0.0401 MAE=0.0362 Epoch: 10/150MAE=0.0325 MAE=0.0315 MAE=0.0293 MAE=0.0287 MAE=0.0274 MAE=0.0270 MAE=0.0271 MAE=0.0274 MAE=0.0265 MAE=0.0263 Epoch: 20/150MAE=0.0261 MAE=0.0269 MAE=0.0278 MAE=0.0276 MAE=0.0273 MAE=0.0269 MAE=0.0261 MAE=0.0261 MAE=0.0260 MAE=0.0260 Epoch: 30/150MAE=0.0264 MAE=0.0260 MAE=0.0256 MAE=0.0254 MAE=0.0254 MAE=0.0259 MAE=0.0256 MAE=0.0255 MAE=0.0253 MAE=0.0253 Epoch: 40/150MAE=0.0254 MAE=0.0252 MAE=0.0252 MAE=0.0251 MAE=0.0251 MAE=0.0249 MAE=0.0246 MAE=0.0249 MAE=0.0247 MAE=0.0247 Epoch: 50/150MAE=0.0245 MAE=0.0245 MAE=0.0246 MAE=0.0247 MAE=0.0247 MAE=0.0242 MAE=0.0242 MAE=0.0243 MAE=0.0242 MAE=0.0242 Epoch: 60/150MAE=0.0241 MAE=0.0241 MAE=0.0242 MAE=0.0240 MAE=0.0240 MAE=0.0240 MAE=0.0242 MAE=0.0241 MAE=0.0239 MAE=0.0240 Epoch: 70/150MAE=0.0240 MAE=0.0240 MAE=0.0239 MAE=0.0239 MAE=0.0239 MAE=0.0239 MAE=0.0239 MAE=0.0238 MAE=0.0238 MAE=0.0239 Epoch: 80/150MAE=0.0238 MAE=0.0239 MAE=0.0239 MAE=0.0238 MAE=0.0238 MAE=0.0238 MAE=0.0237 MAE=0.0238 MAE=0.0238 MAE=0.0238 Epoch: 90/150MAE=0.0238 MAE=0.0238 MAE=0.0238 MAE=0.0238 MAE=0.0238 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 Epoch: 100/150MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0238 MAE=0.0237 MAE=0.0237 Epoch: 110/150MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 Epoch: 120/150MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0237 MAE=0.0243 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.025 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1372 MAE=0.1164 MAE=0.0781 MAE=0.0549 MAE=0.0489 MAE=0.0450 MAE=0.0401 MAE=0.0389 MAE=0.0359 Epoch: 10/150MAE=0.0346 MAE=0.0332 MAE=0.0319 MAE=0.0314 MAE=0.0293 MAE=0.0305 MAE=0.0293 MAE=0.0287 MAE=0.0286 MAE=0.0296 Epoch: 20/150MAE=0.0283 MAE=0.0276 MAE=0.0276 MAE=0.0282 MAE=0.0295 MAE=0.0300 MAE=0.0291 MAE=0.0259 MAE=0.0258 MAE=0.0265 Epoch: 30/150MAE=0.0256 MAE=0.0258 MAE=0.0256 MAE=0.0253 MAE=0.0255 MAE=0.0255 MAE=0.0255 MAE=0.0256 MAE=0.0249 MAE=0.0250 Epoch: 40/150MAE=0.0249 MAE=0.0250 MAE=0.0249 MAE=0.0249 MAE=0.0248 MAE=0.0247 MAE=0.0260 MAE=0.0262 MAE=0.0260 MAE=0.0261 Epoch: 50/150MAE=0.0257 MAE=0.0257 MAE=0.0258 MAE=0.0257 MAE=0.0256 MAE=0.0256 MAE=0.0253 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.025 +/- 0.001\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.1317 MAE=0.1027 MAE=0.0762 MAE=0.0590 MAE=0.0525 MAE=0.0484 MAE=0.0406 MAE=0.0381 MAE=0.0355 Epoch: 10/150MAE=0.0339 MAE=0.0342 MAE=0.0330 MAE=0.0318 MAE=0.0290 MAE=0.0301 MAE=0.0310 MAE=0.0293 MAE=0.0321 MAE=0.0296 Epoch: 20/150MAE=0.0292 MAE=0.0278 MAE=0.0282 MAE=0.0280 MAE=0.0274 MAE=0.0278 MAE=0.0271 MAE=0.0281 MAE=0.0266 MAE=0.0270 Epoch: 30/150MAE=0.0272 MAE=0.0267 MAE=0.0273 MAE=0.0261 MAE=0.0266 MAE=0.0257 MAE=0.0260 MAE=0.0258 MAE=0.0262 MAE=0.0256 Epoch: 40/150MAE=0.0257 MAE=0.0259 MAE=0.0260 MAE=0.0256 MAE=0.0254 MAE=0.0254 MAE=0.0255 MAE=0.0253 MAE=0.0253 MAE=0.0251 Epoch: 50/150MAE=0.0252 MAE=0.0253 MAE=0.0252 MAE=0.0252 MAE=0.0253 MAE=0.0253 MAE=0.0253 MAE=0.0251 MAE=0.0251 MAE=0.0251 Epoch: 60/150MAE=0.0251 MAE=0.0251 MAE=0.0251 MAE=0.0250 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0249 Epoch: 70/150MAE=0.0248 MAE=0.0250 MAE=0.0250 MAE=0.0249 MAE=0.0250 MAE=0.0250 MAE=0.0250 MAE=0.0250 MAE=0.0250 MAE=0.0250 Epoch: 80/150MAE=0.0249 MAE=0.0253 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.025 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.1241 MAE=0.0923 MAE=0.0629 MAE=0.0522 MAE=0.0465 MAE=0.0417 MAE=0.0413 MAE=0.0399 MAE=0.0347 Epoch: 10/150MAE=0.0338 MAE=0.0319 MAE=0.0299 MAE=0.0332 MAE=0.0298 MAE=0.0302 MAE=0.0287 MAE=0.0282 MAE=0.0282 MAE=0.0270 Epoch: 20/150MAE=0.0277 MAE=0.0264 MAE=0.0268 MAE=0.0266 MAE=0.0264 MAE=0.0252 MAE=0.0258 MAE=0.0252 MAE=0.0251 MAE=0.0253 Epoch: 30/150MAE=0.0251 MAE=0.0251 MAE=0.0247 MAE=0.0258 MAE=0.0244 MAE=0.0241 MAE=0.0238 MAE=0.0250 MAE=0.0238 MAE=0.0233 Epoch: 40/150MAE=0.0233 MAE=0.0252 MAE=0.0245 MAE=0.0236 MAE=0.0233 MAE=0.0228 MAE=0.0226 MAE=0.0222 MAE=0.0226 MAE=0.0223 Epoch: 50/150MAE=0.0223 MAE=0.0224 MAE=0.0221 MAE=0.0221 MAE=0.0219 MAE=0.0219 MAE=0.0218 MAE=0.0220 MAE=0.0219 MAE=0.0219 Epoch: 60/150MAE=0.0219 MAE=0.0220 MAE=0.0217 MAE=0.0216 MAE=0.0217 MAE=0.0219 MAE=0.0217 MAE=0.0217 MAE=0.0215 MAE=0.0216 Epoch: 70/150MAE=0.0217 MAE=0.0217 MAE=0.0216 MAE=0.0215 MAE=0.0215 MAE=0.0214 MAE=0.0215 MAE=0.0215 MAE=0.0214 MAE=0.0214 Epoch: 80/150MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0215 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 Epoch: 90/150MAE=0.0214 MAE=0.0214 MAE=0.0213 MAE=0.0213 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0213 Epoch: 100/150MAE=0.0214 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0214 Epoch: 110/150MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0218 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.024 +/- 0.002\n",
      "\n",
      "Epoch: 1/150MAE=0.1217 MAE=0.0974 MAE=0.0682 MAE=0.0504 MAE=0.0467 MAE=0.0438 MAE=0.0415 MAE=0.0359 MAE=0.0345 Epoch: 10/150MAE=0.0328 MAE=0.0324 MAE=0.0308 MAE=0.0305 MAE=0.0290 MAE=0.0283 MAE=0.0278 MAE=0.0284 MAE=0.0287 MAE=0.0284 Epoch: 20/150MAE=0.0269 MAE=0.0260 MAE=0.0256 MAE=0.0294 MAE=0.0312 MAE=0.0264 MAE=0.0254 MAE=0.0259 MAE=0.0253 MAE=0.0252 Epoch: 30/150MAE=0.0252 MAE=0.0255 MAE=0.0250 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0247 MAE=0.0249 MAE=0.0254 MAE=0.0258 Epoch: 40/150MAE=0.0250 MAE=0.0253 MAE=0.0244 MAE=0.0252 MAE=0.0258 MAE=0.0270 MAE=0.0286 MAE=0.0309 MAE=0.0364 MAE=0.0387 Epoch: 50/150MAE=0.0430 MAE=0.0564 MAE=0.0721 MAE=0.0251 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.024 +/- 0.002\n",
      "\n",
      "Epoch: 1/150MAE=0.1295 MAE=0.0965 MAE=0.0663 MAE=0.0476 MAE=0.0451 MAE=0.0429 MAE=0.0426 MAE=0.0394 MAE=0.0364 Epoch: 10/150MAE=0.0328 MAE=0.0300 MAE=0.0313 MAE=0.0305 MAE=0.0346 MAE=0.0303 MAE=0.0276 MAE=0.0281 MAE=0.0275 MAE=0.0263 Epoch: 20/150MAE=0.0275 MAE=0.0260 MAE=0.0261 MAE=0.0266 MAE=0.0381 MAE=0.0407 MAE=0.0303 MAE=0.0271 MAE=0.0257 MAE=0.0252 Epoch: 30/150MAE=0.0250 MAE=0.0249 MAE=0.0249 MAE=0.0253 MAE=0.0251 MAE=0.0247 MAE=0.0247 MAE=0.0263 MAE=0.0261 MAE=0.0250 Epoch: 40/150MAE=0.0245 MAE=0.0242 MAE=0.0237 MAE=0.0241 MAE=0.0241 MAE=0.0239 MAE=0.0237 MAE=0.0239 MAE=0.0234 MAE=0.0242 Epoch: 50/150MAE=0.0243 MAE=0.0234 MAE=0.0247 MAE=0.0235 MAE=0.0234 MAE=0.0229 MAE=0.0230 MAE=0.0231 MAE=0.0229 MAE=0.0229 Epoch: 60/150MAE=0.0227 MAE=0.0225 MAE=0.0229 MAE=0.0227 MAE=0.0228 MAE=0.0227 MAE=0.0229 MAE=0.0225 MAE=0.0225 MAE=0.0227 Epoch: 70/150MAE=0.0226 MAE=0.0226 MAE=0.0226 MAE=0.0224 MAE=0.0223 MAE=0.0224 MAE=0.0223 MAE=0.0222 MAE=0.0221 MAE=0.0222 Epoch: 80/150MAE=0.0222 MAE=0.0223 MAE=0.0223 MAE=0.0223 MAE=0.0222 MAE=0.0220 MAE=0.0220 MAE=0.0220 MAE=0.0220 MAE=0.0221 Epoch: 90/150MAE=0.0221 MAE=0.0221 MAE=0.0222 MAE=0.0221 MAE=0.0222 MAE=0.0221 MAE=0.0221 MAE=0.0221 MAE=0.0221 MAE=0.0225 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.024 +/- 0.002\n",
      "\n",
      "Epoch: 1/150MAE=0.1023 MAE=0.0849 MAE=0.0659 MAE=0.0506 MAE=0.0442 MAE=0.0398 MAE=0.0398 MAE=0.0377 MAE=0.0357 Epoch: 10/150MAE=0.0350 MAE=0.0346 MAE=0.0319 MAE=0.0308 MAE=0.0306 MAE=0.0296 MAE=0.0295 MAE=0.0288 MAE=0.0275 MAE=0.0275 Epoch: 20/150MAE=0.0304 MAE=0.0281 MAE=0.0292 MAE=0.0252 MAE=0.0255 MAE=0.0253 MAE=0.0249 MAE=0.0248 MAE=0.0252 MAE=0.0248 Epoch: 30/150MAE=0.0252 MAE=0.0243 MAE=0.0256 MAE=0.0250 MAE=0.0252 MAE=0.0249 MAE=0.0243 MAE=0.0244 MAE=0.0240 MAE=0.0238 Epoch: 40/150MAE=0.0242 MAE=0.0240 MAE=0.0247 MAE=0.0242 MAE=0.0236 MAE=0.0237 MAE=0.0237 MAE=0.0239 MAE=0.0238 MAE=0.0236 Epoch: 50/150MAE=0.0234 MAE=0.0236 MAE=0.0236 MAE=0.0233 MAE=0.0235 MAE=0.0235 MAE=0.0234 MAE=0.0234 MAE=0.0233 MAE=0.0234 Epoch: 60/150MAE=0.0233 MAE=0.0233 MAE=0.0232 MAE=0.0232 MAE=0.0232 MAE=0.0232 MAE=0.0232 MAE=0.0232 MAE=0.0231 MAE=0.0232 Epoch: 70/150MAE=0.0231 MAE=0.0232 MAE=0.0232 MAE=0.0233 MAE=0.0233 MAE=0.0233 MAE=0.0233 MAE=0.0233 MAE=0.0233 MAE=0.0233 Epoch: 80/150MAE=0.0233 MAE=0.0236 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.024 +/- 0.001\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.1185 MAE=0.0842 MAE=0.0716 MAE=0.0582 MAE=0.0509 MAE=0.0466 MAE=0.0466 MAE=0.0417 MAE=0.0406 Epoch: 10/150MAE=0.0372 MAE=0.0362 MAE=0.0335 MAE=0.0340 MAE=0.0352 MAE=0.0358 MAE=0.0336 MAE=0.0311 MAE=0.0297 MAE=0.0289 Epoch: 20/150MAE=0.0284 MAE=0.0279 MAE=0.0304 MAE=0.0280 MAE=0.0277 MAE=0.0274 MAE=0.0279 MAE=0.0270 MAE=0.0274 MAE=0.0282 Epoch: 30/150MAE=0.0272 MAE=0.0272 MAE=0.0269 MAE=0.0266 MAE=0.0262 MAE=0.0258 MAE=0.0262 MAE=0.0258 MAE=0.0264 MAE=0.0264 Epoch: 40/150MAE=0.0258 MAE=0.0253 MAE=0.0257 MAE=0.0258 MAE=0.0257 MAE=0.0260 MAE=0.0255 MAE=0.0254 MAE=0.0255 MAE=0.0256 Epoch: 50/150MAE=0.0254 MAE=0.0250 MAE=0.0251 MAE=0.0252 MAE=0.0251 MAE=0.0251 MAE=0.0251 MAE=0.0251 MAE=0.0252 MAE=0.0251 Epoch: 60/150MAE=0.0251 MAE=0.0251 MAE=0.0254 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.025 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.1199 MAE=0.0883 MAE=0.0577 MAE=0.0533 MAE=0.0481 MAE=0.0437 MAE=0.0405 MAE=0.0384 MAE=0.0359 Epoch: 10/150MAE=0.0337 MAE=0.0317 MAE=0.0304 MAE=0.0312 MAE=0.0331 MAE=0.0295 MAE=0.0289 MAE=0.0317 MAE=0.0261 MAE=0.0254 Epoch: 20/150MAE=0.0279 MAE=0.0253 MAE=0.0251 MAE=0.0251 MAE=0.0248 MAE=0.0241 MAE=0.0242 MAE=0.0255 MAE=0.0244 MAE=0.0257 Epoch: 30/150MAE=0.0233 MAE=0.0228 MAE=0.0222 MAE=0.0224 MAE=0.0217 MAE=0.0218 MAE=0.0217 MAE=0.0215 MAE=0.0218 MAE=0.0219 Epoch: 40/150MAE=0.0214 MAE=0.0214 MAE=0.0220 MAE=0.0215 MAE=0.0213 MAE=0.0211 MAE=0.0211 MAE=0.0212 MAE=0.0211 MAE=0.0212 Epoch: 50/150MAE=0.0210 MAE=0.0208 MAE=0.0206 MAE=0.0206 MAE=0.0207 MAE=0.0206 MAE=0.0206 MAE=0.0208 MAE=0.0204 MAE=0.0205 Epoch: 60/150MAE=0.0203 MAE=0.0203 MAE=0.0204 MAE=0.0203 MAE=0.0203 MAE=0.0204 MAE=0.0204 MAE=0.0202 MAE=0.0203 MAE=0.0203 Epoch: 70/150MAE=0.0203 MAE=0.0204 MAE=0.0202 MAE=0.0203 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 Epoch: 80/150MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0201 MAE=0.0200 MAE=0.0200 MAE=0.0201 MAE=0.0200 Epoch: 90/150MAE=0.0200 MAE=0.0201 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 Epoch: 100/150MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0199 MAE=0.0200 Epoch: 110/150MAE=0.0199 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0200 MAE=0.0199 MAE=0.0200 MAE=0.0200 MAE=0.0206 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.023 +/- 0.002\n",
      "\n",
      "Epoch: 1/150MAE=0.1341 MAE=0.1127 MAE=0.0797 MAE=0.0520 MAE=0.0424 MAE=0.0396 MAE=0.0382 MAE=0.0361 MAE=0.0356 Epoch: 10/150MAE=0.0334 MAE=0.0318 MAE=0.0302 MAE=0.0311 MAE=0.0292 MAE=0.0284 MAE=0.0301 MAE=0.0283 MAE=0.0280 MAE=0.0287 Epoch: 20/150MAE=0.0271 MAE=0.0287 MAE=0.0285 MAE=0.0260 MAE=0.0268 MAE=0.0263 MAE=0.0268 MAE=0.0264 MAE=0.0250 MAE=0.0247 Epoch: 30/150MAE=0.0242 MAE=0.0242 MAE=0.0242 MAE=0.0233 MAE=0.0233 MAE=0.0230 MAE=0.0233 MAE=0.0237 MAE=0.0235 MAE=0.0234 Epoch: 40/150MAE=0.0225 MAE=0.0235 MAE=0.0230 MAE=0.0225 MAE=0.0225 MAE=0.0223 MAE=0.0225 MAE=0.0230 MAE=0.0226 MAE=0.0227 Epoch: 50/150MAE=0.0230 MAE=0.0222 MAE=0.0223 MAE=0.0223 MAE=0.0222 MAE=0.0225 MAE=0.0222 MAE=0.0221 MAE=0.0219 MAE=0.0218 Epoch: 60/150MAE=0.0221 MAE=0.0219 MAE=0.0225 MAE=0.0220 MAE=0.0219 MAE=0.0219 MAE=0.0219 MAE=0.0221 MAE=0.0221 MAE=0.0218 MAE=0.0224 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.023 +/- 0.002\n",
      "\n",
      "Epoch: 1/150MAE=0.1355 MAE=0.0881 MAE=0.0662 MAE=0.0581 MAE=0.0502 MAE=0.0479 MAE=0.0472 MAE=0.0398 MAE=0.0395 Epoch: 10/150MAE=0.0351 MAE=0.0354 MAE=0.0327 MAE=0.0334 MAE=0.0306 MAE=0.0307 MAE=0.0300 MAE=0.0322 MAE=0.0288 MAE=0.0290 Epoch: 20/150MAE=0.0281 MAE=0.0274 MAE=0.0267 MAE=0.0281 MAE=0.0257 MAE=0.0271 MAE=0.0271 MAE=0.0281 MAE=0.0280 MAE=0.0264 Epoch: 30/150MAE=0.0264 MAE=0.0261 MAE=0.0252 MAE=0.0249 MAE=0.0249 MAE=0.0249 MAE=0.0248 MAE=0.0251 MAE=0.0251 MAE=0.0248 Epoch: 40/150MAE=0.0253 MAE=0.0243 MAE=0.0251 MAE=0.0241 MAE=0.0242 MAE=0.0243 MAE=0.0243 MAE=0.0241 MAE=0.0239 MAE=0.0244 Epoch: 50/150MAE=0.0246 MAE=0.0243 MAE=0.0245 MAE=0.0239 MAE=0.0237 MAE=0.0237 MAE=0.0243 MAE=0.0243 MAE=0.0239 MAE=0.0237 Epoch: 60/150MAE=0.0236 MAE=0.0235 MAE=0.0235 MAE=0.0237 MAE=0.0237 MAE=0.0236 MAE=0.0236 MAE=0.0235 MAE=0.0235 MAE=0.0233 Epoch: 70/150MAE=0.0233 MAE=0.0233 MAE=0.0234 MAE=0.0234 MAE=0.0235 MAE=0.0234 MAE=0.0232 MAE=0.0233 MAE=0.0233 MAE=0.0232 Epoch: 80/150MAE=0.0232 MAE=0.0231 MAE=0.0232 MAE=0.0232 MAE=0.0231 MAE=0.0231 MAE=0.0231 MAE=0.0231 MAE=0.0232 MAE=0.0231 Epoch: 90/150MAE=0.0232 MAE=0.0231 MAE=0.0231 MAE=0.0231 MAE=0.0231 MAE=0.0232 MAE=0.0231 MAE=0.0231 MAE=0.0232 MAE=0.0231 Epoch: 100/150MAE=0.0232 MAE=0.0231 MAE=0.0231 MAE=0.0231 MAE=0.0231 MAE=0.0231 MAE=0.0231 MAE=0.0232 MAE=0.0231 MAE=0.0231 MAE=0.0234 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.023 +/- 0.002\n",
      "\n",
      "Epoch: 1/150MAE=0.1025 MAE=0.0707 MAE=0.0505 MAE=0.0473 MAE=0.0471 MAE=0.0458 MAE=0.0435 MAE=0.0388 MAE=0.0380 Epoch: 10/150MAE=0.0348 MAE=0.0325 MAE=0.0330 MAE=0.0311 MAE=0.0299 MAE=0.0309 MAE=0.0298 MAE=0.0311 MAE=0.0274 MAE=0.0277 Epoch: 20/150MAE=0.0287 MAE=0.0276 MAE=0.0260 MAE=0.0272 MAE=0.0263 MAE=0.0259 MAE=0.0253 MAE=0.0251 MAE=0.0277 MAE=0.0265 Epoch: 30/150MAE=0.0258 MAE=0.0265 MAE=0.0247 MAE=0.0241 MAE=0.0237 MAE=0.0239 MAE=0.0245 MAE=0.0239 MAE=0.0244 MAE=0.0237 Epoch: 40/150MAE=0.0234 MAE=0.0239 MAE=0.0236 MAE=0.0232 MAE=0.0239 MAE=0.0237 MAE=0.0233 MAE=0.0239 MAE=0.0232 MAE=0.0233 Epoch: 50/150MAE=0.0231 MAE=0.0229 MAE=0.0233 MAE=0.0232 MAE=0.0230 MAE=0.0232 MAE=0.0230 MAE=0.0228 MAE=0.0228 MAE=0.0230 Epoch: 60/150MAE=0.0230 MAE=0.0231 MAE=0.0230 MAE=0.0227 MAE=0.0226 MAE=0.0227 MAE=0.0227 MAE=0.0228 MAE=0.0229 MAE=0.0228 Epoch: 70/150MAE=0.0227 MAE=0.0226 MAE=0.0226 MAE=0.0226 MAE=0.0226 MAE=0.0225 MAE=0.0225 MAE=0.0226 MAE=0.0226 MAE=0.0226 Epoch: 80/150MAE=0.0227 MAE=0.0227 MAE=0.0226 MAE=0.0225 MAE=0.0226 MAE=0.0226 MAE=0.0226 MAE=0.0231 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.023 +/- 0.002\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.1247 MAE=0.0892 MAE=0.0665 MAE=0.0587 MAE=0.0526 MAE=0.0456 MAE=0.0437 MAE=0.0392 MAE=0.0392 Epoch: 10/150MAE=0.0368 MAE=0.0352 MAE=0.0357 MAE=0.0335 MAE=0.0322 MAE=0.0311 MAE=0.0300 MAE=0.0301 MAE=0.0284 MAE=0.0278 Epoch: 20/150MAE=0.0265 MAE=0.0263 MAE=0.0259 MAE=0.0277 MAE=0.0262 MAE=0.0254 MAE=0.0257 MAE=0.0253 MAE=0.0256 MAE=0.0254 Epoch: 30/150MAE=0.0258 MAE=0.0260 MAE=0.0230 MAE=0.0233 MAE=0.0234 MAE=0.0233 MAE=0.0232 MAE=0.0226 MAE=0.0227 MAE=0.0226 Epoch: 40/150MAE=0.0225 MAE=0.0227 MAE=0.0223 MAE=0.0223 MAE=0.0226 MAE=0.0221 MAE=0.0221 MAE=0.0222 MAE=0.0223 MAE=0.0220 Epoch: 50/150MAE=0.0221 MAE=0.0225 MAE=0.0221 MAE=0.0216 MAE=0.0220 MAE=0.0215 MAE=0.0220 MAE=0.0217 MAE=0.0225 MAE=0.0217 Epoch: 60/150MAE=0.0212 MAE=0.0216 MAE=0.0214 MAE=0.0211 MAE=0.0214 MAE=0.0211 MAE=0.0211 MAE=0.0210 MAE=0.0209 MAE=0.0209 Epoch: 70/150MAE=0.0215 MAE=0.0210 MAE=0.0212 MAE=0.0207 MAE=0.0210 MAE=0.0207 MAE=0.0207 MAE=0.0208 MAE=0.0206 MAE=0.0206 Epoch: 80/150MAE=0.0207 MAE=0.0207 MAE=0.0207 MAE=0.0208 MAE=0.0206 MAE=0.0205 MAE=0.0206 MAE=0.0207 MAE=0.0209 MAE=0.0207 Epoch: 90/150MAE=0.0205 MAE=0.0204 MAE=0.0204 MAE=0.0204 MAE=0.0205 MAE=0.0204 MAE=0.0205 MAE=0.0205 MAE=0.0204 MAE=0.0205 Epoch: 100/150MAE=0.0204 MAE=0.0205 MAE=0.0207 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.021 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.1383 MAE=0.1125 MAE=0.0935 MAE=0.0813 MAE=0.0664 MAE=0.0578 MAE=0.0499 MAE=0.0444 MAE=0.0431 Epoch: 10/150MAE=0.0411 MAE=0.0413 MAE=0.0377 MAE=0.0387 MAE=0.0363 MAE=0.0345 MAE=0.0339 MAE=0.0331 MAE=0.0321 MAE=0.0310 Epoch: 20/150MAE=0.0317 MAE=0.0312 MAE=0.0312 MAE=0.0293 MAE=0.0285 MAE=0.0278 MAE=0.0284 MAE=0.0278 MAE=0.0274 MAE=0.0274 Epoch: 30/150MAE=0.0264 MAE=0.0273 MAE=0.0264 MAE=0.0274 MAE=0.0268 MAE=0.0252 MAE=0.0251 MAE=0.0252 MAE=0.0254 MAE=0.0242 Epoch: 40/150MAE=0.0248 MAE=0.0235 MAE=0.0235 MAE=0.0235 MAE=0.0238 MAE=0.0244 MAE=0.0236 MAE=0.0237 MAE=0.0230 MAE=0.0228 Epoch: 50/150MAE=0.0230 MAE=0.0229 MAE=0.0230 MAE=0.0231 MAE=0.0229 MAE=0.0225 MAE=0.0225 MAE=0.0225 MAE=0.0226 MAE=0.0224 Epoch: 60/150MAE=0.0221 MAE=0.0224 MAE=0.0224 MAE=0.0226 MAE=0.0224 MAE=0.0221 MAE=0.0218 MAE=0.0220 MAE=0.0220 MAE=0.0220 Epoch: 70/150MAE=0.0221 MAE=0.0220 MAE=0.0217 MAE=0.0217 MAE=0.0217 MAE=0.0217 MAE=0.0217 MAE=0.0217 MAE=0.0217 MAE=0.0216 Epoch: 80/150MAE=0.0216 MAE=0.0216 MAE=0.0216 MAE=0.0217 MAE=0.0217 MAE=0.0216 MAE=0.0217 MAE=0.0216 MAE=0.0215 MAE=0.0215 Epoch: 90/150MAE=0.0215 MAE=0.0215 MAE=0.0215 MAE=0.0215 MAE=0.0215 MAE=0.0214 MAE=0.0215 MAE=0.0215 MAE=0.0215 MAE=0.0216 Epoch: 100/150MAE=0.0215 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 Epoch: 110/150MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0214 MAE=0.0218 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.021 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1365 MAE=0.1085 MAE=0.0829 MAE=0.0629 MAE=0.0533 MAE=0.0507 MAE=0.0472 MAE=0.0443 MAE=0.0421 Epoch: 10/150MAE=0.0422 MAE=0.0391 MAE=0.0386 MAE=0.0367 MAE=0.0367 MAE=0.0443 MAE=0.0601 MAE=0.0382 MAE=0.0321 MAE=0.0347 Epoch: 20/150MAE=0.0305 MAE=0.0300 MAE=0.0283 MAE=0.0276 MAE=0.0272 MAE=0.0269 MAE=0.0272 MAE=0.0264 MAE=0.0264 MAE=0.0272 Epoch: 30/150MAE=0.0264 MAE=0.0261 MAE=0.0252 MAE=0.0252 MAE=0.0252 MAE=0.0246 MAE=0.0246 MAE=0.0249 MAE=0.0245 MAE=0.0240 Epoch: 40/150MAE=0.0243 MAE=0.0235 MAE=0.0236 MAE=0.0325 MAE=0.0230 MAE=0.0256 MAE=0.0286 MAE=0.0242 MAE=0.0237 MAE=0.0229 Epoch: 50/150MAE=0.0226 MAE=0.0223 MAE=0.0225 MAE=0.0221 MAE=0.0223 MAE=0.0221 MAE=0.0221 MAE=0.0224 MAE=0.0220 MAE=0.0220 Epoch: 60/150MAE=0.0219 MAE=0.0221 MAE=0.0218 MAE=0.0216 MAE=0.0217 MAE=0.0218 MAE=0.0214 MAE=0.0214 MAE=0.0213 MAE=0.0215 Epoch: 70/150MAE=0.0213 MAE=0.0214 MAE=0.0213 MAE=0.0211 MAE=0.0209 MAE=0.0210 MAE=0.0209 MAE=0.0211 MAE=0.0214 MAE=0.0208 Epoch: 80/150MAE=0.0208 MAE=0.0209 MAE=0.0211 MAE=0.0211 MAE=0.0217 MAE=0.0222 MAE=0.0219 MAE=0.0216 MAE=0.0213 MAE=0.0212 Epoch: 90/150MAE=0.0211 MAE=0.0209 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.021 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.0938 MAE=0.0627 MAE=0.0548 MAE=0.0505 MAE=0.0476 MAE=0.0445 MAE=0.0429 MAE=0.0419 MAE=0.0415 Epoch: 10/150MAE=0.0402 MAE=0.0371 MAE=0.0395 MAE=0.0374 MAE=0.0346 MAE=0.0343 MAE=0.0306 MAE=0.0342 MAE=0.0298 MAE=0.0300 Epoch: 20/150MAE=0.0298 MAE=0.0283 MAE=0.0274 MAE=0.0278 MAE=0.0260 MAE=0.0273 MAE=0.0260 MAE=0.0270 MAE=0.0243 MAE=0.0814 Epoch: 30/150MAE=0.0342 MAE=0.0256 MAE=0.0237 MAE=0.0245 MAE=0.0246 MAE=0.0237 MAE=0.0230 MAE=0.0222 MAE=0.0228 MAE=0.0223 Epoch: 40/150MAE=0.0226 MAE=0.0229 MAE=0.0220 MAE=0.0227 MAE=0.0223 MAE=0.0217 MAE=0.0223 MAE=0.0215 MAE=0.0216 MAE=0.0214 Epoch: 50/150MAE=0.0214 MAE=0.0210 MAE=0.0215 MAE=0.0212 MAE=0.0209 MAE=0.0210 MAE=0.0206 MAE=0.0208 MAE=0.0210 MAE=0.0208 Epoch: 60/150MAE=0.0205 MAE=0.0215 MAE=0.0203 MAE=0.0206 MAE=0.0205 MAE=0.0203 MAE=0.0206 MAE=0.0205 MAE=0.0202 MAE=0.0201 Epoch: 70/150MAE=0.0205 MAE=0.0201 MAE=0.0199 MAE=0.0197 MAE=0.0200 MAE=0.0201 MAE=0.0198 MAE=0.0203 MAE=0.0201 MAE=0.0201 Epoch: 80/150MAE=0.0201 MAE=0.0200 MAE=0.0198 MAE=0.0197 MAE=0.0197 MAE=0.0198 MAE=0.0198 MAE=0.0197 MAE=0.0196 MAE=0.0196 Epoch: 90/150MAE=0.0196 MAE=0.0195 MAE=0.0195 MAE=0.0196 MAE=0.0196 MAE=0.0196 MAE=0.0196 MAE=0.0195 MAE=0.0195 MAE=0.0195 Epoch: 100/150MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0194 MAE=0.0194 MAE=0.0195 MAE=0.0195 MAE=0.0196 MAE=0.0195 Epoch: 110/150MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0199 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.021 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1005 MAE=0.0755 MAE=0.0652 MAE=0.0593 MAE=0.0510 MAE=0.0468 MAE=0.0445 MAE=0.0434 MAE=0.0415 Epoch: 10/150MAE=0.0406 MAE=0.0387 MAE=0.0372 MAE=0.0355 MAE=0.0329 MAE=0.0329 MAE=0.0320 MAE=0.0302 MAE=0.0294 MAE=0.0284 Epoch: 20/150MAE=0.0292 MAE=0.0277 MAE=0.0270 MAE=0.0269 MAE=0.0262 MAE=0.0253 MAE=0.0260 MAE=0.0263 MAE=0.0252 MAE=0.0247 Epoch: 30/150MAE=0.0243 MAE=0.0237 MAE=0.0241 MAE=0.0245 MAE=0.0234 MAE=0.0238 MAE=0.0236 MAE=0.0228 MAE=0.0223 MAE=0.0224 Epoch: 40/150MAE=0.0224 MAE=0.0220 MAE=0.0229 MAE=0.0224 MAE=0.0216 MAE=0.0236 MAE=0.0226 MAE=0.0215 MAE=0.0212 MAE=0.0212 Epoch: 50/150MAE=0.0213 MAE=0.0208 MAE=0.0205 MAE=0.0214 MAE=0.0208 MAE=0.0209 MAE=0.0210 MAE=0.0198 MAE=0.0196 MAE=0.0193 Epoch: 60/150MAE=0.0194 MAE=0.0192 MAE=0.0193 MAE=0.0195 MAE=0.0193 MAE=0.0192 MAE=0.0193 MAE=0.0191 MAE=0.0192 MAE=0.0191 Epoch: 70/150MAE=0.0192 MAE=0.0191 MAE=0.0195 MAE=0.0194 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0187 Epoch: 80/150MAE=0.0186 MAE=0.0186 MAE=0.0185 MAE=0.0187 MAE=0.0185 MAE=0.0185 MAE=0.0188 MAE=0.0185 MAE=0.0184 MAE=0.0184 Epoch: 90/150MAE=0.0184 MAE=0.0186 MAE=0.0185 MAE=0.0183 MAE=0.0185 MAE=0.0183 MAE=0.0186 MAE=0.0185 MAE=0.0184 MAE=0.0184 Epoch: 100/150MAE=0.0183 MAE=0.0181 MAE=0.0182 MAE=0.0182 MAE=0.0181 MAE=0.0182 MAE=0.0181 MAE=0.0180 MAE=0.0179 MAE=0.0179 Epoch: 110/150MAE=0.0181 MAE=0.0179 MAE=0.0180 MAE=0.0179 MAE=0.0180 MAE=0.0179 MAE=0.0180 MAE=0.0180 MAE=0.0179 MAE=0.0179 Epoch: 120/150MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0178 MAE=0.0178 MAE=0.0178 Epoch: 130/150MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0179 MAE=0.0178 MAE=0.0178 MAE=0.0179 MAE=0.0179 MAE=0.0185 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.020 +/- 0.001\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150MAE=0.0947 MAE=0.0829 MAE=0.0716 MAE=0.0637 MAE=0.0577 MAE=0.0485 MAE=0.0475 MAE=0.0405 MAE=0.0417 Epoch: 10/150MAE=0.0361 MAE=0.0347 MAE=0.0326 MAE=0.0560 MAE=0.0449 MAE=0.0350 MAE=0.0307 MAE=0.0288 MAE=0.0339 MAE=0.0332 Epoch: 20/150MAE=0.0278 MAE=0.0264 MAE=0.0252 MAE=0.0251 MAE=0.0250 MAE=0.0346 MAE=0.0339 MAE=0.0257 MAE=0.0247 MAE=0.0232 Epoch: 30/150MAE=0.0235 MAE=0.0236 MAE=0.0232 MAE=0.0220 MAE=0.0227 MAE=0.0251 MAE=0.0265 MAE=0.0247 MAE=0.0228 MAE=0.0217 Epoch: 40/150MAE=0.0212 MAE=0.0215 MAE=0.0212 MAE=0.0211 MAE=0.0207 MAE=0.0205 MAE=0.0209 MAE=0.0203 MAE=0.0210 MAE=0.0207 Epoch: 50/150MAE=0.0205 MAE=0.0209 MAE=0.0198 MAE=0.0193 MAE=0.0195 MAE=0.0201 MAE=0.0195 MAE=0.0194 MAE=0.0192 MAE=0.0193 Epoch: 60/150MAE=0.0193 MAE=0.0193 MAE=0.0192 MAE=0.0191 MAE=0.0190 MAE=0.0190 MAE=0.0192 MAE=0.0191 MAE=0.0192 MAE=0.0192 Epoch: 70/150MAE=0.0190 MAE=0.0189 MAE=0.0189 MAE=0.0190 MAE=0.0189 MAE=0.0190 MAE=0.0190 MAE=0.0189 MAE=0.0188 MAE=0.0188 Epoch: 80/150MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0188 MAE=0.0189 MAE=0.0189 MAE=0.0188 MAE=0.0188 MAE=0.0187 MAE=0.0187 Epoch: 90/150MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0188 MAE=0.0188 MAE=0.0187 MAE=0.0187 Epoch: 100/150MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 Epoch: 110/150MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 Epoch: 120/150MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0187 MAE=0.0191 \n",
      "********************1's fold 1's run over********************\n",
      "MAE: 0.019 +/- 0.000\n",
      "\n",
      "Epoch: 1/150MAE=0.1154 MAE=0.0901 MAE=0.0793 MAE=0.0654 MAE=0.0547 MAE=0.0489 MAE=0.0450 MAE=0.0522 MAE=0.0495 Epoch: 10/150MAE=0.0403 MAE=0.0352 MAE=0.0355 MAE=0.0351 MAE=0.0340 MAE=0.0329 MAE=0.0335 MAE=0.0281 MAE=0.0299 MAE=0.0366 Epoch: 20/150MAE=0.0381 MAE=0.0283 MAE=0.0278 MAE=0.0354 MAE=0.0273 MAE=0.0258 MAE=0.0296 MAE=0.0340 MAE=0.0268 MAE=0.0272 Epoch: 30/150MAE=0.0253 MAE=0.0243 MAE=0.0241 MAE=0.0237 MAE=0.0235 MAE=0.0240 MAE=0.0240 MAE=0.0237 MAE=0.0233 MAE=0.0240 Epoch: 40/150MAE=0.0232 MAE=0.0227 MAE=0.0234 MAE=0.0230 MAE=0.0227 MAE=0.0227 MAE=0.0223 MAE=0.0219 MAE=0.0222 MAE=0.0221 Epoch: 50/150MAE=0.0221 MAE=0.0221 MAE=0.0219 MAE=0.0217 MAE=0.0218 MAE=0.0218 MAE=0.0217 MAE=0.0216 MAE=0.0216 MAE=0.0217 Epoch: 60/150MAE=0.0217 MAE=0.0218 MAE=0.0219 MAE=0.0218 MAE=0.0217 MAE=0.0216 MAE=0.0215 MAE=0.0215 MAE=0.0214 MAE=0.0214 Epoch: 70/150MAE=0.0214 MAE=0.0213 MAE=0.0212 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 MAE=0.0213 Epoch: 80/150MAE=0.0213 MAE=0.0213 MAE=0.0212 MAE=0.0212 MAE=0.0212 MAE=0.0212 MAE=0.0212 MAE=0.0212 MAE=0.0211 MAE=0.0211 Epoch: 90/150MAE=0.0211 MAE=0.0211 MAE=0.0211 MAE=0.0211 MAE=0.0211 MAE=0.0211 MAE=0.0211 MAE=0.0211 MAE=0.0211 MAE=0.0211 Epoch: 100/150MAE=0.0211 MAE=0.0211 MAE=0.0211 MAE=0.0211 MAE=0.0215 \n",
      "********************1's fold 2's run over********************\n",
      "MAE: 0.020 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1037 MAE=0.0929 MAE=0.0740 MAE=0.0625 MAE=0.0549 MAE=0.0472 MAE=0.0421 MAE=0.0402 MAE=0.0372 Epoch: 10/150MAE=0.0385 MAE=0.0343 MAE=0.0333 MAE=0.0323 MAE=0.0313 MAE=0.0316 MAE=0.0303 MAE=0.0293 MAE=0.0289 MAE=0.0290 Epoch: 20/150MAE=0.0300 MAE=0.0268 MAE=0.0284 MAE=0.0270 MAE=0.0294 MAE=0.0282 MAE=0.0278 MAE=0.0246 MAE=0.0243 MAE=0.0247 Epoch: 30/150MAE=0.0241 MAE=0.0229 MAE=0.0239 MAE=0.0230 MAE=0.0245 MAE=0.0232 MAE=0.0224 MAE=0.0222 MAE=0.0224 MAE=0.0221 Epoch: 40/150MAE=0.0223 MAE=0.0225 MAE=0.0218 MAE=0.0222 MAE=0.0224 MAE=0.0220 MAE=0.0219 MAE=0.0216 MAE=0.0215 MAE=0.0213 Epoch: 50/150MAE=0.0210 MAE=0.0212 MAE=0.0214 MAE=0.0212 MAE=0.0214 MAE=0.0213 MAE=0.0213 MAE=0.0212 MAE=0.0210 MAE=0.0210 Epoch: 60/150MAE=0.0209 MAE=0.0209 MAE=0.0208 MAE=0.0207 MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0208 Epoch: 70/150MAE=0.0208 MAE=0.0208 MAE=0.0208 MAE=0.0207 MAE=0.0210 \n",
      "********************1's fold 3's run over********************\n",
      "MAE: 0.021 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1036 MAE=0.0924 MAE=0.0657 MAE=0.0543 MAE=0.0480 MAE=0.0476 MAE=0.0405 MAE=0.0386 MAE=0.0382 Epoch: 10/150MAE=0.0424 MAE=0.0334 MAE=0.0908 MAE=0.0481 MAE=0.0359 MAE=0.0319 MAE=0.0291 MAE=0.0285 MAE=0.0287 MAE=0.0264 Epoch: 20/150MAE=0.0345 MAE=0.0600 MAE=0.0385 MAE=0.0295 MAE=0.0253 MAE=0.0240 MAE=0.0238 MAE=0.0239 MAE=0.0236 MAE=0.0232 Epoch: 30/150MAE=0.0230 MAE=0.0225 MAE=0.0228 MAE=0.0226 MAE=0.0220 MAE=0.0223 MAE=0.0230 MAE=0.0222 MAE=0.0214 MAE=0.0214 Epoch: 40/150MAE=0.0214 MAE=0.0215 MAE=0.0214 MAE=0.0213 MAE=0.0210 MAE=0.0210 MAE=0.0209 MAE=0.0208 MAE=0.0209 MAE=0.0207 Epoch: 50/150MAE=0.0208 MAE=0.0209 MAE=0.0208 MAE=0.0205 MAE=0.0208 MAE=0.0205 MAE=0.0206 MAE=0.0204 MAE=0.0202 MAE=0.0204 Epoch: 60/150MAE=0.0204 MAE=0.0203 MAE=0.0203 MAE=0.0202 MAE=0.0200 MAE=0.0199 MAE=0.0203 MAE=0.0204 MAE=0.0203 MAE=0.0199 Epoch: 70/150MAE=0.0200 MAE=0.0201 MAE=0.0197 MAE=0.0199 MAE=0.0202 MAE=0.0199 MAE=0.0200 MAE=0.0200 MAE=0.0199 MAE=0.0199 Epoch: 80/150MAE=0.0198 MAE=0.0198 MAE=0.0198 MAE=0.0205 \n",
      "********************1's fold 4's run over********************\n",
      "MAE: 0.021 +/- 0.001\n",
      "\n",
      "Epoch: 1/150MAE=0.1074 MAE=0.0748 MAE=0.0673 MAE=0.0671 MAE=0.0571 MAE=0.0471 MAE=0.0421 MAE=0.0377 MAE=0.0372 Epoch: 10/150MAE=0.0395 MAE=0.0359 MAE=0.0335 MAE=0.0320 MAE=0.0316 MAE=0.0672 MAE=0.0347 MAE=0.0288 MAE=0.0280 MAE=0.0264 Epoch: 20/150MAE=0.0271 MAE=0.0326 MAE=0.0244 MAE=0.0352 MAE=0.0256 MAE=0.0302 MAE=0.0320 MAE=0.0255 MAE=0.0232 MAE=0.0224 Epoch: 30/150MAE=0.0215 MAE=0.0217 MAE=0.0222 MAE=0.0219 MAE=0.0213 MAE=0.0213 MAE=0.0212 MAE=0.0218 MAE=0.0212 MAE=0.0210 Epoch: 40/150MAE=0.0210 MAE=0.0241 MAE=0.0433 MAE=0.0323 MAE=0.0240 MAE=0.0226 MAE=0.0219 MAE=0.0212 MAE=0.0211 MAE=0.0209 Epoch: 50/150MAE=0.0208 MAE=0.0207 MAE=0.0207 MAE=0.0207 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0205 MAE=0.0204 MAE=0.0205 Epoch: 60/150MAE=0.0203 MAE=0.0204 MAE=0.0202 MAE=0.0201 MAE=0.0202 MAE=0.0202 MAE=0.0201 MAE=0.0200 MAE=0.0199 MAE=0.0199 Epoch: 70/150MAE=0.0199 MAE=0.0199 MAE=0.0197 MAE=0.0198 MAE=0.0198 MAE=0.0198 MAE=0.0199 MAE=0.0200 MAE=0.0199 MAE=0.0197 Epoch: 80/150MAE=0.0197 MAE=0.0196 MAE=0.0196 MAE=0.0196 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0195 MAE=0.0195 Epoch: 90/150MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0194 MAE=0.0193 MAE=0.0194 MAE=0.0193 MAE=0.0193 MAE=0.0193 Epoch: 100/150MAE=0.0193 MAE=0.0193 MAE=0.0193 MAE=0.0193 MAE=0.0193 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 Epoch: 110/150MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 Epoch: 120/150MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0192 MAE=0.0195 \n",
      "********************1's fold 5's run over********************\n",
      "MAE: 0.020 +/- 0.001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --co_ratio=0.1 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --co_ratio=0.3 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --co_ratio=0.5 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --co_ratio=0.7 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --co_ratio=0.9 --pooling='CO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150Epoch: 40/150Epoch: 50/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.839 +/- 0.000\n",
      "AUPRC: 0.805 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150Epoch: 40/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.827 +/- 0.012\n",
      "AUPRC: 0.786 +/- 0.019\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150Epoch: 40/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.824 +/- 0.010\n",
      "AUPRC: 0.780 +/- 0.018\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150Epoch: 40/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.823 +/- 0.009\n",
      "AUPRC: 0.784 +/- 0.018\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.824 +/- 0.009\n",
      "AUPRC: 0.789 +/- 0.018\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150Epoch: 40/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.820 +/- 0.000\n",
      "AUPRC: 0.781 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.841 +/- 0.021\n",
      "AUPRC: 0.820 +/- 0.040\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150Epoch: 40/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.834 +/- 0.020\n",
      "AUPRC: 0.810 +/- 0.035\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.823 +/- 0.026\n",
      "AUPRC: 0.802 +/- 0.034\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.820 +/- 0.024\n",
      "AUPRC: 0.797 +/- 0.032\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.802 +/- 0.000\n",
      "AUPRC: 0.742 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.830 +/- 0.028\n",
      "AUPRC: 0.783 +/- 0.041\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.830 +/- 0.023\n",
      "AUPRC: 0.784 +/- 0.034\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.833 +/- 0.021\n",
      "AUPRC: 0.792 +/- 0.032\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.835 +/- 0.019\n",
      "AUPRC: 0.796 +/- 0.030\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.839 +/- 0.000\n",
      "AUPRC: 0.796 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.847 +/- 0.008\n",
      "AUPRC: 0.810 +/- 0.014\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.850 +/- 0.008\n",
      "AUPRC: 0.819 +/- 0.017\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.846 +/- 0.010\n",
      "AUPRC: 0.810 +/- 0.021\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.847 +/- 0.009\n",
      "AUPRC: 0.805 +/- 0.021\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: bace, include 1513 molecules and 1 classification tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 1's run over********************\n",
      "AUROC: 0.843 +/- 0.000\n",
      "AUPRC: 0.822 +/- 0.000\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 2's run over********************\n",
      "AUROC: 0.826 +/- 0.017\n",
      "AUPRC: 0.799 +/- 0.023\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 3's run over********************\n",
      "AUROC: 0.830 +/- 0.015\n",
      "AUPRC: 0.794 +/- 0.020\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150\n",
      "********************1's fold 4's run over********************\n",
      "AUROC: 0.833 +/- 0.014\n",
      "AUPRC: 0.801 +/- 0.021\n",
      "\n",
      "Epoch: 1/150Epoch: 10/150Epoch: 20/150Epoch: 30/150\n",
      "********************1's fold 5's run over********************\n",
      "AUROC: 0.831 +/- 0.013\n",
      "AUPRC: 0.793 +/- 0.024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --co_ratio=0.1 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --co_ratio=0.3 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --co_ratio=0.5 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --co_ratio=0.7 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --co_ratio=0.9 --pooling='CO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.5650 RMSE=1.3964 RMSE=1.4950 RMSE=1.2556 RMSE=1.2993 RMSE=1.2439 RMSE=1.2286 RMSE=1.3725 RMSE=1.1922 Epoch: 10/150RMSE=1.2696 RMSE=1.5961 RMSE=1.3705 RMSE=1.3077 RMSE=1.3156 RMSE=1.3182 RMSE=1.2859 RMSE=1.3351 RMSE=1.2906 RMSE=1.2643 Epoch: 20/150RMSE=1.2661 RMSE=1.2632 RMSE=1.1968 RMSE=1.2187 RMSE=1.2631 RMSE=1.2214 RMSE=1.2307 RMSE=1.2434 RMSE=1.2361 RMSE=1.2740 RMSE=1.5812 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.581 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.6883 RMSE=1.4866 RMSE=1.4878 RMSE=1.4642 RMSE=1.4679 RMSE=1.4483 RMSE=1.4553 RMSE=1.3822 RMSE=1.4342 Epoch: 10/150RMSE=1.4078 RMSE=1.4169 RMSE=1.3717 RMSE=1.3256 RMSE=1.3051 RMSE=1.3004 RMSE=1.3797 RMSE=1.3566 RMSE=1.2365 RMSE=1.2871 Epoch: 20/150RMSE=1.2410 RMSE=1.3291 RMSE=1.3049 RMSE=1.2286 RMSE=1.2349 RMSE=1.2469 RMSE=1.2233 RMSE=1.3533 RMSE=1.2543 RMSE=1.2702 Epoch: 30/150RMSE=1.2791 RMSE=1.2572 RMSE=1.2861 RMSE=1.3376 RMSE=1.3210 RMSE=1.2977 RMSE=1.3481 RMSE=1.3406 RMSE=1.2925 RMSE=1.2986 Epoch: 40/150RMSE=1.2737 RMSE=1.3330 RMSE=1.2641 RMSE=1.2933 RMSE=1.3035 RMSE=1.2901 RMSE=1.3180 RMSE=1.4216 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.501 +/- 0.080\n",
      "\n",
      "Epoch: 1/150RMSE=1.5613 RMSE=1.5514 RMSE=1.5072 RMSE=1.4457 RMSE=1.4481 RMSE=1.4459 RMSE=1.3622 RMSE=1.4297 RMSE=1.4631 Epoch: 10/150RMSE=1.4671 RMSE=1.4819 RMSE=1.4406 RMSE=1.4393 RMSE=1.4386 RMSE=1.4639 RMSE=1.4560 RMSE=1.4614 RMSE=1.4403 RMSE=1.4404 Epoch: 20/150RMSE=1.4363 RMSE=1.4713 RMSE=1.4230 RMSE=1.4608 RMSE=1.4355 RMSE=1.4358 RMSE=1.4553 RMSE=1.4468 RMSE=2.0102 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.671 +/- 0.249\n",
      "\n",
      "Epoch: 1/150RMSE=1.8340 RMSE=1.7612 RMSE=1.5463 RMSE=1.4440 RMSE=1.5531 RMSE=1.4104 RMSE=1.3744 RMSE=1.4669 RMSE=1.2599 Epoch: 10/150RMSE=1.3168 RMSE=1.2934 RMSE=1.2899 RMSE=1.3698 RMSE=1.3658 RMSE=1.3116 RMSE=1.3351 RMSE=1.3161 RMSE=1.3348 RMSE=1.3165 Epoch: 20/150RMSE=1.2765 RMSE=1.3169 RMSE=1.3283 RMSE=1.3479 RMSE=1.3331 RMSE=1.2783 RMSE=1.2844 RMSE=1.3183 RMSE=1.2661 RMSE=1.3580 RMSE=1.5525 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.641 +/- 0.221\n",
      "\n",
      "Epoch: 1/150RMSE=1.6744 RMSE=1.8885 RMSE=1.5157 RMSE=1.4650 RMSE=1.3450 RMSE=1.4193 RMSE=1.2925 RMSE=1.3972 RMSE=1.4817 Epoch: 10/150RMSE=1.4282 RMSE=1.4879 RMSE=1.3532 RMSE=1.5206 RMSE=1.3720 RMSE=1.4589 RMSE=1.3616 RMSE=1.3901 RMSE=1.3469 RMSE=1.3713 Epoch: 20/150RMSE=1.3962 RMSE=1.3130 RMSE=1.2998 RMSE=1.2930 RMSE=1.3029 RMSE=1.3216 RMSE=1.3373 RMSE=1.3229 RMSE=1.6231 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.638 +/- 0.198\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.6920 RMSE=1.4629 RMSE=1.5349 RMSE=1.3850 RMSE=1.3259 RMSE=1.1324 RMSE=1.2244 RMSE=1.1474 RMSE=1.1352 Epoch: 10/150RMSE=1.1779 RMSE=1.1996 RMSE=1.1843 RMSE=1.3302 RMSE=1.1606 RMSE=1.1068 RMSE=1.1632 RMSE=1.1693 RMSE=1.1794 RMSE=1.1723 Epoch: 20/150RMSE=1.1548 RMSE=1.2180 RMSE=1.1055 RMSE=1.1301 RMSE=1.1761 RMSE=1.1724 RMSE=1.0699 RMSE=1.1770 RMSE=1.1758 RMSE=1.1168 Epoch: 30/150RMSE=1.1280 RMSE=1.1073 RMSE=1.1277 RMSE=1.1260 RMSE=1.2016 RMSE=1.1567 RMSE=1.1634 RMSE=1.1287 RMSE=1.1312 RMSE=1.1195 Epoch: 40/150RMSE=1.1665 RMSE=1.1152 RMSE=1.1243 RMSE=1.1275 RMSE=1.1234 RMSE=1.1547 RMSE=1.1562 RMSE=1.2235 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.223 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.6956 RMSE=1.3491 RMSE=1.2948 RMSE=1.3084 RMSE=1.2598 RMSE=1.2270 RMSE=1.1301 RMSE=1.1599 RMSE=1.1611 Epoch: 10/150RMSE=1.2292 RMSE=1.1152 RMSE=1.1409 RMSE=1.2221 RMSE=1.1896 RMSE=1.1015 RMSE=1.2012 RMSE=1.1397 RMSE=1.1697 RMSE=1.1797 Epoch: 20/150RMSE=1.1298 RMSE=1.2024 RMSE=1.1550 RMSE=1.1049 RMSE=1.1621 RMSE=1.1260 RMSE=1.1290 RMSE=1.1415 RMSE=1.1483 RMSE=1.1170 Epoch: 30/150RMSE=1.1406 RMSE=1.1331 RMSE=1.1274 RMSE=1.1232 RMSE=1.1144 RMSE=1.1401 RMSE=1.2457 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.235 +/- 0.011\n",
      "\n",
      "Epoch: 1/150RMSE=1.4302 RMSE=1.3253 RMSE=1.2471 RMSE=1.2969 RMSE=1.2252 RMSE=1.1802 RMSE=1.2860 RMSE=1.2355 RMSE=1.2036 Epoch: 10/150RMSE=1.2026 RMSE=1.1654 RMSE=1.1729 RMSE=1.1765 RMSE=1.1864 RMSE=1.2577 RMSE=1.2173 RMSE=1.1843 RMSE=1.1989 RMSE=1.2154 Epoch: 20/150RMSE=1.1942 RMSE=1.2099 RMSE=1.1822 RMSE=1.1964 RMSE=1.2146 RMSE=1.1827 RMSE=1.2329 RMSE=1.1996 RMSE=1.2024 RMSE=1.2140 Epoch: 30/150RMSE=1.2172 RMSE=1.1879 RMSE=1.2358 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.235 +/- 0.009\n",
      "\n",
      "Epoch: 1/150RMSE=1.7020 RMSE=1.4611 RMSE=1.4980 RMSE=1.4256 RMSE=1.5677 RMSE=1.3488 RMSE=1.2808 RMSE=1.2925 RMSE=1.2868 Epoch: 10/150RMSE=1.2272 RMSE=1.2591 RMSE=1.3574 RMSE=1.2596 RMSE=1.1603 RMSE=1.2122 RMSE=1.2672 RMSE=1.2923 RMSE=1.3110 RMSE=1.2148 Epoch: 20/150RMSE=1.2149 RMSE=1.2865 RMSE=1.3262 RMSE=1.2806 RMSE=1.3035 RMSE=1.2774 RMSE=1.2915 RMSE=1.2078 RMSE=1.2776 RMSE=1.2441 Epoch: 30/150RMSE=1.2674 RMSE=1.2476 RMSE=1.2719 RMSE=1.2670 RMSE=1.2754 RMSE=1.1561 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.215 +/- 0.035\n",
      "\n",
      "Epoch: 1/150RMSE=1.5334 RMSE=1.3693 RMSE=1.2560 RMSE=1.2426 RMSE=1.3558 RMSE=1.3117 RMSE=1.2580 RMSE=1.2130 RMSE=1.2191 Epoch: 10/150RMSE=1.2321 RMSE=1.2302 RMSE=1.1770 RMSE=1.1726 RMSE=1.1070 RMSE=1.2782 RMSE=1.1299 RMSE=1.0990 RMSE=1.1205 RMSE=1.1548 Epoch: 20/150RMSE=1.1537 RMSE=1.1667 RMSE=1.1356 RMSE=1.1260 RMSE=1.1304 RMSE=1.2894 RMSE=1.1789 RMSE=1.1421 RMSE=1.1747 RMSE=1.1823 Epoch: 30/150RMSE=1.1397 RMSE=1.1447 RMSE=1.1667 RMSE=1.1841 RMSE=1.1689 RMSE=1.2875 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.230 +/- 0.043\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.4831 RMSE=1.2956 RMSE=1.2587 RMSE=1.2542 RMSE=1.2236 RMSE=1.1454 RMSE=1.2186 RMSE=1.1614 RMSE=1.2012 Epoch: 10/150RMSE=1.3217 RMSE=1.1034 RMSE=1.1283 RMSE=1.1410 RMSE=1.1270 RMSE=1.1160 RMSE=1.0868 RMSE=1.0538 RMSE=1.1156 RMSE=1.0435 Epoch: 20/150RMSE=1.0153 RMSE=1.0377 RMSE=1.0626 RMSE=1.1268 RMSE=1.0721 RMSE=1.0567 RMSE=1.0275 RMSE=1.0595 RMSE=1.0217 RMSE=1.0600 Epoch: 30/150RMSE=1.0637 RMSE=1.0939 RMSE=1.0819 RMSE=1.0691 RMSE=1.0755 RMSE=1.0742 RMSE=1.0521 RMSE=1.0784 RMSE=1.0837 RMSE=1.0623 Epoch: 40/150RMSE=1.0713 RMSE=1.0587 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.059 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.5921 RMSE=1.3983 RMSE=1.3027 RMSE=1.0799 RMSE=1.2836 RMSE=1.2273 RMSE=1.1874 RMSE=1.2071 RMSE=1.1890 Epoch: 10/150RMSE=1.1737 RMSE=1.1435 RMSE=1.1154 RMSE=1.1229 RMSE=1.1318 RMSE=1.1365 RMSE=1.1163 RMSE=1.1019 RMSE=1.1093 RMSE=1.1075 Epoch: 20/150RMSE=1.1287 RMSE=1.0970 RMSE=1.0828 RMSE=1.0773 RMSE=1.0702 RMSE=1.2944 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.177 +/- 0.118\n",
      "\n",
      "Epoch: 1/150RMSE=1.5023 RMSE=1.3323 RMSE=1.2437 RMSE=1.2977 RMSE=1.2429 RMSE=1.1326 RMSE=1.2503 RMSE=1.1887 RMSE=1.1101 Epoch: 10/150RMSE=1.0488 RMSE=1.1040 RMSE=1.1310 RMSE=1.2028 RMSE=1.1315 RMSE=1.1005 RMSE=1.0687 RMSE=1.0307 RMSE=1.0532 RMSE=1.0355 Epoch: 20/150RMSE=1.0138 RMSE=1.0637 RMSE=0.9918 RMSE=1.0132 RMSE=1.0542 RMSE=1.0774 RMSE=1.0668 RMSE=1.0327 RMSE=1.1330 RMSE=1.0535 Epoch: 30/150RMSE=1.0782 RMSE=1.0625 RMSE=1.0322 RMSE=1.0204 RMSE=1.0520 RMSE=1.0272 RMSE=1.0246 RMSE=1.0334 RMSE=1.0346 RMSE=1.0132 Epoch: 40/150RMSE=1.0336 RMSE=1.0207 RMSE=1.0336 RMSE=1.2844 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.212 +/- 0.109\n",
      "\n",
      "Epoch: 1/150RMSE=1.6431 RMSE=1.3643 RMSE=1.3642 RMSE=1.2409 RMSE=1.1576 RMSE=1.2683 RMSE=1.1054 RMSE=1.1313 RMSE=1.1715 Epoch: 10/150RMSE=1.0508 RMSE=1.1721 RMSE=1.2305 RMSE=1.2128 RMSE=1.1985 RMSE=1.2168 RMSE=1.2633 RMSE=1.2189 RMSE=1.1852 RMSE=1.1794 Epoch: 20/150RMSE=1.1321 RMSE=1.2173 RMSE=1.2315 RMSE=1.1675 RMSE=1.2057 RMSE=1.2456 RMSE=1.1872 RMSE=1.1561 RMSE=1.1619 RMSE=1.2020 Epoch: 30/150RMSE=1.1770 RMSE=1.3576 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.249 +/- 0.113\n",
      "\n",
      "Epoch: 1/150RMSE=1.9977 RMSE=1.5970 RMSE=1.2397 RMSE=1.4012 RMSE=1.2655 RMSE=1.2623 RMSE=1.4392 RMSE=1.1651 RMSE=1.1921 Epoch: 10/150RMSE=1.2783 RMSE=1.2358 RMSE=1.2834 RMSE=1.2208 RMSE=1.2064 RMSE=1.2387 RMSE=1.2228 RMSE=1.1856 RMSE=1.2120 RMSE=1.2364 Epoch: 20/150RMSE=1.2749 RMSE=1.2572 RMSE=1.2569 RMSE=1.2356 RMSE=1.2455 RMSE=1.2239 RMSE=1.2227 RMSE=1.2163 RMSE=1.2196 RMSE=1.3184 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.263 +/- 0.105\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.4957 RMSE=1.2607 RMSE=1.1564 RMSE=1.1644 RMSE=1.0826 RMSE=1.1632 RMSE=1.0000 RMSE=1.2099 RMSE=1.0175 Epoch: 10/150RMSE=1.0881 RMSE=0.9326 RMSE=0.9204 RMSE=0.9555 RMSE=0.9401 RMSE=0.8976 RMSE=0.8409 RMSE=0.8534 RMSE=0.9963 RMSE=0.9911 Epoch: 20/150RMSE=0.9703 RMSE=0.9196 RMSE=0.8892 RMSE=0.9253 RMSE=0.9258 RMSE=0.9395 RMSE=0.8397 RMSE=0.9298 RMSE=0.9376 RMSE=0.8709 Epoch: 30/150RMSE=0.8844 RMSE=0.8929 RMSE=0.8593 RMSE=0.8569 RMSE=0.8746 RMSE=0.8763 RMSE=0.8342 RMSE=0.8700 RMSE=0.8514 RMSE=0.8468 Epoch: 40/150RMSE=0.8248 RMSE=0.8939 RMSE=0.8761 RMSE=0.8845 RMSE=0.8590 RMSE=0.9035 RMSE=0.8592 RMSE=0.8725 RMSE=0.8521 RMSE=0.8540 Epoch: 50/150RMSE=0.8484 RMSE=0.8509 RMSE=0.8560 RMSE=0.8434 RMSE=0.8698 RMSE=0.8577 RMSE=0.8645 RMSE=0.8745 RMSE=0.8746 RMSE=0.8757 Epoch: 60/150RMSE=0.8658 RMSE=1.0431 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.043 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.5337 RMSE=1.4074 RMSE=1.4308 RMSE=1.2897 RMSE=1.2564 RMSE=1.3006 RMSE=1.6403 RMSE=1.2509 RMSE=1.2644 Epoch: 10/150RMSE=1.2065 RMSE=1.1803 RMSE=1.1496 RMSE=1.1109 RMSE=1.1574 RMSE=1.1884 RMSE=1.1710 RMSE=1.2402 RMSE=1.1412 RMSE=1.1002 Epoch: 20/150RMSE=1.1984 RMSE=1.1493 RMSE=1.1532 RMSE=1.1224 RMSE=1.1360 RMSE=1.1311 RMSE=1.1531 RMSE=1.1438 RMSE=1.1210 RMSE=1.1227 Epoch: 30/150RMSE=1.1272 RMSE=1.1078 RMSE=1.1267 RMSE=1.1190 RMSE=1.1163 RMSE=1.1256 RMSE=1.1103 RMSE=1.1236 RMSE=1.1049 RMSE=1.1288 Epoch: 40/150RMSE=1.1359 RMSE=1.1147 RMSE=1.1109 RMSE=1.1147 RMSE=1.1160 RMSE=1.1299 RMSE=1.1275 RMSE=1.1282 RMSE=1.1171 RMSE=1.1423 Epoch: 50/150RMSE=1.1321 RMSE=1.1202 RMSE=1.1312 RMSE=1.1230 RMSE=1.1313 RMSE=1.1225 RMSE=1.1394 RMSE=1.1369 RMSE=1.1223 RMSE=1.1767 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.110 +/- 0.067\n",
      "\n",
      "Epoch: 1/150RMSE=1.4065 RMSE=1.2353 RMSE=1.2608 RMSE=1.3147 RMSE=1.6644 RMSE=1.2148 RMSE=1.0605 RMSE=0.9516 RMSE=1.1218 Epoch: 10/150RMSE=1.1912 RMSE=1.2031 RMSE=1.3567 RMSE=1.3277 RMSE=1.0850 RMSE=1.1548 RMSE=1.0917 RMSE=1.0964 RMSE=1.1353 RMSE=1.1601 Epoch: 20/150RMSE=1.0878 RMSE=1.1821 RMSE=1.1273 RMSE=1.1363 RMSE=1.2028 RMSE=1.1657 RMSE=1.1415 RMSE=1.1555 RMSE=1.1741 RMSE=1.0197 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.080 +/- 0.069\n",
      "\n",
      "Epoch: 1/150RMSE=1.6392 RMSE=1.3610 RMSE=1.2484 RMSE=1.1527 RMSE=1.1797 RMSE=1.1780 RMSE=1.2601 RMSE=1.1977 RMSE=1.2106 Epoch: 10/150RMSE=1.1455 RMSE=1.1524 RMSE=1.1411 RMSE=1.1129 RMSE=1.1067 RMSE=1.2566 RMSE=1.0991 RMSE=1.0943 RMSE=1.1517 RMSE=1.0804 Epoch: 20/150RMSE=1.0624 RMSE=1.1958 RMSE=1.0045 RMSE=0.9899 RMSE=1.0324 RMSE=1.0292 RMSE=1.0672 RMSE=0.9896 RMSE=1.0568 RMSE=1.0552 Epoch: 30/150RMSE=1.0003 RMSE=1.0310 RMSE=1.0133 RMSE=1.0434 RMSE=1.0323 RMSE=0.9890 RMSE=0.9884 RMSE=0.9960 RMSE=0.9892 RMSE=0.9930 Epoch: 40/150RMSE=0.9868 RMSE=0.9743 RMSE=0.9839 RMSE=0.9507 RMSE=0.9629 RMSE=0.9674 RMSE=0.9895 RMSE=0.9481 RMSE=0.9740 RMSE=0.9825 Epoch: 50/150RMSE=0.9733 RMSE=0.9767 RMSE=0.9760 RMSE=0.9949 RMSE=0.9776 RMSE=0.9764 RMSE=0.9847 RMSE=0.9707 RMSE=0.9900 RMSE=1.0117 Epoch: 60/150RMSE=0.9908 RMSE=0.9972 RMSE=0.9799 RMSE=0.9918 RMSE=0.9661 RMSE=0.9914 RMSE=0.9947 RMSE=0.9673 RMSE=0.9428 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.046 +/- 0.084\n",
      "\n",
      "Epoch: 1/150RMSE=1.7370 RMSE=1.4364 RMSE=1.2212 RMSE=1.3339 RMSE=1.2841 RMSE=1.1373 RMSE=1.2702 RMSE=1.3211 RMSE=1.2845 Epoch: 10/150RMSE=1.1528 RMSE=1.1580 RMSE=1.1420 RMSE=1.1434 RMSE=1.1560 RMSE=1.1255 RMSE=1.0948 RMSE=1.0953 RMSE=1.0513 RMSE=1.0672 Epoch: 20/150RMSE=1.1145 RMSE=1.0842 RMSE=1.0902 RMSE=1.0923 RMSE=1.1404 RMSE=1.1543 RMSE=1.0728 RMSE=1.2764 RMSE=1.0574 RMSE=1.0735 Epoch: 30/150RMSE=1.0537 RMSE=1.0760 RMSE=1.0863 RMSE=1.1223 RMSE=1.1563 RMSE=1.0928 RMSE=1.0943 RMSE=1.0868 RMSE=1.0747 RMSE=1.0971 Epoch: 40/150RMSE=1.1203 RMSE=1.1225 RMSE=1.1004 RMSE=1.0529 RMSE=1.0868 RMSE=1.1015 RMSE=1.0998 RMSE=1.0918 RMSE=1.0909 RMSE=1.0881 Epoch: 50/150RMSE=1.1201 RMSE=1.1293 RMSE=1.0863 RMSE=1.1339 RMSE=1.1376 RMSE=1.1135 RMSE=1.1249 RMSE=1.0799 RMSE=1.1005 RMSE=1.0811 Epoch: 60/150RMSE=1.0956 RMSE=1.1038 RMSE=1.0991 RMSE=1.0992 RMSE=1.0818 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.053 +/- 0.077\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.5106 RMSE=1.3644 RMSE=1.3137 RMSE=1.4479 RMSE=1.1256 RMSE=1.0532 RMSE=1.0197 RMSE=1.1611 RMSE=1.0413 Epoch: 10/150RMSE=1.0941 RMSE=1.0045 RMSE=0.9329 RMSE=0.9607 RMSE=1.0225 RMSE=1.0160 RMSE=1.1535 RMSE=1.0307 RMSE=1.0219 RMSE=1.0422 Epoch: 20/150RMSE=0.9654 RMSE=1.0066 RMSE=1.0310 RMSE=1.0081 RMSE=1.0170 RMSE=1.0234 RMSE=0.9920 RMSE=0.9882 RMSE=0.9860 RMSE=0.9952 Epoch: 30/150RMSE=0.9739 RMSE=0.9837 RMSE=0.9796 RMSE=0.8877 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.888 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.3112 RMSE=1.3340 RMSE=1.4189 RMSE=1.1740 RMSE=1.2449 RMSE=1.0628 RMSE=1.0038 RMSE=1.0332 RMSE=1.0088 Epoch: 10/150RMSE=1.0580 RMSE=1.0198 RMSE=0.8953 RMSE=0.9195 RMSE=0.9152 RMSE=0.8844 RMSE=0.8813 RMSE=0.9724 RMSE=0.9870 RMSE=0.9518 Epoch: 20/150RMSE=0.8815 RMSE=0.9175 RMSE=0.9482 RMSE=0.9087 RMSE=0.8837 RMSE=0.9087 RMSE=0.8767 RMSE=0.9052 RMSE=0.9387 RMSE=0.9158 Epoch: 30/150RMSE=0.9026 RMSE=0.9345 RMSE=0.9356 RMSE=0.9099 RMSE=0.9086 RMSE=0.9022 RMSE=0.9254 RMSE=0.9089 RMSE=0.9487 RMSE=0.9303 Epoch: 40/150RMSE=0.9226 RMSE=0.9241 RMSE=0.9370 RMSE=0.9338 RMSE=0.9324 RMSE=0.9400 RMSE=0.9354 RMSE=1.0144 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 0.951 +/- 0.063\n",
      "\n",
      "Epoch: 1/150RMSE=1.5046 RMSE=1.1927 RMSE=1.4123 RMSE=1.0559 RMSE=1.0709 RMSE=1.0753 RMSE=1.0361 RMSE=0.9525 RMSE=0.8266 Epoch: 10/150RMSE=0.9544 RMSE=0.9411 RMSE=1.0074 RMSE=0.8790 RMSE=0.8902 RMSE=0.8877 RMSE=0.8189 RMSE=0.9329 RMSE=0.8794 RMSE=0.8817 Epoch: 20/150RMSE=0.8684 RMSE=0.9066 RMSE=0.9266 RMSE=0.8997 RMSE=0.8963 RMSE=0.9278 RMSE=0.8975 RMSE=0.9017 RMSE=0.8984 RMSE=0.9007 Epoch: 30/150RMSE=0.9099 RMSE=0.9149 RMSE=0.8937 RMSE=0.9245 RMSE=0.9150 RMSE=0.9001 RMSE=0.9217 RMSE=0.9066 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 0.936 +/- 0.056\n",
      "\n",
      "Epoch: 1/150RMSE=1.7024 RMSE=1.3544 RMSE=1.3714 RMSE=1.4327 RMSE=1.3157 RMSE=1.0532 RMSE=1.1276 RMSE=1.1839 RMSE=1.1627 Epoch: 10/150RMSE=1.1996 RMSE=1.0765 RMSE=1.1385 RMSE=1.1688 RMSE=1.0420 RMSE=1.0552 RMSE=1.0625 RMSE=1.0813 RMSE=1.0219 RMSE=1.0451 Epoch: 20/150RMSE=1.0896 RMSE=1.0521 RMSE=1.0186 RMSE=1.0404 RMSE=1.0372 RMSE=1.0332 RMSE=1.0223 RMSE=1.1182 RMSE=1.0443 RMSE=0.9768 Epoch: 30/150RMSE=0.9924 RMSE=1.0368 RMSE=1.0300 RMSE=1.0507 RMSE=0.9954 RMSE=0.9908 RMSE=1.0489 RMSE=1.0416 RMSE=1.0390 RMSE=1.0362 Epoch: 40/150RMSE=1.0398 RMSE=1.0768 RMSE=1.0727 RMSE=1.0786 RMSE=1.0733 RMSE=1.0495 RMSE=1.0669 RMSE=1.0533 RMSE=1.0487 RMSE=1.0451 RMSE=1.0319 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 0.960 +/- 0.064\n",
      "\n",
      "Epoch: 1/150RMSE=1.3966 RMSE=1.6291 RMSE=1.6741 RMSE=1.4404 RMSE=1.4544 RMSE=1.2372 RMSE=1.2080 RMSE=1.2558 RMSE=1.4018 Epoch: 10/150RMSE=1.2476 RMSE=1.3502 RMSE=1.2641 RMSE=1.2446 RMSE=1.1061 RMSE=1.0606 RMSE=1.0551 RMSE=1.1225 RMSE=1.1245 RMSE=1.1407 Epoch: 20/150RMSE=1.1538 RMSE=1.1267 RMSE=1.1144 RMSE=1.1011 RMSE=1.1523 RMSE=1.1468 RMSE=1.1268 RMSE=1.1252 RMSE=1.1549 RMSE=1.1315 Epoch: 30/150RMSE=1.1163 RMSE=1.1250 RMSE=1.1354 RMSE=1.1478 RMSE=1.1189 RMSE=1.1275 RMSE=1.1457 RMSE=1.0574 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 0.980 +/- 0.069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --co_ratio=0.1 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --co_ratio=0.3 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --co_ratio=0.5 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --co_ratio=0.7 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --co_ratio=0.9 --pooling='CO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freesolv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.1588 RMSE=4.3907 RMSE=3.6676 RMSE=2.7378 RMSE=2.4986 RMSE=2.4437 RMSE=2.4213 RMSE=2.1056 RMSE=2.5684 Epoch: 10/150RMSE=2.4880 RMSE=2.4404 RMSE=2.6973 RMSE=2.2776 RMSE=2.1642 RMSE=2.2796 RMSE=2.0681 RMSE=2.1840 RMSE=2.1889 RMSE=2.1902 Epoch: 20/150RMSE=2.2006 RMSE=2.2405 RMSE=2.3109 RMSE=2.2345 RMSE=2.1170 RMSE=2.0191 RMSE=1.9778 RMSE=2.0128 RMSE=2.0126 RMSE=2.1002 Epoch: 30/150RMSE=2.0785 RMSE=2.1043 RMSE=2.0690 RMSE=2.0217 RMSE=2.2268 RMSE=2.0828 RMSE=2.1348 RMSE=2.1767 RMSE=2.0472 RMSE=2.2173 Epoch: 40/150RMSE=2.1714 RMSE=2.2160 RMSE=2.1820 RMSE=2.1831 RMSE=2.1295 RMSE=2.2319 RMSE=2.1703 RMSE=2.8630 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.863 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.2355 RMSE=4.2974 RMSE=2.9535 RMSE=2.3861 RMSE=2.1795 RMSE=2.5151 RMSE=2.1888 RMSE=2.3978 RMSE=2.0328 Epoch: 10/150RMSE=2.2001 RMSE=2.1805 RMSE=1.9508 RMSE=2.1620 RMSE=1.7163 RMSE=1.7804 RMSE=2.2704 RMSE=2.1881 RMSE=2.1240 RMSE=1.8902 Epoch: 20/150RMSE=2.0498 RMSE=2.0101 RMSE=1.8836 RMSE=1.8119 RMSE=1.8839 RMSE=1.8247 RMSE=1.8217 RMSE=1.8494 RMSE=1.8619 RMSE=1.9647 Epoch: 30/150RMSE=1.9367 RMSE=1.9498 RMSE=1.9075 RMSE=2.0243 RMSE=1.9491 RMSE=2.2971 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 2.580 +/- 0.283\n",
      "\n",
      "Epoch: 1/150RMSE=5.3044 RMSE=4.7444 RMSE=3.6924 RMSE=2.7259 RMSE=2.3212 RMSE=2.2414 RMSE=2.1458 RMSE=2.3536 RMSE=2.3202 Epoch: 10/150RMSE=1.9814 RMSE=2.0560 RMSE=1.9275 RMSE=1.8633 RMSE=2.0310 RMSE=1.8694 RMSE=2.2479 RMSE=2.0232 RMSE=1.8492 RMSE=1.7429 Epoch: 20/150RMSE=1.8580 RMSE=1.8490 RMSE=1.7452 RMSE=1.7345 RMSE=1.8694 RMSE=1.9135 RMSE=1.7610 RMSE=1.8977 RMSE=1.7462 RMSE=1.7897 Epoch: 30/150RMSE=1.8876 RMSE=1.8274 RMSE=1.8275 RMSE=1.8091 RMSE=1.8257 RMSE=1.7728 RMSE=1.8276 RMSE=1.8198 RMSE=1.8340 RMSE=1.7493 Epoch: 40/150RMSE=1.8211 RMSE=1.8580 RMSE=1.7407 RMSE=1.7843 RMSE=2.2030 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 2.454 +/- 0.291\n",
      "\n",
      "Epoch: 1/150RMSE=5.1105 RMSE=3.8892 RMSE=2.7658 RMSE=2.5066 RMSE=2.3354 RMSE=1.9467 RMSE=2.1858 RMSE=2.1880 RMSE=1.9907 Epoch: 10/150RMSE=2.2773 RMSE=1.9376 RMSE=1.9544 RMSE=1.9013 RMSE=1.8392 RMSE=1.8937 RMSE=1.9304 RMSE=2.0015 RMSE=2.0468 RMSE=1.9654 Epoch: 20/150RMSE=1.9633 RMSE=2.0409 RMSE=1.8619 RMSE=2.0108 RMSE=2.0671 RMSE=2.0147 RMSE=2.0849 RMSE=2.0216 RMSE=1.9829 RMSE=1.9726 Epoch: 30/150RMSE=2.0339 RMSE=2.0018 RMSE=1.9651 RMSE=2.0220 RMSE=2.0053 RMSE=3.0094 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 2.593 +/- 0.349\n",
      "\n",
      "Epoch: 1/150RMSE=5.1324 RMSE=4.2241 RMSE=3.6212 RMSE=3.4137 RMSE=2.9393 RMSE=3.0738 RMSE=3.2014 RMSE=3.2185 RMSE=2.8512 Epoch: 10/150RMSE=3.1983 RMSE=2.7983 RMSE=2.8552 RMSE=2.7404 RMSE=2.7994 RMSE=2.3875 RMSE=2.7158 RMSE=2.6152 RMSE=2.8590 RMSE=2.4988 Epoch: 20/150RMSE=2.5871 RMSE=2.6318 RMSE=2.5045 RMSE=2.6206 RMSE=2.7092 RMSE=2.5922 RMSE=2.5290 RMSE=2.5708 RMSE=2.6700 RMSE=2.5285 Epoch: 30/150RMSE=2.6034 RMSE=2.6204 RMSE=2.5570 RMSE=2.5832 RMSE=2.5567 RMSE=2.5246 RMSE=3.4681 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 2.768 +/- 0.469\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.2502 RMSE=4.4564 RMSE=3.2337 RMSE=2.4552 RMSE=2.5735 RMSE=2.2663 RMSE=2.3412 RMSE=2.1670 RMSE=2.0583 Epoch: 10/150RMSE=2.0260 RMSE=1.8389 RMSE=1.9125 RMSE=1.7984 RMSE=1.7139 RMSE=2.1746 RMSE=1.8282 RMSE=2.0310 RMSE=1.6370 RMSE=2.0012 Epoch: 20/150RMSE=1.9716 RMSE=1.8047 RMSE=1.9264 RMSE=1.6632 RMSE=1.6253 RMSE=1.4725 RMSE=1.6636 RMSE=1.5165 RMSE=1.4319 RMSE=1.6942 Epoch: 30/150RMSE=1.5324 RMSE=1.6478 RMSE=1.5090 RMSE=1.5165 RMSE=1.6135 RMSE=1.5106 RMSE=1.6503 RMSE=1.4964 RMSE=1.5437 RMSE=1.5693 Epoch: 40/150RMSE=1.6099 RMSE=1.5762 RMSE=1.5938 RMSE=1.6572 RMSE=1.7596 RMSE=1.4917 RMSE=1.5055 RMSE=1.5066 RMSE=1.7710 RMSE=2.3452 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.345 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.2060 RMSE=4.5671 RMSE=3.2135 RMSE=2.3624 RMSE=2.4439 RMSE=2.2487 RMSE=2.4350 RMSE=2.3253 RMSE=2.2063 Epoch: 10/150RMSE=2.1618 RMSE=2.1434 RMSE=2.2778 RMSE=2.2141 RMSE=1.8333 RMSE=2.0839 RMSE=2.1168 RMSE=1.9069 RMSE=2.4399 RMSE=1.9958 Epoch: 20/150RMSE=1.9202 RMSE=2.2101 RMSE=2.0932 RMSE=2.0696 RMSE=1.7971 RMSE=2.2390 RMSE=2.0493 RMSE=1.9747 RMSE=1.8525 RMSE=2.0354 Epoch: 30/150RMSE=1.8745 RMSE=1.9985 RMSE=1.9643 RMSE=2.0421 RMSE=1.9151 RMSE=1.9450 RMSE=1.8658 RMSE=1.8964 RMSE=1.8564 RMSE=1.9012 Epoch: 40/150RMSE=1.8024 RMSE=1.9026 RMSE=1.8950 RMSE=1.9468 RMSE=1.8084 RMSE=2.2693 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 2.307 +/- 0.038\n",
      "\n",
      "Epoch: 1/150RMSE=5.2587 RMSE=4.6023 RMSE=3.4268 RMSE=2.5547 RMSE=2.0187 RMSE=2.1714 RMSE=1.8887 RMSE=2.0271 RMSE=2.9961 Epoch: 10/150RMSE=2.1321 RMSE=2.1827 RMSE=2.0663 RMSE=1.8553 RMSE=1.7420 RMSE=1.8896 RMSE=1.7227 RMSE=1.9163 RMSE=2.0586 RMSE=1.9865 Epoch: 20/150RMSE=2.0212 RMSE=1.8511 RMSE=1.8365 RMSE=1.8748 RMSE=1.9073 RMSE=2.1485 RMSE=1.9879 RMSE=1.9945 RMSE=2.0261 RMSE=1.9902 Epoch: 30/150RMSE=1.9145 RMSE=2.0093 RMSE=1.9595 RMSE=1.9213 RMSE=2.0467 RMSE=2.0130 RMSE=1.9579 RMSE=3.7957 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 2.803 +/- 0.702\n",
      "\n",
      "Epoch: 1/150RMSE=5.2851 RMSE=4.7365 RMSE=3.0630 RMSE=2.4115 RMSE=2.2111 RMSE=2.2537 RMSE=2.1701 RMSE=2.2109 RMSE=2.4905 Epoch: 10/150RMSE=2.0569 RMSE=2.1488 RMSE=2.2757 RMSE=2.1093 RMSE=1.9501 RMSE=1.9758 RMSE=2.4202 RMSE=1.9065 RMSE=2.1684 RMSE=1.8517 Epoch: 20/150RMSE=2.0213 RMSE=1.9087 RMSE=2.1978 RMSE=1.7958 RMSE=2.2608 RMSE=2.1245 RMSE=1.8872 RMSE=2.2636 RMSE=1.9051 RMSE=1.8662 Epoch: 30/150RMSE=1.7515 RMSE=1.8892 RMSE=1.8665 RMSE=1.7960 RMSE=2.0891 RMSE=1.8928 RMSE=1.8585 RMSE=1.9262 RMSE=2.0886 RMSE=1.9891 Epoch: 40/150RMSE=1.9761 RMSE=1.9400 RMSE=1.9265 RMSE=1.9715 RMSE=1.9510 RMSE=1.9492 RMSE=2.0632 RMSE=1.9427 RMSE=2.0330 RMSE=2.0323 Epoch: 50/150RMSE=2.0131 RMSE=2.3207 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 2.683 +/- 0.643\n",
      "\n",
      "Epoch: 1/150RMSE=5.1472 RMSE=4.3695 RMSE=3.3399 RMSE=2.6612 RMSE=2.1534 RMSE=1.9976 RMSE=2.0196 RMSE=2.0310 RMSE=1.9189 Epoch: 10/150RMSE=1.9256 RMSE=1.8653 RMSE=2.4743 RMSE=2.0882 RMSE=1.6368 RMSE=2.6248 RMSE=1.7050 RMSE=1.7287 RMSE=1.7509 RMSE=1.6490 Epoch: 20/150RMSE=1.5027 RMSE=1.6304 RMSE=1.6471 RMSE=1.6581 RMSE=1.5600 RMSE=1.6339 RMSE=1.4949 RMSE=1.5686 RMSE=1.7386 RMSE=1.6203 Epoch: 30/150RMSE=1.6703 RMSE=1.6971 RMSE=1.6692 RMSE=1.7049 RMSE=1.7680 RMSE=1.6475 RMSE=1.6723 RMSE=1.7039 RMSE=1.6739 RMSE=1.6890 Epoch: 40/150RMSE=1.7066 RMSE=1.6726 RMSE=1.7042 RMSE=1.7393 RMSE=1.7063 RMSE=1.6565 RMSE=1.6933 RMSE=1.8217 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 2.510 +/- 0.670\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.2070 RMSE=4.4605 RMSE=3.5588 RMSE=2.8628 RMSE=2.5514 RMSE=2.4212 RMSE=2.3031 RMSE=2.8436 RMSE=2.5625 Epoch: 10/150RMSE=2.7709 RMSE=2.3099 RMSE=2.2567 RMSE=2.3319 RMSE=2.2314 RMSE=2.3446 RMSE=2.0638 RMSE=2.2799 RMSE=2.4345 RMSE=2.1872 Epoch: 20/150RMSE=2.1645 RMSE=2.1257 RMSE=2.2083 RMSE=2.1359 RMSE=2.1045 RMSE=2.2018 RMSE=2.1834 RMSE=2.1255 RMSE=2.1509 RMSE=2.0518 Epoch: 30/150RMSE=2.0301 RMSE=2.0746 RMSE=2.1949 RMSE=2.1760 RMSE=2.1108 RMSE=2.1383 RMSE=2.1807 RMSE=2.2463 RMSE=2.1783 RMSE=2.1472 Epoch: 40/150RMSE=2.1447 RMSE=2.1633 RMSE=2.1966 RMSE=2.1550 RMSE=2.2025 RMSE=2.1907 RMSE=2.1621 RMSE=2.1805 RMSE=2.1599 RMSE=2.1383 Epoch: 50/150RMSE=2.1421 RMSE=2.3882 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.388 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.0715 RMSE=4.0849 RMSE=2.9859 RMSE=2.1662 RMSE=2.3873 RMSE=2.1747 RMSE=1.9801 RMSE=1.9185 RMSE=1.9323 Epoch: 10/150RMSE=1.9015 RMSE=2.0053 RMSE=2.2779 RMSE=1.8637 RMSE=2.3509 RMSE=2.1817 RMSE=2.0483 RMSE=1.9067 RMSE=1.9017 RMSE=1.7742 Epoch: 20/150RMSE=1.7043 RMSE=1.8074 RMSE=1.6738 RMSE=1.7255 RMSE=1.8536 RMSE=1.6818 RMSE=1.7538 RMSE=1.7033 RMSE=1.7809 RMSE=1.6716 Epoch: 30/150RMSE=1.7150 RMSE=1.7247 RMSE=1.6606 RMSE=1.6178 RMSE=1.7082 RMSE=1.7356 RMSE=1.7512 RMSE=1.7505 RMSE=1.7401 RMSE=1.6958 Epoch: 40/150RMSE=1.7006 RMSE=1.7238 RMSE=1.8361 RMSE=1.7211 RMSE=1.7163 RMSE=1.7905 RMSE=1.7388 RMSE=1.7245 RMSE=1.7175 RMSE=1.7810 Epoch: 50/150RMSE=1.6850 RMSE=1.6972 RMSE=1.6624 RMSE=1.7498 RMSE=2.0939 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 2.241 +/- 0.147\n",
      "\n",
      "Epoch: 1/150RMSE=5.0479 RMSE=4.2691 RMSE=3.1421 RMSE=2.4383 RMSE=2.1530 RMSE=2.0601 RMSE=2.2028 RMSE=2.2175 RMSE=2.1753 Epoch: 10/150RMSE=1.9751 RMSE=2.5318 RMSE=1.9320 RMSE=1.9237 RMSE=2.0555 RMSE=1.8511 RMSE=2.0403 RMSE=1.9276 RMSE=1.7205 RMSE=2.0034 Epoch: 20/150RMSE=1.8177 RMSE=1.8852 RMSE=1.8210 RMSE=1.7719 RMSE=1.7744 RMSE=1.7751 RMSE=1.8687 RMSE=1.8066 RMSE=1.6708 RMSE=1.6990 Epoch: 30/150RMSE=1.7959 RMSE=1.8449 RMSE=1.9009 RMSE=1.8129 RMSE=1.8122 RMSE=1.8333 RMSE=1.8150 RMSE=1.8746 RMSE=1.8594 RMSE=1.7806 Epoch: 40/150RMSE=1.7783 RMSE=1.7673 RMSE=1.7462 RMSE=1.7757 RMSE=1.8511 RMSE=1.7978 RMSE=1.8300 RMSE=1.8216 RMSE=1.8047 RMSE=1.7943 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 2.092 +/- 0.242\n",
      "\n",
      "Epoch: 1/150RMSE=5.1407 RMSE=4.5978 RMSE=3.7490 RMSE=3.0454 RMSE=2.8638 RMSE=2.7475 RMSE=2.4248 RMSE=2.7408 RMSE=2.5831 Epoch: 10/150RMSE=2.8835 RMSE=2.4688 RMSE=2.5810 RMSE=2.3692 RMSE=2.5327 RMSE=2.2832 RMSE=2.3711 RMSE=2.4630 RMSE=2.4970 RMSE=2.2356 Epoch: 20/150RMSE=2.6067 RMSE=2.7465 RMSE=2.8605 RMSE=2.6171 RMSE=2.6143 RMSE=2.2626 RMSE=2.4246 RMSE=2.3343 RMSE=2.4969 RMSE=2.4191 Epoch: 30/150RMSE=2.6862 RMSE=2.3572 RMSE=2.4122 RMSE=2.3408 RMSE=2.4420 RMSE=2.5760 RMSE=2.3987 RMSE=2.4102 RMSE=2.6475 RMSE=2.6072 RMSE=3.1216 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 2.349 +/- 0.493\n",
      "\n",
      "Epoch: 1/150RMSE=5.1495 RMSE=4.5874 RMSE=3.2426 RMSE=2.5288 RMSE=2.6285 RMSE=2.5177 RMSE=2.2244 RMSE=1.8213 RMSE=1.7248 Epoch: 10/150RMSE=1.9391 RMSE=1.9407 RMSE=2.0463 RMSE=1.8158 RMSE=1.5193 RMSE=1.6817 RMSE=1.5663 RMSE=1.6580 RMSE=1.6705 RMSE=1.4780 Epoch: 20/150RMSE=1.6056 RMSE=1.4863 RMSE=1.7341 RMSE=1.5232 RMSE=1.4736 RMSE=1.6336 RMSE=1.4516 RMSE=1.4866 RMSE=1.4676 RMSE=1.7507 Epoch: 30/150RMSE=1.5066 RMSE=1.4552 RMSE=1.4341 RMSE=1.6037 RMSE=1.6720 RMSE=1.5110 RMSE=1.5914 RMSE=1.5616 RMSE=1.5678 RMSE=1.5723 Epoch: 40/150RMSE=1.5412 RMSE=1.5149 RMSE=1.5483 RMSE=1.5729 RMSE=1.5353 RMSE=1.5623 RMSE=1.5337 RMSE=1.5175 RMSE=1.5471 RMSE=1.5418 Epoch: 50/150RMSE=1.5603 RMSE=1.5975 RMSE=1.5055 RMSE=2.1108 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 2.302 +/- 0.451\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.2191 RMSE=4.5669 RMSE=3.0793 RMSE=2.8605 RMSE=2.3850 RMSE=1.9573 RMSE=2.1371 RMSE=2.0421 RMSE=1.8279 Epoch: 10/150RMSE=1.7567 RMSE=1.6433 RMSE=1.8599 RMSE=1.7728 RMSE=1.6638 RMSE=1.8533 RMSE=1.8057 RMSE=1.6239 RMSE=1.6289 RMSE=1.7149 Epoch: 20/150RMSE=1.6481 RMSE=1.6312 RMSE=1.6772 RMSE=1.7014 RMSE=1.4801 RMSE=1.5309 RMSE=1.6251 RMSE=1.6968 RMSE=1.6288 RMSE=1.5295 Epoch: 30/150RMSE=1.5972 RMSE=1.5693 RMSE=1.6504 RMSE=1.5535 RMSE=1.4563 RMSE=1.5138 RMSE=1.6288 RMSE=1.5731 RMSE=1.6038 RMSE=1.6180 Epoch: 40/150RMSE=1.5363 RMSE=1.5556 RMSE=1.5875 RMSE=1.5657 RMSE=1.5901 RMSE=1.5608 RMSE=1.5259 RMSE=1.5612 RMSE=1.5571 RMSE=1.5091 Epoch: 50/150RMSE=1.5928 RMSE=1.5617 RMSE=1.5807 RMSE=1.5007 RMSE=1.5262 RMSE=1.6486 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.649 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.0278 RMSE=4.4212 RMSE=3.7003 RMSE=3.3470 RMSE=3.0175 RMSE=2.8150 RMSE=2.6662 RMSE=2.8044 RMSE=2.4009 Epoch: 10/150RMSE=2.6057 RMSE=2.4130 RMSE=2.1867 RMSE=2.2545 RMSE=2.5717 RMSE=2.3959 RMSE=2.2165 RMSE=2.3698 RMSE=2.3109 RMSE=2.2421 Epoch: 20/150RMSE=2.2922 RMSE=2.3557 RMSE=2.3029 RMSE=2.1787 RMSE=2.2391 RMSE=2.2543 RMSE=2.2430 RMSE=2.1640 RMSE=2.1506 RMSE=2.4122 Epoch: 30/150RMSE=2.2758 RMSE=2.3332 RMSE=2.1238 RMSE=2.1523 RMSE=2.3529 RMSE=2.1696 RMSE=2.3108 RMSE=2.4153 RMSE=2.2880 RMSE=2.2463 Epoch: 40/150RMSE=2.1588 RMSE=2.0564 RMSE=2.0971 RMSE=2.1984 RMSE=2.1869 RMSE=2.2752 RMSE=2.2266 RMSE=2.2070 RMSE=2.2407 RMSE=2.2259 Epoch: 50/150RMSE=2.3350 RMSE=2.3859 RMSE=2.3311 RMSE=2.4054 RMSE=2.3729 RMSE=2.3431 RMSE=2.3403 RMSE=2.3370 RMSE=2.2090 RMSE=2.3919 Epoch: 60/150RMSE=2.3825 RMSE=2.3425 RMSE=2.2235 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.936 +/- 0.287\n",
      "\n",
      "Epoch: 1/150RMSE=4.9875 RMSE=3.4244 RMSE=2.6905 RMSE=2.5802 RMSE=2.7785 RMSE=2.4940 RMSE=2.4974 RMSE=2.2954 RMSE=2.1147 Epoch: 10/150RMSE=2.0471 RMSE=1.7913 RMSE=1.6863 RMSE=1.8165 RMSE=1.8917 RMSE=1.7477 RMSE=1.7108 RMSE=1.8019 RMSE=1.6191 RMSE=1.5954 Epoch: 20/150RMSE=1.5688 RMSE=1.6100 RMSE=1.5468 RMSE=1.6744 RMSE=1.6041 RMSE=1.4806 RMSE=1.5326 RMSE=1.5759 RMSE=1.6281 RMSE=1.4529 Epoch: 30/150RMSE=1.6720 RMSE=1.4473 RMSE=1.5419 RMSE=1.8066 RMSE=1.5348 RMSE=1.6038 RMSE=1.6405 RMSE=1.4864 RMSE=1.5628 RMSE=1.4617 Epoch: 40/150RMSE=1.5518 RMSE=1.4801 RMSE=1.4756 RMSE=1.4500 RMSE=1.4532 RMSE=1.4318 RMSE=1.4671 RMSE=1.4874 RMSE=1.5233 RMSE=1.4395 Epoch: 50/150RMSE=1.4867 RMSE=1.4141 RMSE=1.4755 RMSE=1.5173 RMSE=1.4950 RMSE=1.4662 RMSE=1.4629 RMSE=1.4800 RMSE=1.4328 RMSE=1.5353 Epoch: 60/150RMSE=1.4692 RMSE=1.4391 RMSE=1.5375 RMSE=1.4547 RMSE=1.4511 RMSE=1.5424 RMSE=1.5130 RMSE=1.4559 RMSE=1.4703 RMSE=1.5555 Epoch: 70/150RMSE=1.4175 RMSE=1.4781 RMSE=1.8233 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.898 +/- 0.241\n",
      "\n",
      "Epoch: 1/150RMSE=5.0558 RMSE=4.1911 RMSE=3.5584 RMSE=2.8500 RMSE=2.4970 RMSE=2.4477 RMSE=2.2421 RMSE=2.2993 RMSE=2.6828 Epoch: 10/150RMSE=2.3878 RMSE=2.3225 RMSE=2.3129 RMSE=2.2866 RMSE=2.1265 RMSE=2.3176 RMSE=2.0710 RMSE=2.3223 RMSE=2.1007 RMSE=2.0695 Epoch: 20/150RMSE=2.0887 RMSE=2.4313 RMSE=2.2286 RMSE=2.1054 RMSE=2.0760 RMSE=2.1014 RMSE=2.0700 RMSE=1.9666 RMSE=2.0426 RMSE=2.0489 Epoch: 30/150RMSE=2.1757 RMSE=2.0164 RMSE=1.9912 RMSE=2.0544 RMSE=2.0793 RMSE=2.0194 RMSE=2.0858 RMSE=2.0427 RMSE=2.1639 RMSE=2.0356 Epoch: 40/150RMSE=2.1399 RMSE=2.0930 RMSE=2.0971 RMSE=2.1586 RMSE=2.0742 RMSE=2.1623 RMSE=2.0533 RMSE=2.0897 RMSE=1.9881 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.921 +/- 0.212\n",
      "\n",
      "Epoch: 1/150RMSE=4.9987 RMSE=4.3099 RMSE=2.9381 RMSE=2.5870 RMSE=2.1904 RMSE=2.2690 RMSE=2.1596 RMSE=1.9024 RMSE=2.0535 Epoch: 10/150RMSE=2.0131 RMSE=1.7962 RMSE=1.9702 RMSE=1.9484 RMSE=1.7936 RMSE=1.7094 RMSE=2.5433 RMSE=2.0659 RMSE=1.7716 RMSE=1.9075 Epoch: 20/150RMSE=1.9647 RMSE=1.6396 RMSE=1.7021 RMSE=1.8579 RMSE=1.8528 RMSE=1.7159 RMSE=1.7369 RMSE=1.7075 RMSE=1.6320 RMSE=1.7866 Epoch: 30/150RMSE=1.7630 RMSE=1.7408 RMSE=1.6596 RMSE=1.6337 RMSE=1.6631 RMSE=1.7156 RMSE=1.6931 RMSE=1.6559 RMSE=1.6884 RMSE=1.7122 Epoch: 40/150RMSE=1.6848 RMSE=1.6325 RMSE=1.6764 RMSE=1.7248 RMSE=1.7436 RMSE=1.7092 RMSE=1.6941 RMSE=1.7367 RMSE=1.7179 RMSE=1.5638 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.849 +/- 0.237\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=4.8673 RMSE=3.9399 RMSE=2.9501 RMSE=2.1364 RMSE=2.0008 RMSE=1.9283 RMSE=1.8615 RMSE=1.9267 RMSE=1.9694 Epoch: 10/150RMSE=1.8155 RMSE=1.7867 RMSE=1.7878 RMSE=1.5519 RMSE=1.5721 RMSE=1.4331 RMSE=1.5268 RMSE=1.5700 RMSE=1.4814 RMSE=1.4525 Epoch: 20/150RMSE=1.4659 RMSE=1.2867 RMSE=1.4434 RMSE=1.2891 RMSE=1.3745 RMSE=1.5399 RMSE=1.3626 RMSE=1.4128 RMSE=1.3095 RMSE=1.2656 Epoch: 30/150RMSE=1.4260 RMSE=1.3953 RMSE=1.3920 RMSE=1.2991 RMSE=1.3015 RMSE=1.3089 RMSE=1.2319 RMSE=1.3047 RMSE=1.3496 RMSE=1.3624 Epoch: 40/150RMSE=1.2983 RMSE=1.3095 RMSE=1.2793 RMSE=1.2985 RMSE=1.3142 RMSE=1.2814 RMSE=1.3281 RMSE=1.3073 RMSE=1.3202 RMSE=1.3127 Epoch: 50/150RMSE=1.2385 RMSE=1.3012 RMSE=1.3051 RMSE=1.2604 RMSE=1.2742 RMSE=1.2708 RMSE=1.2588 RMSE=2.4994 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.499 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=4.8672 RMSE=3.8158 RMSE=3.0116 RMSE=2.2699 RMSE=1.9667 RMSE=1.9311 RMSE=2.2286 RMSE=2.0613 RMSE=1.9670 Epoch: 10/150RMSE=1.6779 RMSE=1.6996 RMSE=1.6022 RMSE=1.5535 RMSE=1.6634 RMSE=1.5297 RMSE=1.6321 RMSE=2.1267 RMSE=1.8019 RMSE=1.6772 Epoch: 20/150RMSE=1.5749 RMSE=1.5308 RMSE=1.4946 RMSE=1.4795 RMSE=1.5363 RMSE=1.4796 RMSE=1.3109 RMSE=1.4923 RMSE=1.4776 RMSE=1.4777 Epoch: 30/150RMSE=1.6512 RMSE=1.4947 RMSE=1.5051 RMSE=1.5313 RMSE=1.5170 RMSE=1.5141 RMSE=1.5299 RMSE=1.5472 RMSE=1.4501 RMSE=1.4592 Epoch: 40/150RMSE=1.5081 RMSE=1.4654 RMSE=1.4672 RMSE=1.4286 RMSE=1.4294 RMSE=1.4787 RMSE=1.4654 RMSE=1.4550 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.977 +/- 0.522\n",
      "\n",
      "Epoch: 1/150RMSE=4.8358 RMSE=4.0571 RMSE=2.5861 RMSE=2.1856 RMSE=2.1258 RMSE=1.6350 RMSE=1.9792 RMSE=1.5046 RMSE=1.6031 Epoch: 10/150RMSE=1.6579 RMSE=1.7564 RMSE=1.7645 RMSE=1.3430 RMSE=1.3661 RMSE=1.4910 RMSE=1.5119 RMSE=1.3626 RMSE=1.3536 RMSE=1.3268 Epoch: 20/150RMSE=1.3340 RMSE=1.3048 RMSE=1.3501 RMSE=1.3648 RMSE=1.4006 RMSE=1.3315 RMSE=1.3244 RMSE=1.3688 RMSE=1.2737 RMSE=1.3265 Epoch: 30/150RMSE=1.2678 RMSE=1.3597 RMSE=1.3658 RMSE=1.2832 RMSE=1.3422 RMSE=1.3600 RMSE=1.3107 RMSE=1.3532 RMSE=1.2833 RMSE=1.2918 Epoch: 40/150RMSE=1.3335 RMSE=1.3483 RMSE=1.3326 RMSE=1.3549 RMSE=1.3689 RMSE=1.3776 RMSE=1.3194 RMSE=1.3007 RMSE=1.3032 RMSE=1.3731 Epoch: 50/150RMSE=1.3424 RMSE=2.0190 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.991 +/- 0.427\n",
      "\n",
      "Epoch: 1/150RMSE=5.2508 RMSE=4.5472 RMSE=3.8969 RMSE=3.0187 RMSE=2.4865 RMSE=2.3974 RMSE=2.0821 RMSE=1.7452 RMSE=1.9103 Epoch: 10/150RMSE=2.0450 RMSE=1.8686 RMSE=1.9097 RMSE=1.6691 RMSE=1.5149 RMSE=1.7514 RMSE=1.7370 RMSE=1.6924 RMSE=1.5712 RMSE=1.5872 Epoch: 20/150RMSE=1.6894 RMSE=1.7170 RMSE=1.5203 RMSE=1.4917 RMSE=1.6335 RMSE=1.7212 RMSE=1.5878 RMSE=1.6486 RMSE=1.6061 RMSE=1.6104 Epoch: 30/150RMSE=1.6927 RMSE=1.4645 RMSE=1.6021 RMSE=1.6244 RMSE=1.6041 RMSE=1.5300 RMSE=1.5533 RMSE=1.6237 RMSE=1.7172 RMSE=1.6020 Epoch: 40/150RMSE=1.7132 RMSE=1.5864 RMSE=1.4839 RMSE=1.7103 RMSE=1.5946 RMSE=1.5789 RMSE=1.5693 RMSE=1.7114 RMSE=1.7208 RMSE=1.6284 Epoch: 50/150RMSE=1.5806 RMSE=1.7187 RMSE=3.8316 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 2.451 +/- 0.878\n",
      "\n",
      "Epoch: 1/150RMSE=5.0530 RMSE=4.0816 RMSE=2.5367 RMSE=2.3686 RMSE=2.4744 RMSE=2.4177 RMSE=2.0891 RMSE=1.7647 RMSE=2.0417 Epoch: 10/150RMSE=1.8681 RMSE=1.8967 RMSE=1.7047 RMSE=1.7612 RMSE=1.8147 RMSE=1.6203 RMSE=1.7657 RMSE=1.7626 RMSE=1.6138 RMSE=1.8311 Epoch: 20/150RMSE=1.8352 RMSE=1.7711 RMSE=1.9279 RMSE=1.7184 RMSE=1.6123 RMSE=1.7447 RMSE=1.7127 RMSE=1.5993 RMSE=1.5315 RMSE=1.6374 Epoch: 30/150RMSE=1.7229 RMSE=1.5855 RMSE=1.6342 RMSE=1.6984 RMSE=1.6246 RMSE=1.5776 RMSE=1.5782 RMSE=1.5518 RMSE=1.5575 RMSE=1.5146 Epoch: 40/150RMSE=1.6047 RMSE=1.5828 RMSE=1.5602 RMSE=1.6089 RMSE=1.5772 RMSE=1.5943 RMSE=1.5944 RMSE=1.5393 RMSE=1.6130 RMSE=1.5585 Epoch: 50/150RMSE=1.5105 RMSE=1.5234 RMSE=1.5550 RMSE=1.5525 RMSE=1.5646 RMSE=1.5959 RMSE=1.5486 RMSE=1.6019 RMSE=1.5943 RMSE=1.5456 Epoch: 60/150RMSE=1.5637 RMSE=1.5702 RMSE=1.5150 RMSE=1.5464 RMSE=1.5271 RMSE=1.6052 RMSE=1.5903 RMSE=1.5139 RMSE=1.5574 RMSE=1.5490 Epoch: 70/150RMSE=1.5632 RMSE=1.8659 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 2.334 +/- 0.820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --co_ratio=0.1 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --co_ratio=0.3 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --co_ratio=0.5 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --co_ratio=0.7 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --co_ratio=0.9 --pooling='CO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipophilicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.3593 RMSE=1.2427 RMSE=1.2660 RMSE=1.2301 RMSE=1.2611 RMSE=1.2509 RMSE=1.2021 RMSE=1.1906 RMSE=1.1400 Epoch: 10/150RMSE=1.1342 RMSE=1.1542 RMSE=1.1523 RMSE=1.1465 RMSE=1.0978 RMSE=1.1577 RMSE=1.1104 RMSE=1.1353 RMSE=1.1129 RMSE=1.0916 Epoch: 20/150RMSE=1.1026 RMSE=1.0944 RMSE=1.0712 RMSE=1.0703 RMSE=1.0657 RMSE=1.0636 RMSE=1.0781 RMSE=1.0864 RMSE=1.0710 RMSE=1.0875 Epoch: 30/150RMSE=1.0698 RMSE=1.0596 RMSE=1.0501 RMSE=1.0690 RMSE=1.0708 RMSE=1.0640 RMSE=1.0479 RMSE=1.0811 RMSE=1.0705 RMSE=1.0581 Epoch: 40/150RMSE=1.0524 RMSE=1.0406 RMSE=1.0556 RMSE=1.0578 RMSE=1.0616 RMSE=1.0426 RMSE=1.0540 RMSE=1.0499 RMSE=1.0555 RMSE=1.0471 Epoch: 50/150RMSE=1.0437 RMSE=1.0509 RMSE=1.0485 RMSE=1.0538 RMSE=1.0325 RMSE=1.0466 RMSE=1.0416 RMSE=1.0567 RMSE=1.0471 RMSE=1.0398 Epoch: 60/150RMSE=1.0520 RMSE=1.0422 RMSE=1.0637 RMSE=1.0616 RMSE=1.0557 RMSE=1.0531 RMSE=1.0480 RMSE=1.0506 RMSE=1.0571 RMSE=1.0576 Epoch: 70/150RMSE=1.0532 RMSE=1.0574 RMSE=1.0576 RMSE=1.0550 RMSE=1.0508 RMSE=0.9718 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.972 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.2790 RMSE=1.2701 RMSE=1.2655 RMSE=1.2544 RMSE=1.2659 RMSE=1.2607 RMSE=1.2100 RMSE=1.2140 RMSE=1.2195 Epoch: 10/150RMSE=1.2161 RMSE=1.2062 RMSE=1.2237 RMSE=1.2320 RMSE=1.1946 RMSE=1.2232 RMSE=1.2315 RMSE=1.2818 RMSE=1.1877 RMSE=1.2100 Epoch: 20/150RMSE=1.2041 RMSE=1.1882 RMSE=1.1999 RMSE=1.2095 RMSE=1.2027 RMSE=1.2183 RMSE=1.1766 RMSE=1.1675 RMSE=1.1859 RMSE=1.1940 Epoch: 30/150RMSE=1.1345 RMSE=1.1733 RMSE=1.1588 RMSE=1.1418 RMSE=1.1778 RMSE=1.1353 RMSE=1.1509 RMSE=1.1498 RMSE=1.1509 RMSE=1.1460 Epoch: 40/150RMSE=1.1681 RMSE=1.1598 RMSE=1.1734 RMSE=1.1402 RMSE=1.1443 RMSE=1.1487 RMSE=1.1555 RMSE=1.1695 RMSE=1.1593 RMSE=1.1607 Epoch: 50/150RMSE=1.1557 RMSE=1.1036 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.038 +/- 0.066\n",
      "\n",
      "Epoch: 1/150RMSE=1.2425 RMSE=1.2918 RMSE=1.2691 RMSE=1.2345 RMSE=1.2548 RMSE=1.2582 RMSE=1.2085 RMSE=1.1965 RMSE=1.2058 Epoch: 10/150RMSE=1.2257 RMSE=1.2028 RMSE=1.2093 RMSE=1.1829 RMSE=1.1726 RMSE=1.1667 RMSE=1.1719 RMSE=1.1719 RMSE=1.1725 RMSE=1.1616 Epoch: 20/150RMSE=1.1511 RMSE=1.1792 RMSE=1.1580 RMSE=1.1633 RMSE=1.1681 RMSE=1.1545 RMSE=1.3223 RMSE=1.1395 RMSE=1.1530 RMSE=1.1443 Epoch: 30/150RMSE=1.1253 RMSE=1.1387 RMSE=1.1317 RMSE=1.1524 RMSE=1.1302 RMSE=1.1432 RMSE=1.1490 RMSE=1.1232 RMSE=1.1417 RMSE=1.1204 Epoch: 40/150RMSE=1.1175 RMSE=1.1235 RMSE=1.1410 RMSE=1.1358 RMSE=1.1428 RMSE=1.1487 RMSE=1.1557 RMSE=1.1422 RMSE=1.1486 RMSE=1.1464 Epoch: 50/150RMSE=1.1528 RMSE=1.1447 RMSE=1.1444 RMSE=1.1398 RMSE=1.1529 RMSE=1.1397 RMSE=1.1501 RMSE=1.1410 RMSE=1.1530 RMSE=1.1318 Epoch: 60/150RMSE=1.1386 RMSE=1.0740 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.050 +/- 0.056\n",
      "\n",
      "Epoch: 1/150RMSE=1.2560 RMSE=1.2569 RMSE=1.2385 RMSE=1.2402 RMSE=1.2227 RMSE=1.2478 RMSE=1.2548 RMSE=1.2847 RMSE=1.2759 Epoch: 10/150RMSE=1.2610 RMSE=1.2968 RMSE=1.2540 RMSE=1.3172 RMSE=1.2752 RMSE=1.2764 RMSE=1.2730 RMSE=1.2450 RMSE=1.2709 RMSE=1.2730 Epoch: 20/150RMSE=1.2398 RMSE=1.2370 RMSE=1.2517 RMSE=1.2292 RMSE=1.2563 RMSE=1.2506 RMSE=1.1402 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 1.072 +/- 0.063\n",
      "\n",
      "Epoch: 1/150RMSE=1.3170 RMSE=1.3554 RMSE=1.2838 RMSE=1.2700 RMSE=1.2141 RMSE=1.2123 RMSE=1.2984 RMSE=1.1922 RMSE=1.2116 Epoch: 10/150RMSE=1.2232 RMSE=1.1766 RMSE=1.2030 RMSE=1.1963 RMSE=1.2092 RMSE=1.1698 RMSE=1.2066 RMSE=1.1670 RMSE=1.1621 RMSE=1.1745 Epoch: 20/150RMSE=1.1627 RMSE=1.1451 RMSE=1.1486 RMSE=1.1624 RMSE=1.2204 RMSE=2.0284 RMSE=1.2176 RMSE=1.1621 RMSE=1.1172 RMSE=1.1496 Epoch: 30/150RMSE=1.1288 RMSE=1.1590 RMSE=1.1082 RMSE=1.1424 RMSE=1.1592 RMSE=1.1824 RMSE=1.1862 RMSE=1.1540 RMSE=1.1497 RMSE=1.1726 Epoch: 40/150RMSE=1.1804 RMSE=1.1725 RMSE=1.1729 RMSE=1.1659 RMSE=1.1559 RMSE=1.1586 RMSE=1.1748 RMSE=1.1379 RMSE=1.1456 RMSE=1.1722 Epoch: 50/150RMSE=1.1604 RMSE=1.1298 RMSE=1.1381 RMSE=1.0708 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 1.072 +/- 0.056\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.3995 RMSE=1.2770 RMSE=1.1723 RMSE=1.1585 RMSE=1.1174 RMSE=1.0988 RMSE=1.1243 RMSE=1.1208 RMSE=1.0596 Epoch: 10/150RMSE=1.0484 RMSE=1.0611 RMSE=1.0210 RMSE=1.0194 RMSE=1.0117 RMSE=1.0080 RMSE=1.0046 RMSE=0.9911 RMSE=0.9969 RMSE=0.9930 Epoch: 20/150RMSE=1.0052 RMSE=0.9831 RMSE=1.0025 RMSE=1.0347 RMSE=0.9928 RMSE=0.9878 RMSE=0.9763 RMSE=0.9670 RMSE=0.9554 RMSE=0.9654 Epoch: 30/150RMSE=0.9773 RMSE=0.9592 RMSE=0.9780 RMSE=0.9583 RMSE=0.9598 RMSE=0.9596 RMSE=0.9601 RMSE=0.9548 RMSE=0.9421 RMSE=0.9426 Epoch: 40/150RMSE=0.9533 RMSE=0.9505 RMSE=0.9503 RMSE=0.9521 RMSE=0.9532 RMSE=0.9522 RMSE=0.9428 RMSE=0.9448 RMSE=0.9426 RMSE=0.9458 Epoch: 50/150RMSE=0.9486 RMSE=0.9455 RMSE=0.9440 RMSE=0.9467 RMSE=0.9510 RMSE=0.9469 RMSE=0.9493 RMSE=0.9504 RMSE=0.9504 RMSE=0.9295 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.930 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.4982 RMSE=1.3359 RMSE=1.2583 RMSE=1.2061 RMSE=1.2026 RMSE=1.1368 RMSE=1.1474 RMSE=1.1091 RMSE=1.1191 Epoch: 10/150RMSE=1.1481 RMSE=1.0966 RMSE=1.1159 RMSE=1.0530 RMSE=1.0660 RMSE=1.0944 RMSE=1.0555 RMSE=1.0585 RMSE=1.0403 RMSE=1.0269 Epoch: 20/150RMSE=1.0175 RMSE=1.0136 RMSE=1.0502 RMSE=1.0521 RMSE=1.0239 RMSE=1.0198 RMSE=1.0200 RMSE=1.0258 RMSE=1.0163 RMSE=1.0123 Epoch: 30/150RMSE=1.0169 RMSE=1.0255 RMSE=0.9997 RMSE=1.0132 RMSE=1.0016 RMSE=1.0219 RMSE=1.0056 RMSE=0.9941 RMSE=0.9927 RMSE=1.0025 Epoch: 40/150RMSE=0.9917 RMSE=1.0007 RMSE=0.9984 RMSE=0.9967 RMSE=0.9973 RMSE=0.9937 RMSE=0.9969 RMSE=1.0035 RMSE=0.9959 RMSE=1.0015 Epoch: 50/150RMSE=1.0028 RMSE=0.9917 RMSE=0.9993 RMSE=0.9980 RMSE=1.0065 RMSE=0.9989 RMSE=0.9968 RMSE=1.0101 RMSE=1.0031 RMSE=1.0055 Epoch: 60/150RMSE=0.9919 RMSE=1.0020 RMSE=1.0070 RMSE=1.0132 RMSE=1.0124 RMSE=1.0037 RMSE=1.0031 RMSE=1.0062 RMSE=1.0045 RMSE=1.0100 Epoch: 70/150RMSE=1.0055 RMSE=1.0090 RMSE=0.9527 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 0.941 +/- 0.012\n",
      "\n",
      "Epoch: 1/150RMSE=1.4450 RMSE=1.3364 RMSE=1.3149 RMSE=1.3131 RMSE=1.2086 RMSE=1.2013 RMSE=1.2005 RMSE=1.2067 RMSE=1.1974 Epoch: 10/150RMSE=1.2246 RMSE=1.1556 RMSE=1.2031 RMSE=1.1911 RMSE=1.1422 RMSE=1.1655 RMSE=1.1091 RMSE=1.4859 RMSE=1.1456 RMSE=1.1488 Epoch: 20/150RMSE=1.1777 RMSE=1.0974 RMSE=1.1043 RMSE=1.0946 RMSE=1.1131 RMSE=1.1184 RMSE=1.0863 RMSE=1.0956 RMSE=1.1066 RMSE=1.1639 Epoch: 30/150RMSE=1.1483 RMSE=1.1096 RMSE=1.1286 RMSE=1.1106 RMSE=1.1252 RMSE=1.1154 RMSE=1.1037 RMSE=1.1088 RMSE=1.1119 RMSE=1.0964 Epoch: 40/150RMSE=1.1013 RMSE=1.1028 RMSE=1.1036 RMSE=1.1053 RMSE=1.1065 RMSE=1.0981 RMSE=1.1129 RMSE=1.0235 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 0.969 +/- 0.040\n",
      "\n",
      "Epoch: 1/150RMSE=1.5220 RMSE=1.3420 RMSE=1.2640 RMSE=1.2358 RMSE=1.2183 RMSE=1.2687 RMSE=1.2277 RMSE=1.1931 RMSE=1.1491 Epoch: 10/150RMSE=1.1938 RMSE=1.1666 RMSE=1.1299 RMSE=1.1472 RMSE=1.1513 RMSE=1.1417 RMSE=1.1527 RMSE=1.0825 RMSE=1.0899 RMSE=1.0631 Epoch: 20/150RMSE=1.0768 RMSE=1.0532 RMSE=1.0787 RMSE=1.0927 RMSE=1.0796 RMSE=1.0561 RMSE=1.0484 RMSE=1.0518 RMSE=1.0354 RMSE=1.0524 Epoch: 30/150RMSE=1.0501 RMSE=1.0502 RMSE=1.0689 RMSE=1.0497 RMSE=1.0585 RMSE=1.0475 RMSE=1.0724 RMSE=1.0629 RMSE=1.0559 RMSE=1.0622 Epoch: 40/150RMSE=1.0468 RMSE=1.0592 RMSE=1.0620 RMSE=1.0544 RMSE=1.0575 RMSE=1.0600 RMSE=1.0701 RMSE=1.0674 RMSE=1.0702 RMSE=1.0315 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 0.984 +/- 0.044\n",
      "\n",
      "Epoch: 1/150RMSE=1.4440 RMSE=1.2676 RMSE=1.2181 RMSE=1.1415 RMSE=1.1538 RMSE=1.1572 RMSE=1.1351 RMSE=1.1112 RMSE=1.0894 Epoch: 10/150RMSE=1.1216 RMSE=1.0634 RMSE=1.1011 RMSE=1.0811 RMSE=1.0746 RMSE=1.0565 RMSE=1.0813 RMSE=1.0502 RMSE=1.0764 RMSE=1.0069 Epoch: 20/150RMSE=1.0525 RMSE=1.0365 RMSE=1.0341 RMSE=1.0214 RMSE=1.0158 RMSE=0.9961 RMSE=0.9853 RMSE=1.0186 RMSE=0.9932 RMSE=0.9957 Epoch: 30/150RMSE=0.9916 RMSE=0.9831 RMSE=0.9922 RMSE=0.9695 RMSE=0.9751 RMSE=0.9746 RMSE=0.9721 RMSE=0.9860 RMSE=0.9695 RMSE=0.9640 Epoch: 40/150RMSE=0.9863 RMSE=0.9711 RMSE=0.9826 RMSE=0.9851 RMSE=0.9720 RMSE=0.9956 RMSE=0.9883 RMSE=0.9816 RMSE=0.9733 RMSE=0.9897 Epoch: 50/150RMSE=0.9789 RMSE=0.9901 RMSE=0.9884 RMSE=0.9848 RMSE=0.9841 RMSE=0.9883 RMSE=0.9891 RMSE=0.9921 RMSE=0.9925 RMSE=0.9913 RMSE=0.9684 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 0.981 +/- 0.040\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.4883 RMSE=1.2627 RMSE=1.2427 RMSE=1.2505 RMSE=1.2616 RMSE=1.1950 RMSE=1.2729 RMSE=1.1798 RMSE=1.1992 Epoch: 10/150RMSE=1.1441 RMSE=1.1481 RMSE=1.0693 RMSE=1.0555 RMSE=1.0877 RMSE=1.0462 RMSE=1.0164 RMSE=1.0892 RMSE=1.0388 RMSE=1.0275 Epoch: 20/150RMSE=1.0636 RMSE=1.0142 RMSE=1.0018 RMSE=1.0134 RMSE=1.0256 RMSE=1.0312 RMSE=1.0234 RMSE=0.9849 RMSE=0.9858 RMSE=0.9900 Epoch: 30/150RMSE=0.9831 RMSE=0.9941 RMSE=0.9866 RMSE=0.9970 RMSE=0.9861 RMSE=0.9802 RMSE=0.9864 RMSE=0.9993 RMSE=0.9769 RMSE=0.9919 Epoch: 40/150RMSE=1.0027 RMSE=0.9901 RMSE=0.9800 RMSE=0.9945 RMSE=0.9917 RMSE=0.9946 RMSE=0.9959 RMSE=0.9999 RMSE=0.9940 RMSE=0.9979 Epoch: 50/150RMSE=0.9945 RMSE=0.9972 RMSE=0.9985 RMSE=0.9981 RMSE=1.0007 RMSE=0.9901 RMSE=0.9970 RMSE=0.9959 RMSE=0.9983 RMSE=0.9192 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.919 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.3707 RMSE=1.2043 RMSE=1.1685 RMSE=1.0939 RMSE=1.0881 RMSE=1.0843 RMSE=1.0417 RMSE=1.0256 RMSE=1.0466 Epoch: 10/150RMSE=1.0805 RMSE=0.9992 RMSE=1.0213 RMSE=0.9899 RMSE=0.9833 RMSE=1.0114 RMSE=0.9835 RMSE=0.9996 RMSE=0.9849 RMSE=0.9378 Epoch: 20/150RMSE=0.9411 RMSE=0.9181 RMSE=0.9447 RMSE=0.9622 RMSE=0.9546 RMSE=0.9374 RMSE=0.9271 RMSE=0.9159 RMSE=0.9173 RMSE=0.9205 Epoch: 30/150RMSE=0.9434 RMSE=0.9202 RMSE=0.9162 RMSE=0.9213 RMSE=0.9107 RMSE=0.9221 RMSE=0.9122 RMSE=0.9072 RMSE=0.9291 RMSE=0.9206 Epoch: 40/150RMSE=0.9362 RMSE=0.9192 RMSE=0.9122 RMSE=0.9208 RMSE=0.9158 RMSE=0.9214 RMSE=0.9238 RMSE=0.9215 RMSE=0.9272 RMSE=0.9179 Epoch: 50/150RMSE=0.9170 RMSE=0.9195 RMSE=0.9120 RMSE=0.9209 RMSE=0.9171 RMSE=0.9196 RMSE=0.9254 RMSE=0.9206 RMSE=0.8672 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 0.893 +/- 0.026\n",
      "\n",
      "Epoch: 1/150RMSE=1.4039 RMSE=1.3389 RMSE=1.2872 RMSE=1.2575 RMSE=1.2384 RMSE=1.2145 RMSE=1.2637 RMSE=1.2183 RMSE=1.2125 Epoch: 10/150RMSE=1.1784 RMSE=1.1706 RMSE=1.1916 RMSE=1.1698 RMSE=1.1368 RMSE=1.1347 RMSE=1.1388 RMSE=1.1294 RMSE=1.1658 RMSE=1.1773 Epoch: 20/150RMSE=1.1177 RMSE=1.1483 RMSE=1.1307 RMSE=1.0825 RMSE=1.1074 RMSE=1.1293 RMSE=1.0932 RMSE=1.1157 RMSE=1.0817 RMSE=1.1009 Epoch: 30/150RMSE=1.0833 RMSE=1.0638 RMSE=1.1232 RMSE=1.0938 RMSE=1.1260 RMSE=1.1314 RMSE=1.0689 RMSE=1.0567 RMSE=1.0763 RMSE=1.0814 Epoch: 40/150RMSE=1.0664 RMSE=1.0804 RMSE=1.0715 RMSE=1.0757 RMSE=1.0871 RMSE=1.0552 RMSE=1.0712 RMSE=1.0527 RMSE=1.0687 RMSE=1.0755 Epoch: 50/150RMSE=1.0584 RMSE=1.0654 RMSE=1.0671 RMSE=1.0597 RMSE=1.0591 RMSE=1.0711 RMSE=1.0738 RMSE=1.0876 RMSE=1.0844 RMSE=1.0858 Epoch: 60/150RMSE=1.0861 RMSE=1.1020 RMSE=1.0840 RMSE=1.0842 RMSE=1.0770 RMSE=1.0825 RMSE=1.0860 RMSE=1.0933 RMSE=1.0198 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 0.935 +/- 0.063\n",
      "\n",
      "Epoch: 1/150RMSE=1.3689 RMSE=1.2241 RMSE=1.1071 RMSE=1.1113 RMSE=1.0688 RMSE=1.0655 RMSE=1.0926 RMSE=1.0432 RMSE=1.0166 Epoch: 10/150RMSE=0.9764 RMSE=0.9606 RMSE=0.9779 RMSE=0.9677 RMSE=0.9384 RMSE=0.9438 RMSE=1.0685 RMSE=0.9188 RMSE=0.9442 RMSE=0.9225 Epoch: 20/150RMSE=0.8976 RMSE=0.9329 RMSE=0.8999 RMSE=0.9410 RMSE=0.9241 RMSE=0.9173 RMSE=0.9176 RMSE=0.8914 RMSE=0.8990 RMSE=0.9114 Epoch: 30/150RMSE=0.9213 RMSE=0.9046 RMSE=0.9092 RMSE=0.9171 RMSE=0.9023 RMSE=0.9277 RMSE=0.8896 RMSE=0.8919 RMSE=0.8932 RMSE=0.8957 Epoch: 40/150RMSE=0.9056 RMSE=0.9032 RMSE=0.8939 RMSE=0.9019 RMSE=0.8961 RMSE=0.8921 RMSE=0.8987 RMSE=0.8963 RMSE=0.8978 RMSE=0.8968 Epoch: 50/150RMSE=0.8935 RMSE=0.9000 RMSE=0.9026 RMSE=0.9049 RMSE=0.9041 RMSE=0.9019 RMSE=0.8959 RMSE=0.8266 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 0.908 +/- 0.072\n",
      "\n",
      "Epoch: 1/150RMSE=1.5167 RMSE=1.2870 RMSE=1.1839 RMSE=1.1478 RMSE=1.0965 RMSE=1.0646 RMSE=1.0686 RMSE=1.0213 RMSE=1.0144 Epoch: 10/150RMSE=1.0231 RMSE=0.9828 RMSE=0.9671 RMSE=0.9590 RMSE=0.9566 RMSE=0.9707 RMSE=1.0279 RMSE=0.9631 RMSE=1.0078 RMSE=0.9360 Epoch: 20/150RMSE=0.9333 RMSE=0.9223 RMSE=0.9159 RMSE=0.9306 RMSE=0.9108 RMSE=0.9022 RMSE=0.9155 RMSE=0.9325 RMSE=0.9342 RMSE=0.9285 Epoch: 30/150RMSE=0.9299 RMSE=0.9155 RMSE=0.9245 RMSE=0.9501 RMSE=0.9112 RMSE=0.9150 RMSE=0.8947 RMSE=0.9076 RMSE=0.9067 RMSE=0.9211 Epoch: 40/150RMSE=0.8922 RMSE=0.9398 RMSE=0.9127 RMSE=0.9252 RMSE=0.9219 RMSE=0.9139 RMSE=0.9079 RMSE=0.9319 RMSE=0.9151 RMSE=0.9075 Epoch: 50/150RMSE=0.9089 RMSE=0.9044 RMSE=0.9137 RMSE=0.9088 RMSE=0.9040 RMSE=0.9057 RMSE=0.9117 RMSE=0.9097 RMSE=0.9126 RMSE=0.9227 Epoch: 60/150RMSE=0.9148 RMSE=0.8928 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 0.905 +/- 0.065\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.4279 RMSE=1.3064 RMSE=1.2507 RMSE=1.2075 RMSE=1.1765 RMSE=1.1740 RMSE=1.1300 RMSE=1.1299 RMSE=1.1948 Epoch: 10/150RMSE=1.1241 RMSE=1.1055 RMSE=1.1248 RMSE=1.1386 RMSE=1.1626 RMSE=1.1051 RMSE=1.0682 RMSE=1.0887 RMSE=1.0741 RMSE=1.0648 Epoch: 20/150RMSE=1.0323 RMSE=1.0467 RMSE=1.0851 RMSE=1.0554 RMSE=1.1702 RMSE=1.0241 RMSE=1.0277 RMSE=1.0255 RMSE=1.0414 RMSE=1.0464 Epoch: 30/150RMSE=1.0325 RMSE=1.0284 RMSE=1.0234 RMSE=1.0327 RMSE=1.0283 RMSE=1.0208 RMSE=1.0345 RMSE=1.0242 RMSE=1.0193 RMSE=1.0358 Epoch: 40/150RMSE=1.0312 RMSE=1.0322 RMSE=1.0254 RMSE=1.0274 RMSE=1.0292 RMSE=1.0194 RMSE=1.0388 RMSE=1.0278 RMSE=1.0336 RMSE=1.0419 Epoch: 50/150RMSE=1.0267 RMSE=1.0266 RMSE=1.0306 RMSE=1.0293 RMSE=1.0270 RMSE=1.0263 RMSE=1.0244 RMSE=1.0268 RMSE=1.0262 RMSE=0.9646 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.965 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.3630 RMSE=1.2566 RMSE=1.1070 RMSE=1.0707 RMSE=0.9686 RMSE=0.9692 RMSE=1.0256 RMSE=0.9464 RMSE=0.9675 Epoch: 10/150RMSE=0.9385 RMSE=0.9253 RMSE=0.9667 RMSE=0.9535 RMSE=0.9874 RMSE=0.9324 RMSE=0.8617 RMSE=0.8664 RMSE=0.8787 RMSE=0.8566 Epoch: 20/150RMSE=0.8595 RMSE=0.8854 RMSE=0.8520 RMSE=0.8767 RMSE=0.9285 RMSE=0.9041 RMSE=0.8894 RMSE=0.8782 RMSE=0.8658 RMSE=0.8622 Epoch: 30/150RMSE=0.8654 RMSE=0.8599 RMSE=0.8668 RMSE=0.8666 RMSE=0.8650 RMSE=0.8701 RMSE=0.8679 RMSE=0.8656 RMSE=0.8604 RMSE=0.8591 Epoch: 40/150RMSE=0.8518 RMSE=0.8588 RMSE=0.8508 RMSE=0.8569 RMSE=0.8502 RMSE=0.8496 RMSE=0.8567 RMSE=0.8524 RMSE=0.8502 RMSE=0.8518 Epoch: 50/150RMSE=0.8479 RMSE=0.8508 RMSE=0.8508 RMSE=0.8521 RMSE=0.8522 RMSE=0.8483 RMSE=0.8479 RMSE=0.8442 RMSE=0.8531 RMSE=0.8498 Epoch: 60/150RMSE=0.8504 RMSE=0.8547 RMSE=0.8474 RMSE=0.8556 RMSE=0.8491 RMSE=0.8506 RMSE=0.8558 RMSE=0.8497 RMSE=0.8559 RMSE=0.8538 Epoch: 70/150RMSE=0.8543 RMSE=0.8572 RMSE=0.8510 RMSE=0.8561 RMSE=0.8586 RMSE=0.8537 RMSE=0.8546 RMSE=0.8577 RMSE=0.7766 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 0.871 +/- 0.094\n",
      "\n",
      "Epoch: 1/150RMSE=1.4144 RMSE=1.3530 RMSE=1.2855 RMSE=1.1597 RMSE=1.1620 RMSE=1.1674 RMSE=1.1629 RMSE=1.1414 RMSE=1.1637 Epoch: 10/150RMSE=1.0862 RMSE=1.1765 RMSE=1.0754 RMSE=1.0818 RMSE=0.9892 RMSE=1.0226 RMSE=1.1245 RMSE=1.0330 RMSE=1.0937 RMSE=3.7152 Epoch: 20/150RMSE=1.0547 RMSE=1.0218 RMSE=0.9915 RMSE=1.0027 RMSE=1.0102 RMSE=1.0301 RMSE=1.0283 RMSE=1.0008 RMSE=0.9886 RMSE=1.0087 Epoch: 30/150RMSE=1.0178 RMSE=1.0120 RMSE=0.9966 RMSE=1.0229 RMSE=1.0200 RMSE=1.0174 RMSE=1.0138 RMSE=1.0109 RMSE=1.0266 RMSE=1.0351 Epoch: 40/150RMSE=1.0260 RMSE=1.0213 RMSE=1.0196 RMSE=1.0270 RMSE=1.0202 RMSE=1.0239 RMSE=1.0164 RMSE=1.0254 RMSE=1.0197 RMSE=0.8487 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 0.863 +/- 0.077\n",
      "\n",
      "Epoch: 1/150RMSE=1.6038 RMSE=1.2722 RMSE=1.1479 RMSE=1.1596 RMSE=1.1362 RMSE=1.1253 RMSE=1.1429 RMSE=1.0341 RMSE=1.0886 Epoch: 10/150RMSE=1.0636 RMSE=1.0867 RMSE=1.1149 RMSE=0.9958 RMSE=0.9852 RMSE=0.9937 RMSE=1.0118 RMSE=1.0009 RMSE=0.9853 RMSE=1.0213 Epoch: 20/150RMSE=0.9526 RMSE=0.9953 RMSE=0.9734 RMSE=0.9931 RMSE=0.9880 RMSE=0.9395 RMSE=0.9637 RMSE=0.9263 RMSE=0.9373 RMSE=0.9381 Epoch: 30/150RMSE=0.9405 RMSE=0.9475 RMSE=0.9267 RMSE=0.9295 RMSE=0.9235 RMSE=0.9272 RMSE=0.9108 RMSE=0.9237 RMSE=0.9231 RMSE=0.9388 Epoch: 40/150RMSE=0.9297 RMSE=0.9413 RMSE=0.9328 RMSE=0.9401 RMSE=0.9415 RMSE=0.9382 RMSE=0.9320 RMSE=0.9282 RMSE=0.9301 RMSE=0.9251 Epoch: 50/150RMSE=0.9268 RMSE=0.9364 RMSE=0.9246 RMSE=0.9231 RMSE=0.9296 RMSE=0.9243 RMSE=0.9285 RMSE=0.9055 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 0.874 +/- 0.070\n",
      "\n",
      "Epoch: 1/150RMSE=1.4155 RMSE=1.2081 RMSE=1.1694 RMSE=1.2194 RMSE=1.1467 RMSE=1.0697 RMSE=1.0510 RMSE=1.1355 RMSE=1.0428 Epoch: 10/150RMSE=1.0188 RMSE=1.0083 RMSE=0.9964 RMSE=1.0183 RMSE=0.9891 RMSE=0.9937 RMSE=0.9747 RMSE=0.9964 RMSE=1.0061 RMSE=0.9884 Epoch: 20/150RMSE=1.0250 RMSE=0.9292 RMSE=0.9369 RMSE=0.9462 RMSE=0.9587 RMSE=0.9327 RMSE=0.9326 RMSE=0.9238 RMSE=0.9249 RMSE=0.9254 Epoch: 30/150RMSE=0.9304 RMSE=0.9211 RMSE=0.9326 RMSE=0.9361 RMSE=0.9426 RMSE=0.9354 RMSE=0.9167 RMSE=0.9070 RMSE=0.9134 RMSE=0.9101 Epoch: 40/150RMSE=0.9138 RMSE=0.9103 RMSE=0.9043 RMSE=0.9096 RMSE=0.9072 RMSE=0.9097 RMSE=0.9091 RMSE=0.9128 RMSE=0.9023 RMSE=0.9094 Epoch: 50/150RMSE=0.9055 RMSE=0.9065 RMSE=0.9054 RMSE=0.9064 RMSE=0.9034 RMSE=0.9031 RMSE=0.9047 RMSE=0.9078 RMSE=0.9007 RMSE=0.9146 Epoch: 60/150RMSE=0.9089 RMSE=0.9077 RMSE=0.9088 RMSE=0.9108 RMSE=0.9129 RMSE=0.9074 RMSE=0.9052 RMSE=0.9079 RMSE=0.9105 RMSE=0.9050 Epoch: 70/150RMSE=0.9004 RMSE=0.9095 RMSE=0.9037 RMSE=0.9083 RMSE=0.9047 RMSE=0.9040 RMSE=0.9017 RMSE=0.9004 RMSE=0.9074 RMSE=0.9181 Epoch: 80/150RMSE=0.9085 RMSE=0.9112 RMSE=0.9068 RMSE=0.9148 RMSE=0.9110 RMSE=0.9182 RMSE=0.9095 RMSE=0.9062 RMSE=0.9175 RMSE=0.9090 Epoch: 90/150RMSE=0.9051 RMSE=0.9398 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 0.887 +/- 0.068\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Generating dataset...\n",
      "Packaging molecules, finish 100.0%\n",
      "\n",
      "Current dataset: lipo, include 4200 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.5703 RMSE=1.3042 RMSE=1.1222 RMSE=1.0593 RMSE=1.0719 RMSE=0.9575 RMSE=0.9826 RMSE=1.0010 RMSE=1.0116 Epoch: 10/150RMSE=0.9830 RMSE=0.9320 RMSE=0.9100 RMSE=0.8931 RMSE=0.8949 RMSE=0.8764 RMSE=0.8968 RMSE=0.8989 RMSE=0.9202 RMSE=0.9030 Epoch: 20/150RMSE=0.8873 RMSE=0.8591 RMSE=0.8665 RMSE=0.8691 RMSE=0.8751 RMSE=0.8504 RMSE=0.8525 RMSE=0.8667 RMSE=0.8680 RMSE=0.8696 Epoch: 30/150RMSE=0.8511 RMSE=0.8524 RMSE=0.8629 RMSE=0.8555 RMSE=0.8537 RMSE=0.8432 RMSE=0.8455 RMSE=0.8500 RMSE=0.8588 RMSE=0.8517 Epoch: 40/150RMSE=0.8579 RMSE=0.8442 RMSE=0.8472 RMSE=0.8520 RMSE=0.8447 RMSE=0.8610 RMSE=0.8449 RMSE=0.8536 RMSE=0.8486 RMSE=0.8438 Epoch: 50/150RMSE=0.8554 RMSE=0.8539 RMSE=0.8479 RMSE=0.8474 RMSE=0.8396 RMSE=0.8414 RMSE=0.8618 RMSE=0.8436 RMSE=0.8530 RMSE=0.8496 Epoch: 60/150RMSE=0.8601 RMSE=0.8480 RMSE=0.8442 RMSE=0.8424 RMSE=0.8582 RMSE=0.8505 RMSE=0.8409 RMSE=0.8540 RMSE=0.8487 RMSE=0.8403 Epoch: 70/150RMSE=0.8531 RMSE=0.8587 RMSE=0.8579 RMSE=0.8465 RMSE=0.8453 RMSE=0.8484 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 0.848 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=1.6890 RMSE=1.2661 RMSE=1.1837 RMSE=1.1124 RMSE=1.1292 RMSE=1.1630 RMSE=1.0469 RMSE=1.0093 RMSE=1.0180 Epoch: 10/150RMSE=1.0498 RMSE=1.0911 RMSE=0.9561 RMSE=1.0521 RMSE=0.9740 RMSE=1.0329 RMSE=0.9941 RMSE=0.9391 RMSE=0.9352 RMSE=0.9196 Epoch: 20/150RMSE=0.9496 RMSE=0.9599 RMSE=1.0093 RMSE=0.9640 RMSE=0.8988 RMSE=0.9033 RMSE=0.9129 RMSE=0.8914 RMSE=0.9124 RMSE=0.9178 Epoch: 30/150RMSE=0.9113 RMSE=0.9105 RMSE=0.9211 RMSE=0.9261 RMSE=0.9256 RMSE=0.9132 RMSE=0.9110 RMSE=0.9076 RMSE=0.9079 RMSE=0.9066 Epoch: 40/150RMSE=0.9089 RMSE=0.9093 RMSE=0.9086 RMSE=0.9057 RMSE=0.9020 RMSE=0.9013 RMSE=0.9080 RMSE=0.9078 RMSE=0.8834 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 0.866 +/- 0.018\n",
      "\n",
      "Epoch: 1/150RMSE=1.5914 RMSE=1.2512 RMSE=1.2453 RMSE=1.1329 RMSE=1.0424 RMSE=1.0273 RMSE=1.0628 RMSE=0.9976 RMSE=0.9706 Epoch: 10/150RMSE=1.0186 RMSE=0.9349 RMSE=0.9883 RMSE=0.9148 RMSE=0.9797 RMSE=0.9680 RMSE=0.9976 RMSE=0.8740 RMSE=1.1552 RMSE=0.8933 Epoch: 20/150RMSE=0.8911 RMSE=0.8625 RMSE=0.8845 RMSE=0.8680 RMSE=0.8755 RMSE=0.8983 RMSE=0.8423 RMSE=0.8510 RMSE=0.8252 RMSE=0.8380 Epoch: 30/150RMSE=0.8715 RMSE=0.8565 RMSE=0.8399 RMSE=0.8035 RMSE=0.8080 RMSE=0.7973 RMSE=0.7981 RMSE=0.8063 RMSE=0.8117 RMSE=0.8139 Epoch: 40/150RMSE=0.8096 RMSE=0.8181 RMSE=0.8112 RMSE=0.8113 RMSE=0.8003 RMSE=0.8072 RMSE=0.7997 RMSE=0.8017 RMSE=0.7985 RMSE=0.8098 Epoch: 50/150RMSE=0.8080 RMSE=0.8100 RMSE=0.8016 RMSE=0.8131 RMSE=0.8024 RMSE=0.8128 RMSE=0.8224 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 0.851 +/- 0.025\n",
      "\n",
      "Epoch: 1/150RMSE=1.5186 RMSE=1.1977 RMSE=1.0808 RMSE=1.1304 RMSE=1.0437 RMSE=1.0184 RMSE=1.0111 RMSE=1.0442 RMSE=1.0212 Epoch: 10/150RMSE=0.9625 RMSE=0.9451 RMSE=0.9720 RMSE=0.9554 RMSE=0.9560 RMSE=0.9912 RMSE=0.9111 RMSE=0.9050 RMSE=0.8971 RMSE=0.9008 Epoch: 20/150RMSE=0.8899 RMSE=0.8880 RMSE=0.9121 RMSE=0.8922 RMSE=0.8811 RMSE=0.8633 RMSE=0.8781 RMSE=0.8737 RMSE=0.8442 RMSE=0.8780 Epoch: 30/150RMSE=0.8828 RMSE=0.8702 RMSE=0.8822 RMSE=0.8562 RMSE=0.8487 RMSE=0.8401 RMSE=0.8410 RMSE=0.8527 RMSE=0.8676 RMSE=0.8620 Epoch: 40/150RMSE=0.8394 RMSE=0.8480 RMSE=0.8407 RMSE=0.8494 RMSE=0.8455 RMSE=0.8502 RMSE=0.8573 RMSE=0.8495 RMSE=0.8583 RMSE=0.8579 Epoch: 50/150RMSE=0.8609 RMSE=0.8563 RMSE=0.8529 RMSE=0.8629 RMSE=0.8584 RMSE=0.8624 RMSE=0.8574 RMSE=0.8602 RMSE=0.8604 RMSE=0.8621 Epoch: 60/150RMSE=0.8608 RMSE=0.8494 \n",
      "********************1's fold 4's run over********************\n",
      "RMSE: 0.851 +/- 0.022\n",
      "\n",
      "Epoch: 1/150RMSE=1.3540 RMSE=1.1703 RMSE=1.1399 RMSE=1.1603 RMSE=1.0479 RMSE=0.9972 RMSE=1.0035 RMSE=0.9569 RMSE=0.9330 Epoch: 10/150RMSE=0.9285 RMSE=0.9553 RMSE=0.8935 RMSE=0.8928 RMSE=0.8964 RMSE=0.9174 RMSE=0.9064 RMSE=0.8669 RMSE=0.9083 RMSE=0.9457 Epoch: 20/150RMSE=0.8689 RMSE=0.8805 RMSE=0.8324 RMSE=0.8311 RMSE=0.8463 RMSE=0.8452 RMSE=0.8589 RMSE=0.8538 RMSE=0.8437 RMSE=0.8315 Epoch: 30/150RMSE=0.8393 RMSE=0.8208 RMSE=0.8420 RMSE=0.8377 RMSE=0.8328 RMSE=0.8208 RMSE=0.8421 RMSE=0.8295 RMSE=0.8325 RMSE=0.8385 Epoch: 40/150RMSE=0.8281 RMSE=0.8256 RMSE=0.8266 RMSE=0.8213 RMSE=0.8164 RMSE=0.8186 RMSE=0.8257 RMSE=0.8211 RMSE=0.8254 RMSE=0.8275 Epoch: 50/150RMSE=0.8255 RMSE=0.8207 RMSE=0.8218 RMSE=0.8201 RMSE=0.8214 RMSE=0.8194 RMSE=0.8271 RMSE=0.8268 RMSE=0.8225 RMSE=0.8235 Epoch: 60/150RMSE=0.8199 RMSE=0.8175 RMSE=0.8269 RMSE=0.8235 RMSE=0.8189 RMSE=0.8226 \n",
      "********************1's fold 5's run over********************\n",
      "RMSE: 0.845 +/- 0.022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --co_ratio=0.1 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --co_ratio=0.3 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --co_ratio=0.5 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --co_ratio=0.7 --pooling='CO'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --co_ratio=0.9 --pooling='CO'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CG-ODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
