{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "visible_devices = os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "print(f\"Using GPU(s): {visible_devices}\")\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f'Available GPUs: {num_gpus}')\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import CitationFull\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dataset_sparse = WebKB(root=\"/data/zeyu/Pooling\", name='Cornell')\n",
    "dataset_sparse = dataset_sparse.shuffle()\n",
    "print(dataset_sparse[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TopKPooling with HierarchicalGCN (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]  # 根据需求调整\n",
    "\n",
    "class HierarchicalGCN_TOPK(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_TOPK, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(TopKPooling(channels, ratio=0.5))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            #print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_TOPK(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# 计算总平均值和标准差\n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVPooling with HierarchicalGCN (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import degree\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
    "from torch_scatter import scatter_add\n",
    "from torch_sparse import spspmm, coalesce\n",
    "from typing import Callable, Optional, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from torch_geometric.utils import scatter, softmax\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "\n",
    "def scatter_sort(x, batch, fill_value=-1e16):\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n",
    "    dense_x[index] = x\n",
    "    dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n",
    "    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n",
    "    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n",
    "\n",
    "    sorted_x = sorted_x.view(-1)\n",
    "    filled_index = sorted_x != fill_value\n",
    "\n",
    "    sorted_x = sorted_x[filled_index]\n",
    "    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n",
    "\n",
    "    return sorted_x, cumsum_sorted_x\n",
    "\n",
    "\n",
    "def _make_ix_like(batch):\n",
    "    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n",
    "    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n",
    "    idx = torch.cat(idx, dim=0)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _threshold_and_support(x, batch):\n",
    "    \"\"\"Sparsemax building block: compute the threshold\n",
    "    Args:\n",
    "        x: input tensor to apply the sparsemax\n",
    "        batch: group indicators\n",
    "    Returns:\n",
    "        the threshold value\n",
    "    \"\"\"\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    sorted_input, input_cumsum = scatter_sort(x, batch)\n",
    "    input_cumsum = input_cumsum - 1.0\n",
    "    rhos = _make_ix_like(batch).to(x.dtype)\n",
    "    support = rhos * sorted_input > input_cumsum\n",
    "\n",
    "    support_size = scatter_add(support.to(batch.dtype), batch)\n",
    "    # mask invalid index, for example, if batch is not start from 0 or not continuous, it may result in negative index\n",
    "    idx = support_size + cum_num_nodes - 1\n",
    "    mask = idx < 0\n",
    "    idx[mask] = 0\n",
    "    tau = input_cumsum.gather(0, idx)\n",
    "    tau /= support_size.to(x.dtype)\n",
    "\n",
    "    return tau, support_size\n",
    "\n",
    "\n",
    "class SparsemaxFunction(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, batch):\n",
    "        \"\"\"sparsemax: normalizing sparse transform\n",
    "        Parameters:\n",
    "            ctx: context object\n",
    "            x (Tensor): shape (N, )\n",
    "            batch: group indicator\n",
    "        Returns:\n",
    "            output (Tensor): same shape as input\n",
    "        \"\"\"\n",
    "        max_val, _ = scatter_max(x, batch)\n",
    "        x -= max_val[batch]\n",
    "        tau, supp_size = _threshold_and_support(x, batch)\n",
    "        output = torch.clamp(x - tau[batch], min=0)\n",
    "        ctx.save_for_backward(supp_size, output, batch)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output, batch = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n",
    "\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        return sparsemax(x, batch)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sparse_attention = Sparsemax()\n",
    "    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n",
    "    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n",
    "    res = sparse_attention(input_x, input_batch)\n",
    "    print(res)\n",
    "\n",
    "class TwoHopNeighborhood(object):\n",
    "    def __call__(self, data):\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "        n = data.num_nodes\n",
    "\n",
    "        fill = 1e16\n",
    "        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n",
    "\n",
    "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n",
    "\n",
    "        edge_index = torch.cat([edge_index, index], dim=1)\n",
    "        if edge_attr is None:\n",
    "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
    "        else:\n",
    "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
    "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
    "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
    "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min')\n",
    "            edge_attr[edge_attr >= fill] = 0\n",
    "            data.edge_attr = edge_attr\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)\n",
    "\n",
    "\n",
    "class GCN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
    "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            nn.init.zeros_(self.bias.data)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        edge_weight = deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n",
    "\n",
    "\n",
    "class PageRankScore(MessagePassing):\n",
    "    def __init__(self, channels, k=10, alpha=0.1, **kwargs):\n",
    "        super(PageRankScore, self).__init__(aggr='add', **kwargs)\n",
    "        self.channels = channels\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.gnn = GCN(channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        edge_index, norm = GCN.norm(edge_index, x.size(0), edge_weight, dtype=x.dtype)\n",
    "\n",
    "        x = self.gnn(x, edge_index, edge_weight)\n",
    "        hidden = x\n",
    "        for k in range(self.k):\n",
    "            x = self.propagate(edge_index, x=x, norm=norm)\n",
    "            x = x * (1 - self.alpha)\n",
    "            x = x + self.alpha * hidden\n",
    "\n",
    "        return x\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "\n",
    "class MVPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.5, negative_slop=0.2):\n",
    "        super(MVPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.sample = True\n",
    "        self.sparse = True\n",
    "        self.sl = True\n",
    "        self.hc = False\n",
    "        self.h_hop = 2\n",
    "        self.lamb = 0.0\n",
    "        self.negative_slop = negative_slop\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att.data)\n",
    "        self.weight = Parameter(torch.Tensor(1, in_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        self.view_att = Parameter(torch.Tensor(3, 3))\n",
    "        nn.init.xavier_uniform_(self.view_att.data)\n",
    "        self.view_bias = Parameter(torch.Tensor(3))\n",
    "        nn.init.zeros_(self.view_bias.data)\n",
    "        self.alpha = Parameter(torch.Tensor(1))\n",
    "        nn.init.ones_(self.alpha.data)\n",
    "        self.beta = Parameter(torch.Tensor(1))\n",
    "        nn.init.ones_(self.beta.data)\n",
    "        self.sparse_attention = Sparsemax()\n",
    "        self.neighbor_augment = TwoHopNeighborhood()\n",
    "        self.calc_pagerank_score = PageRankScore(in_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        row, col = edge_index\n",
    "        score1 = torch.sigmoid(self.alpha * torch.log(degree(row, num_nodes=x.size(0)) + 1e-16) + self.beta).view(-1, 1)\n",
    "        x_score2 = (x * self.weight).sum(dim=-1)\n",
    "        score2 = torch.sigmoid(x_score2 / self.weight.norm(p=2, dim=-1)).view(-1, 1)\n",
    "        x_score3 = self.calc_pagerank_score(x, edge_index, edge_attr)\n",
    "        score3 = torch.sigmoid(x_score3).view(-1, 1)\n",
    "\n",
    "        score_cat = torch.cat([score1, score2, score3], dim=1)\n",
    "        max_value, _ = torch.max(torch.abs(score_cat), dim=0)\n",
    "        score_cat = score_cat / max_value\n",
    "        score_weight = torch.sigmoid(torch.matmul(score_cat, self.view_att) + self.view_bias)\n",
    "        score_weight = torch.softmax(score_weight, dim=1)\n",
    "        score = torch.sigmoid(torch.sum(score_cat * score_weight, dim=1))\n",
    "        # score = score2.view(-1)\n",
    "\n",
    "        # Graph Pooling\n",
    "        original_x = x\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        batch = batch[perm]\n",
    "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        # Discard structure learning layer, directly return\n",
    "        if self.sl is False:\n",
    "            return x, induced_edge_index, induced_edge_attr, batch, perm\n",
    "\n",
    "        # Structure Learning\n",
    "        if self.sample:\n",
    "            # A fast mode for large graphs.\n",
    "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
    "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
    "            # edge weights between them.\n",
    "            if edge_attr is None:\n",
    "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
    "\n",
    "            if self.h_hop >= 2:\n",
    "                hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "                for _ in range(self.h_hop - 1):\n",
    "                    hop_data = self.neighbor_augment(hop_data)\n",
    "                hop_edge_index = hop_data.edge_index\n",
    "                hop_edge_attr = hop_data.edge_attr\n",
    "                new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
    "                if self.hc is True:\n",
    "                    return x, new_edge_index, new_edge_attr, batch, perm\n",
    "            else:\n",
    "                new_edge_index = induced_edge_index\n",
    "                new_edge_attr = induced_edge_attr\n",
    "                if self.hc is True:\n",
    "                    return x, new_edge_index, new_edge_attr, batch, perm\n",
    "\n",
    "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
    "            row, col = new_edge_index\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            adj[row, col] = weights\n",
    "            new_edge_index, weights = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
    "            if edge_attr is None:\n",
    "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
    "                                               device=induced_edge_index.device)\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            # Construct batch fully connected graph in block diagonal matirx format\n",
    "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
    "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
    "            new_edge_index, _ = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop)\n",
    "            adj[row, col] = weights\n",
    "            induced_row, induced_col = induced_edge_index\n",
    "\n",
    "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
    "            weights = adj[row, col]\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x, new_edge_index, new_edge_attr, batch, perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAGPooling with HierarchicalGCN (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, SAGPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]  # 根据需求调整\n",
    "\n",
    "class HierarchicalGCN_SAG(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_SAG, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(SAGPooling(channels, ratio=0.5))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            #print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_SAG(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# 计算总平均值和标准差\n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASAPooling with HierarchicalGCN (2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]  # 根据需求调整\n",
    "\n",
    "class HierarchicalGCN_ASA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_ASA, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(ASAPooling(channels, ratio=0.5))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            #print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_ASA(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# 计算总平均值和标准差\n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PANPooling with HierarchicalGCN (2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "class PANPooling(torch.nn.Module):\n",
    "    r\"\"\" General Graph pooling layer based on PAN, which can work with all layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, ratio=0.5, pan_pool_weight=None, min_score=None, multiplier=1,\n",
    "                 nonlinearity=torch.tanh, filter_size=3, panpool_filter_weight=None):\n",
    "        super(PANPooling, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.min_score = min_score\n",
    "        self.multiplier = multiplier\n",
    "        self.nonlinearity = nonlinearity\n",
    "\n",
    "        self.filter_size = filter_size\n",
    "        if panpool_filter_weight is None:\n",
    "            self.panpool_filter_weight = torch.nn.Parameter(0.5 * torch.ones(filter_size), requires_grad=True)\n",
    "\n",
    "        self.transform = Parameter(torch.ones(in_channels), requires_grad=True)\n",
    "\n",
    "        if pan_pool_weight is None:\n",
    "            #self.weight = torch.tensor([0.7, 0.3], device=self.transform.device)\n",
    "            self.pan_pool_weight = torch.nn.Parameter(0.5 * torch.ones(2), requires_grad=True)\n",
    "        else:\n",
    "            self.pan_pool_weight = pan_pool_weight\n",
    "\n",
    "    def forward(self, x, edge_index, M=None, batch=None, num_nodes=None):\n",
    "\n",
    "        \"\"\"\"\"\"\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        # Path integral\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "        edge_index, edge_weight = self.panentropy_sparse(edge_index, num_nodes)\n",
    "\n",
    "        # weighted degree\n",
    "        num_nodes = x.size(0)\n",
    "        degree = torch.zeros(num_nodes, device=edge_index.device)\n",
    "        degree = scatter_add(edge_weight, edge_index[0], out=degree)\n",
    "\n",
    "        # linear transform\n",
    "        xtransform = torch.matmul(x, self.transform)\n",
    "\n",
    "        # aggregate score\n",
    "        x_transform_norm = xtransform #/ xtransform.norm(p=2, dim=-1)\n",
    "        degree_norm = degree #/ degree.norm(p=2, dim=-1)\n",
    "        score = self.pan_pool_weight[0] * x_transform_norm + self.pan_pool_weight[1] * degree_norm\n",
    "\n",
    "        if self.min_score is None:\n",
    "            score = self.nonlinearity(score)\n",
    "        else:\n",
    "            score = softmax(score, batch)\n",
    "\n",
    "        perm = self.topk(score, self.ratio, batch, self.min_score)\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        x = self.multiplier * x if self.multiplier != 1 else x\n",
    "\n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_weight = self.filter_adj(edge_index, edge_weight, perm, num_nodes=score.size(0))\n",
    "\n",
    "        return x, edge_index, edge_weight, batch, perm, score[perm]\n",
    "\n",
    "    def topk(self, x, ratio, batch, min_score=None, tol=1e-7):\n",
    "\n",
    "        if min_score is not None:\n",
    "            # Make sure that we do not drop all nodes in a graph.\n",
    "            scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "            scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "            perm = torch.nonzero(x > scores_min).view(-1)\n",
    "        else:\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "            cum_num_nodes = torch.cat(\n",
    "                [num_nodes.new_zeros(1),\n",
    "                 num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "            index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "            index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "            dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n",
    "            dense_x[index] = x\n",
    "            dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "            _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "            perm = perm + cum_num_nodes.view(-1, 1)\n",
    "            perm = perm.view(-1)\n",
    "\n",
    "            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "            mask = [\n",
    "                torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "                i * max_num_nodes for i in range(batch_size)\n",
    "            ]\n",
    "            mask = torch.cat(mask, dim=0)\n",
    "\n",
    "            perm = perm[mask]\n",
    "\n",
    "        return perm\n",
    "\n",
    "    def filter_adj(self, edge_index, edge_weight, perm, num_nodes=None):\n",
    "\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "        mask = perm.new_full((num_nodes, ), -1)\n",
    "        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "        mask[perm] = i\n",
    "\n",
    "        row, col = edge_index\n",
    "        row, col = mask[row], mask[col]\n",
    "        mask = (row >= 0) & (col >= 0)\n",
    "        row, col = row[mask], col[mask]\n",
    "\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight[mask]\n",
    "\n",
    "        return torch.stack([row, col], dim=0), edge_weight\n",
    "\n",
    "    def panentropy_sparse(self, edge_index, num_nodes):\n",
    "\n",
    "        edge_value = torch.ones(edge_index.size(1), device=edge_index.device)\n",
    "        edge_index, edge_value = coalesce(edge_index, edge_value, num_nodes, num_nodes)\n",
    "\n",
    "        # iteratively add weighted matrix power\n",
    "        pan_index, pan_value = eye(num_nodes, device=edge_index.device)\n",
    "        indextmp = pan_index.clone().to(edge_index.device)\n",
    "        valuetmp = pan_value.clone().to(edge_index.device)\n",
    "\n",
    "        pan_value = self.panpool_filter_weight[0] * pan_value\n",
    "\n",
    "        for i in range(self.filter_size - 1):\n",
    "            #indextmp, valuetmp = coalesce(indextmp, valuetmp, num_nodes, num_nodes)\n",
    "            indextmp, valuetmp = spspmm(indextmp, valuetmp, edge_index, edge_value, num_nodes, num_nodes, num_nodes)\n",
    "            valuetmp = valuetmp * self.panpool_filter_weight[i+1]\n",
    "            indextmp, valuetmp = coalesce(indextmp, valuetmp, num_nodes, num_nodes)\n",
    "            pan_index = torch.cat((pan_index, indextmp), 1)\n",
    "            pan_value = torch.cat((pan_value, valuetmp))\n",
    "\n",
    "        return coalesce(pan_index, pan_value, num_nodes, num_nodes, op='add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]  # 根据需求调整\n",
    "\n",
    "class HierarchicalGCN_PAN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_PAN, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(PANPooling(channels))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm, score_perm = self.pools[i - 1](x, edge_index, batch=batch, M=None)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            #print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_PAN(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# 计算总平均值和标准差\n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoPooling with HierarchicalGCN (2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "class GPR_prop(MessagePassing):\n",
    "    '''\n",
    "    propagation class for GPR_GNN\n",
    "    '''\n",
    "\n",
    "    def __init__(self, K, alpha, Init, Gamma=None, bias=True, **kwargs):\n",
    "        super(GPR_prop, self).__init__(aggr='add', **kwargs)\n",
    "        self.K = K\n",
    "        self.Init = Init\n",
    "        self.alpha = alpha\n",
    "\n",
    "        assert Init in ['SGC', 'PPR', 'NPPR', 'Random', 'WS']\n",
    "        if Init == 'SGC':\n",
    "            # SGC-like\n",
    "            TEMP = 0.0*np.ones(K+1)\n",
    "            TEMP[alpha] = 1.0\n",
    "        elif Init == 'PPR':\n",
    "            # PPR-like\n",
    "            TEMP = alpha*(1-alpha)**np.arange(K+1)\n",
    "            TEMP[-1] = (1-alpha)**K\n",
    "        elif Init == 'NPPR':\n",
    "            # Negative PPR\n",
    "            TEMP = (alpha)**np.arange(K+1)\n",
    "            TEMP = TEMP/np.sum(np.abs(TEMP))\n",
    "        elif Init == 'Random':\n",
    "            # Random\n",
    "            bound = np.sqrt(3/(K+1))\n",
    "            TEMP = np.random.uniform(-bound, bound, K+1)\n",
    "            TEMP = TEMP/np.sum(np.abs(TEMP))\n",
    "        elif Init == 'WS':\n",
    "            # Specify Gamma\n",
    "            TEMP = Gamma\n",
    "\n",
    "        self.temp = Parameter(torch.tensor(TEMP))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.zeros_(self.temp)\n",
    "        for k in range(self.K+1):\n",
    "            self.temp.data[k] = self.alpha*(1-self.alpha)**k\n",
    "        self.temp.data[-1] = (1-self.alpha)**self.K\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        edge_index, norm = gcn_norm(\n",
    "            edge_index, edge_weight, num_nodes=x.size(0), dtype=x.dtype)\n",
    "\n",
    "        hidden = x*(self.temp[0])\n",
    "        for k in range(self.K):\n",
    "            x = self.propagate(edge_index, x=x, norm=norm)\n",
    "            gamma = self.temp[k+1]\n",
    "            hidden = hidden + gamma*x\n",
    "        return hidden\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}(K={}, temp={})'.format(self.__class__.__name__, self.K,\n",
    "                                           self.temp)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes) # in case all the edges are removed\n",
    "\n",
    "        edge_index = edge_index.type(torch.long)\n",
    "        row, col = edge_index\n",
    "        # print(row, col)\n",
    "        # print(edge_weight.shape, row.shape, num_nodes)\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        # row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "class graph_attention(torch.nn.Module):\n",
    "    # reference: https://github.com/gordicaleksa/pytorch-GAT/blob/39c8f0ee634477033e8b1a6e9a6da3c7ed71bbd1/models/definitions/GAT.py#L324\n",
    "    src_nodes_dim = 0  # position of source nodes in edge index\n",
    "    trg_nodes_dim = 1  # position of target nodes in edge index\n",
    "\n",
    "    nodes_dim = 0      # node dimension/axis\n",
    "    head_dim = 1       # attention head dimension/axis\n",
    "\n",
    "    def __init__(self, num_in_features, num_out_features, num_of_heads, dropout_prob=0.6, log_attention_weights=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Saving these as we'll need them in forward propagation in children layers (imp1/2/3)\n",
    "        self.num_of_heads = num_of_heads\n",
    "        self.num_out_features = num_out_features\n",
    "        #\n",
    "        # Trainable weights: linear projection matrix (denoted as \"W\" in the paper), attention target/source\n",
    "        # (denoted as \"a\" in the paper) and bias (not mentioned in the paper but present in the official GAT repo)\n",
    "        #\n",
    "\n",
    "        # You can treat this one matrix as num_of_heads independent W matrices\n",
    "        self.linear_proj = nn.Linear(num_in_features, num_of_heads * num_out_features, bias=False)\n",
    "\n",
    "        # After we concatenate target node (node i) and source node (node j) we apply the additive scoring function\n",
    "        # which gives us un-normalized score \"e\". Here we split the \"a\" vector - but the semantics remain the same.\n",
    "\n",
    "        # Basically instead of doing [x, y] (concatenation, x/y are node feature vectors) and dot product with \"a\"\n",
    "        # we instead do a dot product between x and \"a_left\" and y and \"a_right\" and we sum them up\n",
    "        self.scoring_fn_target = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "        self.scoring_fn_source = nn.Parameter(torch.Tensor(1, num_of_heads, num_out_features))\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        \"\"\"\n",
    "        The reason we're using Glorot (aka Xavier uniform) initialization is because it's a default TF initialization:\n",
    "            https://stackoverflow.com/questions/37350131/what-is-the-default-variable-initializer-in-tensorflow\n",
    "        The original repo was developed in TensorFlow (TF) and they used the default initialization.\n",
    "        Feel free to experiment - there may be better initializations depending on your problem.\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.linear_proj.weight)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_target)\n",
    "        nn.init.xavier_uniform_(self.scoring_fn_source)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #\n",
    "        # Step 1: Linear Projection + regularization\n",
    "        #\n",
    "\n",
    "        in_nodes_features = x  # unpack data\n",
    "        num_of_nodes = in_nodes_features.shape[self.nodes_dim]\n",
    "\n",
    "        # shape = (N, FIN) * (FIN, NH*FOUT) -> (N, NH, FOUT) where NH - number of heads, FOUT - num of output features\n",
    "        # We project the input node features into NH independent output features (one for each attention head)\n",
    "        nodes_features_proj = self.linear_proj(in_nodes_features).view(-1, self.num_of_heads, self.num_out_features)\n",
    "\n",
    "        #\n",
    "        # Step 2: Edge attention calculation\n",
    "        #\n",
    "\n",
    "        # Apply the scoring function (* represents element-wise (a.k.a. Hadamard) product)\n",
    "        # shape = (N, NH, FOUT) * (1, NH, FOUT) -> (N, NH, 1) -> (N, NH) because sum squeezes the last dimension\n",
    "        # Optimization note: torch.sum() is as performant as .sum() in my experiments\n",
    "        scores_source = (nodes_features_proj * self.scoring_fn_source).sum(dim=-1)\n",
    "        scores_target = (nodes_features_proj * self.scoring_fn_target).sum(dim=-1)\n",
    "\n",
    "        # We simply copy (lift) the scores for source/target nodes based on the edge index. Instead of preparing all\n",
    "        # the possible combinations of scores we just prepare those that will actually be used and those are defined\n",
    "        # by the edge index.\n",
    "        # scores shape = (E, NH), nodes_features_proj_lifted shape = (E, NH, FOUT), E - number of edges in the graph\n",
    "        scores_source_lifted, scores_target_lifted, nodes_features_proj_lifted = self.lift(scores_source, scores_target, nodes_features_proj, edge_index)\n",
    "        scores_per_edge = scores_source_lifted + scores_target_lifted\n",
    "\n",
    "        return torch.sigmoid(scores_per_edge)\n",
    "\n",
    "    def lift(self, scores_source, scores_target, nodes_features_matrix_proj, edge_index):\n",
    "        \"\"\"\n",
    "        Lifts i.e. duplicates certain vectors depending on the edge index.\n",
    "        One of the tensor dims goes from N -> E (that's where the \"lift\" comes from).\n",
    "        \"\"\"\n",
    "        src_nodes_index = edge_index[self.src_nodes_dim]\n",
    "        trg_nodes_index = edge_index[self.trg_nodes_dim]\n",
    "\n",
    "        # Using index_select is faster than \"normal\" indexing (scores_source[src_nodes_index]) in PyTorch!\n",
    "        scores_source = scores_source.index_select(self.nodes_dim, src_nodes_index)\n",
    "        scores_target = scores_target.index_select(self.nodes_dim, trg_nodes_index)\n",
    "        nodes_features_matrix_proj_lifted = nodes_features_matrix_proj.index_select(self.nodes_dim, src_nodes_index)\n",
    "\n",
    "        return scores_source, scores_target, nodes_features_matrix_proj_lifted\n",
    "\n",
    "\n",
    "\n",
    "class CoPooling(torch.nn.Module):\n",
    "    # reference for GAT code: https://github.com/PetarV-/GAT\n",
    "    # reference for generalized pagerank code: https://github.com/jianhao2016/GPRGNN\n",
    "    def __init__(self, ratio=0.5, K=0.05, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=None):\n",
    "        super(CoPooling, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "        self.edge_ratio = edge_ratio\n",
    "\n",
    "        self.prop1 = GPR_prop(K, alpha, Init, Gamma)\n",
    "\n",
    "        score_dim = 32\n",
    "        self.G_att = graph_attention(num_in_features=nhid, num_out_features=score_dim, num_of_heads=1)\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(2*nhid, nhid))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        self.bias = Parameter(torch.Tensor(nhid))\n",
    "        nn.init.zeros_(self.bias.data)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "        nn.init.zeros_(self.bias.data)\n",
    "        self.prop1.reset_parameters()\n",
    "        self.G_att.init_params()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch=None, nodes_index=None, node_attr=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        ori_batch = batch.clone()\n",
    "        device = x.device\n",
    "        num_nodes = x.shape[0]\n",
    "\n",
    "        # cut edges based on scores\n",
    "        x_cut = self.prop1(x, edge_index) # run generalized pagerank to update features\n",
    "\n",
    "        attention = self.G_att(x_cut, edge_index) # get the attention weights after sigmoid\n",
    "        attention = attention.sum(dim=1) #sum the weights on head dim\n",
    "        edge_index, attention = add_self_loops(edge_index, attention, 1.0, num_nodes) # add self loops in case no edges\n",
    "\n",
    "        # to get a systemitic adj matrix\n",
    "        edge_index_t, attention_t = transpose(edge_index, attention, num_nodes, num_nodes)\n",
    "        edge_tmp = torch.cat((edge_index, edge_index_t), 1)\n",
    "        att_tmp = torch.cat((attention, attention_t),0)\n",
    "        edge_index, attention = coalesce(edge_tmp, att_tmp, num_nodes, num_nodes, 'mean')\n",
    "\n",
    "        attention_np = attention.cpu().data.numpy()\n",
    "        cut_val = np.percentile(attention_np, int(100*(1-self.edge_ratio))) # this is for keep the top edge_ratio edges\n",
    "        attention = attention * (attention >= cut_val) # keep the edge_ratio higher weights of edges\n",
    "\n",
    "        kep_idx = attention > 0.0\n",
    "        cut_edge_index, cut_edge_attr = edge_index[:, kep_idx], attention[kep_idx]\n",
    "\n",
    "        # Graph Pooling based on nodes\n",
    "        x_information_score = self.calc_information_score(x, cut_edge_index, cut_edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x_topk = x[perm]\n",
    "        batch = batch[perm]\n",
    "        if nodes_index is not None:\n",
    "            nodes_index = nodes_index[perm]\n",
    "\n",
    "        if node_attr is not None:\n",
    "            node_attr = node_attr[perm]\n",
    "        if cut_edge_index is not None or cut_edge_index.nelement() != 0:\n",
    "            induced_edge_index, induced_edge_attr = filter_adj(cut_edge_index, cut_edge_attr, perm, num_nodes=num_nodes)\n",
    "        else:\n",
    "            print('All edges are cut!')\n",
    "            induced_edge_index, induced_edge_attr = cut_edge_index, cut_edge_attr\n",
    "\n",
    "        # update node features\n",
    "        attention_dense = (to_dense_adj(cut_edge_index, edge_attr=cut_edge_attr, max_num_nodes=num_nodes)).squeeze()\n",
    "        x = F.relu(torch.matmul(torch.cat((x_topk, torch.matmul(attention_dense[perm],x)), 1), self.weight) + self.bias)\n",
    "\n",
    "        return x, induced_edge_index, perm, induced_edge_attr, batch, nodes_index, node_attr, attention_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]  # 根据需求调整\n",
    "\n",
    "class HierarchicalGCN_CO(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_CO, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(CoPooling(ratio=0.5, K=1, edge_ratio=0.6, nhid=64, alpha=0.1, Init='Random', Gamma=1.0))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, perm, _, batch, _, _, _ = self.pools[i - 1](x, edge_index, edge_attr=None, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            #print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_CO(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# 计算总平均值和标准差\n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGIPooling with HierarchicalGCN (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "@dataclass(init=False)\n",
    "class SelectOutput:\n",
    "    r\"\"\"The output of the :class:`Select` method, which holds an assignment\n",
    "    from selected nodes to their respective cluster(s).\n",
    "\n",
    "    Args:\n",
    "        node_index (torch.Tensor): The indices of the selected nodes.\n",
    "        num_nodes (int): The number of nodes.\n",
    "        cluster_index (torch.Tensor): The indices of the clusters each node in\n",
    "            :obj:`node_index` is assigned to.\n",
    "        num_clusters (int): The number of clusters.\n",
    "        weight (torch.Tensor, optional): A weight vector, denoting the strength\n",
    "            of the assignment of a node to its cluster. (default: :obj:`None`)\n",
    "    \"\"\"\n",
    "    node_index: Tensor\n",
    "    num_nodes: int\n",
    "    cluster_index: Tensor\n",
    "    num_clusters: int\n",
    "    weight: Optional[Tensor] = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_index: Tensor,\n",
    "        num_nodes: int,\n",
    "        cluster_index: Tensor,\n",
    "        num_clusters: int,\n",
    "        weight: Optional[Tensor] = None,\n",
    "    ):\n",
    "        if node_index.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'node_index' to be one-dimensional \"\n",
    "                             f\"(got {node_index.dim()} dimensions)\")\n",
    "\n",
    "        if cluster_index.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'cluster_index' to be one-dimensional \"\n",
    "                             f\"(got {cluster_index.dim()} dimensions)\")\n",
    "\n",
    "        if node_index.numel() != cluster_index.numel():\n",
    "            raise ValueError(f\"Expected 'node_index' and 'cluster_index' to \"\n",
    "                             f\"hold the same number of values (got \"\n",
    "                             f\"{node_index.numel()} and \"\n",
    "                             f\"{cluster_index.numel()} values)\")\n",
    "\n",
    "        if weight is not None and weight.dim() != 1:\n",
    "            raise ValueError(f\"Expected 'weight' vector to be one-dimensional \"\n",
    "                             f\"(got {weight.dim()} dimensions)\")\n",
    "\n",
    "        if weight is not None and weight.numel() != node_index.numel():\n",
    "            raise ValueError(f\"Expected 'weight' to hold {node_index.numel()} \"\n",
    "                             f\"values (got {weight.numel()} values)\")\n",
    "\n",
    "        self.node_index = node_index\n",
    "        self.num_nodes = num_nodes\n",
    "        self.cluster_index = cluster_index\n",
    "        self.num_clusters = num_clusters\n",
    "        self.weight = weight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Select(torch.nn.Module):\n",
    "    r\"\"\"An abstract base class for implementing custom node selections as\n",
    "    described in the `\"Understanding Pooling in Graph Neural Networks\"\n",
    "    <https://arxiv.org/abs/1905.05178>`_ paper, which maps the nodes of an\n",
    "    input graph to supernodes in the coarsened graph.\n",
    "\n",
    "    Specifically, :class:`Select` returns a :class:`SelectOutput` output, which\n",
    "    holds a (sparse) mapping :math:`\\mathbf{C} \\in {[0, 1]}^{N \\times C}` that\n",
    "    assigns selected nodes to one or more of :math:`C` super nodes.\n",
    "    \"\"\"\n",
    "    def reset_parameters(self):\n",
    "        r\"\"\"Resets all learnable parameters of the module.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, *args, **kwargs) -> SelectOutput:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f'{self.__class__.__name__}()'\n",
    "\n",
    "def cumsum(x: Tensor, dim: int = 0) -> Tensor:\n",
    "    r\"\"\"Returns the cumulative sum of elements of :obj:`x`.\n",
    "    In contrast to :meth:`torch.cumsum`, prepends the output with zero.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "        dim (int, optional): The dimension to do the operation over.\n",
    "            (default: :obj:`0`)\n",
    "\n",
    "    Example:\n",
    "        >>> x = torch.tensor([2, 4, 1])\n",
    "        >>> cumsum(x)\n",
    "        tensor([0, 2, 6, 7])\n",
    "\n",
    "    \"\"\"\n",
    "    size = x.size()[:dim] + (x.size(dim) + 1, ) + x.size()[dim + 1:]\n",
    "    out = x.new_empty(size)\n",
    "\n",
    "    out.narrow(dim, 0, 1).zero_()\n",
    "    torch.cumsum(x, dim=dim, out=out.narrow(dim, 1, x.size(dim)))\n",
    "\n",
    "    return out\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "def topk(\n",
    "    x: Tensor,\n",
    "    ratio: Optional[Union[float, int]],\n",
    "    batch: Tensor,\n",
    "    min_score: Optional[float] = None,\n",
    "    tol: float = 1e-7,\n",
    ") -> Tensor:\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter(x, batch, reduce='max')[batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero().view(-1)\n",
    "        return perm\n",
    "\n",
    "    if ratio is not None:\n",
    "        num_nodes = scatter(batch.new_ones(x.size(0)), batch, reduce='sum')\n",
    "\n",
    "        if ratio >= 1:\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), int(ratio))\n",
    "        else:\n",
    "            k = (float(ratio) * num_nodes.to(x.dtype)).ceil().to(torch.long)\n",
    "\n",
    "        x, x_perm = torch.sort(x.view(-1), descending=True)\n",
    "        batch = batch[x_perm]\n",
    "        batch, batch_perm = torch.sort(batch, descending=False, stable=True)\n",
    "\n",
    "        arange = torch.arange(x.size(0), dtype=torch.long, device=x.device)\n",
    "        ptr = cumsum(num_nodes)\n",
    "        batched_arange = arange - ptr[batch]\n",
    "        mask = batched_arange < k[batch]\n",
    "\n",
    "        return x_perm[batch_perm[mask]]\n",
    "\n",
    "    raise ValueError(\"At least one of the 'ratio' and 'min_score' parameters \"\n",
    "                     \"must be specified\")\n",
    "\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_channels * 2, in_channels)\n",
    "        self.fc2 = nn.Linear(in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CGIPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.5, non_lin=torch.tanh):\n",
    "        super(CGIPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.non_lin = non_lin\n",
    "        self.hidden_dim = in_channels\n",
    "        self.transform = GraphConv(in_channels, self.hidden_dim)\n",
    "        self.pp_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n",
    "        self.np_conv = GraphConv(self.hidden_dim, self.hidden_dim)\n",
    "\n",
    "        self.positive_pooling = GraphConv(self.hidden_dim, 1)\n",
    "        self.negative_pooling = GraphConv(self.hidden_dim, 1)\n",
    "\n",
    "        self.discriminator = Discriminator(self.hidden_dim)\n",
    "        self.loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        device = x.device  # 获取输入张量的设备信息\n",
    "\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_transform = F.leaky_relu(self.transform(x, edge_index), 0.2)\n",
    "        x_tp = F.leaky_relu(self.pp_conv(x, edge_index), 0.2)\n",
    "        x_tn = F.leaky_relu(self.np_conv(x, edge_index), 0.2)\n",
    "        s_pp = self.positive_pooling(x_tp, edge_index).squeeze()\n",
    "        s_np = self.negative_pooling(x_tn, edge_index).squeeze()\n",
    "\n",
    "        perm_positive = topk(s_pp, 1, batch)\n",
    "        perm_negative = topk(s_np, 1, batch)\n",
    "        x_pp = x_transform[perm_positive] * self.non_lin(s_pp[perm_positive]).view(-1, 1)\n",
    "        x_np = x_transform[perm_negative] * self.non_lin(s_np[perm_negative]).view(-1, 1)\n",
    "\n",
    "        x_pp_readout = gap(x_pp, batch[perm_positive])\n",
    "        x_np_readout = gap(x_np, batch[perm_negative])\n",
    "        x_readout = gap(x_transform, batch)\n",
    "\n",
    "        positive_pair = torch.cat([x_pp_readout, x_readout], dim=1)\n",
    "        negative_pair = torch.cat([x_np_readout, x_readout], dim=1)\n",
    "\n",
    "        real = torch.ones(positive_pair.shape[0], device=device)  # 将张量移动到相应设备\n",
    "        fake = torch.zeros(negative_pair.shape[0], device=device)  # 将张量移动到相应设备\n",
    "        #real_loss = self.loss_fn(self.discriminator(positive_pair), real)\n",
    "        #fake_loss = self.loss_fn(self.discriminator(negative_pair), fake)\n",
    "        #discrimination_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        score = (s_pp - s_np)\n",
    "\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x_transform[perm] * self.non_lin(score[perm]).view(-1, 1)\n",
    "        batch = batch[perm]\n",
    "\n",
    "        filter_edge_index, filter_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "        return x, filter_edge_index, filter_edge_attr, batch, perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]  # 根据需求调整\n",
    "\n",
    "class HierarchicalGCN_CGI(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_CGI, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(CGIPool(channels))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            #print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_CGI(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# 计算总平均值和标准差\n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMISPooling with HierarchicalGCN (2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Tuple, Union\n",
    "from torch_geometric.typing import Adj, OptTensor, PairTensor, Tensor\n",
    "Scorer = Callable[[Tensor, Adj, OptTensor, OptTensor], Tensor]\n",
    "from torch_sparse import SparseTensor, remove_diag\n",
    "from torch_geometric.nn.aggr import Aggregation\n",
    "from torch_geometric.nn.dense import Linear\n",
    "from torch.nn import Module\n",
    "from torch_scatter import scatter_max, scatter_min\n",
    "\n",
    "def maximal_independent_set(edge_index: Adj, k: int = 1,\n",
    "                            perm: OptTensor = None) -> Tensor:\n",
    "    r\"\"\"Returns a Maximal :math:`k`-Independent Set of a graph, i.e., a set of\n",
    "    nodes (as a :class:`ByteTensor`) such that none of them are :math:`k`-hop\n",
    "    neighbors, and any node in the graph has a :math:`k`-hop neighbor in the\n",
    "    returned set.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method follows `Blelloch's Alogirithm\n",
    "    <https://arxiv.org/abs/1202.3205>`_ for :math:`k = 1`, and its\n",
    "    generalization by `Bacciu et al. <https://arxiv.org/abs/2208.03523>`_ for\n",
    "    higher values of :math:`k`.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: :class:`ByteTensor`\n",
    "    \"\"\"\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "        device = edge_index.device()\n",
    "        n = edge_index.size(0)\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        device = row.device\n",
    "        n = edge_index.max().item() + 1\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    mis = torch.zeros(n, dtype=torch.bool, device=device)\n",
    "    mask = mis.clone()\n",
    "    min_rank = rank.clone()\n",
    "\n",
    "    while not mask.all():\n",
    "        for _ in range(k):\n",
    "            min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "            scatter_min(min_rank[row], col, out=min_neigh)\n",
    "            torch.minimum(min_neigh, min_rank, out=min_rank)  # self-loops\n",
    "\n",
    "        mis = mis | torch.eq(rank, min_rank)\n",
    "        mask = mis.clone().byte()\n",
    "\n",
    "        for _ in range(k):\n",
    "            max_neigh = torch.full_like(mask, fill_value=0)\n",
    "            scatter_max(mask[row], col, out=max_neigh)\n",
    "            torch.maximum(max_neigh, mask, out=mask)  # self-loops\n",
    "\n",
    "        mask = mask.to(dtype=torch.bool)\n",
    "        min_rank = rank.clone()\n",
    "        min_rank[mask] = n\n",
    "\n",
    "    return mis\n",
    "\n",
    "def maximal_independent_set_cluster(edge_index: Adj, k: int = 1,\n",
    "                                    perm: OptTensor = None) -> PairTensor:\n",
    "    r\"\"\"Computes the Maximal :math:`k`-Independent Set (:math:`k`-MIS)\n",
    "    clustering of a graph, as defined in `\"Generalizing Downsampling from\n",
    "    Regular Data to Graphs\" <https://arxiv.org/abs/2208.03523>`_.\n",
    "    The algorithm greedily selects the nodes in their canonical order. If a\n",
    "    permutation :obj:`perm` is provided, the nodes are extracted following\n",
    "    that permutation instead.\n",
    "    This method returns both the :math:`k`-MIS and the clustering, where the\n",
    "    :math:`c`-th cluster refers to the :math:`c`-th element of the\n",
    "    :math:`k`-MIS.\n",
    "    Args:\n",
    "        edge_index (Tensor or SparseTensor): The graph connectivity.\n",
    "        k (int): The :math:`k` value (defaults to 1).\n",
    "        perm (LongTensor, optional): Permutation vector. Must be of size\n",
    "            :obj:`(n,)` (defaults to :obj:`None`).\n",
    "    :rtype: (:class:`ByteTensor`, :class:`LongTensor`)\n",
    "    \"\"\"\n",
    "    mis = maximal_independent_set(edge_index=edge_index, k=k, perm=perm)\n",
    "    n, device = mis.size(0), mis.device\n",
    "\n",
    "    if isinstance(edge_index, SparseTensor):\n",
    "        row, col, _ = edge_index.coo()\n",
    "    else:\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "\n",
    "    if perm is None:\n",
    "        rank = torch.arange(n, dtype=torch.long, device=device)\n",
    "    else:\n",
    "        rank = torch.zeros_like(perm)\n",
    "        rank[perm] = torch.arange(n, dtype=torch.long, device=device)\n",
    "\n",
    "    min_rank = torch.full((n, ), fill_value=n, dtype=torch.long, device=device)\n",
    "    rank_mis = rank[mis]\n",
    "    min_rank[mis] = rank_mis\n",
    "\n",
    "    for _ in range(k):\n",
    "        min_neigh = torch.full_like(min_rank, fill_value=n)\n",
    "        scatter_min(min_rank[row], col, out=min_neigh)\n",
    "        torch.minimum(min_neigh, min_rank, out=min_rank)\n",
    "\n",
    "    _, clusters = torch.unique(min_rank, return_inverse=True)\n",
    "    perm = torch.argsort(rank_mis)\n",
    "    return mis, perm[clusters]\n",
    "\n",
    "\n",
    "class KMISPooling(Module):\n",
    "\n",
    "    _heuristics = {None, 'greedy', 'w-greedy'}\n",
    "    _passthroughs = {None, 'before', 'after'}\n",
    "    _scorers = {\n",
    "        'linear',\n",
    "        'random',\n",
    "        'constant',\n",
    "        'canonical',\n",
    "        'first',\n",
    "        'last',\n",
    "    }\n",
    "\n",
    "    def __init__(self, in_channels: Optional[int] = None, k: int = 1,\n",
    "                 scorer: Union[Scorer, str] = 'linear',\n",
    "                 score_heuristic: Optional[str] = 'greedy',\n",
    "                 score_passthrough: Optional[str] = 'before',\n",
    "                 aggr_x: Optional[Union[str, Aggregation]] = None,\n",
    "                 aggr_edge: str = 'sum',\n",
    "                 aggr_score: Callable[[Tensor, Tensor], Tensor] = torch.mul,\n",
    "                 remove_self_loops: bool = True) -> None:\n",
    "        super(KMISPooling, self).__init__()\n",
    "        assert score_heuristic in self._heuristics, \\\n",
    "            \"Unrecognized `score_heuristic` value.\"\n",
    "        assert score_passthrough in self._passthroughs, \\\n",
    "            \"Unrecognized `score_passthrough` value.\"\n",
    "\n",
    "        if not callable(scorer):\n",
    "            assert scorer in self._scorers, \\\n",
    "                \"Unrecognized `scorer` value.\"\n",
    "\n",
    "        self.k = k\n",
    "        self.scorer = scorer\n",
    "        self.score_heuristic = score_heuristic\n",
    "        self.score_passthrough = score_passthrough\n",
    "\n",
    "        self.aggr_x = aggr_x\n",
    "        self.aggr_edge = aggr_edge\n",
    "        self.aggr_score = aggr_score\n",
    "        self.remove_self_loops = remove_self_loops\n",
    "\n",
    "        if scorer == 'linear':\n",
    "            assert self.score_passthrough is not None, \\\n",
    "                \"`'score_passthrough'` must not be `None`\" \\\n",
    "                \" when using `'linear'` scorer\"\n",
    "\n",
    "            self.lin = Linear(in_features=in_channels, out_features=1)\n",
    "\n",
    "    def _apply_heuristic(self, x: Tensor, adj: SparseTensor) -> Tensor:\n",
    "        if self.score_heuristic is None:\n",
    "            return x\n",
    "\n",
    "        row, col, _ = adj.coo()\n",
    "        x = x.view(-1)\n",
    "\n",
    "        if self.score_heuristic == 'greedy':\n",
    "            k_sums = torch.ones_like(x)\n",
    "        else:\n",
    "            k_sums = x.clone()\n",
    "\n",
    "        for _ in range(self.k):\n",
    "            scatter_add(k_sums[row], col, out=k_sums)\n",
    "\n",
    "        return x / k_sums\n",
    "\n",
    "    def _scorer(self, x: Tensor, edge_index: Adj, edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) -> Tensor:\n",
    "        if self.scorer == 'linear':\n",
    "            return self.lin(x).sigmoid()\n",
    "\n",
    "        if self.scorer == 'random':\n",
    "            return torch.rand((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'constant':\n",
    "            return torch.ones((x.size(0), 1), device=x.device)\n",
    "\n",
    "        if self.scorer == 'canonical':\n",
    "            return -torch.arange(x.size(0), device=x.device).view(-1, 1)\n",
    "\n",
    "        if self.scorer == 'first':\n",
    "            return x[..., [0]]\n",
    "\n",
    "        if self.scorer == 'last':\n",
    "            return x[..., [-1]]\n",
    "\n",
    "        return self.scorer(x, edge_index, edge_attr, batch)\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Adj,\n",
    "                edge_attr: OptTensor = None,\n",
    "                batch: OptTensor = None) \\\n",
    "            -> Tuple[Tensor, Adj, OptTensor, OptTensor, Tensor, Tensor]:\n",
    "        \"\"\"\"\"\"\n",
    "        edge_index = edge_index.long()\n",
    "        adj, n = edge_index, x.size(0)\n",
    "\n",
    "        if not isinstance(edge_index, SparseTensor):\n",
    "            adj = SparseTensor.from_edge_index(edge_index, edge_attr, (n, n))\n",
    "\n",
    "        score = self._scorer(x, edge_index, edge_attr, batch)\n",
    "        updated_score = self._apply_heuristic(score, adj)\n",
    "        perm = torch.argsort(updated_score.view(-1), 0, descending=True)\n",
    "\n",
    "        mis, cluster = maximal_independent_set_cluster(adj, self.k, perm)\n",
    "\n",
    "        row, col, val = adj.coo()\n",
    "        c = mis.sum()\n",
    "\n",
    "        if val is None:\n",
    "            val = torch.ones_like(row, dtype=torch.float)\n",
    "\n",
    "        adj = SparseTensor(row=cluster[row], col=cluster[col], value=val,\n",
    "                           is_sorted=False,\n",
    "                           sparse_sizes=(c, c)).coalesce(self.aggr_edge)\n",
    "\n",
    "        if self.remove_self_loops:\n",
    "            adj = remove_diag(adj)\n",
    "\n",
    "        if self.score_passthrough == 'before':\n",
    "            x = self.aggr_score(x, score)\n",
    "\n",
    "        if self.aggr_x is None:\n",
    "            x = x[mis]\n",
    "        elif isinstance(self.aggr_x, str):\n",
    "            x = scatter(x, cluster, dim=0, dim_size=mis.sum(),\n",
    "                        reduce=self.aggr_x)\n",
    "        else:\n",
    "            x = self.aggr_x(x, cluster, dim_size=c)\n",
    "\n",
    "        if self.score_passthrough == 'after':\n",
    "            x = self.aggr_score(x, score[mis])\n",
    "\n",
    "        if isinstance(edge_index, SparseTensor):\n",
    "            edge_index, edge_attr = adj, None\n",
    "\n",
    "        else:\n",
    "            row, col, edge_attr = adj.coo()\n",
    "            edge_index = torch.stack([row, col])\n",
    "\n",
    "        if batch is not None:\n",
    "            batch = batch[mis]\n",
    "\n",
    "        #attn = x\n",
    "        #select_out = topk(attn, batch)\n",
    "        perm = perm[mis]\n",
    "\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, mis, cluster, perm\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.scorer == 'linear':\n",
    "            channels = f\"in_channels={self.lin.in_channels}, \"\n",
    "        else:\n",
    "            channels = \"\"\n",
    "\n",
    "        return f'{self.__class__.__name__}({channels}k={self.k})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]  # 根据需求调整\n",
    "\n",
    "class HierarchicalGCN_KMIS(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_KMIS, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(KMISPooling(64, k=5, aggr_x='sum'))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, _, cluster, perm = self.pools[i - 1](x, edge_index, batch=batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            #print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_KMIS(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# 计算总平均值和标准差\n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GSAPooling with HierarchicalGCN (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Union, Optional, Callable\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, ChebConv, GraphConv\n",
    "\n",
    "def uniform(size, tensor):\n",
    "    if tensor is not None:\n",
    "        bound = 1.0 / math.sqrt(size)\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "\n",
    "def topk(x, ratio, batch, min_score=None, tol=1e-7):\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n",
    "    else:\n",
    "        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "        cum_num_nodes = torch.cat(\n",
    "            [num_nodes.new_zeros(1),\n",
    "             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "        dense_x = x.new_full((batch_size * max_num_nodes, ),\n",
    "                             torch.finfo(x.dtype).min)\n",
    "        dense_x[index] = x\n",
    "        dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "        _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "        perm = perm + cum_num_nodes.view(-1, 1)\n",
    "        perm = perm.view(-1)\n",
    "\n",
    "        if isinstance(ratio, int):\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n",
    "            k = torch.min(k, num_nodes)\n",
    "        else:\n",
    "            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "\n",
    "        mask = [\n",
    "            torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "            i * max_num_nodes for i in range(batch_size)\n",
    "        ]\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "\n",
    "        perm = perm[mask]\n",
    "\n",
    "    return perm\n",
    "\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "\n",
    "class GSAPool(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n",
    "                        min_score=None, multiplier=1,\n",
    "                        non_linearity=torch.tanh,\n",
    "                        cus_drop_ratio =0):\n",
    "        super(GSAPool,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.ratio = pooling_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.sbtl_layer = GCNConv(in_channels,1)\n",
    "        self.fbtl_layer = nn.Linear(in_channels, 1)\n",
    "        self.fusion = GCNConv(in_channels,in_channels)\n",
    "\n",
    "        self.min_score = min_score\n",
    "        self.multiplier = multiplier\n",
    "        self.fusion_flag = 0\n",
    "        self.non_linearity = non_linearity\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n",
    "\n",
    "    def conv_selection(self, conv, in_channels, conv_type=0):\n",
    "        if(conv_type == 0):\n",
    "            out_channels = 1\n",
    "        elif(conv_type == 1):\n",
    "            out_channels = in_channels\n",
    "        if(conv == \"GCNConv\"):\n",
    "            return GCNConv(in_channels,out_channels)\n",
    "        elif(conv == \"ChebConv\"):\n",
    "            return ChebConv(in_channels,out_channels,1)\n",
    "        elif(conv == \"SAGEConv\"):\n",
    "            return SAGEConv(in_channels,out_channels)\n",
    "        elif(conv == \"GATConv\"):\n",
    "            return GATConv(in_channels,out_channels, heads=1, concat=True)\n",
    "        elif(conv == \"GraphConv\"):\n",
    "            return GraphConv(in_channels,out_channels)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "\n",
    "        #SBTL\n",
    "        score_s = self.sbtl_layer(x,edge_index).squeeze()\n",
    "        #FBTL\n",
    "        score_f = self.fbtl_layer(x).squeeze()\n",
    "        #hyperparametr alpha\n",
    "        score = score_s*self.alpha + score_f*(1-self.alpha)\n",
    "\n",
    "        score = score.unsqueeze(-1) if score.dim()==0 else score\n",
    "\n",
    "        if self.min_score is None:\n",
    "            score = self.non_linearity(score)\n",
    "        else:\n",
    "            score = softmax(score, batch)\n",
    "\n",
    "        sc = self.dropout(score)\n",
    "        perm = topk(sc, self.ratio, batch)\n",
    "\n",
    "        #fusion\n",
    "        if(self.fusion_flag == 1):\n",
    "            x = self.fusion(x, edge_index)\n",
    "        x_ae = x[perm]\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        x = self.multiplier * x if self.multiplier != 1 else x\n",
    "\n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_attr = filter_adj(\n",
    "            edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, perm, x_ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]  # 根据需求调整\n",
    "\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(GSAPool(64, pooling_ratio=0.5, alpha = 0.6, cus_drop_ratio = 0))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, edge_attr, batch, perm, _ = self.pools[i - 1](x, edge_index, None, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            #print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_GSA(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# 计算总平均值和标准差\n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HGPSLPooling with HierarchicalGCN (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from torch_geometric.utils import softmax, dense_to_sparse, add_remaining_self_loops\n",
    "def topk(x, ratio, batch, min_score=None, tol=1e-7):\n",
    "\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = torch.nonzero(x > scores_min).view(-1)\n",
    "    else:\n",
    "        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "        cum_num_nodes = torch.cat(\n",
    "            [num_nodes.new_zeros(1),\n",
    "            num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "        dense_x = x.new_full((batch_size * max_num_nodes, ), -2)\n",
    "        dense_x[index] = x\n",
    "        dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "        _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "        perm = perm + cum_num_nodes.view(-1, 1)\n",
    "        perm = perm.view(-1)\n",
    "\n",
    "        k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "        mask = [\n",
    "            torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "            i * max_num_nodes for i in range(batch_size)\n",
    "        ]\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "\n",
    "        perm = perm[mask]\n",
    "\n",
    "    return perm\n",
    "\n",
    "def filter_adj(edge_index, edge_weight, perm, num_nodes=None):\n",
    "\n",
    "        num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "        mask = perm.new_full((num_nodes, ), -1)\n",
    "        i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "        mask[perm] = i\n",
    "\n",
    "        row, col = edge_index\n",
    "        row, col = mask[row], mask[col]\n",
    "        mask = (row >= 0) & (col >= 0)\n",
    "        row, col = row[mask], col[mask]\n",
    "\n",
    "        if edge_weight is not None:\n",
    "            edge_weight = edge_weight[mask]\n",
    "\n",
    "        return torch.stack([row, col], dim=0), edge_weight\n",
    "\n",
    "def scatter_sort(x, batch, fill_value=-1e16):\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "    index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "    dense_x = x.new_full((batch_size * max_num_nodes,), fill_value)\n",
    "    dense_x[index] = x\n",
    "    dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "    sorted_x, _ = dense_x.sort(dim=-1, descending=True)\n",
    "    cumsum_sorted_x = sorted_x.cumsum(dim=-1)\n",
    "    cumsum_sorted_x = cumsum_sorted_x.view(-1)\n",
    "\n",
    "    sorted_x = sorted_x.view(-1)\n",
    "    filled_index = sorted_x != fill_value\n",
    "\n",
    "    sorted_x = sorted_x[filled_index]\n",
    "    cumsum_sorted_x = cumsum_sorted_x[filled_index]\n",
    "\n",
    "    return sorted_x, cumsum_sorted_x\n",
    "\n",
    "\n",
    "def _make_ix_like(batch):\n",
    "    num_nodes = scatter_add(batch.new_ones(batch.size(0)), batch, dim=0)\n",
    "    idx = [torch.arange(1, i + 1, dtype=torch.long, device=batch.device) for i in num_nodes]\n",
    "    idx = torch.cat(idx, dim=0)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def _threshold_and_support(x, batch):\n",
    "    \"\"\"Sparsemax building block: compute the threshold\n",
    "    Args:\n",
    "        x: input tensor to apply the sparsemax\n",
    "        batch: group indicators\n",
    "    Returns:\n",
    "        the threshold value\n",
    "    \"\"\"\n",
    "    num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "    cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "    sorted_input, input_cumsum = scatter_sort(x, batch)\n",
    "    input_cumsum = input_cumsum - 1.0\n",
    "    rhos = _make_ix_like(batch).to(x.dtype)\n",
    "    support = rhos * sorted_input > input_cumsum\n",
    "\n",
    "    support_size = scatter_add(support.to(batch.dtype), batch)\n",
    "    # mask invalid index, for example, if batch is not start from 0 or not continuous, it may result in negative index\n",
    "    idx = support_size + cum_num_nodes - 1\n",
    "    mask = idx < 0\n",
    "    idx[mask] = 0\n",
    "    tau = input_cumsum.gather(0, idx)\n",
    "    tau /= support_size.to(x.dtype)\n",
    "\n",
    "    return tau, support_size\n",
    "\n",
    "\n",
    "class SparsemaxFunction(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, batch):\n",
    "        \"\"\"sparsemax: normalizing sparse transform\n",
    "        Parameters:\n",
    "            ctx: context object\n",
    "            x (Tensor): shape (N, )\n",
    "            batch: group indicator\n",
    "        Returns:\n",
    "            output (Tensor): same shape as input\n",
    "        \"\"\"\n",
    "        max_val, _ = scatter_max(x, batch)\n",
    "        x -= max_val[batch]\n",
    "        tau, supp_size = _threshold_and_support(x, batch)\n",
    "        output = torch.clamp(x - tau[batch], min=0)\n",
    "        ctx.save_for_backward(supp_size, output, batch)\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        supp_size, output, batch = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[output == 0] = 0\n",
    "\n",
    "        v_hat = scatter_add(grad_input, batch) / supp_size.to(output.dtype)\n",
    "        grad_input = torch.where(output != 0, grad_input - v_hat[batch], grad_input)\n",
    "\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "sparsemax = SparsemaxFunction.apply\n",
    "\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Sparsemax, self).__init__()\n",
    "\n",
    "    def forward(self, x, batch):\n",
    "        return sparsemax(x, batch)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sparse_attention = Sparsemax()\n",
    "    input_x = torch.tensor([1.7301, 0.6792, -1.0565, 1.6614, -0.3196, -0.7790, -0.3877, -0.4943, 0.1831, -0.0061])\n",
    "    input_batch = torch.cat([torch.zeros(4, dtype=torch.long), torch.ones(6, dtype=torch.long)], dim=0)\n",
    "    res = sparse_attention(input_x, input_batch)\n",
    "    print(res)\n",
    "\n",
    "class TwoHopNeighborhood(object):\n",
    "    def __call__(self, data):\n",
    "        edge_index, edge_attr = data.edge_index, data.edge_attr\n",
    "        n = data.num_nodes\n",
    "\n",
    "        fill = 1e16\n",
    "        value = edge_index.new_full((edge_index.size(1),), fill, dtype=torch.float)\n",
    "\n",
    "        index, value = spspmm(edge_index, value, edge_index, value, n, n, n, True)\n",
    "\n",
    "        edge_index = torch.cat([edge_index, index], dim=1)\n",
    "        if edge_attr is None:\n",
    "            data.edge_index, _ = coalesce(edge_index, None, n, n)\n",
    "        else:\n",
    "            value = value.view(-1, *[1 for _ in range(edge_attr.dim() - 1)])\n",
    "            value = value.expand(-1, *list(edge_attr.size())[1:])\n",
    "            edge_attr = torch.cat([edge_attr, value], dim=0)\n",
    "            data.edge_index, edge_attr = coalesce(edge_index, edge_attr, n, n, op='min', fill_value=fill)\n",
    "            edge_attr[edge_attr >= fill] = 0\n",
    "            data.edge_attr = edge_attr\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)\n",
    "\n",
    "\n",
    "class GCN(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, cached=False, bias=True, **kwargs):\n",
    "        super(GCN, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        nn.init.xavier_uniform_(self.weight.data)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "            nn.init.zeros_(self.bias.data)\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        return edge_index, deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        x = torch.matmul(x, self.weight)\n",
    "\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        if self.bias is not None:\n",
    "            aggr_out = aggr_out + self.bias\n",
    "        return aggr_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {})'.format(self.__class__.__name__, self.in_channels, self.out_channels)\n",
    "\n",
    "\n",
    "class NodeInformationScore(MessagePassing):\n",
    "    def __init__(self, improved=False, cached=False, **kwargs):\n",
    "        super(NodeInformationScore, self).__init__(aggr='add', **kwargs)\n",
    "\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.cached_result = None\n",
    "        self.cached_num_edges = None\n",
    "\n",
    "    @staticmethod\n",
    "    def norm(edge_index, num_nodes, edge_weight, dtype=None):\n",
    "        if edge_weight is None:\n",
    "            edge_weight = torch.ones((edge_index.size(1),), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        row, col = edge_index\n",
    "        deg = scatter_add(edge_weight, row, dim=0, dim_size=num_nodes)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "\n",
    "        edge_index, edge_weight = add_remaining_self_loops(edge_index, edge_weight, 0, num_nodes)\n",
    "\n",
    "        row, col = edge_index\n",
    "        expand_deg = torch.zeros((edge_weight.size(0),), dtype=dtype, device=edge_index.device)\n",
    "        expand_deg[-num_nodes:] = torch.ones((num_nodes,), dtype=dtype, device=edge_index.device)\n",
    "\n",
    "        return edge_index, expand_deg - deg_inv_sqrt[row] * edge_weight * deg_inv_sqrt[col]\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        if self.cached and self.cached_result is not None:\n",
    "            if edge_index.size(1) != self.cached_num_edges:\n",
    "                raise RuntimeError(\n",
    "                    'Cached {} number of edges, but found {}'.format(self.cached_num_edges, edge_index.size(1)))\n",
    "\n",
    "        if not self.cached or self.cached_result is None:\n",
    "            self.cached_num_edges = edge_index.size(1)\n",
    "            edge_index, norm = self.norm(edge_index, x.size(0), edge_weight, x.dtype)\n",
    "            self.cached_result = edge_index, norm\n",
    "\n",
    "        edge_index, norm = self.cached_result\n",
    "\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "class HGPSLPool(torch.nn.Module):\n",
    "    def __init__(self, in_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2):\n",
    "        super(HGPSLPool, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.ratio = ratio\n",
    "        self.sample = sample\n",
    "        self.sparse = sparse\n",
    "        self.sl = sl\n",
    "        self.negative_slop = negative_slop\n",
    "        self.lamb = lamb\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, self.in_channels * 2))\n",
    "        nn.init.xavier_uniform_(self.att.data)\n",
    "        self.sparse_attention = Sparsemax()\n",
    "        self.neighbor_augment = TwoHopNeighborhood()\n",
    "        self.calc_information_score = NodeInformationScore()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "\n",
    "        x_information_score = self.calc_information_score(x, edge_index, edge_attr)\n",
    "        score = torch.sum(torch.abs(x_information_score), dim=1)\n",
    "\n",
    "        # Graph Pooling\n",
    "        original_x = x\n",
    "        perm = topk(score, self.ratio, batch)\n",
    "        x = x[perm]\n",
    "        batch = batch[perm]\n",
    "        induced_edge_index, induced_edge_attr = filter_adj(edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        # Discard structure learning layer, directly return\n",
    "        if self.sl is False:\n",
    "            return x, induced_edge_index, induced_edge_attr, batch\n",
    "\n",
    "        # Structure Learning\n",
    "        if self.sample:\n",
    "            # A fast mode for large graphs.\n",
    "            # In large graphs, learning the possible edge weights between each pair of nodes is time consuming.\n",
    "            # To accelerate this process, we sample it's K-Hop neighbors for each node and then learn the\n",
    "            # edge weights between them.\n",
    "            k_hop = 3\n",
    "            if edge_attr is None:\n",
    "                edge_attr = torch.ones((edge_index.size(1),), dtype=torch.float, device=edge_index.device)\n",
    "\n",
    "            hop_data = Data(x=original_x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            for _ in range(k_hop - 1):\n",
    "                hop_data = self.neighbor_augment(hop_data)\n",
    "            hop_edge_index = hop_data.edge_index\n",
    "            hop_edge_attr = hop_data.edge_attr\n",
    "            new_edge_index, new_edge_attr = filter_adj(hop_edge_index, hop_edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "            new_edge_index, new_edge_attr = add_remaining_self_loops(new_edge_index, new_edge_attr, 0, x.size(0))\n",
    "            row, col = new_edge_index\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop) + new_edge_attr * self.lamb\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            adj[row, col] = weights\n",
    "            new_edge_index, weights = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Learning the possible edge weights between each pair of nodes in the pooled subgraph, relative slower.\n",
    "            if edge_attr is None:\n",
    "                induced_edge_attr = torch.ones((induced_edge_index.size(1),), dtype=x.dtype,\n",
    "                                               device=induced_edge_index.device)\n",
    "            num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "            shift_cum_num_nodes = torch.cat([num_nodes.new_zeros(1), num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "            cum_num_nodes = num_nodes.cumsum(dim=0)\n",
    "            adj = torch.zeros((x.size(0), x.size(0)), dtype=torch.float, device=x.device)\n",
    "            # Construct batch fully connected graph in block diagonal matirx format\n",
    "            for idx_i, idx_j in zip(shift_cum_num_nodes, cum_num_nodes):\n",
    "                adj[idx_i:idx_j, idx_i:idx_j] = 1.0\n",
    "            new_edge_index, _ = dense_to_sparse(adj)\n",
    "            row, col = new_edge_index\n",
    "\n",
    "            weights = (torch.cat([x[row], x[col]], dim=1) * self.att).sum(dim=-1)\n",
    "            weights = F.leaky_relu(weights, self.negative_slop)\n",
    "            adj[row, col] = weights\n",
    "            induced_row, induced_col = induced_edge_index\n",
    "\n",
    "            adj[induced_row, induced_col] += induced_edge_attr * self.lamb\n",
    "            weights = adj[row, col]\n",
    "            if self.sparse:\n",
    "                new_edge_attr = self.sparse_attention(weights, row)\n",
    "            else:\n",
    "                new_edge_attr = softmax(weights, row, num_nodes=x.size(0))\n",
    "            # filter out zero weight edges\n",
    "            adj[row, col] = new_edge_attr\n",
    "            new_edge_index, new_edge_attr = dense_to_sparse(adj)\n",
    "            # release gpu memory\n",
    "            del adj\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return x, new_edge_index, new_edge_attr, batch, perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "import torch\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, TopKPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Callable, Optional, Union\n",
    "\n",
    "dataset = dataset_sparse\n",
    "graph = dataset[0]\n",
    "num_classes = dataset.num_classes\n",
    "in_channels = dataset.num_features\n",
    "hidden_channels = 64\n",
    "out_channels = num_classes\n",
    "depth = 2\n",
    "pool_ratios = [0.5, 0.5]  # 根据需求调整\n",
    "\n",
    "class HierarchicalGCN_HGPSL(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, depth, pool_ratios, act=F.relu, sum_res=False):\n",
    "        super(HierarchicalGCN_HGPSL, self).__init__()\n",
    "        assert depth >= 1\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.depth = depth\n",
    "        self.pool_ratios = pool_ratios\n",
    "        self.act = act\n",
    "        self.sum_res = sum_res\n",
    "\n",
    "        channels = self.hidden_channels\n",
    "\n",
    "        self.down_convs = torch.nn.ModuleList()\n",
    "        self.pools = torch.nn.ModuleList()\n",
    "        self.down_convs.append(SAGEConv(self.in_channels, channels))\n",
    "        for i in range(self.depth):\n",
    "            self.pools.append(HGPSLPool(hidden_channels, ratio=0.5, sample=False, sparse=False, sl=True, lamb=1.0, negative_slop=0.2))\n",
    "            self.down_convs.append(SAGEConv(channels, channels))\n",
    "\n",
    "        in_channels = channels if sum_res else 2 * channels\n",
    "\n",
    "        self.up_convs = torch.nn.ModuleList()\n",
    "        for i in range(self.depth):\n",
    "            self.up_convs.append(SAGEConv(in_channels, channels))\n",
    "        self.up_convs.append(SAGEConv(channels, self.out_channels))\n",
    "\n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        x, edge_index = x.to(device), edge_index.to(device)\n",
    "        edge_attr = None\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        if batch is not None:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.down_convs[0](x, edge_index)\n",
    "        x = self.act(x)\n",
    "\n",
    "        xs = [x]\n",
    "        edge_indices = [edge_index]\n",
    "        perms = []\n",
    "\n",
    "        for i in range(1, self.depth + 1):\n",
    "            x, edge_index, _, batch, perm = self.pools[i - 1](x, edge_index, edge_attr, batch)\n",
    "            x = self.down_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "            if i < self.depth:\n",
    "                xs.append(x)\n",
    "                edge_indices.append(edge_index)\n",
    "            perms.append(perm)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            j = self.depth - 1 - i\n",
    "\n",
    "            res = xs[j]\n",
    "            edge_index = edge_indices[j]\n",
    "            perm = perms[j]\n",
    "\n",
    "            up = torch.zeros_like(res)\n",
    "            up[perm] = x\n",
    "            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)\n",
    "            x = self.up_convs[i](x, edge_index)\n",
    "            x = self.act(x)\n",
    "        x = self.up_convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask, n_epochs=200, patience=150, min_delta=0.0001):\n",
    "    best_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    model.to(device)\n",
    "    graph = graph.to(device)  \n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph.x, graph.edge_index)\n",
    "        loss = criterion(out[train_mask], graph.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        val_acc = eval_node_classifier(model, graph, val_mask)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_acc > best_val_acc + min_delta:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0  # Reset patience counter if improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment patience counter if no improvement\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            #print(f'Early stopping at epoch {epoch} for fold {fold + 1}')\n",
    "            break\n",
    "\n",
    "    return model, best_val_acc\n",
    "\n",
    "def eval_node_classifier(model, graph, mask):\n",
    "    model.eval()\n",
    "    pred = model(graph.x, graph.edge_index).argmax(dim=1)\n",
    "    correct = (pred[mask] == graph.y[mask]).sum()\n",
    "    acc = int(correct) / int(mask.sum())\n",
    "    return acc\n",
    "\n",
    "# Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "\n",
    "seeds = [42, 123, 456, 789, 101]\n",
    "results = []\n",
    "val_accuracies_list = []\n",
    "times = []\n",
    "memories = []\n",
    "gpu_memories = []\n",
    "\n",
    "for seed in seeds:\n",
    "    graph = graph.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    val_accuracies = []\n",
    "    start_time = time.time()\n",
    "    tracemalloc.start()\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(graph.x)):\n",
    "        #print(f'Seed {seed}, Fold {fold + 1}')\n",
    "\n",
    "        # Reset model\n",
    "        model = HierarchicalGCN_HGPSL(in_channels, hidden_channels, out_channels, depth, pool_ratios).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Create train and test masks\n",
    "        train_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(graph.num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[train_index] = True\n",
    "        test_mask[test_index] = True\n",
    "\n",
    "        val_mask = test_mask  # In cross-validation, we use test_mask as val_mask\n",
    "\n",
    "        model, best_val_acc = train_node_classifier(model, graph, optimizer, criterion, train_mask, val_mask)\n",
    "\n",
    "        val_accuracies.append(best_val_acc)\n",
    "        print(f'Seed {seed}, Fold {fold + 1} Val Acc: {best_val_acc:.3f}')\n",
    "\n",
    "    mean_val_acc = np.mean(val_accuracies)\n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    memory_usage = peak / 10**6  # Convert to MB\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory_usage = torch.cuda.memory_reserved(device) / 10**6  # Convert to MB\n",
    "    else:\n",
    "        gpu_memory_usage = 0\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    results.append({\n",
    "        'seed': seed,\n",
    "        'mean_val_acc': mean_val_acc,\n",
    "        'time': elapsed_time,\n",
    "        'memory': memory_usage,\n",
    "        'gpu_memory': gpu_memory_usage\n",
    "    })\n",
    "    print(f'Seed {seed} Results: Mean Val Acc: {mean_val_acc:.3f}, Time: {elapsed_time:.3f} seconds, Memory: {memory_usage:.3f} MB, GPU Memory: {gpu_memory_usage:.3f} MB')\n",
    "\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "mean_val_acc_values = [result['mean_val_acc'] for result in results]\n",
    "\n",
    "# 计算总平均值和标准差\n",
    "total_mean_val_acc = np.mean(mean_val_acc_values)\n",
    "standard_deviation = np.std(mean_val_acc_values)\n",
    "\n",
    "print(f\"Total Mean Val Acc: {total_mean_val_acc:.2f}\")\n",
    "print(f\"Standard Deviation: {standard_deviation:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
