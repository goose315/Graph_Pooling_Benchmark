{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Graph Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nimport sys\nimport torch\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nimport torch.nn.functional as F\nimport torch_geometric.transforms as T\nfrom ogb.graphproppred import PygGraphPropPredDataset, Evaluator\nfrom torch_geometric.loader import DataLoader\nimport os\nimport random\nimport pandas as pd\nimport torch\nimport torch_geometric.transforms as T\nfrom typing import Optional\nimport torch\nfrom torch import Tensor\nfrom torch_geometric.data import Data\nfrom torch_geometric.data.datapipes import functional_transform\nfrom torch_geometric.transforms import BaseTransform\nimport torch_geometric.transforms as T\nfrom torch_geometric.datasets import Planetoid\nfrom torch_geometric.datasets import WebKB\nfrom torch_geometric.datasets import Actor\nfrom torch_geometric.datasets import GNNBenchmarkDataset\nfrom torch_geometric.datasets import TUDataset\nfrom sklearn.metrics import r2_score\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import MoleculeNet\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\nfrom torch_geometric.utils import to_networkx\nfrom torch.nn import Linear\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport time\nimport psutil\nimport torch\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### MUTAG"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Early stopping at epoch 151 for seed 43\n",
                        "Average Time: 114.61 seconds\n",
                        "Var Time: 768.89 seconds\n",
                        "Average Memory: 80.00 MB\n",
                        "Average Best Val Acc: 0.6667\n",
                        "Std Best Test Acc: 0.0709\n",
                        "Average Test Acc: 0.6667\n"
                    ]
                }
            ],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nmax_nodes = 150\ndata_path = \"/data/XXX/Pooling/1\"\ndataset_sparse = TUDataset(root=data_path, name=\"MUTAG\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_geometric.nn import BatchNorm\nclass HierarchicalGCN_ASA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_ASA, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = ASAPooling(hidden_channels, ratio=0.9)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = ASAPooling(hidden_channels, ratio=0.9)\n        self.conv3 = GCNConv(hidden_channels, out_channels)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 32)\n        self.lin2 = torch.nn.Linear(32, num_classes)\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _ = self.pool1(x, edge_index, batch=batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _ = self.pool2(x, edge_index, batch=batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_ASA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\ndef train():\n    model.train()\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        data = data.to(device)\n        out = model(data)\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n    return correct / len(loader.dataset)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 150\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    train_ratio = 0.7\n    val_ratio = 0.15\n    val_ratio = 0.15\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n    model = HierarchicalGCN_ASA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 201):\n        loss = train()\n        val_acc = test(valid_loader)\n        test_acc = test(test_loader)\n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\nprint(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### DD"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 0.6919, Val Acc: 0.5495, Test Acc: 0.5586\n",
                        "Seed: 42, Epoch: 002, Loss: 0.6912, Val Acc: 0.5495, Test Acc: 0.5586\n",
                        "Seed: 42, Epoch: 003, Loss: 0.6908, Val Acc: 0.5495, Test Acc: 0.5586\n"
                    ]
                }
            ],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nmax_nodes = 500\ndata_path = \"/data/XXX/Pooling/\"\ndataset_sparse = TUDataset(root=data_path, name=\"DD\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_geometric.nn import BatchNorm\nclass HierarchicalGCN_ASA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_ASA, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = ASAPooling(hidden_channels, ratio=0.7)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = ASAPooling(hidden_channels, ratio=0.7)\n        self.conv3 = GCNConv(hidden_channels, out_channels)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 32)\n        self.lin2 = torch.nn.Linear(32, num_classes)\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _ = self.pool1(x, edge_index, batch=batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _ = self.pool2(x, edge_index, batch=batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_ASA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\ndef train():\n    model.train()\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        data = data.to(device)\n        out = model(data)\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n    return correct / len(loader.dataset)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 150\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    train_ratio = 0.7\n    val_ratio = 0.15\n    val_ratio = 0.15\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n    model = HierarchicalGCN_ASA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 201):\n        loss = train()\n        val_acc = test(valid_loader)\n        test_acc = test(test_loader)\n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\nprint(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### IMDB-MULTI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Seed: 42, Epoch: 001, Loss: 1.1033, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 002, Loss: 1.1026, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 003, Loss: 1.1021, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 004, Loss: 1.1016, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 005, Loss: 1.1009, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 006, Loss: 1.1004, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 007, Loss: 1.0997, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 008, Loss: 1.0990, Val Acc: 0.2711, Test Acc: 0.3511\n",
                        "Seed: 42, Epoch: 009, Loss: 1.0984, Val Acc: 0.2711, Test Acc: 0.3600\n",
                        "Seed: 42, Epoch: 010, Loss: 1.0972, Val Acc: 0.2711, Test Acc: 0.3600\n",
                        "Seed: 42, Epoch: 011, Loss: 1.0968, Val Acc: 0.2711, Test Acc: 0.3600\n",
                        "Seed: 42, Epoch: 012, Loss: 1.0951, Val Acc: 0.2711, Test Acc: 0.3600\n",
                        "Seed: 42, Epoch: 013, Loss: 1.0939, Val Acc: 0.2756, Test Acc: 0.3556\n",
                        "Seed: 42, Epoch: 014, Loss: 1.0932, Val Acc: 0.2800, Test Acc: 0.3689\n",
                        "Seed: 42, Epoch: 015, Loss: 1.0919, Val Acc: 0.2800, Test Acc: 0.3778\n",
                        "Seed: 42, Epoch: 016, Loss: 1.0911, Val Acc: 0.2844, Test Acc: 0.3600\n",
                        "Seed: 42, Epoch: 017, Loss: 1.0880, Val Acc: 0.2978, Test Acc: 0.3689\n",
                        "Seed: 42, Epoch: 018, Loss: 1.0876, Val Acc: 0.3200, Test Acc: 0.3822\n",
                        "Seed: 42, Epoch: 019, Loss: 1.0844, Val Acc: 0.3111, Test Acc: 0.3644\n",
                        "Seed: 42, Epoch: 020, Loss: 1.0810, Val Acc: 0.3200, Test Acc: 0.3467\n",
                        "Seed: 42, Epoch: 021, Loss: 1.0766, Val Acc: 0.3378, Test Acc: 0.3644\n",
                        "Seed: 42, Epoch: 022, Loss: 1.0744, Val Acc: 0.3422, Test Acc: 0.3911\n",
                        "Seed: 42, Epoch: 023, Loss: 1.0719, Val Acc: 0.3422, Test Acc: 0.3956\n",
                        "Seed: 42, Epoch: 024, Loss: 1.0708, Val Acc: 0.3511, Test Acc: 0.4089\n",
                        "Seed: 42, Epoch: 025, Loss: 1.0661, Val Acc: 0.3511, Test Acc: 0.3867\n",
                        "Seed: 42, Epoch: 026, Loss: 1.0628, Val Acc: 0.3733, Test Acc: 0.3867\n",
                        "Seed: 42, Epoch: 027, Loss: 1.0586, Val Acc: 0.3511, Test Acc: 0.3956\n",
                        "Seed: 42, Epoch: 028, Loss: 1.0554, Val Acc: 0.3956, Test Acc: 0.4222\n",
                        "Seed: 42, Epoch: 029, Loss: 1.0536, Val Acc: 0.4889, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 030, Loss: 1.0433, Val Acc: 0.4756, Test Acc: 0.4400\n",
                        "Seed: 42, Epoch: 031, Loss: 1.0475, Val Acc: 0.4756, Test Acc: 0.4444\n",
                        "Seed: 42, Epoch: 032, Loss: 1.0446, Val Acc: 0.4711, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 033, Loss: 1.0409, Val Acc: 0.4800, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 034, Loss: 1.0364, Val Acc: 0.4933, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 035, Loss: 1.0299, Val Acc: 0.4933, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 036, Loss: 1.0269, Val Acc: 0.4844, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 037, Loss: 1.0133, Val Acc: 0.4844, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 038, Loss: 1.0138, Val Acc: 0.5022, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 039, Loss: 1.0089, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 040, Loss: 1.0090, Val Acc: 0.4800, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 041, Loss: 1.0094, Val Acc: 0.4933, Test Acc: 0.4444\n",
                        "Seed: 42, Epoch: 042, Loss: 1.0138, Val Acc: 0.4889, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 043, Loss: 1.0061, Val Acc: 0.4978, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 044, Loss: 1.0092, Val Acc: 0.4978, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 045, Loss: 1.0065, Val Acc: 0.4889, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 046, Loss: 0.9985, Val Acc: 0.4933, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 047, Loss: 0.9920, Val Acc: 0.4756, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 048, Loss: 0.9811, Val Acc: 0.4800, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 049, Loss: 0.9867, Val Acc: 0.4756, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 050, Loss: 0.9893, Val Acc: 0.4800, Test Acc: 0.4489\n",
                        "Seed: 42, Epoch: 051, Loss: 0.9835, Val Acc: 0.4844, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 052, Loss: 0.9864, Val Acc: 0.4933, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 053, Loss: 0.9791, Val Acc: 0.4800, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 054, Loss: 0.9822, Val Acc: 0.4844, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 055, Loss: 0.9792, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 056, Loss: 0.9813, Val Acc: 0.4889, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 057, Loss: 0.9843, Val Acc: 0.4800, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 058, Loss: 0.9703, Val Acc: 0.4933, Test Acc: 0.4489\n",
                        "Seed: 42, Epoch: 059, Loss: 0.9792, Val Acc: 0.4933, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 060, Loss: 0.9903, Val Acc: 0.4844, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 061, Loss: 0.9947, Val Acc: 0.4711, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 062, Loss: 0.9905, Val Acc: 0.4889, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 063, Loss: 0.9869, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 064, Loss: 0.9716, Val Acc: 0.4844, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 065, Loss: 0.9851, Val Acc: 0.4800, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 066, Loss: 0.9936, Val Acc: 0.4933, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 067, Loss: 0.9923, Val Acc: 0.4844, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 068, Loss: 0.9930, Val Acc: 0.4756, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 069, Loss: 0.9816, Val Acc: 0.4711, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 070, Loss: 0.9764, Val Acc: 0.4889, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 071, Loss: 0.9700, Val Acc: 0.4889, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 072, Loss: 0.9722, Val Acc: 0.4889, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 073, Loss: 0.9793, Val Acc: 0.4844, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 074, Loss: 0.9761, Val Acc: 0.4800, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 075, Loss: 0.9772, Val Acc: 0.4711, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 076, Loss: 0.9723, Val Acc: 0.4889, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 077, Loss: 0.9742, Val Acc: 0.4889, Test Acc: 0.5067\n",
                        "Seed: 42, Epoch: 078, Loss: 0.9868, Val Acc: 0.4933, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 079, Loss: 0.9830, Val Acc: 0.4933, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 080, Loss: 0.9768, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 081, Loss: 0.9846, Val Acc: 0.4667, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 082, Loss: 0.9689, Val Acc: 0.4800, Test Acc: 0.5067\n",
                        "Seed: 42, Epoch: 083, Loss: 0.9721, Val Acc: 0.4933, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 084, Loss: 0.9657, Val Acc: 0.4933, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 085, Loss: 0.9649, Val Acc: 0.4933, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 086, Loss: 0.9682, Val Acc: 0.4933, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 087, Loss: 0.9676, Val Acc: 0.4978, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 088, Loss: 0.9644, Val Acc: 0.4933, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 089, Loss: 0.9619, Val Acc: 0.4889, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 090, Loss: 0.9577, Val Acc: 0.4800, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 091, Loss: 0.9573, Val Acc: 0.4756, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 092, Loss: 0.9675, Val Acc: 0.4889, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 093, Loss: 0.9667, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 094, Loss: 0.9780, Val Acc: 0.4889, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 095, Loss: 0.9772, Val Acc: 0.4889, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 096, Loss: 0.9770, Val Acc: 0.4978, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 097, Loss: 0.9711, Val Acc: 0.4933, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 098, Loss: 0.9722, Val Acc: 0.4756, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 099, Loss: 0.9578, Val Acc: 0.4844, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 100, Loss: 0.9622, Val Acc: 0.4933, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 101, Loss: 0.9691, Val Acc: 0.4933, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 102, Loss: 0.9627, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 103, Loss: 0.9661, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 104, Loss: 0.9581, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 105, Loss: 0.9553, Val Acc: 0.4844, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 106, Loss: 0.9552, Val Acc: 0.4711, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 107, Loss: 0.9530, Val Acc: 0.4756, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 108, Loss: 0.9550, Val Acc: 0.4800, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 109, Loss: 0.9562, Val Acc: 0.4711, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 110, Loss: 0.9585, Val Acc: 0.4889, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 111, Loss: 0.9459, Val Acc: 0.4933, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 112, Loss: 0.9508, Val Acc: 0.4800, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 113, Loss: 0.9488, Val Acc: 0.4800, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 114, Loss: 0.9534, Val Acc: 0.4889, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 115, Loss: 0.9479, Val Acc: 0.4844, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 116, Loss: 0.9503, Val Acc: 0.4711, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 117, Loss: 0.9577, Val Acc: 0.4844, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 118, Loss: 0.9582, Val Acc: 0.4844, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 119, Loss: 0.9603, Val Acc: 0.4844, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 120, Loss: 0.9551, Val Acc: 0.4711, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 121, Loss: 0.9580, Val Acc: 0.4756, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 122, Loss: 0.9535, Val Acc: 0.4756, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 123, Loss: 0.9571, Val Acc: 0.4800, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 124, Loss: 0.9581, Val Acc: 0.4800, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 125, Loss: 0.9536, Val Acc: 0.4800, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 126, Loss: 0.9422, Val Acc: 0.4756, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 127, Loss: 0.9480, Val Acc: 0.4800, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 128, Loss: 0.9515, Val Acc: 0.4889, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 129, Loss: 0.9420, Val Acc: 0.4844, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 130, Loss: 0.9504, Val Acc: 0.4889, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 131, Loss: 0.9484, Val Acc: 0.4800, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 132, Loss: 0.9515, Val Acc: 0.4800, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 133, Loss: 0.9440, Val Acc: 0.4800, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 134, Loss: 0.9463, Val Acc: 0.4933, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 135, Loss: 0.9492, Val Acc: 0.4889, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 136, Loss: 0.9479, Val Acc: 0.4800, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 137, Loss: 0.9433, Val Acc: 0.4578, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 138, Loss: 0.9476, Val Acc: 0.4800, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 139, Loss: 0.9461, Val Acc: 0.4933, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 140, Loss: 0.9473, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 141, Loss: 0.9459, Val Acc: 0.4800, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 142, Loss: 0.9428, Val Acc: 0.4844, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 143, Loss: 0.9452, Val Acc: 0.5022, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 144, Loss: 0.9448, Val Acc: 0.4933, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 145, Loss: 0.9475, Val Acc: 0.4844, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 146, Loss: 0.9507, Val Acc: 0.4756, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 147, Loss: 0.9494, Val Acc: 0.4889, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 148, Loss: 0.9554, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 149, Loss: 0.9597, Val Acc: 0.4933, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 150, Loss: 0.9533, Val Acc: 0.4933, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 151, Loss: 0.9631, Val Acc: 0.4933, Test Acc: 0.4489\n",
                        "Seed: 42, Epoch: 152, Loss: 0.9659, Val Acc: 0.4933, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 153, Loss: 0.9560, Val Acc: 0.5022, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 154, Loss: 0.9452, Val Acc: 0.5067, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 155, Loss: 0.9532, Val Acc: 0.5022, Test Acc: 0.4578\n",
                        "Seed: 42, Epoch: 156, Loss: 0.9502, Val Acc: 0.4978, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 157, Loss: 0.9368, Val Acc: 0.5022, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 158, Loss: 0.9411, Val Acc: 0.5022, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 159, Loss: 0.9350, Val Acc: 0.4978, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 160, Loss: 0.9379, Val Acc: 0.4800, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 161, Loss: 0.9428, Val Acc: 0.4844, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 162, Loss: 0.9318, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 163, Loss: 0.9416, Val Acc: 0.4800, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 164, Loss: 0.9421, Val Acc: 0.4756, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 165, Loss: 0.9434, Val Acc: 0.4756, Test Acc: 0.4622\n",
                        "Seed: 42, Epoch: 166, Loss: 0.9383, Val Acc: 0.4756, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 167, Loss: 0.9460, Val Acc: 0.4756, Test Acc: 0.4711\n",
                        "Seed: 42, Epoch: 168, Loss: 0.9411, Val Acc: 0.4844, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 169, Loss: 0.9364, Val Acc: 0.4800, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 170, Loss: 0.9440, Val Acc: 0.4800, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 171, Loss: 0.9460, Val Acc: 0.4711, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 172, Loss: 0.9354, Val Acc: 0.4756, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 173, Loss: 0.9395, Val Acc: 0.4844, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 174, Loss: 0.9287, Val Acc: 0.4978, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 175, Loss: 0.9385, Val Acc: 0.5067, Test Acc: 0.4667\n",
                        "Seed: 42, Epoch: 176, Loss: 0.9327, Val Acc: 0.4889, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 177, Loss: 0.9355, Val Acc: 0.4889, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 178, Loss: 0.9359, Val Acc: 0.4844, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 179, Loss: 0.9336, Val Acc: 0.4800, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 180, Loss: 0.9447, Val Acc: 0.4756, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 181, Loss: 0.9412, Val Acc: 0.4756, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 182, Loss: 0.9323, Val Acc: 0.4844, Test Acc: 0.5022\n",
                        "Seed: 42, Epoch: 183, Loss: 0.9346, Val Acc: 0.4800, Test Acc: 0.4978\n",
                        "Seed: 42, Epoch: 184, Loss: 0.9268, Val Acc: 0.4800, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 185, Loss: 0.9335, Val Acc: 0.4933, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 186, Loss: 0.9346, Val Acc: 0.4933, Test Acc: 0.4533\n",
                        "Seed: 42, Epoch: 187, Loss: 0.9297, Val Acc: 0.4978, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 188, Loss: 0.9351, Val Acc: 0.4978, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 189, Loss: 0.9373, Val Acc: 0.4978, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 190, Loss: 0.9336, Val Acc: 0.4978, Test Acc: 0.4844\n",
                        "Seed: 42, Epoch: 191, Loss: 0.9299, Val Acc: 0.4933, Test Acc: 0.5156\n",
                        "Seed: 42, Epoch: 192, Loss: 0.9248, Val Acc: 0.4933, Test Acc: 0.4933\n",
                        "Seed: 42, Epoch: 193, Loss: 0.9322, Val Acc: 0.4889, Test Acc: 0.5200\n",
                        "Seed: 42, Epoch: 194, Loss: 0.9258, Val Acc: 0.4933, Test Acc: 0.4889\n",
                        "Seed: 42, Epoch: 195, Loss: 0.9259, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 196, Loss: 0.9302, Val Acc: 0.4933, Test Acc: 0.5111\n",
                        "Seed: 42, Epoch: 197, Loss: 0.9327, Val Acc: 0.4933, Test Acc: 0.4756\n",
                        "Seed: 42, Epoch: 198, Loss: 0.9292, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 199, Loss: 0.9366, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 42, Epoch: 200, Loss: 0.9329, Val Acc: 0.4978, Test Acc: 0.4711\n",
                        "Seed: 43, Epoch: 001, Loss: 1.0991, Val Acc: 0.3600, Test Acc: 0.3378\n",
                        "Seed: 43, Epoch: 002, Loss: 1.0989, Val Acc: 0.3600, Test Acc: 0.3378\n",
                        "Seed: 43, Epoch: 003, Loss: 1.0987, Val Acc: 0.3378, Test Acc: 0.3289\n",
                        "Seed: 43, Epoch: 004, Loss: 1.0986, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 005, Loss: 1.0985, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 006, Loss: 1.0984, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 007, Loss: 1.0983, Val Acc: 0.3333, Test Acc: 0.3244\n",
                        "Seed: 43, Epoch: 008, Loss: 1.0980, Val Acc: 0.3511, Test Acc: 0.3289\n",
                        "Seed: 43, Epoch: 009, Loss: 1.0976, Val Acc: 0.3556, Test Acc: 0.3422\n",
                        "Seed: 43, Epoch: 010, Loss: 1.0974, Val Acc: 0.4222, Test Acc: 0.3556\n",
                        "Seed: 43, Epoch: 011, Loss: 1.0968, Val Acc: 0.4133, Test Acc: 0.3689\n",
                        "Seed: 43, Epoch: 012, Loss: 1.0963, Val Acc: 0.3422, Test Acc: 0.3689\n",
                        "Seed: 43, Epoch: 013, Loss: 1.0955, Val Acc: 0.3067, Test Acc: 0.3378\n",
                        "Seed: 43, Epoch: 014, Loss: 1.0942, Val Acc: 0.3067, Test Acc: 0.3556\n",
                        "Seed: 43, Epoch: 015, Loss: 1.0937, Val Acc: 0.3733, Test Acc: 0.3822\n",
                        "Seed: 43, Epoch: 016, Loss: 1.0917, Val Acc: 0.4089, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 017, Loss: 1.0912, Val Acc: 0.4311, Test Acc: 0.4044\n",
                        "Seed: 43, Epoch: 018, Loss: 1.0889, Val Acc: 0.4044, Test Acc: 0.3956\n",
                        "Seed: 43, Epoch: 019, Loss: 1.0870, Val Acc: 0.3822, Test Acc: 0.3822\n",
                        "Seed: 43, Epoch: 020, Loss: 1.0847, Val Acc: 0.3600, Test Acc: 0.3822\n",
                        "Seed: 43, Epoch: 021, Loss: 1.0831, Val Acc: 0.3600, Test Acc: 0.3822\n",
                        "Seed: 43, Epoch: 022, Loss: 1.0796, Val Acc: 0.3911, Test Acc: 0.3911\n",
                        "Seed: 43, Epoch: 023, Loss: 1.0757, Val Acc: 0.4400, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 024, Loss: 1.0710, Val Acc: 0.4489, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 025, Loss: 1.0685, Val Acc: 0.4489, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 026, Loss: 1.0633, Val Acc: 0.4756, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 027, Loss: 1.0594, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 43, Epoch: 028, Loss: 1.0486, Val Acc: 0.4978, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 029, Loss: 1.0465, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 030, Loss: 1.0417, Val Acc: 0.4933, Test Acc: 0.4578\n",
                        "Seed: 43, Epoch: 031, Loss: 1.0330, Val Acc: 0.4889, Test Acc: 0.4578\n",
                        "Seed: 43, Epoch: 032, Loss: 1.0316, Val Acc: 0.4800, Test Acc: 0.4533\n",
                        "Seed: 43, Epoch: 033, Loss: 1.0270, Val Acc: 0.4978, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 034, Loss: 1.0178, Val Acc: 0.4933, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 035, Loss: 1.0145, Val Acc: 0.4889, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 036, Loss: 1.0078, Val Acc: 0.5244, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 037, Loss: 1.0014, Val Acc: 0.5378, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 038, Loss: 1.0009, Val Acc: 0.5333, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 039, Loss: 0.9929, Val Acc: 0.5333, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 040, Loss: 0.9933, Val Acc: 0.5289, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 041, Loss: 0.9836, Val Acc: 0.5289, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 042, Loss: 0.9764, Val Acc: 0.5333, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 043, Loss: 0.9815, Val Acc: 0.5333, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 044, Loss: 0.9721, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 045, Loss: 0.9731, Val Acc: 0.4844, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 046, Loss: 0.9784, Val Acc: 0.4844, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 047, Loss: 0.9729, Val Acc: 0.5067, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 048, Loss: 0.9630, Val Acc: 0.5022, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 049, Loss: 0.9683, Val Acc: 0.4756, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 050, Loss: 0.9725, Val Acc: 0.4800, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 051, Loss: 0.9714, Val Acc: 0.5067, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 052, Loss: 0.9637, Val Acc: 0.5289, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 053, Loss: 0.9729, Val Acc: 0.4978, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 054, Loss: 0.9705, Val Acc: 0.5200, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 055, Loss: 0.9623, Val Acc: 0.4933, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 056, Loss: 0.9633, Val Acc: 0.4756, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 057, Loss: 0.9618, Val Acc: 0.4667, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 058, Loss: 0.9629, Val Acc: 0.4667, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 059, Loss: 0.9656, Val Acc: 0.4889, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 060, Loss: 0.9643, Val Acc: 0.4889, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 061, Loss: 0.9587, Val Acc: 0.5156, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 062, Loss: 0.9571, Val Acc: 0.5111, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 063, Loss: 0.9565, Val Acc: 0.5156, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 064, Loss: 0.9574, Val Acc: 0.5200, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 065, Loss: 0.9570, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 066, Loss: 0.9522, Val Acc: 0.5022, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 067, Loss: 0.9575, Val Acc: 0.5067, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 068, Loss: 0.9506, Val Acc: 0.5200, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 069, Loss: 0.9492, Val Acc: 0.5067, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 070, Loss: 0.9504, Val Acc: 0.5022, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 071, Loss: 0.9485, Val Acc: 0.4889, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 072, Loss: 0.9457, Val Acc: 0.4756, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 073, Loss: 0.9446, Val Acc: 0.4889, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 074, Loss: 0.9434, Val Acc: 0.4933, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 075, Loss: 0.9414, Val Acc: 0.4800, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 076, Loss: 0.9381, Val Acc: 0.4800, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 077, Loss: 0.9390, Val Acc: 0.4889, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 078, Loss: 0.9387, Val Acc: 0.5022, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 079, Loss: 0.9318, Val Acc: 0.5111, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 080, Loss: 0.9423, Val Acc: 0.5289, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 081, Loss: 0.9416, Val Acc: 0.5244, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 082, Loss: 0.9391, Val Acc: 0.4978, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 083, Loss: 0.9408, Val Acc: 0.4889, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 084, Loss: 0.9411, Val Acc: 0.4889, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 085, Loss: 0.9411, Val Acc: 0.5111, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 086, Loss: 0.9419, Val Acc: 0.5111, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 087, Loss: 0.9408, Val Acc: 0.5111, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 088, Loss: 0.9456, Val Acc: 0.5244, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 089, Loss: 0.9494, Val Acc: 0.5111, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 090, Loss: 0.9627, Val Acc: 0.5156, Test Acc: 0.4489\n",
                        "Seed: 43, Epoch: 091, Loss: 0.9575, Val Acc: 0.5244, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 092, Loss: 0.9546, Val Acc: 0.5111, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 093, Loss: 0.9473, Val Acc: 0.5111, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 094, Loss: 0.9371, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 095, Loss: 0.9449, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 096, Loss: 0.9408, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 097, Loss: 0.9402, Val Acc: 0.5067, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 098, Loss: 0.9386, Val Acc: 0.5111, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 099, Loss: 0.9382, Val Acc: 0.5067, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 100, Loss: 0.9362, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 101, Loss: 0.9370, Val Acc: 0.5022, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 102, Loss: 0.9376, Val Acc: 0.4844, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 103, Loss: 0.9435, Val Acc: 0.4711, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 104, Loss: 0.9410, Val Acc: 0.5022, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 105, Loss: 0.9379, Val Acc: 0.5111, Test Acc: 0.4044\n",
                        "Seed: 43, Epoch: 106, Loss: 0.9380, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 107, Loss: 0.9280, Val Acc: 0.5156, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 108, Loss: 0.9374, Val Acc: 0.5200, Test Acc: 0.4444\n",
                        "Seed: 43, Epoch: 109, Loss: 0.9415, Val Acc: 0.5022, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 110, Loss: 0.9498, Val Acc: 0.5022, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 111, Loss: 0.9502, Val Acc: 0.5022, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 112, Loss: 0.9538, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 113, Loss: 0.9506, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 114, Loss: 0.9409, Val Acc: 0.5111, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 115, Loss: 0.9336, Val Acc: 0.5067, Test Acc: 0.4044\n",
                        "Seed: 43, Epoch: 116, Loss: 0.9476, Val Acc: 0.4844, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 117, Loss: 0.9550, Val Acc: 0.4889, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 118, Loss: 0.9489, Val Acc: 0.5067, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 119, Loss: 0.9405, Val Acc: 0.5111, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 120, Loss: 0.9368, Val Acc: 0.5111, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 121, Loss: 0.9314, Val Acc: 0.5067, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 122, Loss: 0.9346, Val Acc: 0.5111, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 123, Loss: 0.9347, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 124, Loss: 0.9354, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 125, Loss: 0.9358, Val Acc: 0.5156, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 126, Loss: 0.9347, Val Acc: 0.5111, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 127, Loss: 0.9334, Val Acc: 0.5156, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 128, Loss: 0.9387, Val Acc: 0.5156, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 129, Loss: 0.9315, Val Acc: 0.5111, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 130, Loss: 0.9295, Val Acc: 0.5022, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 131, Loss: 0.9320, Val Acc: 0.5067, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 132, Loss: 0.9330, Val Acc: 0.5022, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 133, Loss: 0.9333, Val Acc: 0.5111, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 134, Loss: 0.9288, Val Acc: 0.5111, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 135, Loss: 0.9309, Val Acc: 0.4978, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 136, Loss: 0.9304, Val Acc: 0.5022, Test Acc: 0.4133\n",
                        "Seed: 43, Epoch: 137, Loss: 0.9336, Val Acc: 0.4978, Test Acc: 0.3956\n",
                        "Seed: 43, Epoch: 138, Loss: 0.9311, Val Acc: 0.4622, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 139, Loss: 0.9300, Val Acc: 0.4622, Test Acc: 0.4044\n",
                        "Seed: 43, Epoch: 140, Loss: 0.9343, Val Acc: 0.4978, Test Acc: 0.4089\n",
                        "Seed: 43, Epoch: 141, Loss: 0.9288, Val Acc: 0.4978, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 142, Loss: 0.9266, Val Acc: 0.5022, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 143, Loss: 0.9252, Val Acc: 0.5111, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 144, Loss: 0.9273, Val Acc: 0.5156, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 145, Loss: 0.9264, Val Acc: 0.5022, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 146, Loss: 0.9291, Val Acc: 0.4978, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 147, Loss: 0.9295, Val Acc: 0.4978, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 148, Loss: 0.9318, Val Acc: 0.4933, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 149, Loss: 0.9318, Val Acc: 0.4978, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 150, Loss: 0.9284, Val Acc: 0.5022, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 151, Loss: 0.9228, Val Acc: 0.5022, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 152, Loss: 0.9286, Val Acc: 0.4978, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 153, Loss: 0.9285, Val Acc: 0.4933, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 154, Loss: 0.9253, Val Acc: 0.4933, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 155, Loss: 0.9279, Val Acc: 0.4978, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 156, Loss: 0.9240, Val Acc: 0.5067, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 157, Loss: 0.9239, Val Acc: 0.5067, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 158, Loss: 0.9243, Val Acc: 0.5067, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 159, Loss: 0.9241, Val Acc: 0.5022, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 160, Loss: 0.9226, Val Acc: 0.4978, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 161, Loss: 0.9286, Val Acc: 0.4978, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 162, Loss: 0.9221, Val Acc: 0.5022, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 163, Loss: 0.9218, Val Acc: 0.5067, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 164, Loss: 0.9231, Val Acc: 0.5067, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 165, Loss: 0.9257, Val Acc: 0.5067, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 166, Loss: 0.9246, Val Acc: 0.5067, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 167, Loss: 0.9234, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 168, Loss: 0.9252, Val Acc: 0.4978, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 169, Loss: 0.9256, Val Acc: 0.4933, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 170, Loss: 0.9292, Val Acc: 0.4933, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 171, Loss: 0.9269, Val Acc: 0.5022, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 172, Loss: 0.9265, Val Acc: 0.5111, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 173, Loss: 0.9210, Val Acc: 0.5022, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 174, Loss: 0.9273, Val Acc: 0.5022, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 175, Loss: 0.9226, Val Acc: 0.5022, Test Acc: 0.4044\n",
                        "Seed: 43, Epoch: 176, Loss: 0.9247, Val Acc: 0.5022, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 177, Loss: 0.9244, Val Acc: 0.4933, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 178, Loss: 0.9261, Val Acc: 0.4889, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 179, Loss: 0.9259, Val Acc: 0.5067, Test Acc: 0.4178\n",
                        "Seed: 43, Epoch: 180, Loss: 0.9257, Val Acc: 0.5156, Test Acc: 0.4044\n",
                        "Seed: 43, Epoch: 181, Loss: 0.9191, Val Acc: 0.5022, Test Acc: 0.4267\n",
                        "Seed: 43, Epoch: 182, Loss: 0.9219, Val Acc: 0.5022, Test Acc: 0.4356\n",
                        "Seed: 43, Epoch: 183, Loss: 0.9268, Val Acc: 0.5067, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 184, Loss: 0.9255, Val Acc: 0.5022, Test Acc: 0.4400\n",
                        "Seed: 43, Epoch: 185, Loss: 0.9214, Val Acc: 0.5022, Test Acc: 0.4311\n",
                        "Seed: 43, Epoch: 186, Loss: 0.9151, Val Acc: 0.5111, Test Acc: 0.4222\n",
                        "Seed: 43, Epoch: 187, Loss: 0.9215, Val Acc: 0.4978, Test Acc: 0.4267\n",
                        "Early stopping at epoch 187 for seed 43\n",
                        "Seed: 44, Epoch: 001, Loss: 1.1032, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 002, Loss: 1.1026, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 003, Loss: 1.1021, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 004, Loss: 1.1017, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 005, Loss: 1.1012, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 006, Loss: 1.1008, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 007, Loss: 1.1003, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 008, Loss: 1.0997, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 009, Loss: 1.0989, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 010, Loss: 1.0984, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 011, Loss: 1.0975, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 012, Loss: 1.0962, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 013, Loss: 1.0950, Val Acc: 0.3600, Test Acc: 0.3111\n",
                        "Seed: 44, Epoch: 014, Loss: 1.0941, Val Acc: 0.3644, Test Acc: 0.3422\n",
                        "Seed: 44, Epoch: 015, Loss: 1.0922, Val Acc: 0.3733, Test Acc: 0.3422\n",
                        "Seed: 44, Epoch: 016, Loss: 1.0903, Val Acc: 0.3911, Test Acc: 0.3556\n",
                        "Seed: 44, Epoch: 017, Loss: 1.0867, Val Acc: 0.3822, Test Acc: 0.3556\n",
                        "Seed: 44, Epoch: 018, Loss: 1.0860, Val Acc: 0.3733, Test Acc: 0.4400\n",
                        "Seed: 44, Epoch: 019, Loss: 1.0823, Val Acc: 0.3911, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 020, Loss: 1.0790, Val Acc: 0.4178, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 021, Loss: 1.0763, Val Acc: 0.4800, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 022, Loss: 1.0705, Val Acc: 0.4933, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 023, Loss: 1.0659, Val Acc: 0.4978, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 024, Loss: 1.0618, Val Acc: 0.4933, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 025, Loss: 1.0558, Val Acc: 0.4844, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 026, Loss: 1.0460, Val Acc: 0.4978, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 027, Loss: 1.0434, Val Acc: 0.5022, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 028, Loss: 1.0387, Val Acc: 0.4933, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 029, Loss: 1.0360, Val Acc: 0.4978, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 030, Loss: 1.0326, Val Acc: 0.4933, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 031, Loss: 1.0273, Val Acc: 0.5022, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 032, Loss: 1.0225, Val Acc: 0.4800, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 033, Loss: 1.0170, Val Acc: 0.4844, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 034, Loss: 1.0117, Val Acc: 0.4800, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 035, Loss: 1.0076, Val Acc: 0.4889, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 036, Loss: 1.0068, Val Acc: 0.4933, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 037, Loss: 1.0004, Val Acc: 0.4933, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 038, Loss: 1.0007, Val Acc: 0.4756, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 039, Loss: 1.0022, Val Acc: 0.4800, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 040, Loss: 1.0032, Val Acc: 0.4800, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 041, Loss: 0.9931, Val Acc: 0.4978, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 042, Loss: 0.9960, Val Acc: 0.4844, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 043, Loss: 0.9974, Val Acc: 0.4711, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 044, Loss: 0.9998, Val Acc: 0.4978, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 045, Loss: 0.9925, Val Acc: 0.4933, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 046, Loss: 0.9904, Val Acc: 0.4978, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 047, Loss: 0.9893, Val Acc: 0.4978, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 048, Loss: 0.9887, Val Acc: 0.5022, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 049, Loss: 0.9854, Val Acc: 0.4844, Test Acc: 0.4622\n",
                        "Seed: 44, Epoch: 050, Loss: 0.9873, Val Acc: 0.4800, Test Acc: 0.4489\n",
                        "Seed: 44, Epoch: 051, Loss: 0.9901, Val Acc: 0.4844, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 052, Loss: 0.9843, Val Acc: 0.4844, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 053, Loss: 0.9844, Val Acc: 0.4889, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 054, Loss: 0.9813, Val Acc: 0.4933, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 055, Loss: 0.9849, Val Acc: 0.4889, Test Acc: 0.4489\n",
                        "Seed: 44, Epoch: 056, Loss: 0.9871, Val Acc: 0.4844, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 057, Loss: 0.9829, Val Acc: 0.4889, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 058, Loss: 0.9803, Val Acc: 0.4933, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 059, Loss: 0.9811, Val Acc: 0.4889, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 060, Loss: 0.9770, Val Acc: 0.4933, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 061, Loss: 0.9777, Val Acc: 0.4933, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 062, Loss: 0.9733, Val Acc: 0.4978, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 063, Loss: 0.9698, Val Acc: 0.4844, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 064, Loss: 0.9715, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 065, Loss: 0.9663, Val Acc: 0.4978, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 066, Loss: 0.9701, Val Acc: 0.5067, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 067, Loss: 0.9703, Val Acc: 0.4889, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 068, Loss: 0.9650, Val Acc: 0.4933, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 069, Loss: 0.9685, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 070, Loss: 0.9729, Val Acc: 0.4889, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 071, Loss: 0.9661, Val Acc: 0.4933, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 072, Loss: 0.9630, Val Acc: 0.4800, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 073, Loss: 0.9640, Val Acc: 0.4800, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 074, Loss: 0.9597, Val Acc: 0.5067, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 075, Loss: 0.9601, Val Acc: 0.5022, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 076, Loss: 0.9595, Val Acc: 0.4933, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 077, Loss: 0.9577, Val Acc: 0.4933, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 078, Loss: 0.9581, Val Acc: 0.4889, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 079, Loss: 0.9596, Val Acc: 0.4889, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 080, Loss: 0.9550, Val Acc: 0.4933, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 081, Loss: 0.9663, Val Acc: 0.4933, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 082, Loss: 0.9604, Val Acc: 0.4800, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 083, Loss: 0.9630, Val Acc: 0.4889, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 084, Loss: 0.9599, Val Acc: 0.4844, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 085, Loss: 0.9570, Val Acc: 0.4889, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 086, Loss: 0.9556, Val Acc: 0.5111, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 087, Loss: 0.9614, Val Acc: 0.5200, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 088, Loss: 0.9665, Val Acc: 0.5200, Test Acc: 0.4311\n",
                        "Seed: 44, Epoch: 089, Loss: 0.9683, Val Acc: 0.5156, Test Acc: 0.4444\n",
                        "Seed: 44, Epoch: 090, Loss: 0.9655, Val Acc: 0.5156, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 091, Loss: 0.9573, Val Acc: 0.4844, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 092, Loss: 0.9521, Val Acc: 0.4933, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 093, Loss: 0.9535, Val Acc: 0.4844, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 094, Loss: 0.9588, Val Acc: 0.4844, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 095, Loss: 0.9580, Val Acc: 0.4844, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 096, Loss: 0.9535, Val Acc: 0.4844, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 097, Loss: 0.9522, Val Acc: 0.4844, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 098, Loss: 0.9518, Val Acc: 0.4889, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 099, Loss: 0.9475, Val Acc: 0.4933, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 100, Loss: 0.9542, Val Acc: 0.5067, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 101, Loss: 0.9521, Val Acc: 0.5156, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 102, Loss: 0.9525, Val Acc: 0.5022, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 103, Loss: 0.9497, Val Acc: 0.4978, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 104, Loss: 0.9451, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 105, Loss: 0.9483, Val Acc: 0.5156, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 106, Loss: 0.9477, Val Acc: 0.5156, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 107, Loss: 0.9469, Val Acc: 0.5244, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 108, Loss: 0.9418, Val Acc: 0.5067, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 109, Loss: 0.9457, Val Acc: 0.4978, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 110, Loss: 0.9429, Val Acc: 0.4889, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 111, Loss: 0.9417, Val Acc: 0.5022, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 112, Loss: 0.9433, Val Acc: 0.4933, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 113, Loss: 0.9440, Val Acc: 0.4978, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 114, Loss: 0.9372, Val Acc: 0.4933, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 115, Loss: 0.9391, Val Acc: 0.4978, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 116, Loss: 0.9372, Val Acc: 0.4844, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 117, Loss: 0.9381, Val Acc: 0.4844, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 118, Loss: 0.9413, Val Acc: 0.4844, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 119, Loss: 0.9408, Val Acc: 0.4889, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 120, Loss: 0.9386, Val Acc: 0.5067, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 121, Loss: 0.9408, Val Acc: 0.4978, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 122, Loss: 0.9410, Val Acc: 0.4844, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 123, Loss: 0.9375, Val Acc: 0.4933, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 124, Loss: 0.9408, Val Acc: 0.5289, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 125, Loss: 0.9420, Val Acc: 0.5244, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 126, Loss: 0.9444, Val Acc: 0.5067, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 127, Loss: 0.9384, Val Acc: 0.4978, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 128, Loss: 0.9408, Val Acc: 0.5067, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 129, Loss: 0.9358, Val Acc: 0.4978, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 130, Loss: 0.9417, Val Acc: 0.4933, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 131, Loss: 0.9438, Val Acc: 0.4889, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 132, Loss: 0.9404, Val Acc: 0.4889, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 133, Loss: 0.9462, Val Acc: 0.4844, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 134, Loss: 0.9425, Val Acc: 0.4889, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 135, Loss: 0.9384, Val Acc: 0.4978, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 136, Loss: 0.9377, Val Acc: 0.4978, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 137, Loss: 0.9354, Val Acc: 0.5067, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 138, Loss: 0.9324, Val Acc: 0.4933, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 139, Loss: 0.9429, Val Acc: 0.4978, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 140, Loss: 0.9554, Val Acc: 0.4978, Test Acc: 0.4356\n",
                        "Seed: 44, Epoch: 141, Loss: 0.9547, Val Acc: 0.4756, Test Acc: 0.4444\n",
                        "Seed: 44, Epoch: 142, Loss: 0.9626, Val Acc: 0.4667, Test Acc: 0.4400\n",
                        "Seed: 44, Epoch: 143, Loss: 0.9639, Val Acc: 0.4622, Test Acc: 0.4222\n",
                        "Seed: 44, Epoch: 144, Loss: 0.9571, Val Acc: 0.4978, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 145, Loss: 0.9462, Val Acc: 0.4978, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 146, Loss: 0.9449, Val Acc: 0.4978, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 147, Loss: 0.9451, Val Acc: 0.5067, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 148, Loss: 0.9462, Val Acc: 0.5022, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 149, Loss: 0.9438, Val Acc: 0.5067, Test Acc: 0.4844\n",
                        "Seed: 44, Epoch: 150, Loss: 0.9421, Val Acc: 0.5111, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 151, Loss: 0.9435, Val Acc: 0.5156, Test Acc: 0.4667\n",
                        "Seed: 44, Epoch: 152, Loss: 0.9463, Val Acc: 0.5156, Test Acc: 0.4711\n",
                        "Seed: 44, Epoch: 153, Loss: 0.9529, Val Acc: 0.4978, Test Acc: 0.4622\n",
                        "Seed: 44, Epoch: 154, Loss: 0.9514, Val Acc: 0.5022, Test Acc: 0.4533\n",
                        "Seed: 44, Epoch: 155, Loss: 0.9476, Val Acc: 0.5111, Test Acc: 0.4533\n",
                        "Seed: 44, Epoch: 156, Loss: 0.9441, Val Acc: 0.5156, Test Acc: 0.4578\n",
                        "Seed: 44, Epoch: 157, Loss: 0.9419, Val Acc: 0.5111, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 158, Loss: 0.9342, Val Acc: 0.5200, Test Acc: 0.4667\n",
                        "Seed: 44, Epoch: 159, Loss: 0.9346, Val Acc: 0.5111, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 160, Loss: 0.9329, Val Acc: 0.4978, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 161, Loss: 0.9309, Val Acc: 0.4889, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 162, Loss: 0.9288, Val Acc: 0.4978, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 163, Loss: 0.9335, Val Acc: 0.5111, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 164, Loss: 0.9262, Val Acc: 0.5022, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 165, Loss: 0.9280, Val Acc: 0.4933, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 166, Loss: 0.9259, Val Acc: 0.4889, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 167, Loss: 0.9332, Val Acc: 0.4889, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 168, Loss: 0.9304, Val Acc: 0.4844, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 169, Loss: 0.9246, Val Acc: 0.5022, Test Acc: 0.4800\n",
                        "Seed: 44, Epoch: 170, Loss: 0.9273, Val Acc: 0.4978, Test Acc: 0.4889\n",
                        "Seed: 44, Epoch: 171, Loss: 0.9296, Val Acc: 0.5022, Test Acc: 0.4933\n",
                        "Seed: 44, Epoch: 172, Loss: 0.9251, Val Acc: 0.4978, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 173, Loss: 0.9246, Val Acc: 0.4844, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 174, Loss: 0.9276, Val Acc: 0.4933, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 175, Loss: 0.9228, Val Acc: 0.5022, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 176, Loss: 0.9218, Val Acc: 0.5067, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 177, Loss: 0.9294, Val Acc: 0.5111, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 178, Loss: 0.9235, Val Acc: 0.5111, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 179, Loss: 0.9231, Val Acc: 0.5022, Test Acc: 0.5333\n",
                        "Seed: 44, Epoch: 180, Loss: 0.9234, Val Acc: 0.5022, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 181, Loss: 0.9259, Val Acc: 0.4978, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 182, Loss: 0.9270, Val Acc: 0.4933, Test Acc: 0.5378\n",
                        "Seed: 44, Epoch: 183, Loss: 0.9200, Val Acc: 0.5022, Test Acc: 0.5378\n",
                        "Seed: 44, Epoch: 184, Loss: 0.9244, Val Acc: 0.5022, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 185, Loss: 0.9289, Val Acc: 0.5022, Test Acc: 0.5111\n",
                        "Seed: 44, Epoch: 186, Loss: 0.9235, Val Acc: 0.5067, Test Acc: 0.5022\n",
                        "Seed: 44, Epoch: 187, Loss: 0.9237, Val Acc: 0.5067, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 188, Loss: 0.9247, Val Acc: 0.5067, Test Acc: 0.5067\n",
                        "Seed: 44, Epoch: 189, Loss: 0.9245, Val Acc: 0.5067, Test Acc: 0.5156\n",
                        "Seed: 44, Epoch: 190, Loss: 0.9210, Val Acc: 0.5022, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 191, Loss: 0.9257, Val Acc: 0.4889, Test Acc: 0.5378\n",
                        "Seed: 44, Epoch: 192, Loss: 0.9202, Val Acc: 0.5111, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 193, Loss: 0.9229, Val Acc: 0.5111, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 194, Loss: 0.9233, Val Acc: 0.5067, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 195, Loss: 0.9208, Val Acc: 0.4978, Test Acc: 0.5200\n",
                        "Seed: 44, Epoch: 196, Loss: 0.9207, Val Acc: 0.5022, Test Acc: 0.5244\n",
                        "Seed: 44, Epoch: 197, Loss: 0.9213, Val Acc: 0.4933, Test Acc: 0.5289\n",
                        "Seed: 44, Epoch: 198, Loss: 0.9183, Val Acc: 0.5067, Test Acc: 0.4978\n",
                        "Seed: 44, Epoch: 199, Loss: 0.9224, Val Acc: 0.4978, Test Acc: 0.4756\n",
                        "Seed: 44, Epoch: 200, Loss: 0.9256, Val Acc: 0.5067, Test Acc: 0.4711\n",
                        "Average Time: 1025.48 seconds\n",
                        "Var Time: 921.34 seconds\n",
                        "Average Memory: 1052.00 MB\n",
                        "Average Best Val Acc: 0.5244\n",
                        "Std Best Test Acc: 0.0372\n",
                        "Average Test Acc: 0.4696\n"
                    ]
                }
            ],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\nmax_nodes = 500\ndata_path = \"/data/XXX/Pooling/\"\ndataset_sparse = TUDataset(root=data_path, name=\"IMDB-MULTI\", transform=T.Compose([T.OneHotDegree(88)]), use_node_attr=True)\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_geometric.nn import BatchNorm\nclass HierarchicalGCN_ASA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_ASA, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = ASAPooling(hidden_channels, ratio=0.9)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = ASAPooling(hidden_channels, ratio=0.9)\n        self.conv3 = GCNConv(hidden_channels, out_channels)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 32)\n        self.lin2 = torch.nn.Linear(32, num_classes)\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _ = self.pool1(x, edge_index, batch=batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _ = self.pool2(x, edge_index, batch=batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_ASA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\ndef train():\n    model.train()\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        data = data.to(device)\n        out = model(data)\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n    return correct / len(loader.dataset)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 150\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    train_ratio = 0.7\n    val_ratio = 0.15\n    val_ratio = 0.15\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n    model = HierarchicalGCN_ASA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 201):\n        loss = train()\n        val_acc = test(valid_loader)\n        test_acc = test(test_loader)\n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\nprint(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### COLLAB"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": "from torch_geometric.datasets import TUDataset\nimport torch_geometric.transforms as T\nfrom torch_geometric.data import DenseDataLoader\ndata_path = \"/data1/Pooling/\"\ndataset_sparse = TUDataset(root=data_path, name=\"COLLAB\", transform=T.Compose([T.OneHotDegree(491)]), use_node_attr=True)\nimport torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, ASAPooling\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import TUDataset\nfrom torch_geometric.transforms import ToUndirected\nfrom torch.nn import Linear\nimport torch.optim as optim\nfrom torch_geometric.nn import global_mean_pool\nfrom torch_geometric.utils import to_dense_batch\nfrom torch_geometric.nn import BatchNorm\nclass HierarchicalGCN_ASA(torch.nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n        super(HierarchicalGCN_ASA, self).__init__()\n        self.conv1 = GCNConv(in_channels, hidden_channels)\n        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool1 = ASAPooling(hidden_channels, ratio=0.9)\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n        self.pool2 = ASAPooling(hidden_channels, ratio=0.9)\n        self.conv3 = GCNConv(hidden_channels, out_channels)\n        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n        self.lin1 = torch.nn.Linear(out_channels, 32)\n        self.lin2 = torch.nn.Linear(32, num_classes)\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _ = self.pool1(x, edge_index, batch=batch)\n        x = self.conv2(x, edge_index)\n        x = F.relu(x)\n        x, edge_index, _, batch, _ = self.pool2(x, edge_index, batch=batch)\n        x = self.conv3(x, edge_index)\n        x = F.relu(x)\n        x, mask = to_dense_batch(x, batch)\n        x = x.mean(dim=1)\n        x = self.lin1(x).relu()\n        x = self.lin2(x)\n        return F.log_softmax(x, dim=-1)\nnum_classes = dataset_sparse.num_classes\nin_channels = dataset_sparse.num_features\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = HierarchicalGCN_ASA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = torch.nn.CrossEntropyLoss()\ndef train():\n    model.train()\n    total_loss = 0\n    for data in train_loader:\n        data = data.to(device)\n        optimizer.zero_grad()\n        out = model(data)\n        loss = F.nll_loss(out, data.y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * data.num_graphs\n    return total_loss / len(train_loader.dataset)\ndef test(loader):\n    model.eval()\n    correct = 0\n    for data in loader:\n        data = data.to(device)\n        out = model(data)\n        pred = out.argmax(dim=1)\n        correct += (pred == data.y).sum().item()\n    return correct / len(loader.dataset)\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nseeds = [42, 43, 44]\ntimes = []\nmemories = []\nbest_val_accs = []\nbest_test_accs = []\nearly_stop_patience = 150\ntolerance = 0.0001\nfor seed in seeds:\n    set_seed(seed)\n    dataset_sparse = dataset_sparse.shuffle()\n    train_ratio = 0.7\n    val_ratio = 0.15\n    val_ratio = 0.15\n    num_total = len(dataset_sparse)\n    num_train = int(num_total * train_ratio)\n    num_val = int(num_total * val_ratio)\n    num_test = num_total - num_train - num_val\n    train_dataset = dataset_sparse[:num_train]\n    val_dataset = dataset_sparse[num_train:num_train + num_val]\n    test_dataset = dataset_sparse[num_train + num_val:]\n    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n    model = HierarchicalGCN_ASA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    start_time = time.time()\n    best_val_acc = 0\n    epochs_no_improve = 0\n    for epoch in range(1, 201):\n        loss = train()\n        val_acc = test(valid_loader)\n        test_acc = test(test_loader)\n        if val_acc > best_val_acc + tolerance:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n            epochs_no_improve = 0\n        else:\n            epochs_no_improve += 1\n        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n        if epochs_no_improve >= early_stop_patience:\n            print(f'Early stopping at epoch {epoch} for seed {seed}')\n            break\n    end_time = time.time()\n    total_time = end_time - start_time\n    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  \n    times.append(total_time)\n    memories.append(memory_allocated)\n    best_val_accs.append(best_val_acc)\n    best_test_accs.append(best_test_acc)\n    torch.cuda.empty_cache()\nprint(f'Average Time: {np.mean(times):.2f} seconds')\nprint(f'Var Time: {np.var(times):.2f} seconds')\nprint(f'Average Memory: {np.mean(memories):.2f} MB')\nprint(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\nprint(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\nprint(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Graph Regression"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### QM7"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/500MAE=1549.9890 MAE=1548.3679 MAE=1544.8348 MAE=1542.1368 MAE=1540.3928 MAE=1536.1909 MAE=1533.2390 MAE=1526.7466 MAE=1523.4377 Epoch: 10/500MAE=1518.5776 MAE=1513.7883 MAE=1512.1201 MAE=1506.9475 MAE=1494.6902 MAE=1487.8940 MAE=1475.9880 MAE=1472.7158 MAE=1464.3601 MAE=1459.8149 Epoch: 20/500MAE=1453.1532 MAE=1440.7437 MAE=1435.9729 MAE=1433.3440 MAE=1416.6311 MAE=1395.0476 MAE=1384.8713 MAE=1373.6216 MAE=1366.0151 MAE=1349.3988 Epoch: 30/500MAE=1345.4993 MAE=1326.1885 MAE=1317.5447 MAE=1301.8408 MAE=1287.2114 MAE=1278.6711 MAE=1258.0403 MAE=1246.9518 MAE=1228.2086 MAE=1183.9573 Epoch: 40/500MAE=1184.0028 MAE=1190.0552 MAE=1167.0938 MAE=1140.5122 MAE=1130.6899 MAE=1098.1836 MAE=1108.6740 MAE=1081.7876 MAE=1036.7192 MAE=1010.5583 Epoch: 50/500MAE=993.3591 MAE=987.4745 MAE=949.9429 MAE=934.1104 MAE=931.1755 MAE=908.2782 MAE=889.4403 MAE=856.1349 MAE=840.6167 MAE=819.2380 Epoch: 60/500MAE=780.6344 MAE=770.6805 MAE=752.4740 MAE=739.1320 MAE=678.1364 MAE=654.0548 MAE=640.2821 MAE=625.2075 MAE=614.1457 MAE=578.7004 Epoch: 70/500MAE=552.8197 MAE=530.3209 MAE=454.1618 MAE=473.1555 MAE=483.5346 MAE=446.4643 MAE=405.2269 MAE=390.0589 MAE=392.1377 MAE=389.0171 Epoch: 80/500MAE=393.9325 MAE=350.2927 MAE=310.9747 MAE=318.7128 MAE=259.6586 MAE=287.1808 MAE=235.7720 MAE=374.8552 MAE=213.5759 MAE=175.1612 Epoch: 90/500MAE=217.5020 MAE=164.9398 MAE=196.5553 MAE=162.5648 MAE=169.9659 MAE=147.0465 MAE=143.5445 MAE=138.9405 MAE=139.1188 MAE=140.7010 Epoch: 100/500MAE=147.7390 MAE=136.4899 MAE=141.7521 MAE=136.8030 MAE=140.4187 MAE=137.4375 MAE=135.4600 MAE=135.1615 MAE=134.6024 MAE=134.4340 Epoch: 110/500MAE=135.2153 MAE=137.0461 MAE=134.1080 MAE=134.7296 MAE=134.5259 MAE=135.0870 MAE=136.5313 MAE=134.7881 MAE=136.2917 MAE=134.2546 Epoch: 120/500MAE=133.8846 MAE=135.4263 MAE=133.3512 MAE=133.6736 MAE=134.4852 MAE=134.2543 MAE=133.6230 MAE=133.2229 MAE=133.5085 MAE=133.4200 Epoch: 130/500MAE=133.0434 MAE=133.1660 MAE=133.0998 MAE=132.6141 MAE=133.1213 MAE=133.0057 MAE=133.6052 MAE=133.8414 MAE=133.5561 MAE=133.6903 Epoch: 140/500MAE=133.5566 MAE=133.4730 MAE=133.4094 MAE=133.1095 MAE=133.0856 MAE=132.9983 MAE=133.0883 MAE=133.3893 MAE=133.4964 MAE=133.5288 Epoch: 150/500MAE=133.5237 MAE=133.5144 MAE=133.5029 MAE=133.4497 MAE=133.3457 MAE=133.3233 MAE=133.3152 MAE=133.2883 MAE=133.2836 MAE=133.4009 Epoch: 160/500MAE=133.4087 MAE=133.4122 MAE=133.4155 MAE=133.4200 MAE=133.4184 MAE=133.4184 MAE=133.4129 MAE=133.4739 MAE=133.4041 MAE=133.4059 Epoch: 170/500MAE=133.4006 MAE=133.4724 MAE=133.4666 MAE=133.4596 MAE=133.3785 MAE=133.3770 MAE=133.3788 MAE=133.3843 MAE=133.3903 MAE=133.4638 Epoch: 180/500MAE=133.3899 MAE=133.3879 MAE=133.4600 MAE=133.3907 MAE=133.4564 MAE=133.3854 MAE=133.4602 MAE=133.4496 MAE=133.3377 MAE=133.3362 Epoch: 190/500MAE=133.4511 MAE=133.4414 MAE=133.3684 MAE=133.4384 MAE=133.4284 MAE=133.4258 MAE=133.4212 MAE=133.4209 MAE=133.4127 MAE=133.4126 Epoch: 200/500MAE=133.4176 MAE=133.4160 MAE=133.4133 MAE=133.4131 MAE=133.4170 MAE=133.4112 MAE=133.4152 MAE=133.4099 MAE=133.4078 MAE=133.4062 Epoch: 210/500MAE=133.4143 MAE=133.4103 MAE=133.4100 MAE=133.4244 MAE=133.4298 MAE=133.4352 MAE=133.4353 MAE=133.4465 MAE=133.4399 MAE=133.4535 Epoch: 220/500MAE=133.4391 MAE=133.4388 MAE=133.4376 MAE=133.4047 MAE=133.4347 MAE=133.4413 MAE=133.4491 MAE=133.4434 MAE=133.4400 MAE=133.4418 Epoch: 230/500MAE=133.4409 MAE=133.3750 MAE=133.4416 MAE=133.3690 MAE=133.4438 MAE=133.4351 MAE=133.3556 MAE=133.4329 MAE=133.3387 MAE=133.3403 Epoch: 240/500MAE=133.4069 MAE=133.4051 MAE=133.4058 MAE=133.4019 MAE=133.3969 MAE=133.3888 MAE=133.2654 MAE=133.3807 MAE=133.2151 MAE=133.3832 Epoch: 250/500MAE=133.3891 MAE=133.3858 MAE=133.3895 MAE=133.3908 MAE=133.3895 MAE=133.4094 MAE=133.4073 MAE=133.4112 MAE=133.2944 MAE=133.2928 Epoch: 260/500MAE=133.2933 MAE=133.2878 MAE=133.2900 MAE=133.2916 MAE=133.4090 MAE=133.4025 MAE=133.4145 MAE=133.2206 MAE=133.2944 MAE=133.2814 Epoch: 270/500MAE=133.3998 MAE=133.2934 MAE=133.2811 MAE=133.2755 MAE=133.3893 MAE=133.2799 MAE=133.2726 MAE=133.2746 MAE=133.2755 MAE=133.2737 Epoch: 280/500MAE=133.2888 MAE=133.2840 MAE=133.2719 MAE=133.2832 MAE=140.8870 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 140.887 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/500MAE=1550.0396 MAE=1548.2708 MAE=1545.7837 MAE=1542.8484 MAE=1539.9604 MAE=1537.1892 MAE=1533.2915 MAE=1529.9634 MAE=1525.6152 Epoch: 10/500MAE=1520.3337 MAE=1514.7153 MAE=1508.1321 MAE=1503.3035 MAE=1498.4082 MAE=1491.2764 MAE=1482.7260 MAE=1477.4252 MAE=1466.0225 MAE=1462.1990 Epoch: 20/500MAE=1452.6177 MAE=1460.4949 MAE=1438.2468 MAE=1421.5575 MAE=1398.4988 MAE=1401.2290 MAE=1385.2869 MAE=1365.3005 MAE=1351.0264 MAE=1345.1991 Epoch: 30/500MAE=1329.6929 MAE=1318.0209 MAE=1305.8179 MAE=1296.7405 MAE=1265.4177 MAE=1253.8799 MAE=1266.0447 MAE=1237.7148 MAE=1205.0023 MAE=1205.6196 Epoch: 40/500MAE=1178.9952 MAE=1182.9290 MAE=1163.4880 MAE=1139.8669 MAE=986.8997 MAE=1127.3245 MAE=1090.1053 MAE=1083.1963 MAE=1042.3441 MAE=1040.8920 Epoch: 50/500MAE=1047.0684 MAE=1012.7299 MAE=1032.6064 MAE=994.3669 MAE=1007.7036 MAE=989.0200 MAE=980.9912 MAE=984.9182 MAE=979.5933 MAE=973.9384 Epoch: 60/500MAE=968.9568 MAE=963.4542 MAE=956.1763 MAE=965.0436 MAE=948.3787 MAE=946.7953 MAE=946.5704 MAE=931.2264 MAE=939.4473 MAE=925.7549 Epoch: 70/500MAE=918.5925 MAE=925.4257 MAE=902.5414 MAE=911.3761 MAE=902.2231 MAE=902.4366 MAE=892.9554 MAE=889.6520 MAE=879.4269 MAE=878.4358 Epoch: 80/500MAE=875.9989 MAE=868.6184 MAE=863.5964 MAE=859.4768 MAE=855.0623 MAE=849.7072 MAE=831.8868 MAE=854.3065 MAE=877.2167 MAE=853.7252 Epoch: 90/500MAE=827.6525 MAE=820.4777 MAE=817.1982 MAE=816.3459 MAE=810.3452 MAE=801.9785 MAE=796.4689 MAE=796.0425 MAE=789.8462 MAE=776.7815 Epoch: 100/500MAE=848.1104 MAE=776.0763 MAE=781.2302 MAE=761.3405 MAE=797.9852 MAE=773.3904 MAE=776.2872 MAE=749.2460 MAE=744.7059 MAE=736.2496 Epoch: 110/500MAE=738.6011 MAE=756.8081 MAE=711.9263 MAE=778.6398 MAE=737.5576 MAE=704.0539 MAE=702.3851 MAE=708.7386 MAE=691.0983 MAE=688.9658 Epoch: 120/500MAE=686.2860 MAE=685.1870 MAE=674.1245 MAE=673.9969 MAE=668.3846 MAE=659.9943 MAE=661.9739 MAE=648.5677 MAE=645.8126 MAE=640.3129 Epoch: 130/500MAE=647.5925 MAE=634.1064 MAE=630.7258 MAE=616.1891 MAE=629.4962 MAE=629.6044 MAE=617.4071 MAE=608.4094 MAE=611.7922 MAE=599.9501 Epoch: 140/500MAE=593.0251 MAE=593.8242 MAE=579.9795 MAE=581.5646 MAE=553.7939 MAE=575.1857 MAE=574.7032 MAE=565.1090 MAE=582.7993 MAE=548.5646 Epoch: 150/500MAE=565.5295 MAE=566.2851 MAE=547.3221 MAE=537.7372 MAE=569.0452 MAE=538.3262 MAE=531.1038 MAE=564.4470 MAE=519.9591 MAE=560.5850 Epoch: 160/500MAE=538.4094 MAE=538.0791 MAE=529.4643 MAE=511.1966 MAE=519.2097 MAE=524.9805 MAE=510.1544 MAE=517.6624 MAE=507.3280 MAE=509.9787 Epoch: 170/500MAE=510.6825 MAE=511.7288 MAE=507.4179 MAE=515.8519 MAE=510.3137 MAE=505.9615 MAE=511.8528 MAE=510.8636 MAE=501.7778 MAE=500.2295 Epoch: 180/500MAE=503.6635 MAE=502.5292 MAE=502.0518 MAE=506.5113 MAE=499.2214 MAE=497.7236 MAE=498.0533 MAE=501.2922 MAE=501.4504 MAE=496.6541 Epoch: 190/500MAE=498.0962 MAE=499.4616 MAE=499.2177 MAE=496.9254 MAE=497.6849 MAE=499.3850 MAE=499.3809 MAE=497.1226 MAE=497.0247 MAE=496.8242 Epoch: 200/500MAE=496.7102 MAE=496.2401 MAE=495.8649 MAE=496.5779 MAE=496.2627 MAE=495.9105 MAE=496.7158 MAE=496.3116 MAE=495.9801 MAE=495.9695 Epoch: 210/500MAE=495.5537 MAE=495.9498 MAE=495.9292 MAE=495.9550 MAE=495.9850 MAE=495.8829 MAE=495.9677 MAE=495.8651 MAE=495.8315 MAE=495.7372 Epoch: 220/500MAE=495.7422 MAE=495.6586 MAE=495.7409 MAE=495.8586 MAE=495.6591 MAE=495.7150 MAE=495.6735 MAE=495.8064 MAE=495.6086 MAE=495.8063 Epoch: 230/500MAE=495.6439 MAE=495.5786 MAE=495.7002 MAE=495.5670 MAE=495.5682 MAE=495.3719 MAE=495.3635 MAE=495.3307 MAE=495.3317 MAE=495.4617 Epoch: 240/500MAE=495.4961 MAE=495.4899 MAE=495.4145 MAE=495.2877 MAE=495.2057 MAE=495.3565 MAE=495.1663 MAE=495.1391 MAE=495.1003 MAE=495.0581 Epoch: 250/500MAE=495.1949 MAE=495.1500 MAE=495.1667 MAE=495.2787 MAE=495.2873 MAE=495.0228 MAE=495.0856 MAE=494.9455 MAE=494.9145 MAE=494.9390 Epoch: 260/500MAE=494.7207 MAE=494.7949 MAE=494.8494 MAE=495.0259 MAE=495.0859 MAE=494.9778 MAE=495.1107 MAE=494.9152 MAE=494.8874 MAE=494.7970 Epoch: 270/500MAE=494.9461 MAE=495.0074 MAE=495.0612 MAE=494.9079 MAE=494.8151 MAE=494.8836 MAE=494.7830 MAE=494.8871 MAE=494.9637 MAE=495.1254 Epoch: 280/500MAE=495.0179 MAE=494.6958 MAE=494.6095 MAE=494.6871 MAE=494.5342 MAE=494.5232 MAE=494.5071 MAE=494.5540 MAE=494.6980 MAE=494.6679 Epoch: 290/500MAE=494.6829 MAE=494.6482 MAE=494.5311 MAE=494.4696 MAE=494.4038 MAE=494.5727 MAE=494.5485 MAE=494.5008 MAE=494.5181 MAE=494.5414 Epoch: 300/500MAE=494.3924 MAE=494.5347 MAE=494.4387 MAE=494.4015 MAE=494.3471 MAE=494.4047 MAE=494.2548 MAE=494.0953 MAE=493.9976 MAE=494.0442 Epoch: 310/500MAE=494.1459 MAE=494.1414 MAE=494.0921 MAE=494.2008 MAE=494.0871 MAE=494.0216 MAE=494.0264 MAE=494.1208 MAE=494.0756 MAE=493.8954 Epoch: 320/500MAE=493.8150 MAE=493.8437 MAE=493.8462 MAE=493.9826 MAE=494.0671 MAE=493.9025 MAE=493.9018 MAE=493.8768 MAE=493.8752 MAE=493.8685 Epoch: 330/500MAE=493.8415 MAE=493.9485 MAE=493.9347 MAE=493.9122 MAE=493.7673 MAE=493.6294 MAE=493.6418 MAE=493.9010 MAE=493.7974 MAE=493.5723 Epoch: 340/500MAE=493.7677 MAE=493.8544 MAE=493.7827 MAE=493.6720 MAE=493.5425 MAE=493.5489 MAE=493.6858 MAE=493.6860 MAE=493.5194 MAE=493.6023 Epoch: 350/500MAE=493.4393 MAE=493.3755 MAE=493.4741 MAE=493.2975 MAE=493.5732 MAE=493.4168 MAE=493.4542 MAE=493.2701 MAE=493.4337 MAE=493.2940 Epoch: 360/500MAE=493.3355 MAE=493.2211 MAE=493.2941 MAE=493.3043 MAE=493.2611 MAE=493.1755 MAE=493.1984 MAE=493.1613 MAE=493.2815 MAE=493.2550 Epoch: 370/500MAE=493.3497 MAE=493.1804 MAE=493.0385 MAE=493.1426 MAE=493.0982 MAE=493.1951 MAE=493.2101 MAE=493.1347 MAE=493.2061 MAE=493.1235 Epoch: 380/500MAE=493.1374 MAE=493.1888 MAE=493.0255 MAE=492.9905 MAE=493.0025 MAE=492.9346 MAE=492.9091 MAE=492.7507 MAE=493.0223 MAE=493.0409 Epoch: 390/500MAE=492.7553 MAE=492.8441 MAE=492.8453 MAE=492.9390 MAE=492.9888 MAE=492.7255 MAE=492.7945 MAE=492.7608 MAE=492.6895 MAE=492.6472 Epoch: 400/500MAE=492.6465 MAE=492.6041 MAE=492.7173 MAE=492.5382 MAE=492.6295 MAE=492.6419 MAE=492.5272 MAE=492.5828 MAE=492.5307 MAE=492.6284 Epoch: 410/500MAE=492.5228 MAE=492.3929 MAE=492.5616 MAE=492.4932 MAE=492.5266 MAE=492.5479 MAE=492.6080 MAE=492.4424 MAE=492.3516 MAE=492.4437 Epoch: 420/500MAE=492.4265 MAE=492.4014 MAE=492.4277 MAE=492.3694 MAE=492.2829 MAE=492.3311 MAE=492.2554 MAE=492.3151 MAE=492.3271 MAE=492.1210 Epoch: 430/500MAE=492.0164 MAE=492.0692 MAE=492.2224 MAE=492.3064 MAE=492.1432 MAE=492.0152 MAE=492.0510 MAE=492.1039 MAE=492.1329 MAE=492.1114 Epoch: 440/500MAE=492.0474 MAE=492.0523 MAE=491.9711 MAE=492.0651 MAE=492.0339 MAE=491.9353 MAE=491.8675 MAE=491.7527 MAE=491.8373 MAE=491.9917 Epoch: 450/500MAE=491.8867 MAE=491.8342 MAE=492.0144 MAE=492.0005 MAE=491.8620 MAE=491.8877 MAE=492.0277 MAE=491.9109 MAE=491.8820 MAE=491.6537 Epoch: 460/500MAE=491.6903 MAE=491.5491 MAE=491.8007 MAE=491.6339 MAE=491.6514 MAE=491.4396 MAE=491.6772 MAE=491.7231 MAE=491.5425 MAE=491.5688 Epoch: 470/500MAE=491.5225 MAE=491.3805 MAE=491.4277 MAE=491.4280 MAE=491.2845 MAE=491.3214 MAE=491.2930 MAE=491.2970 MAE=491.4127 MAE=491.4501 Epoch: 480/500MAE=491.3746 MAE=491.1895 MAE=491.3678 MAE=491.3430 MAE=491.3044 MAE=491.2025 MAE=491.3124 MAE=491.3055 MAE=491.1110 MAE=491.1945 Epoch: 490/500MAE=491.2006 MAE=491.3814 MAE=491.1491 MAE=491.0733 MAE=491.0085 MAE=491.0627 MAE=491.0297 MAE=491.0040 MAE=491.1940 MAE=491.0828 Epoch: 500/500MAE=490.8878 MAE=477.2813 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 309.084 +/- 168.197\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.9530 MAE=1547.7593 MAE=1544.5864 MAE=1542.1952 MAE=1539.5790 MAE=1536.7233 MAE=1533.8809 MAE=1528.0787 MAE=1524.9792 Epoch: 10/500MAE=1519.1841 MAE=1514.1537 MAE=1504.7256 MAE=1501.5607 MAE=1499.7987 MAE=1488.1440 MAE=1480.2078 MAE=1475.4722 MAE=1468.2107 MAE=1460.3296 Epoch: 20/500MAE=1456.7810 MAE=1444.8137 MAE=1445.2260 MAE=1399.6230 MAE=1401.1465 MAE=1392.4622 MAE=1378.7677 MAE=1374.6853 MAE=1350.1190 MAE=1362.9983 Epoch: 30/500MAE=1326.6356 MAE=1334.1646 MAE=1316.1113 MAE=1301.7500 MAE=1277.7065 MAE=1255.7428 MAE=1261.1520 MAE=1233.0632 MAE=1224.7190 MAE=1227.3004 Epoch: 40/500MAE=1176.2810 MAE=1145.3856 MAE=1150.2156 MAE=1128.3152 MAE=1119.6897 MAE=1101.6029 MAE=1070.5781 MAE=1065.2010 MAE=1025.6316 MAE=1012.3734 Epoch: 50/500MAE=1006.5510 MAE=972.5885 MAE=957.6416 MAE=933.3328 MAE=921.7800 MAE=891.9487 MAE=858.0693 MAE=832.8792 MAE=839.9427 MAE=871.6384 Epoch: 60/500MAE=781.3390 MAE=752.4031 MAE=744.7297 MAE=710.4303 MAE=687.9883 MAE=668.6954 MAE=638.9653 MAE=613.6824 MAE=596.3156 MAE=569.7501 Epoch: 70/500MAE=566.4995 MAE=527.6952 MAE=527.9963 MAE=501.2242 MAE=446.6883 MAE=417.8992 MAE=441.9396 MAE=365.9647 MAE=385.5593 MAE=397.8086 Epoch: 80/500MAE=390.9906 MAE=294.5155 MAE=233.2809 MAE=228.1525 MAE=210.3932 MAE=207.5698 MAE=192.2389 MAE=230.8380 MAE=173.8126 MAE=199.7411 Epoch: 90/500MAE=178.2878 MAE=187.8732 MAE=201.3817 MAE=150.5782 MAE=201.6632 MAE=158.8988 MAE=165.2419 MAE=152.8580 MAE=146.1021 MAE=154.7314 Epoch: 100/500MAE=148.6089 MAE=143.2085 MAE=143.7597 MAE=141.7721 MAE=142.1046 MAE=137.6052 MAE=140.7188 MAE=137.9468 MAE=140.9151 MAE=140.2461 Epoch: 110/500MAE=135.6354 MAE=134.1660 MAE=136.6689 MAE=135.7452 MAE=136.0312 MAE=133.4245 MAE=134.2247 MAE=135.3169 MAE=135.4385 MAE=133.0277 Epoch: 120/500MAE=132.4201 MAE=134.3979 MAE=135.1940 MAE=131.9857 MAE=135.7203 MAE=133.1370 MAE=133.1521 MAE=131.7879 MAE=132.6697 MAE=130.9247 Epoch: 130/500MAE=129.1112 MAE=131.9570 MAE=131.8207 MAE=131.7865 MAE=129.4283 MAE=130.7207 MAE=129.9029 MAE=129.8394 MAE=129.7574 MAE=130.0685 Epoch: 140/500MAE=129.4841 MAE=130.1851 MAE=129.8435 MAE=129.8012 MAE=129.6944 MAE=129.6957 MAE=129.6979 MAE=129.5173 MAE=129.5530 MAE=129.7334 Epoch: 150/500MAE=129.5419 MAE=129.6426 MAE=129.6600 MAE=129.6181 MAE=129.6358 MAE=129.5682 MAE=129.4931 MAE=129.6201 MAE=129.6423 MAE=129.6378 Epoch: 160/500MAE=129.6248 MAE=129.6348 MAE=129.6216 MAE=129.5966 MAE=129.6280 MAE=129.6227 MAE=129.5910 MAE=129.6185 MAE=129.6230 MAE=129.6279 Epoch: 170/500MAE=129.6252 MAE=129.6651 MAE=129.6251 MAE=129.6514 MAE=129.5973 MAE=129.6049 MAE=129.5421 MAE=129.5543 MAE=129.5849 MAE=129.6384 Epoch: 180/500MAE=129.6182 MAE=129.6014 MAE=129.5950 MAE=129.6207 MAE=129.6003 MAE=129.5880 MAE=129.5527 MAE=129.5614 MAE=129.5680 MAE=129.5575 Epoch: 190/500MAE=129.5596 MAE=129.6144 MAE=129.5614 MAE=129.5584 MAE=129.6028 MAE=129.5676 MAE=129.5469 MAE=129.4476 MAE=129.4253 MAE=129.4312 Epoch: 200/500MAE=129.5297 MAE=129.5716 MAE=129.5440 MAE=129.4387 MAE=129.5629 MAE=129.5642 MAE=129.6022 MAE=129.5616 MAE=129.5617 MAE=129.5753 Epoch: 210/500MAE=129.5989 MAE=129.5960 MAE=129.5488 MAE=129.5676 MAE=129.5636 MAE=129.5322 MAE=129.5617 MAE=129.5407 MAE=129.5605 MAE=129.5345 Epoch: 220/500MAE=129.5367 MAE=129.5080 MAE=129.5385 MAE=129.4970 MAE=129.4961 MAE=129.4955 MAE=129.4798 MAE=129.4888 MAE=129.5316 MAE=129.4948 Epoch: 230/500MAE=129.5009 MAE=129.5058 MAE=129.4819 MAE=129.5083 MAE=129.5093 MAE=129.5143 MAE=129.5420 MAE=129.5180 MAE=129.5177 MAE=129.4731 Epoch: 240/500MAE=129.4738 MAE=129.4917 MAE=129.4668 MAE=129.4521 MAE=129.4532 MAE=129.4729 MAE=129.4882 MAE=129.4779 MAE=129.5191 MAE=129.5265 Epoch: 250/500MAE=129.5062 MAE=129.4784 MAE=129.5029 MAE=129.4806 MAE=129.4955 MAE=129.4520 MAE=129.4621 MAE=129.4502 MAE=129.4437 MAE=129.4436 Epoch: 260/500MAE=129.4244 MAE=129.4423 MAE=129.4318 MAE=129.4415 MAE=129.4514 MAE=129.4166 MAE=129.4338 MAE=129.4140 MAE=129.4047 MAE=129.4342 Epoch: 270/500MAE=129.4355 MAE=129.4245 MAE=129.4200 MAE=129.4078 MAE=129.3998 MAE=129.4083 MAE=129.4299 MAE=129.4039 MAE=129.3897 MAE=129.4158 Epoch: 280/500MAE=129.3964 MAE=135.2553 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 251.141 +/- 159.922\n",
                        "\n",
                        "Epoch: 1/500MAE=1550.0051 MAE=1548.0233 MAE=1545.1721 MAE=1543.0940 MAE=1540.8525 MAE=1537.4275 MAE=1533.1531 MAE=1529.9515 MAE=1528.5566 Epoch: 10/500MAE=1521.4226 MAE=1516.4537 MAE=1508.8127 MAE=1504.1080 MAE=1499.7803 MAE=1488.7504 MAE=1483.1904 MAE=1472.9355 MAE=1465.6379 MAE=1458.6394 Epoch: 20/500MAE=1452.5132 MAE=1443.5034 MAE=1449.2722 MAE=1421.1156 MAE=1412.3064 MAE=1401.3000 MAE=1392.8262 MAE=1383.5795 MAE=1368.4243 MAE=1358.0686 Epoch: 30/500MAE=1345.6355 MAE=1334.3553 MAE=1317.2710 MAE=1304.8137 MAE=1292.0896 MAE=1275.9307 MAE=1262.5420 MAE=1248.0577 MAE=1231.0469 MAE=1212.4292 Epoch: 40/500MAE=1198.0615 MAE=1187.5833 MAE=1167.0765 MAE=1157.9626 MAE=1133.7861 MAE=1113.3301 MAE=1100.8708 MAE=1081.4066 MAE=1063.7800 MAE=1045.6050 Epoch: 50/500MAE=1024.4758 MAE=1006.9141 MAE=981.5889 MAE=964.1780 MAE=942.4773 MAE=921.4025 MAE=901.8538 MAE=880.9653 MAE=865.5459 MAE=846.0115 Epoch: 60/500MAE=818.7269 MAE=815.0763 MAE=769.2491 MAE=749.0377 MAE=719.3794 MAE=693.4866 MAE=695.1533 MAE=652.4102 MAE=632.4199 MAE=598.4359 Epoch: 70/500MAE=595.5903 MAE=569.8217 MAE=549.3270 MAE=498.1005 MAE=513.4294 MAE=451.0811 MAE=454.8358 MAE=429.8890 MAE=442.5063 MAE=456.7148 Epoch: 80/500MAE=396.2800 MAE=339.1774 MAE=335.3846 MAE=313.5989 MAE=324.4962 MAE=299.5844 MAE=256.4509 MAE=249.7106 MAE=234.4227 MAE=216.6665 Epoch: 90/500MAE=223.3654 MAE=216.2173 MAE=194.9753 MAE=231.5681 MAE=180.6088 MAE=175.2856 MAE=168.0388 MAE=152.7693 MAE=157.3949 MAE=162.6700 Epoch: 100/500MAE=147.9824 MAE=145.1438 MAE=147.8242 MAE=144.7739 MAE=139.9113 MAE=137.4910 MAE=139.5395 MAE=136.4709 MAE=136.5011 MAE=134.7839 Epoch: 110/500MAE=134.7267 MAE=134.0894 MAE=138.1990 MAE=138.0725 MAE=135.8897 MAE=137.9600 MAE=132.6164 MAE=132.5243 MAE=130.5315 MAE=134.0164 Epoch: 120/500MAE=130.5721 MAE=131.0447 MAE=130.6378 MAE=131.3823 MAE=130.4346 MAE=130.3006 MAE=131.2896 MAE=129.8987 MAE=129.6829 MAE=130.6106 Epoch: 130/500MAE=130.0641 MAE=130.8992 MAE=130.0076 MAE=130.3038 MAE=130.5421 MAE=129.9438 MAE=130.9747 MAE=130.2532 MAE=130.2213 MAE=130.2527 Epoch: 140/500MAE=129.9013 MAE=129.7070 MAE=129.7323 MAE=129.8542 MAE=129.2940 MAE=129.5736 MAE=129.5505 MAE=129.0788 MAE=129.4763 MAE=129.2470 Epoch: 150/500MAE=129.4555 MAE=129.6409 MAE=130.2451 MAE=129.6544 MAE=129.3606 MAE=129.9945 MAE=130.1134 MAE=130.1086 MAE=130.1232 MAE=130.1357 Epoch: 160/500MAE=130.0508 MAE=130.2467 MAE=130.2106 MAE=130.0470 MAE=130.0151 MAE=129.9598 MAE=129.7863 MAE=129.8014 MAE=129.8069 MAE=129.7773 Epoch: 170/500MAE=129.8292 MAE=129.7859 MAE=129.8115 MAE=129.8006 MAE=129.8088 MAE=129.8312 MAE=129.8053 MAE=129.7737 MAE=129.8030 MAE=129.7385 Epoch: 180/500MAE=129.8016 MAE=129.9430 MAE=129.7870 MAE=129.9241 MAE=129.9291 MAE=129.8676 MAE=129.9387 MAE=129.9398 MAE=130.0150 MAE=129.8677 Epoch: 190/500MAE=129.9418 MAE=129.8743 MAE=129.7723 MAE=129.6462 MAE=129.7739 MAE=129.7797 MAE=129.8238 MAE=129.8745 MAE=129.8238 MAE=129.8347 Epoch: 200/500MAE=129.8385 MAE=129.8335 MAE=129.8376 MAE=129.7718 MAE=129.7610 MAE=129.7917 MAE=129.7687 MAE=129.8430 MAE=129.7966 MAE=129.7893 Epoch: 210/500MAE=129.8055 MAE=129.7940 MAE=129.7605 MAE=129.7512 MAE=129.7752 MAE=129.8659 MAE=129.8612 MAE=129.8275 MAE=129.8298 MAE=129.8982 Epoch: 220/500MAE=129.7900 MAE=129.8863 MAE=129.9286 MAE=129.8594 MAE=129.8531 MAE=129.9232 MAE=129.8549 MAE=129.8434 MAE=129.8190 MAE=129.8412 Epoch: 230/500MAE=129.8195 MAE=129.7666 MAE=129.7714 MAE=129.8277 MAE=129.7058 MAE=129.6613 MAE=129.6685 MAE=129.6805 MAE=129.8819 MAE=129.9913 Epoch: 240/500MAE=129.9794 MAE=129.8610 MAE=129.9124 MAE=130.0636 MAE=129.8429 MAE=129.8814 MAE=129.9835 MAE=129.8803 MAE=129.6721 MAE=129.6629 Epoch: 250/500MAE=129.6986 MAE=129.6809 MAE=129.7330 MAE=129.7297 MAE=129.7381 MAE=129.7789 MAE=129.7449 MAE=129.9131 MAE=129.9161 MAE=129.8919 Epoch: 260/500MAE=129.8619 MAE=129.9187 MAE=129.6908 MAE=129.6829 MAE=129.6823 MAE=129.6839 MAE=129.7148 MAE=129.9402 MAE=129.8558 MAE=129.7401 Epoch: 270/500MAE=129.7582 MAE=129.8148 MAE=129.7023 MAE=129.7572 MAE=129.8407 MAE=129.8217 MAE=129.9406 MAE=129.9400 MAE=129.9520 MAE=129.7973 Epoch: 280/500MAE=129.8488 MAE=129.9813 MAE=129.8506 MAE=129.7011 MAE=129.8621 MAE=129.6994 MAE=129.7532 MAE=129.7987 MAE=129.8714 MAE=129.8531 Epoch: 290/500MAE=129.8540 MAE=129.7863 MAE=129.8829 MAE=129.8372 MAE=129.8083 MAE=129.8822 MAE=129.8599 MAE=129.8359 MAE=134.9790 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 222.101 +/- 147.347\n",
                        "\n",
                        "Epoch: 1/500MAE=1550.0110 MAE=1547.8445 MAE=1545.2450 MAE=1542.3663 MAE=1539.0170 MAE=1535.9165 MAE=1532.7252 MAE=1528.6641 MAE=1524.9226 Epoch: 10/500MAE=1519.1572 MAE=1516.1282 MAE=1510.1027 MAE=1502.2400 MAE=1495.3484 MAE=1491.7458 MAE=1484.0801 MAE=1474.7656 MAE=1469.5605 MAE=1459.9470 Epoch: 20/500MAE=1451.9297 MAE=1439.7729 MAE=1439.0677 MAE=1427.8337 MAE=1413.1205 MAE=1400.5349 MAE=1395.6741 MAE=1377.1899 MAE=1371.6945 MAE=1357.4949 Epoch: 30/500MAE=1345.0302 MAE=1336.4199 MAE=1323.2083 MAE=1302.0452 MAE=1293.0229 MAE=1281.6925 MAE=1248.5839 MAE=1235.3602 MAE=1222.6382 MAE=1215.9813 Epoch: 40/500MAE=1201.5420 MAE=1173.4763 MAE=1162.7949 MAE=1139.4857 MAE=1125.5364 MAE=1113.3362 MAE=1107.1782 MAE=1076.6768 MAE=1059.3528 MAE=1036.7969 Epoch: 50/500MAE=1011.7073 MAE=1005.2896 MAE=974.4969 MAE=960.0483 MAE=922.7491 MAE=909.6624 MAE=885.2677 MAE=882.0212 MAE=827.5456 MAE=815.4841 Epoch: 60/500MAE=777.6091 MAE=771.4653 MAE=765.1079 MAE=715.4272 MAE=702.3217 MAE=660.4436 MAE=631.9279 MAE=616.0447 MAE=616.5360 MAE=585.1965 Epoch: 70/500MAE=569.9808 MAE=548.6073 MAE=450.9013 MAE=488.6499 MAE=478.9432 MAE=485.3492 MAE=397.8242 MAE=414.3620 MAE=402.6066 MAE=333.5009 Epoch: 80/500MAE=322.1466 MAE=295.8814 MAE=347.0851 MAE=252.3998 MAE=300.3507 MAE=244.8489 MAE=245.6143 MAE=221.0472 MAE=241.5284 MAE=216.2269 Epoch: 90/500MAE=189.5748 MAE=177.9509 MAE=175.5659 MAE=180.3369 MAE=159.9743 MAE=152.6811 MAE=163.7889 MAE=159.4247 MAE=158.1973 MAE=167.1910 Epoch: 100/500MAE=152.2889 MAE=146.8455 MAE=149.2481 MAE=149.9297 MAE=152.6975 MAE=148.9722 MAE=148.8455 MAE=149.7277 MAE=150.8293 MAE=150.7357 Epoch: 110/500MAE=149.3071 MAE=147.5094 MAE=147.6495 MAE=147.2663 MAE=148.3364 MAE=148.1973 MAE=147.8839 MAE=147.8058 MAE=147.8300 MAE=147.7713 Epoch: 120/500MAE=147.5366 MAE=147.9875 MAE=147.9978 MAE=148.0810 MAE=147.9919 MAE=147.7940 MAE=147.9060 MAE=147.8619 MAE=147.8011 MAE=147.7671 Epoch: 130/500MAE=147.7160 MAE=147.7464 MAE=147.7711 MAE=147.7345 MAE=147.7328 MAE=147.7340 MAE=147.7228 MAE=147.7169 MAE=147.7458 MAE=147.7287 Epoch: 140/500MAE=147.5966 MAE=147.5841 MAE=147.7122 MAE=147.7214 MAE=147.7166 MAE=147.7259 MAE=147.6881 MAE=147.7199 MAE=147.5998 MAE=147.5904 Epoch: 150/500MAE=147.5445 MAE=147.5381 MAE=147.5330 MAE=147.5390 MAE=147.5408 MAE=147.5340 MAE=147.5490 MAE=147.5374 MAE=147.5393 MAE=147.5388 Epoch: 160/500MAE=147.5393 MAE=147.5397 MAE=147.5415 MAE=147.5399 MAE=147.5383 MAE=147.5284 MAE=147.5198 MAE=147.4620 MAE=147.4532 MAE=147.4587 Epoch: 170/500MAE=147.6018 MAE=147.4586 MAE=147.4696 MAE=147.4688 MAE=147.4772 MAE=147.6625 MAE=147.6164 MAE=147.6180 MAE=147.6586 MAE=147.6174 Epoch: 180/500MAE=147.6652 MAE=147.6325 MAE=147.6588 MAE=147.6615 MAE=147.6618 MAE=147.6691 MAE=147.6666 MAE=147.6653 MAE=147.6693 MAE=147.6650 Epoch: 190/500MAE=147.6702 MAE=147.5650 MAE=147.5716 MAE=147.5881 MAE=147.5771 MAE=147.5880 MAE=147.6456 MAE=147.6258 MAE=147.6323 MAE=147.5668 Epoch: 200/500MAE=147.5541 MAE=147.5598 MAE=147.5538 MAE=147.5633 MAE=147.9131 MAE=147.8995 MAE=147.8945 MAE=147.8956 MAE=147.9126 MAE=147.9098 Epoch: 210/500MAE=147.5764 MAE=147.9673 MAE=147.9580 MAE=147.9464 MAE=147.6733 MAE=147.9884 MAE=148.0848 MAE=148.0286 MAE=148.0415 MAE=148.0975 Epoch: 220/500MAE=148.1326 MAE=148.1262 MAE=148.1272 MAE=148.1302 MAE=148.1627 MAE=148.1561 MAE=148.1944 MAE=148.1892 MAE=148.1880 MAE=148.1980 Epoch: 230/500MAE=148.2245 MAE=148.2391 MAE=148.2291 MAE=148.2196 MAE=148.2224 MAE=148.2374 MAE=148.2193 MAE=148.2366 MAE=148.2412 MAE=148.2437 Epoch: 240/500MAE=148.2427 MAE=148.2337 MAE=148.2206 MAE=148.2260 MAE=148.2244 MAE=148.2216 MAE=148.2195 MAE=148.2135 MAE=148.2235 MAE=148.2173 Epoch: 250/500MAE=148.2159 MAE=148.2252 MAE=153.4721 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 208.375 +/- 134.620\n",
                        "\n",
                        "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/500MAE=1549.9492 MAE=1548.1836 MAE=1545.2803 MAE=1542.7960 MAE=1539.9406 MAE=1537.6116 MAE=1533.3992 MAE=1530.7637 MAE=1524.9819 Epoch: 10/500MAE=1520.0881 MAE=1514.8666 MAE=1510.1956 MAE=1505.0374 MAE=1498.8444 MAE=1493.3008 MAE=1487.2307 MAE=1479.6096 MAE=1472.3584 MAE=1464.5087 Epoch: 20/500MAE=1457.9097 MAE=1435.0449 MAE=1426.2699 MAE=1418.0264 MAE=1409.8889 MAE=1414.0293 MAE=1391.5630 MAE=1381.3018 MAE=1367.7963 MAE=1345.2990 Epoch: 30/500MAE=1332.5720 MAE=1323.6493 MAE=1320.0242 MAE=1303.0549 MAE=1281.9751 MAE=1278.6078 MAE=1256.0178 MAE=1233.2893 MAE=1231.7000 MAE=1211.7820 Epoch: 40/500MAE=1203.4232 MAE=1184.3054 MAE=1153.9121 MAE=1151.3118 MAE=1133.5574 MAE=1072.0562 MAE=1111.8455 MAE=1073.6243 MAE=1049.0333 MAE=1031.0798 Epoch: 50/500MAE=983.0027 MAE=971.5651 MAE=979.3068 MAE=931.3644 MAE=934.0659 MAE=871.5491 MAE=861.4516 MAE=850.4276 MAE=828.9916 MAE=799.8698 Epoch: 60/500MAE=786.5919 MAE=777.8501 MAE=740.4414 MAE=732.5651 MAE=750.6237 MAE=640.1243 MAE=646.2371 MAE=619.4697 MAE=598.4756 MAE=558.9701 Epoch: 70/500MAE=549.5866 MAE=490.2898 MAE=507.6958 MAE=543.9825 MAE=495.7446 MAE=440.6028 MAE=431.4169 MAE=327.3718 MAE=289.7328 MAE=378.9461 Epoch: 80/500MAE=377.1022 MAE=242.8449 MAE=283.2221 MAE=223.6079 MAE=251.7424 MAE=207.7997 MAE=180.7252 MAE=176.9191 MAE=163.0160 MAE=152.7404 Epoch: 90/500MAE=125.9005 MAE=105.4661 MAE=208.5810 MAE=91.0013 MAE=90.9935 MAE=102.7887 MAE=89.4523 MAE=90.9347 MAE=123.1334 MAE=78.5221 Epoch: 100/500MAE=123.9990 MAE=77.6670 MAE=79.1164 MAE=87.7471 MAE=83.1349 MAE=77.3770 MAE=72.2691 MAE=95.1223 MAE=82.8478 MAE=73.4233 Epoch: 110/500MAE=79.6545 MAE=75.6708 MAE=100.6302 MAE=89.7160 MAE=79.8538 MAE=68.6949 MAE=66.3719 MAE=72.2377 MAE=67.2550 MAE=66.6502 Epoch: 120/500MAE=67.5457 MAE=65.7712 MAE=66.9492 MAE=66.5000 MAE=65.6189 MAE=65.9505 MAE=66.2280 MAE=67.3668 MAE=66.6522 MAE=65.8671 Epoch: 130/500MAE=65.7428 MAE=65.6997 MAE=65.2387 MAE=66.6160 MAE=66.3815 MAE=65.4159 MAE=66.2398 MAE=65.7257 MAE=65.1037 MAE=65.2154 Epoch: 140/500MAE=66.1068 MAE=65.9647 MAE=65.3399 MAE=65.2947 MAE=65.5481 MAE=65.7136 MAE=65.9292 MAE=65.9137 MAE=65.9197 MAE=66.0681 Epoch: 150/500MAE=66.0034 MAE=65.9077 MAE=65.8685 MAE=65.7695 MAE=65.7349 MAE=65.7874 MAE=65.7979 MAE=65.7395 MAE=65.7118 MAE=65.7533 Epoch: 160/500MAE=65.7502 MAE=65.8109 MAE=65.7873 MAE=65.8165 MAE=65.8322 MAE=65.8382 MAE=65.8738 MAE=65.8654 MAE=65.8575 MAE=65.8616 Epoch: 170/500MAE=65.8586 MAE=65.8079 MAE=65.7885 MAE=65.7971 MAE=65.8429 MAE=65.7994 MAE=65.8286 MAE=65.8506 MAE=65.7827 MAE=65.8301 Epoch: 180/500MAE=65.7833 MAE=65.8371 MAE=65.8464 MAE=65.8179 MAE=65.8234 MAE=65.7489 MAE=65.7853 MAE=65.7653 MAE=65.7966 MAE=65.7942 Epoch: 190/500MAE=65.8004 MAE=65.7388 MAE=65.8084 MAE=65.7533 MAE=65.8154 MAE=65.8170 MAE=65.8098 MAE=65.7677 MAE=65.8237 MAE=65.8005 Epoch: 200/500MAE=65.7278 MAE=65.7965 MAE=65.7824 MAE=65.7969 MAE=65.7167 MAE=65.7919 MAE=65.8168 MAE=65.7654 MAE=65.6544 MAE=65.6570 Epoch: 210/500MAE=65.7003 MAE=65.7599 MAE=65.6892 MAE=65.7720 MAE=65.6536 MAE=65.7422 MAE=65.7514 MAE=65.7273 MAE=65.7276 MAE=65.7185 Epoch: 220/500MAE=65.6632 MAE=65.6641 MAE=65.7200 MAE=65.7592 MAE=65.7472 MAE=65.7308 MAE=65.7201 MAE=65.7559 MAE=65.6764 MAE=65.7288 Epoch: 230/500MAE=65.7293 MAE=65.6357 MAE=65.6528 MAE=65.7036 MAE=65.6430 MAE=65.6800 MAE=65.6369 MAE=65.6360 MAE=65.6419 MAE=65.6963 Epoch: 240/500MAE=65.6904 MAE=65.7065 MAE=65.6271 MAE=65.6773 MAE=65.6673 MAE=65.6704 MAE=65.6326 MAE=65.6770 MAE=65.6789 MAE=65.7268 Epoch: 250/500MAE=65.7373 MAE=65.6854 MAE=65.6877 MAE=65.6277 MAE=65.6342 MAE=65.6958 MAE=65.6862 MAE=65.6173 MAE=65.6369 MAE=65.5770 Epoch: 260/500MAE=65.6402 MAE=65.6060 MAE=65.6356 MAE=65.6339 MAE=65.6282 MAE=65.5701 MAE=65.6594 MAE=65.5972 MAE=65.6627 MAE=65.6621 Epoch: 270/500MAE=65.6658 MAE=65.6915 MAE=65.6711 MAE=65.5632 MAE=65.5675 MAE=65.6032 MAE=65.6350 MAE=65.5900 MAE=65.5928 MAE=65.5281 Epoch: 280/500MAE=65.6281 MAE=65.6586 MAE=65.6661 MAE=65.6744 MAE=65.5845 MAE=65.6386 MAE=65.5554 MAE=65.6723 MAE=65.6465 MAE=66.6303 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 66.630 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.8447 MAE=1547.2290 MAE=1544.6334 MAE=1541.9824 MAE=1538.8098 MAE=1535.9900 MAE=1532.6346 MAE=1528.1379 MAE=1525.5149 Epoch: 10/500MAE=1521.5090 MAE=1515.2000 MAE=1509.1582 MAE=1504.9817 MAE=1497.9028 MAE=1491.3844 MAE=1482.0463 MAE=1481.0752 MAE=1468.6870 MAE=1456.5872 Epoch: 20/500MAE=1447.8440 MAE=1440.8994 MAE=1427.9983 MAE=1422.3315 MAE=1399.2654 MAE=1390.0286 MAE=1378.9102 MAE=1376.1821 MAE=1359.1677 MAE=1345.5959 Epoch: 30/500MAE=1328.3386 MAE=1310.7102 MAE=1299.0560 MAE=1291.4077 MAE=1279.1421 MAE=1268.0525 MAE=1243.3281 MAE=1235.8983 MAE=1220.3132 MAE=1196.7559 Epoch: 40/500MAE=1192.7511 MAE=1151.1218 MAE=1168.8254 MAE=1141.3926 MAE=1123.6060 MAE=1096.9697 MAE=1058.8949 MAE=1044.7244 MAE=1058.1339 MAE=992.9770 Epoch: 50/500MAE=973.8467 MAE=1001.4365 MAE=933.0486 MAE=926.4198 MAE=882.4749 MAE=888.9893 MAE=850.0154 MAE=810.9175 MAE=819.6407 MAE=770.0483 Epoch: 60/500MAE=774.2252 MAE=730.0099 MAE=708.4910 MAE=713.7529 MAE=673.3864 MAE=633.6470 MAE=651.0471 MAE=596.0033 MAE=560.8123 MAE=540.0874 Epoch: 70/500MAE=531.0466 MAE=523.6614 MAE=458.2632 MAE=365.6738 MAE=403.0584 MAE=423.9006 MAE=382.8719 MAE=394.7426 MAE=341.3142 MAE=333.5877 Epoch: 80/500MAE=328.7850 MAE=317.2632 MAE=303.7197 MAE=292.8944 MAE=281.8734 MAE=273.1558 MAE=278.2553 MAE=254.2180 MAE=240.4761 MAE=253.9389 Epoch: 90/500MAE=220.2838 MAE=223.8319 MAE=208.9470 MAE=218.2187 MAE=197.6963 MAE=182.8565 MAE=182.9640 MAE=185.7774 MAE=174.4252 MAE=160.8539 Epoch: 100/500MAE=173.0775 MAE=160.7440 MAE=161.1631 MAE=139.7407 MAE=146.1460 MAE=151.9814 MAE=141.4910 MAE=125.6297 MAE=128.9278 MAE=125.0642 Epoch: 110/500MAE=128.4161 MAE=116.2492 MAE=113.5764 MAE=115.4642 MAE=121.4744 MAE=114.0630 MAE=121.7993 MAE=109.8110 MAE=109.1927 MAE=110.3934 Epoch: 120/500MAE=107.4228 MAE=108.3244 MAE=109.0616 MAE=105.5130 MAE=118.0707 MAE=110.7571 MAE=117.7868 MAE=118.5918 MAE=111.2588 MAE=105.9833 Epoch: 130/500MAE=106.4345 MAE=106.6237 MAE=104.0798 MAE=103.4199 MAE=104.3774 MAE=102.3194 MAE=103.6387 MAE=103.4755 MAE=101.0870 MAE=101.5092 Epoch: 140/500MAE=100.5991 MAE=101.5262 MAE=103.1833 MAE=101.4606 MAE=100.7531 MAE=99.9052 MAE=100.3920 MAE=100.9080 MAE=99.9276 MAE=99.2776 Epoch: 150/500MAE=100.1024 MAE=99.0891 MAE=99.6099 MAE=99.7475 MAE=98.3353 MAE=99.5256 MAE=99.4324 MAE=98.6635 MAE=98.2265 MAE=98.1731 Epoch: 160/500MAE=97.9385 MAE=98.0746 MAE=98.2670 MAE=98.5399 MAE=97.5468 MAE=97.7726 MAE=96.6863 MAE=96.5091 MAE=96.5583 MAE=96.8495 Epoch: 170/500MAE=95.9427 MAE=96.8644 MAE=96.1283 MAE=96.0260 MAE=96.9218 MAE=95.7980 MAE=95.9661 MAE=96.3281 MAE=95.6781 MAE=95.2177 Epoch: 180/500MAE=94.8899 MAE=95.3804 MAE=95.7693 MAE=95.5030 MAE=95.4365 MAE=95.1866 MAE=95.1071 MAE=94.9842 MAE=94.6145 MAE=94.7370 Epoch: 190/500MAE=94.9621 MAE=94.8449 MAE=94.6616 MAE=94.7204 MAE=94.6202 MAE=94.6092 MAE=94.7408 MAE=94.7289 MAE=94.5773 MAE=94.4380 Epoch: 200/500MAE=94.4571 MAE=94.4380 MAE=94.6149 MAE=94.7491 MAE=94.7635 MAE=94.6583 MAE=94.7081 MAE=94.7470 MAE=94.7486 MAE=94.6910 Epoch: 210/500MAE=94.7170 MAE=94.7337 MAE=94.6439 MAE=94.7072 MAE=94.5749 MAE=94.6969 MAE=94.7287 MAE=94.6944 MAE=94.7113 MAE=94.6741 Epoch: 220/500MAE=94.6634 MAE=94.6758 MAE=94.6746 MAE=94.6631 MAE=94.6389 MAE=94.6786 MAE=94.6458 MAE=94.6303 MAE=94.6299 MAE=94.6253 Epoch: 230/500MAE=94.6010 MAE=94.4687 MAE=94.6045 MAE=94.4692 MAE=94.5412 MAE=94.4018 MAE=94.4073 MAE=94.3974 MAE=94.4146 MAE=94.4423 Epoch: 240/500MAE=94.3825 MAE=94.4202 MAE=94.3406 MAE=94.3282 MAE=94.3471 MAE=94.3007 MAE=94.3065 MAE=94.3371 MAE=94.3329 MAE=94.3287 Epoch: 250/500MAE=94.3419 MAE=94.2717 MAE=94.2870 MAE=94.2126 MAE=94.1969 MAE=94.2691 MAE=94.2674 MAE=94.2388 MAE=94.2713 MAE=94.2734 Epoch: 260/500MAE=94.2776 MAE=94.2596 MAE=94.2197 MAE=94.1951 MAE=94.3439 MAE=94.2030 MAE=94.2328 MAE=94.1551 MAE=94.2710 MAE=94.1660 Epoch: 270/500MAE=94.1594 MAE=94.1547 MAE=94.1642 MAE=94.1791 MAE=94.1694 MAE=94.1544 MAE=94.2241 MAE=94.1147 MAE=94.0798 MAE=94.1275 Epoch: 280/500MAE=94.1335 MAE=94.1975 MAE=94.2401 MAE=94.1614 MAE=94.1508 MAE=94.1637 MAE=94.1330 MAE=94.1680 MAE=94.1210 MAE=94.1313 Epoch: 290/500MAE=94.1384 MAE=94.1570 MAE=94.1130 MAE=94.1050 MAE=94.1083 MAE=94.0924 MAE=94.0435 MAE=94.0620 MAE=94.0590 MAE=94.1517 Epoch: 300/500MAE=94.0960 MAE=94.0956 MAE=94.1309 MAE=94.1325 MAE=94.1796 MAE=94.1954 MAE=94.1240 MAE=94.1027 MAE=94.1196 MAE=94.1069 Epoch: 310/500MAE=94.0812 MAE=94.0840 MAE=94.0589 MAE=94.0677 MAE=94.0586 MAE=94.0267 MAE=94.0904 MAE=94.0764 MAE=94.0691 MAE=94.0608 Epoch: 320/500MAE=94.0736 MAE=94.0682 MAE=94.0582 MAE=94.0412 MAE=94.0574 MAE=94.0887 MAE=94.1289 MAE=94.0654 MAE=94.0850 MAE=94.0548 Epoch: 330/500MAE=94.0578 MAE=94.0581 MAE=94.0605 MAE=94.0236 MAE=94.0384 MAE=94.0269 MAE=94.0337 MAE=94.0465 MAE=94.0229 MAE=94.0251 Epoch: 340/500MAE=94.0283 MAE=94.0033 MAE=94.0333 MAE=94.0251 MAE=94.0446 MAE=94.0034 MAE=93.9903 MAE=94.0135 MAE=94.0015 MAE=94.0292 Epoch: 350/500MAE=94.0281 MAE=94.0110 MAE=93.9889 MAE=93.9888 MAE=93.9747 MAE=93.9604 MAE=93.9457 MAE=93.9773 MAE=93.9468 MAE=93.9497 Epoch: 360/500MAE=93.9557 MAE=93.9573 MAE=93.9488 MAE=93.9480 MAE=93.9502 MAE=93.8893 MAE=93.8842 MAE=93.9407 MAE=93.9386 MAE=93.9392 Epoch: 370/500MAE=93.9414 MAE=93.8869 MAE=93.8461 MAE=93.8549 MAE=93.8696 MAE=93.8814 MAE=93.8307 MAE=93.8166 MAE=93.8051 MAE=93.8595 Epoch: 380/500MAE=93.8386 MAE=93.8205 MAE=93.8883 MAE=93.8204 MAE=93.7995 MAE=93.7772 MAE=93.7719 MAE=93.7917 MAE=93.7940 MAE=93.7662 Epoch: 390/500MAE=93.6940 MAE=93.8319 MAE=93.8421 MAE=93.7057 MAE=93.7751 MAE=93.7541 MAE=93.7723 MAE=93.7436 MAE=93.7697 MAE=93.7279 Epoch: 400/500MAE=93.7409 MAE=93.7222 MAE=93.6988 MAE=93.6118 MAE=93.6217 MAE=93.7353 MAE=93.5890 MAE=93.5816 MAE=93.6954 MAE=93.6168 Epoch: 410/500MAE=93.6242 MAE=93.6735 MAE=93.5404 MAE=93.6926 MAE=93.6180 MAE=93.7347 MAE=93.5675 MAE=93.5924 MAE=93.5165 MAE=93.5550 Epoch: 420/500MAE=93.5295 MAE=93.5491 MAE=93.5353 MAE=93.5547 MAE=93.5826 MAE=93.5349 MAE=93.5176 MAE=93.5616 MAE=93.5950 MAE=93.5015 Epoch: 430/500MAE=93.5233 MAE=93.5248 MAE=93.5322 MAE=93.5115 MAE=93.4907 MAE=93.5071 MAE=93.5092 MAE=93.5029 MAE=93.4963 MAE=93.4392 Epoch: 440/500MAE=93.4764 MAE=93.4418 MAE=93.3637 MAE=93.3700 MAE=93.4690 MAE=93.4356 MAE=93.4110 MAE=93.3933 MAE=93.4117 MAE=93.4873 Epoch: 450/500MAE=93.4430 MAE=93.4469 MAE=93.4119 MAE=93.3626 MAE=93.3485 MAE=93.3629 MAE=93.3384 MAE=93.3394 MAE=93.3898 MAE=93.4133 Epoch: 460/500MAE=93.3474 MAE=93.3438 MAE=93.3651 MAE=93.4091 MAE=93.3370 MAE=93.3301 MAE=93.3501 MAE=93.3930 MAE=93.3719 MAE=93.3771 Epoch: 470/500MAE=93.3714 MAE=93.3945 MAE=93.3828 MAE=93.4125 MAE=93.3894 MAE=93.3528 MAE=93.3196 MAE=93.3626 MAE=93.3330 MAE=93.3466 Epoch: 480/500MAE=93.3445 MAE=93.3491 MAE=93.3180 MAE=93.3384 MAE=93.3376 MAE=93.3187 MAE=93.3190 MAE=93.3173 MAE=93.2344 MAE=93.2552 Epoch: 490/500MAE=93.2940 MAE=93.2780 MAE=93.2205 MAE=93.2531 MAE=93.2404 MAE=93.2202 MAE=93.2084 MAE=93.2165 MAE=93.1846 MAE=93.2076 Epoch: 500/500MAE=93.2024 MAE=94.6525 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 80.641 +/- 14.011\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.7273 MAE=1547.5793 MAE=1545.0872 MAE=1542.2565 MAE=1539.4207 MAE=1536.7971 MAE=1532.1760 MAE=1529.0781 MAE=1526.2974 Epoch: 10/500MAE=1520.6890 MAE=1515.3552 MAE=1509.8212 MAE=1504.8003 MAE=1498.6643 MAE=1491.2007 MAE=1483.9323 MAE=1477.8617 MAE=1465.8099 MAE=1458.8726 Epoch: 20/500MAE=1456.9976 MAE=1444.3840 MAE=1436.8965 MAE=1423.8723 MAE=1415.2416 MAE=1404.6205 MAE=1393.5210 MAE=1374.9419 MAE=1368.9596 MAE=1343.0848 Epoch: 30/500MAE=1336.2432 MAE=1330.0054 MAE=1308.1274 MAE=1301.1714 MAE=1286.5038 MAE=1273.8730 MAE=1258.3086 MAE=1239.1830 MAE=1218.3989 MAE=1202.9556 Epoch: 40/500MAE=1197.7251 MAE=1169.4189 MAE=1158.5592 MAE=1100.7838 MAE=1078.0972 MAE=1094.1907 MAE=1077.7290 MAE=1051.4415 MAE=1023.8961 MAE=1015.2845 Epoch: 50/500MAE=998.7816 MAE=984.5389 MAE=964.4750 MAE=944.9698 MAE=939.4269 MAE=911.7147 MAE=895.0861 MAE=893.6014 MAE=807.9163 MAE=792.7090 Epoch: 60/500MAE=760.2567 MAE=727.3869 MAE=732.8049 MAE=717.4686 MAE=639.9549 MAE=722.0294 MAE=611.0322 MAE=581.3555 MAE=612.5329 MAE=556.5497 Epoch: 70/500MAE=519.0750 MAE=463.1070 MAE=492.8721 MAE=445.4903 MAE=431.3599 MAE=365.5976 MAE=348.9431 MAE=353.7769 MAE=324.4582 MAE=326.2870 Epoch: 80/500MAE=297.3328 MAE=287.5621 MAE=293.8533 MAE=264.7827 MAE=225.9320 MAE=286.2050 MAE=225.8673 MAE=218.4283 MAE=165.4250 MAE=151.3359 Epoch: 90/500MAE=154.1978 MAE=165.0056 MAE=193.5014 MAE=130.5966 MAE=149.7976 MAE=141.6786 MAE=145.1195 MAE=147.6668 MAE=121.7239 MAE=124.6589 Epoch: 100/500MAE=132.4466 MAE=129.0568 MAE=124.4731 MAE=115.6717 MAE=115.4676 MAE=102.3261 MAE=105.3568 MAE=106.6079 MAE=106.9550 MAE=109.1992 Epoch: 110/500MAE=103.1156 MAE=100.8960 MAE=104.7111 MAE=101.1245 MAE=103.0540 MAE=99.0797 MAE=100.4819 MAE=98.7017 MAE=101.8411 MAE=97.6539 Epoch: 120/500MAE=98.3982 MAE=101.4373 MAE=97.1880 MAE=96.0257 MAE=97.5193 MAE=94.7899 MAE=94.4873 MAE=94.3717 MAE=94.6924 MAE=95.0801 Epoch: 130/500MAE=96.3973 MAE=93.0325 MAE=92.2681 MAE=88.1470 MAE=93.5866 MAE=92.5868 MAE=89.2112 MAE=89.0689 MAE=93.9622 MAE=89.7019 Epoch: 140/500MAE=90.7787 MAE=89.3459 MAE=89.4963 MAE=89.6962 MAE=90.2853 MAE=89.4943 MAE=89.5893 MAE=89.1964 MAE=88.9649 MAE=88.8433 Epoch: 150/500MAE=89.0371 MAE=88.9109 MAE=89.0324 MAE=88.7903 MAE=88.8319 MAE=88.8607 MAE=88.7911 MAE=88.8485 MAE=88.8887 MAE=88.7650 Epoch: 160/500MAE=88.7332 MAE=88.6272 MAE=88.6399 MAE=88.6370 MAE=88.6057 MAE=88.6882 MAE=88.7108 MAE=88.7378 MAE=88.6897 MAE=88.7282 Epoch: 170/500MAE=88.7398 MAE=88.7028 MAE=88.6947 MAE=88.7433 MAE=88.6962 MAE=88.7103 MAE=88.6974 MAE=88.6752 MAE=88.6292 MAE=88.6005 Epoch: 180/500MAE=88.5795 MAE=88.5765 MAE=88.6013 MAE=88.6080 MAE=88.6178 MAE=88.6338 MAE=88.6539 MAE=88.6386 MAE=88.6362 MAE=88.6230 Epoch: 190/500MAE=88.6803 MAE=88.7096 MAE=88.6891 MAE=88.6881 MAE=88.6731 MAE=88.6279 MAE=88.6522 MAE=88.6081 MAE=88.6309 MAE=88.6213 Epoch: 200/500MAE=88.6417 MAE=88.6507 MAE=88.6855 MAE=88.6555 MAE=88.6405 MAE=88.6512 MAE=88.6923 MAE=88.6674 MAE=88.6721 MAE=88.6176 Epoch: 210/500MAE=88.6261 MAE=88.6467 MAE=88.6488 MAE=88.6637 MAE=88.6277 MAE=88.6144 MAE=88.6160 MAE=88.6419 MAE=88.6642 MAE=88.6698 Epoch: 220/500MAE=88.6421 MAE=88.6226 MAE=88.6438 MAE=88.6444 MAE=88.6413 MAE=88.6214 MAE=88.6525 MAE=88.5909 MAE=88.6210 MAE=88.6105 Epoch: 230/500MAE=88.6320 MAE=88.5742 MAE=88.5850 MAE=88.5693 MAE=88.6237 MAE=88.6084 MAE=88.5926 MAE=88.6001 MAE=88.5509 MAE=88.5621 Epoch: 240/500MAE=88.5480 MAE=88.5683 MAE=88.5615 MAE=88.5645 MAE=88.5664 MAE=88.5261 MAE=88.5248 MAE=88.5650 MAE=88.5782 MAE=88.5637 Epoch: 250/500MAE=88.5291 MAE=88.5087 MAE=88.5415 MAE=88.5410 MAE=88.5337 MAE=88.4764 MAE=88.4544 MAE=88.4872 MAE=88.5032 MAE=88.4774 Epoch: 260/500MAE=88.4580 MAE=88.4896 MAE=88.5018 MAE=88.5107 MAE=88.4801 MAE=88.4766 MAE=88.4210 MAE=88.4106 MAE=88.3818 MAE=88.4122 Epoch: 270/500MAE=88.4133 MAE=88.4555 MAE=88.3866 MAE=88.3764 MAE=88.3745 MAE=88.3909 MAE=88.3650 MAE=88.3822 MAE=88.3659 MAE=88.3917 Epoch: 280/500MAE=88.3651 MAE=88.3622 MAE=88.3504 MAE=88.3220 MAE=89.4704 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 83.584 +/- 12.174\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.5249 MAE=1547.1813 MAE=1545.4553 MAE=1542.7906 MAE=1540.3379 MAE=1537.3889 MAE=1534.5763 MAE=1530.0162 MAE=1526.2595 Epoch: 10/500MAE=1523.2656 MAE=1514.2085 MAE=1509.3287 MAE=1503.0812 MAE=1497.6389 MAE=1496.6538 MAE=1485.6565 MAE=1476.1984 MAE=1471.5222 MAE=1461.1755 Epoch: 20/500MAE=1442.9468 MAE=1436.7388 MAE=1428.6129 MAE=1420.4288 MAE=1408.8506 MAE=1387.6229 MAE=1383.4689 MAE=1366.0562 MAE=1362.8414 MAE=1352.3655 Epoch: 30/500MAE=1331.1780 MAE=1331.1941 MAE=1316.0067 MAE=1254.2965 MAE=1278.2794 MAE=1262.1670 MAE=1262.4702 MAE=1238.8368 MAE=1210.2888 MAE=1194.3826 Epoch: 40/500MAE=1174.6494 MAE=1168.8252 MAE=1143.4926 MAE=1131.5851 MAE=1167.6838 MAE=1072.2249 MAE=1076.5981 MAE=1048.2009 MAE=1030.9897 MAE=1013.9110 Epoch: 50/500MAE=999.4956 MAE=1014.1711 MAE=955.1171 MAE=954.8408 MAE=940.7473 MAE=849.2891 MAE=842.0226 MAE=848.9733 MAE=830.4312 MAE=755.0272 Epoch: 60/500MAE=779.1465 MAE=727.4736 MAE=777.4695 MAE=711.7495 MAE=643.6179 MAE=642.4030 MAE=608.9442 MAE=621.1217 MAE=554.0974 MAE=516.2195 Epoch: 70/500MAE=511.9092 MAE=505.9788 MAE=471.4629 MAE=471.3685 MAE=436.5094 MAE=385.1020 MAE=389.2271 MAE=357.7952 MAE=325.8793 MAE=308.8564 Epoch: 80/500MAE=287.1235 MAE=268.2198 MAE=268.2059 MAE=275.4243 MAE=230.8458 MAE=203.8941 MAE=220.3022 MAE=216.4818 MAE=207.9586 MAE=190.2563 Epoch: 90/500MAE=165.0742 MAE=175.2453 MAE=150.2098 MAE=152.4683 MAE=130.8575 MAE=145.7819 MAE=131.7582 MAE=121.2268 MAE=123.7768 MAE=115.1136 Epoch: 100/500MAE=118.0369 MAE=133.8173 MAE=120.1594 MAE=113.6697 MAE=110.5254 MAE=118.7109 MAE=117.3825 MAE=107.0789 MAE=107.5215 MAE=106.1207 Epoch: 110/500MAE=112.8612 MAE=102.8910 MAE=115.3254 MAE=111.8201 MAE=123.1509 MAE=101.1033 MAE=106.4796 MAE=99.6892 MAE=101.9469 MAE=100.4087 Epoch: 120/500MAE=118.8115 MAE=93.8098 MAE=96.2934 MAE=102.7404 MAE=82.4438 MAE=90.1626 MAE=96.1104 MAE=81.2310 MAE=85.8848 MAE=97.6825 Epoch: 130/500MAE=83.9490 MAE=93.3969 MAE=92.4458 MAE=92.4371 MAE=83.7651 MAE=86.8229 MAE=77.0743 MAE=87.8005 MAE=78.0048 MAE=76.2168 Epoch: 140/500MAE=77.5168 MAE=75.6143 MAE=80.0551 MAE=79.2505 MAE=78.5410 MAE=75.4830 MAE=78.8650 MAE=83.4695 MAE=77.3802 MAE=82.4997 Epoch: 150/500MAE=76.5090 MAE=77.5611 MAE=77.2793 MAE=76.2956 MAE=76.9651 MAE=77.4154 MAE=75.8893 MAE=76.1734 MAE=75.8183 MAE=76.8358 Epoch: 160/500MAE=76.4483 MAE=75.5457 MAE=75.7849 MAE=76.2166 MAE=75.9128 MAE=75.9351 MAE=76.0514 MAE=75.9965 MAE=76.0740 MAE=76.0493 Epoch: 170/500MAE=76.0055 MAE=76.0861 MAE=76.0685 MAE=76.0290 MAE=76.1500 MAE=76.1041 MAE=76.1137 MAE=76.0589 MAE=76.0491 MAE=76.0182 Epoch: 180/500MAE=76.0750 MAE=75.9716 MAE=75.9585 MAE=75.9675 MAE=75.9709 MAE=75.9714 MAE=75.9482 MAE=75.9706 MAE=75.9250 MAE=75.9432 Epoch: 190/500MAE=75.9308 MAE=75.9435 MAE=75.9489 MAE=75.9501 MAE=75.9266 MAE=75.9281 MAE=75.9370 MAE=75.9302 MAE=75.9344 MAE=75.9405 Epoch: 200/500MAE=75.9406 MAE=75.9429 MAE=75.9550 MAE=75.9225 MAE=76.0032 MAE=75.9731 MAE=76.0135 MAE=75.9686 MAE=75.9520 MAE=75.9292 Epoch: 210/500MAE=75.9301 MAE=75.9474 MAE=75.9512 MAE=75.9420 MAE=75.9452 MAE=75.9502 MAE=75.9373 MAE=76.0197 MAE=76.0396 MAE=76.0363 Epoch: 220/500MAE=76.0064 MAE=76.0163 MAE=76.0280 MAE=75.9388 MAE=76.0319 MAE=76.0137 MAE=76.0183 MAE=76.0257 MAE=76.0208 MAE=75.9050 Epoch: 230/500MAE=75.9241 MAE=75.9408 MAE=75.9465 MAE=75.9448 MAE=75.9416 MAE=75.9487 MAE=75.9496 MAE=76.0359 MAE=76.0049 MAE=75.9404 Epoch: 240/500MAE=75.9207 MAE=75.9105 MAE=75.9260 MAE=75.8855 MAE=75.9413 MAE=75.9252 MAE=75.9259 MAE=75.8919 MAE=75.9039 MAE=75.9016 Epoch: 250/500MAE=75.9405 MAE=75.9349 MAE=75.8605 MAE=75.8691 MAE=75.8882 MAE=75.8721 MAE=75.8780 MAE=75.9317 MAE=75.9163 MAE=75.9381 Epoch: 260/500MAE=75.9093 MAE=75.8917 MAE=75.8653 MAE=75.8991 MAE=75.9125 MAE=75.8718 MAE=75.8753 MAE=75.8754 MAE=75.9117 MAE=75.8744 Epoch: 270/500MAE=75.8616 MAE=75.8972 MAE=75.8905 MAE=75.9063 MAE=75.8657 MAE=75.8883 MAE=75.8438 MAE=75.9110 MAE=76.0178 MAE=75.9551 Epoch: 280/500MAE=76.0046 MAE=75.9954 MAE=75.9675 MAE=75.9962 MAE=76.0063 MAE=75.9849 MAE=75.9391 MAE=75.8446 MAE=75.8156 MAE=75.7835 Epoch: 290/500MAE=75.9588 MAE=75.9918 MAE=75.9838 MAE=75.9905 MAE=76.0049 MAE=75.8822 MAE=78.5941 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 82.337 +/- 10.762\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.8690 MAE=1547.6345 MAE=1544.9200 MAE=1542.6450 MAE=1539.3406 MAE=1537.2209 MAE=1533.9528 MAE=1528.7273 MAE=1524.9495 Epoch: 10/500MAE=1517.4280 MAE=1510.9377 MAE=1511.2078 MAE=1501.6125 MAE=1497.7040 MAE=1491.3463 MAE=1480.8906 MAE=1470.9025 MAE=1464.8989 MAE=1455.9785 Epoch: 20/500MAE=1446.9714 MAE=1433.1559 MAE=1425.3250 MAE=1419.6733 MAE=1406.2324 MAE=1393.9954 MAE=1381.4415 MAE=1371.5106 MAE=1361.2614 MAE=1348.3400 Epoch: 30/500MAE=1336.4653 MAE=1321.6980 MAE=1309.9093 MAE=1293.3525 MAE=1279.1863 MAE=1265.7419 MAE=1253.5681 MAE=1236.3555 MAE=1224.3063 MAE=1203.4875 Epoch: 40/500MAE=1189.2134 MAE=1169.8677 MAE=1158.9963 MAE=1135.3987 MAE=1117.3440 MAE=1094.7301 MAE=1075.9054 MAE=1053.2954 MAE=1044.4066 MAE=1028.0098 Epoch: 50/500MAE=998.2966 MAE=965.0515 MAE=981.5043 MAE=919.1019 MAE=909.4557 MAE=891.7972 MAE=920.0308 MAE=867.5701 MAE=807.5475 MAE=779.0051 Epoch: 60/500MAE=741.3775 MAE=820.6746 MAE=739.6902 MAE=772.1962 MAE=682.3936 MAE=652.8897 MAE=599.3309 MAE=578.8781 MAE=559.2322 MAE=579.0080 Epoch: 70/500MAE=528.9571 MAE=517.1560 MAE=454.8173 MAE=520.3344 MAE=387.5186 MAE=377.1598 MAE=427.0767 MAE=346.3440 MAE=361.8661 MAE=306.0337 Epoch: 80/500MAE=305.9923 MAE=285.7696 MAE=310.2778 MAE=275.1705 MAE=222.7534 MAE=247.4856 MAE=202.8039 MAE=207.4744 MAE=184.3257 MAE=164.8135 Epoch: 90/500MAE=155.5078 MAE=161.6342 MAE=115.1448 MAE=117.4888 MAE=112.2142 MAE=151.2864 MAE=109.7410 MAE=104.3470 MAE=119.1460 MAE=106.0793 Epoch: 100/500MAE=110.2122 MAE=116.1028 MAE=104.0725 MAE=107.1071 MAE=103.8675 MAE=111.9002 MAE=103.4846 MAE=108.2384 MAE=119.2101 MAE=111.0450 Epoch: 110/500MAE=96.0850 MAE=101.1445 MAE=96.8784 MAE=98.2270 MAE=100.3351 MAE=91.7403 MAE=92.3811 MAE=87.2522 MAE=86.5488 MAE=94.5387 Epoch: 120/500MAE=84.7883 MAE=90.3805 MAE=85.6604 MAE=91.9773 MAE=83.5863 MAE=83.1407 MAE=91.8569 MAE=85.1676 MAE=84.7180 MAE=92.6804 Epoch: 130/500MAE=81.3743 MAE=82.1060 MAE=89.0652 MAE=81.1519 MAE=83.7344 MAE=82.5938 MAE=85.7616 MAE=82.0902 MAE=82.4992 MAE=83.8956 Epoch: 140/500MAE=80.6466 MAE=82.2222 MAE=81.8013 MAE=81.7177 MAE=81.2162 MAE=80.0309 MAE=81.7700 MAE=82.6161 MAE=80.1259 MAE=80.1920 Epoch: 150/500MAE=80.8670 MAE=80.2425 MAE=80.6238 MAE=80.0333 MAE=80.0085 MAE=80.7806 MAE=80.2177 MAE=80.3094 MAE=80.0843 MAE=80.0062 Epoch: 160/500MAE=80.3631 MAE=80.3067 MAE=80.2466 MAE=80.2564 MAE=80.2498 MAE=80.2431 MAE=80.2767 MAE=80.2063 MAE=80.1976 MAE=80.2017 Epoch: 170/500MAE=80.2003 MAE=80.2353 MAE=80.3085 MAE=80.3164 MAE=80.2592 MAE=80.2958 MAE=80.2816 MAE=80.2812 MAE=80.3248 MAE=80.2775 Epoch: 180/500MAE=80.0686 MAE=80.1017 MAE=80.0329 MAE=80.0616 MAE=80.0570 MAE=80.0387 MAE=80.0696 MAE=80.0354 MAE=80.0044 MAE=80.0078 Epoch: 190/500MAE=80.0171 MAE=80.0472 MAE=80.0374 MAE=80.0359 MAE=80.0496 MAE=80.0581 MAE=80.0611 MAE=80.0557 MAE=80.0768 MAE=80.1262 Epoch: 200/500MAE=80.0633 MAE=80.0744 MAE=80.0690 MAE=80.0758 MAE=80.0579 MAE=80.0707 MAE=80.0629 MAE=80.0268 MAE=80.0321 MAE=79.9911 Epoch: 210/500MAE=80.0257 MAE=80.0654 MAE=80.0262 MAE=80.0224 MAE=80.0339 MAE=80.0066 MAE=80.0409 MAE=80.0338 MAE=80.0299 MAE=80.0193 Epoch: 220/500MAE=80.0025 MAE=80.0015 MAE=80.0094 MAE=79.9908 MAE=80.2587 MAE=79.9690 MAE=80.0209 MAE=80.0472 MAE=80.0379 MAE=79.9994 Epoch: 230/500MAE=80.0143 MAE=79.9999 MAE=79.9942 MAE=80.2365 MAE=80.2194 MAE=80.2265 MAE=79.9429 MAE=80.0053 MAE=79.9749 MAE=79.9884 Epoch: 240/500MAE=79.9732 MAE=79.9382 MAE=79.9731 MAE=79.9827 MAE=79.9516 MAE=79.9514 MAE=79.9618 MAE=79.9281 MAE=79.9417 MAE=79.9306 Epoch: 250/500MAE=79.9417 MAE=79.9723 MAE=79.9476 MAE=79.9553 MAE=79.9514 MAE=79.9757 MAE=79.9993 MAE=79.9997 MAE=79.9125 MAE=79.9498 Epoch: 260/500MAE=79.9809 MAE=79.9760 MAE=79.9607 MAE=79.9390 MAE=79.9639 MAE=79.9321 MAE=79.9215 MAE=79.9234 MAE=79.9279 MAE=79.9623 Epoch: 270/500MAE=79.9356 MAE=79.9262 MAE=79.9448 MAE=79.9416 MAE=79.9153 MAE=79.9344 MAE=79.9412 MAE=79.9639 MAE=79.9585 MAE=79.9587 Epoch: 280/500MAE=79.9520 MAE=79.9341 MAE=79.8884 MAE=79.9165 MAE=79.9130 MAE=79.9038 MAE=79.9328 MAE=79.9294 MAE=79.9001 MAE=79.8911 Epoch: 290/500MAE=79.9076 MAE=79.8800 MAE=79.8630 MAE=79.8785 MAE=79.9050 MAE=79.8931 MAE=79.8910 MAE=79.8859 MAE=79.9285 MAE=79.8914 Epoch: 300/500MAE=79.8804 MAE=79.8812 MAE=79.9160 MAE=79.9360 MAE=79.8747 MAE=79.9197 MAE=79.8994 MAE=79.8991 MAE=79.9038 MAE=79.8813 Epoch: 310/500MAE=79.8863 MAE=79.8719 MAE=79.8780 MAE=79.9010 MAE=79.8966 MAE=79.8927 MAE=79.8824 MAE=79.8787 MAE=79.8600 MAE=79.8582 Epoch: 320/500MAE=79.8804 MAE=79.9064 MAE=79.9239 MAE=79.8938 MAE=79.9105 MAE=79.9190 MAE=79.9100 MAE=79.8966 MAE=79.9006 MAE=79.8985 Epoch: 330/500MAE=79.9315 MAE=79.8971 MAE=79.9079 MAE=79.9170 MAE=79.8672 MAE=79.9101 MAE=79.9242 MAE=79.9114 MAE=79.8769 MAE=79.8982 Epoch: 340/500MAE=79.9108 MAE=79.9133 MAE=79.9089 MAE=79.9025 MAE=79.8963 MAE=79.9068 MAE=79.9196 MAE=79.8862 MAE=79.8856 MAE=79.8732 Epoch: 350/500MAE=79.8638 MAE=79.8567 MAE=79.8308 MAE=79.8462 MAE=80.0442 MAE=80.0355 MAE=80.0302 MAE=80.0168 MAE=80.0111 MAE=80.0216 Epoch: 360/500MAE=80.0403 MAE=80.0258 MAE=79.9878 MAE=79.9846 MAE=79.8027 MAE=79.8295 MAE=79.8642 MAE=79.9959 MAE=80.0013 MAE=79.8493 Epoch: 370/500MAE=79.8354 MAE=79.8327 MAE=79.8292 MAE=79.8341 MAE=79.9922 MAE=79.8453 MAE=80.0042 MAE=80.0720 MAE=80.0013 MAE=79.8476 Epoch: 380/500MAE=79.8184 MAE=79.9517 MAE=79.9846 MAE=79.9935 MAE=79.9919 MAE=79.9942 MAE=79.9964 MAE=79.9780 MAE=79.9668 MAE=79.9701 Epoch: 390/500MAE=79.9846 MAE=79.9897 MAE=79.9732 MAE=79.9696 MAE=80.0025 MAE=79.9950 MAE=79.9875 MAE=80.0081 MAE=79.9657 MAE=80.0070 Epoch: 400/500MAE=79.9965 MAE=79.9726 MAE=79.9935 MAE=79.9463 MAE=79.9577 MAE=79.9664 MAE=79.9875 MAE=79.9683 MAE=79.9745 MAE=79.9987 Epoch: 410/500MAE=79.9834 MAE=79.9658 MAE=79.9918 MAE=79.9776 MAE=79.9758 MAE=80.0011 MAE=80.0234 MAE=79.9585 MAE=79.9626 MAE=79.9854 Epoch: 420/500MAE=79.9641 MAE=79.9935 MAE=79.9666 MAE=79.9291 MAE=79.9472 MAE=79.9264 MAE=79.9157 MAE=79.9987 MAE=80.0034 MAE=79.9745 Epoch: 430/500MAE=79.9713 MAE=79.9552 MAE=79.9693 MAE=79.9402 MAE=79.9372 MAE=79.9588 MAE=79.9575 MAE=79.9744 MAE=79.9642 MAE=79.9662 Epoch: 440/500MAE=79.9422 MAE=79.9207 MAE=79.8958 MAE=79.9127 MAE=79.9067 MAE=79.9344 MAE=79.9445 MAE=79.9769 MAE=79.9371 MAE=79.9192 Epoch: 450/500MAE=79.9335 MAE=79.9070 MAE=79.9037 MAE=79.9285 MAE=79.9546 MAE=79.9197 MAE=79.9288 MAE=79.9204 MAE=79.9269 MAE=79.9768 Epoch: 460/500MAE=79.9104 MAE=79.9044 MAE=79.9213 MAE=79.9032 MAE=79.8773 MAE=79.8757 MAE=79.8852 MAE=79.9541 MAE=79.9323 MAE=79.9117 Epoch: 470/500MAE=79.8687 MAE=79.9041 MAE=79.9029 MAE=79.9280 MAE=79.9436 MAE=79.9321 MAE=79.9302 MAE=79.9305 MAE=79.9243 MAE=79.9200 Epoch: 480/500MAE=79.9103 MAE=79.9508 MAE=79.9505 MAE=80.0068 MAE=79.9784 MAE=79.9518 MAE=79.9461 MAE=79.9167 MAE=79.8537 MAE=79.8743 Epoch: 490/500MAE=79.8757 MAE=79.8789 MAE=79.8718 MAE=79.8942 MAE=79.8709 MAE=79.9103 MAE=79.9210 MAE=79.9032 MAE=79.9288 MAE=79.9262 Epoch: 500/500MAE=79.9228 MAE=82.2234 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 82.314 +/- 9.626\n",
                        "\n",
                        "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/500MAE=1549.9161 MAE=1547.0991 MAE=1543.9565 MAE=1540.7286 MAE=1538.5085 MAE=1534.1326 MAE=1531.6211 MAE=1528.1670 MAE=1523.9050 Epoch: 10/500MAE=1517.9312 MAE=1511.6705 MAE=1507.2329 MAE=1501.1230 MAE=1496.4453 MAE=1487.5415 MAE=1483.1503 MAE=1478.9827 MAE=1474.1658 MAE=1461.6267 Epoch: 20/500MAE=1461.4922 MAE=1437.1372 MAE=1428.5420 MAE=1426.6104 MAE=1404.9856 MAE=1405.0156 MAE=1398.6406 MAE=1387.4485 MAE=1367.2593 MAE=1361.0176 Epoch: 30/500MAE=1349.4694 MAE=1347.0398 MAE=1328.5088 MAE=1304.2271 MAE=1295.4059 MAE=1283.0740 MAE=1275.9510 MAE=1261.3325 MAE=1227.3643 MAE=1224.0559 Epoch: 40/500MAE=1207.5486 MAE=1167.9458 MAE=1175.4143 MAE=1152.9712 MAE=1138.0696 MAE=1100.3359 MAE=1098.9692 MAE=1075.6820 MAE=1053.3954 MAE=1036.5630 Epoch: 50/500MAE=1032.3535 MAE=989.4646 MAE=974.0819 MAE=957.9830 MAE=923.4811 MAE=908.1798 MAE=879.8336 MAE=857.8600 MAE=825.7619 MAE=820.6268 Epoch: 60/500MAE=821.4022 MAE=765.5957 MAE=739.6974 MAE=718.7535 MAE=689.0894 MAE=666.7325 MAE=649.5622 MAE=621.2678 MAE=602.7701 MAE=587.4702 Epoch: 70/500MAE=553.8168 MAE=544.8854 MAE=483.0326 MAE=441.3613 MAE=436.3563 MAE=442.7169 MAE=492.3282 MAE=331.6058 MAE=497.6862 MAE=365.6605 Epoch: 80/500MAE=347.3990 MAE=318.6158 MAE=291.0302 MAE=342.6341 MAE=225.3162 MAE=329.8345 MAE=210.3759 MAE=145.6845 MAE=225.7187 MAE=187.9038 Epoch: 90/500MAE=130.1496 MAE=116.6291 MAE=124.4812 MAE=114.7026 MAE=120.2702 MAE=102.2703 MAE=104.5791 MAE=103.9603 MAE=98.4478 MAE=90.4782 Epoch: 100/500MAE=94.5238 MAE=93.6841 MAE=91.4861 MAE=95.2998 MAE=83.3539 MAE=81.3118 MAE=85.1915 MAE=81.1576 MAE=81.5680 MAE=81.8068 Epoch: 110/500MAE=82.1508 MAE=83.2315 MAE=80.5461 MAE=79.5243 MAE=79.8114 MAE=79.1713 MAE=79.8066 MAE=79.5594 MAE=79.2701 MAE=80.2919 Epoch: 120/500MAE=80.7578 MAE=80.1611 MAE=78.9449 MAE=79.6259 MAE=81.9637 MAE=79.5346 MAE=81.0046 MAE=80.2616 MAE=79.9422 MAE=80.6380 Epoch: 130/500MAE=80.2469 MAE=80.3817 MAE=80.3020 MAE=80.4126 MAE=80.6671 MAE=81.0421 MAE=80.7755 MAE=80.9054 MAE=80.5556 MAE=79.9397 Epoch: 140/500MAE=79.7878 MAE=80.1632 MAE=79.9336 MAE=80.0714 MAE=80.0536 MAE=80.1662 MAE=80.3817 MAE=80.5823 MAE=80.3299 MAE=80.2545 Epoch: 150/500MAE=80.3571 MAE=80.5976 MAE=80.6106 MAE=80.3674 MAE=80.5497 MAE=80.2526 MAE=80.2318 MAE=80.2290 MAE=80.2271 MAE=80.3832 Epoch: 160/500MAE=80.3521 MAE=80.4347 MAE=80.4129 MAE=80.3896 MAE=80.3860 MAE=80.3920 MAE=80.3942 MAE=80.4153 MAE=80.4308 MAE=80.5200 Epoch: 170/500MAE=80.5694 MAE=80.5572 MAE=80.5028 MAE=80.5238 MAE=80.3628 MAE=80.3693 MAE=80.4664 MAE=80.4218 MAE=80.5143 MAE=80.4749 Epoch: 180/500MAE=80.5482 MAE=80.4542 MAE=80.5953 MAE=80.4520 MAE=80.4438 MAE=80.6158 MAE=80.6122 MAE=80.7517 MAE=80.6066 MAE=80.6988 Epoch: 190/500MAE=80.7458 MAE=80.7183 MAE=80.7074 MAE=80.6526 MAE=80.7205 MAE=80.5369 MAE=80.6364 MAE=80.5995 MAE=80.5922 MAE=80.5166 Epoch: 200/500MAE=80.6968 MAE=80.7010 MAE=80.6828 MAE=80.4759 MAE=80.4732 MAE=80.4701 MAE=80.4246 MAE=80.4265 MAE=80.4134 MAE=80.4032 Epoch: 210/500MAE=80.3914 MAE=80.4217 MAE=80.5542 MAE=80.5807 MAE=80.4752 MAE=80.4992 MAE=80.3904 MAE=80.4448 MAE=80.4583 MAE=80.4435 Epoch: 220/500MAE=80.4328 MAE=80.4374 MAE=80.4576 MAE=80.4502 MAE=80.3857 MAE=80.4321 MAE=80.4478 MAE=80.4451 MAE=80.3937 MAE=80.3844 Epoch: 230/500MAE=80.3182 MAE=80.3198 MAE=80.4078 MAE=80.3063 MAE=80.3122 MAE=80.3165 MAE=80.2965 MAE=80.3048 MAE=80.3314 MAE=80.3428 Epoch: 240/500MAE=80.3281 MAE=80.3225 MAE=80.3105 MAE=80.2806 MAE=80.3197 MAE=80.3341 MAE=80.3118 MAE=80.3221 MAE=80.3100 MAE=80.3209 Epoch: 250/500MAE=80.3422 MAE=80.3372 MAE=80.3566 MAE=80.3236 MAE=80.3297 MAE=80.3566 MAE=80.3499 MAE=80.3527 MAE=80.4725 MAE=80.3444 Epoch: 260/500MAE=80.3390 MAE=80.3760 MAE=80.3418 MAE=80.3058 MAE=80.3556 MAE=80.5013 MAE=80.5227 MAE=80.5708 MAE=80.5173 MAE=80.5216 Epoch: 270/500MAE=80.4749 MAE=80.4752 MAE=80.5886 MAE=82.8109 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 82.811 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.9138 MAE=1547.6730 MAE=1545.3362 MAE=1542.6106 MAE=1539.5925 MAE=1536.6847 MAE=1534.1993 MAE=1531.4297 MAE=1526.8134 Epoch: 10/500MAE=1520.5410 MAE=1516.4785 MAE=1509.9590 MAE=1505.6210 MAE=1495.9392 MAE=1491.3843 MAE=1485.1548 MAE=1478.7528 MAE=1464.7648 MAE=1460.0194 Epoch: 20/500MAE=1448.6329 MAE=1446.2937 MAE=1440.5913 MAE=1423.1302 MAE=1414.6466 MAE=1407.8362 MAE=1385.5228 MAE=1372.1566 MAE=1359.9717 MAE=1349.6794 Epoch: 30/500MAE=1338.5779 MAE=1317.0264 MAE=1297.8606 MAE=1304.0983 MAE=1272.9202 MAE=1265.7139 MAE=1233.0110 MAE=1226.4016 MAE=1213.0282 MAE=1219.0050 Epoch: 40/500MAE=1195.2209 MAE=1178.4714 MAE=1138.3920 MAE=1110.2712 MAE=1138.0043 MAE=1084.6584 MAE=1061.9878 MAE=1044.0526 MAE=1031.4128 MAE=1013.1166 Epoch: 50/500MAE=1001.3982 MAE=986.0645 MAE=971.5060 MAE=966.4999 MAE=910.1183 MAE=900.7592 MAE=824.5286 MAE=821.0963 MAE=802.1995 MAE=798.5710 Epoch: 60/500MAE=754.5663 MAE=742.9598 MAE=698.4871 MAE=688.5839 MAE=671.3487 MAE=699.6688 MAE=611.1411 MAE=548.4544 MAE=541.8572 MAE=558.0094 Epoch: 70/500MAE=519.2688 MAE=470.1903 MAE=446.4542 MAE=446.9640 MAE=396.4722 MAE=417.2202 MAE=412.9189 MAE=362.3596 MAE=353.6781 MAE=342.4265 Epoch: 80/500MAE=303.0635 MAE=327.6237 MAE=235.3238 MAE=251.9706 MAE=178.9161 MAE=308.2051 MAE=238.2386 MAE=240.9291 MAE=154.4464 MAE=160.2767 Epoch: 90/500MAE=177.7197 MAE=148.1289 MAE=162.6326 MAE=136.6820 MAE=123.9932 MAE=115.5452 MAE=115.2310 MAE=116.9157 MAE=110.9614 MAE=114.3375 Epoch: 100/500MAE=101.2194 MAE=110.0666 MAE=102.6982 MAE=101.7527 MAE=99.0939 MAE=95.8655 MAE=94.5771 MAE=102.9596 MAE=90.8836 MAE=89.7784 Epoch: 110/500MAE=95.0438 MAE=98.1963 MAE=89.2091 MAE=87.1590 MAE=91.4486 MAE=84.7103 MAE=87.9286 MAE=85.1768 MAE=85.9364 MAE=84.2680 Epoch: 120/500MAE=86.4003 MAE=95.8862 MAE=87.8839 MAE=86.3975 MAE=81.1046 MAE=81.3249 MAE=83.0821 MAE=83.0810 MAE=81.1002 MAE=81.9104 Epoch: 130/500MAE=80.2924 MAE=83.0673 MAE=82.8573 MAE=82.8641 MAE=82.4713 MAE=82.6622 MAE=83.3966 MAE=82.7772 MAE=82.9782 MAE=83.5817 Epoch: 140/500MAE=82.6577 MAE=82.2607 MAE=81.7956 MAE=81.6892 MAE=82.0387 MAE=81.9408 MAE=81.8596 MAE=82.0843 MAE=81.9266 MAE=81.9036 Epoch: 150/500MAE=82.0062 MAE=82.1925 MAE=81.9351 MAE=82.0155 MAE=82.0648 MAE=82.0717 MAE=82.1213 MAE=82.1078 MAE=82.1393 MAE=82.1289 Epoch: 160/500MAE=82.1470 MAE=82.1318 MAE=82.1118 MAE=82.1071 MAE=82.0979 MAE=82.1061 MAE=82.0986 MAE=82.0954 MAE=82.0874 MAE=82.0801 Epoch: 170/500MAE=82.0888 MAE=82.1134 MAE=82.1228 MAE=82.1413 MAE=82.0407 MAE=82.0434 MAE=82.1489 MAE=82.0663 MAE=82.0574 MAE=82.0300 Epoch: 180/500MAE=82.0492 MAE=82.0614 MAE=82.0623 MAE=82.0602 MAE=82.0590 MAE=82.0467 MAE=82.1470 MAE=82.1762 MAE=82.1901 MAE=82.2079 Epoch: 190/500MAE=82.1631 MAE=82.1432 MAE=82.1249 MAE=82.1563 MAE=82.1976 MAE=82.1880 MAE=82.1873 MAE=82.1982 MAE=82.1975 MAE=82.1860 Epoch: 200/500MAE=82.1922 MAE=82.1947 MAE=82.1859 MAE=82.1794 MAE=82.1878 MAE=82.1988 MAE=82.2038 MAE=82.1949 MAE=82.2269 MAE=82.2290 Epoch: 210/500MAE=82.1927 MAE=82.1814 MAE=82.1884 MAE=82.1405 MAE=82.1859 MAE=82.1414 MAE=82.1574 MAE=82.1924 MAE=82.1969 MAE=82.2029 Epoch: 220/500MAE=82.1475 MAE=82.1916 MAE=82.1821 MAE=82.1854 MAE=82.1695 MAE=82.1827 MAE=82.1857 MAE=82.1569 MAE=82.1620 MAE=82.1688 Epoch: 230/500MAE=82.1592 MAE=82.1404 MAE=82.1143 MAE=82.1288 MAE=82.1460 MAE=82.0567 MAE=82.1647 MAE=82.1737 MAE=82.1624 MAE=82.1350 Epoch: 240/500MAE=82.1819 MAE=82.1743 MAE=82.1785 MAE=82.1656 MAE=82.1702 MAE=82.1688 MAE=82.1682 MAE=82.1737 MAE=82.1592 MAE=82.1633 Epoch: 250/500MAE=82.1928 MAE=82.2037 MAE=82.1562 MAE=82.1939 MAE=82.2130 MAE=82.1873 MAE=82.2078 MAE=82.2034 MAE=82.2116 MAE=82.1577 Epoch: 260/500MAE=82.1735 MAE=82.1821 MAE=82.1780 MAE=82.2150 MAE=82.2294 MAE=82.2255 MAE=82.2232 MAE=82.2299 MAE=82.1881 MAE=82.1858 Epoch: 270/500MAE=82.2205 MAE=82.1797 MAE=82.1564 MAE=82.1940 MAE=82.1878 MAE=82.1937 MAE=82.1881 MAE=82.1905 MAE=82.1905 MAE=82.1849 Epoch: 280/500MAE=82.1905 MAE=82.8386 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 82.825 +/- 0.014\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.8594 MAE=1547.5378 MAE=1544.8203 MAE=1542.4492 MAE=1539.9792 MAE=1536.7605 MAE=1533.9580 MAE=1529.8888 MAE=1526.2506 Epoch: 10/500MAE=1521.8251 MAE=1517.0663 MAE=1510.3939 MAE=1501.3257 MAE=1496.9215 MAE=1486.4181 MAE=1480.8215 MAE=1477.9146 MAE=1463.6714 MAE=1465.1909 Epoch: 20/500MAE=1457.3999 MAE=1448.4031 MAE=1434.0728 MAE=1430.8405 MAE=1406.3469 MAE=1398.4258 MAE=1400.3961 MAE=1366.0319 MAE=1355.1294 MAE=1361.9937 Epoch: 30/500MAE=1340.1886 MAE=1309.5376 MAE=1305.8398 MAE=1299.2012 MAE=1286.2229 MAE=1268.6750 MAE=1249.3008 MAE=1269.8606 MAE=1220.3735 MAE=1211.7096 Epoch: 40/500MAE=1171.7289 MAE=1166.7219 MAE=1161.3206 MAE=1127.4487 MAE=1113.4519 MAE=1096.2629 MAE=1074.2424 MAE=1053.2738 MAE=1035.4447 MAE=1012.2197 Epoch: 50/500MAE=1008.3466 MAE=986.8368 MAE=947.2940 MAE=915.9584 MAE=897.2664 MAE=887.7975 MAE=935.3937 MAE=852.6014 MAE=811.8328 MAE=782.2675 Epoch: 60/500MAE=769.5995 MAE=740.8550 MAE=726.4548 MAE=694.4238 MAE=662.6566 MAE=667.3304 MAE=622.9464 MAE=579.4530 MAE=562.3798 MAE=681.5110 Epoch: 70/500MAE=546.9641 MAE=483.0456 MAE=496.5893 MAE=408.6727 MAE=397.8829 MAE=495.5131 MAE=396.9525 MAE=340.8635 MAE=340.2984 MAE=282.4859 Epoch: 80/500MAE=252.2565 MAE=298.0246 MAE=267.2032 MAE=289.2012 MAE=213.4637 MAE=205.6986 MAE=185.8712 MAE=213.8361 MAE=184.7272 MAE=161.5472 Epoch: 90/500MAE=138.1478 MAE=124.2411 MAE=128.0668 MAE=138.1510 MAE=150.2453 MAE=116.8979 MAE=115.5514 MAE=161.5240 MAE=119.3266 MAE=111.0153 Epoch: 100/500MAE=100.1961 MAE=101.5428 MAE=92.2793 MAE=103.5445 MAE=94.2200 MAE=102.6952 MAE=95.4078 MAE=124.5090 MAE=93.4423 MAE=95.0051 Epoch: 110/500MAE=93.8791 MAE=89.5633 MAE=89.2127 MAE=87.3157 MAE=85.4665 MAE=89.8716 MAE=85.5244 MAE=88.2907 MAE=85.8565 MAE=85.9874 Epoch: 120/500MAE=85.6451 MAE=86.5816 MAE=85.5710 MAE=85.5186 MAE=85.3347 MAE=85.4784 MAE=85.5667 MAE=85.8072 MAE=85.3624 MAE=84.7517 Epoch: 130/500MAE=84.6780 MAE=84.7403 MAE=84.5405 MAE=84.6023 MAE=84.6008 MAE=84.4177 MAE=84.4031 MAE=84.5177 MAE=84.7739 MAE=84.2681 Epoch: 140/500MAE=84.5170 MAE=84.5522 MAE=84.3274 MAE=83.8036 MAE=84.6676 MAE=84.5127 MAE=84.4312 MAE=84.3553 MAE=84.3058 MAE=84.6987 Epoch: 150/500MAE=84.4576 MAE=84.3883 MAE=84.2872 MAE=84.3889 MAE=84.3508 MAE=84.3316 MAE=84.3094 MAE=84.4432 MAE=84.3909 MAE=84.2408 Epoch: 160/500MAE=84.2891 MAE=84.2900 MAE=84.2883 MAE=84.4179 MAE=84.4243 MAE=84.4554 MAE=84.4536 MAE=84.4587 MAE=84.2792 MAE=84.4298 Epoch: 170/500MAE=84.4259 MAE=84.2646 MAE=84.2666 MAE=84.2508 MAE=84.2706 MAE=84.2614 MAE=84.2559 MAE=84.2429 MAE=84.2517 MAE=84.2672 Epoch: 180/500MAE=84.2546 MAE=84.2478 MAE=84.2415 MAE=84.2418 MAE=84.2397 MAE=84.2393 MAE=84.2356 MAE=84.2240 MAE=84.2802 MAE=84.2211 Epoch: 190/500MAE=84.2088 MAE=84.1928 MAE=84.2053 MAE=84.2295 MAE=84.2265 MAE=84.2002 MAE=84.2201 MAE=84.2270 MAE=84.2095 MAE=84.1852 Epoch: 200/500MAE=84.1906 MAE=84.2079 MAE=84.1995 MAE=84.2023 MAE=84.2362 MAE=84.1941 MAE=84.2095 MAE=84.1849 MAE=84.1944 MAE=84.2770 Epoch: 210/500MAE=84.2523 MAE=84.2481 MAE=84.2488 MAE=84.2738 MAE=84.2535 MAE=84.1547 MAE=84.2762 MAE=84.1667 MAE=84.2674 MAE=84.2687 Epoch: 220/500MAE=84.1865 MAE=84.2637 MAE=84.2590 MAE=84.2723 MAE=84.2771 MAE=84.2766 MAE=84.2807 MAE=84.2628 MAE=84.1952 MAE=84.2319 Epoch: 230/500MAE=84.2263 MAE=84.2930 MAE=84.2731 MAE=84.2866 MAE=84.2120 MAE=84.1022 MAE=84.1245 MAE=84.1330 MAE=84.1418 MAE=84.0719 Epoch: 240/500MAE=84.2183 MAE=84.1319 MAE=84.1824 MAE=84.1254 MAE=84.1934 MAE=84.2285 MAE=84.0441 MAE=84.2300 MAE=84.2425 MAE=84.2221 Epoch: 250/500MAE=84.2946 MAE=84.2216 MAE=84.2386 MAE=84.2457 MAE=84.2265 MAE=84.1535 MAE=84.1754 MAE=84.1779 MAE=84.2101 MAE=84.2100 Epoch: 260/500MAE=84.2142 MAE=84.2223 MAE=84.2224 MAE=84.2185 MAE=84.1503 MAE=84.1366 MAE=84.2166 MAE=84.2229 MAE=84.1505 MAE=84.2087 Epoch: 270/500MAE=84.1186 MAE=84.1132 MAE=84.1273 MAE=84.1367 MAE=84.1431 MAE=84.1447 MAE=84.1520 MAE=84.1600 MAE=84.1148 MAE=84.1266 Epoch: 280/500MAE=84.1294 MAE=84.1111 MAE=84.1175 MAE=84.1347 MAE=84.1291 MAE=84.1033 MAE=84.1183 MAE=84.1428 MAE=84.1342 MAE=84.1129 Epoch: 290/500MAE=84.0863 MAE=84.1003 MAE=84.0855 MAE=84.0833 MAE=83.9758 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 83.208 +/- 0.543\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.8888 MAE=1547.5981 MAE=1545.0769 MAE=1543.8182 MAE=1540.7417 MAE=1537.4528 MAE=1534.5168 MAE=1528.4170 MAE=1524.3306 Epoch: 10/500MAE=1519.4045 MAE=1515.6370 MAE=1505.0823 MAE=1509.1389 MAE=1498.3306 MAE=1488.0315 MAE=1481.4189 MAE=1469.1150 MAE=1465.2910 MAE=1451.8470 Epoch: 20/500MAE=1453.2976 MAE=1440.8679 MAE=1424.7419 MAE=1419.1353 MAE=1415.0911 MAE=1408.5422 MAE=1394.5985 MAE=1373.6035 MAE=1354.9568 MAE=1345.2515 Epoch: 30/500MAE=1333.0103 MAE=1328.1168 MAE=1320.4293 MAE=1307.1152 MAE=1286.2124 MAE=1273.4250 MAE=1275.8268 MAE=1251.5131 MAE=1233.5682 MAE=1197.1069 Epoch: 40/500MAE=1178.3442 MAE=1161.9941 MAE=1163.9686 MAE=1206.2446 MAE=1125.6997 MAE=1142.7305 MAE=1076.9432 MAE=1073.4294 MAE=1012.5565 MAE=1084.8628 Epoch: 50/500MAE=1017.9677 MAE=1052.6958 MAE=970.1143 MAE=968.9770 MAE=906.7998 MAE=917.2521 MAE=910.1334 MAE=878.3160 MAE=938.3536 MAE=804.4381 Epoch: 60/500MAE=831.7623 MAE=802.5477 MAE=784.5052 MAE=788.9172 MAE=750.1478 MAE=636.7264 MAE=676.9094 MAE=611.0427 MAE=592.7949 MAE=589.8043 Epoch: 70/500MAE=523.2520 MAE=559.0912 MAE=536.9977 MAE=467.6238 MAE=441.9021 MAE=436.1255 MAE=407.6887 MAE=382.1493 MAE=372.9893 MAE=422.1551 Epoch: 80/500MAE=440.4285 MAE=295.5290 MAE=263.9201 MAE=285.2258 MAE=229.4311 MAE=230.6009 MAE=208.4121 MAE=240.3033 MAE=162.2898 MAE=221.1856 Epoch: 90/500MAE=132.7429 MAE=148.5817 MAE=115.7113 MAE=108.3408 MAE=103.7037 MAE=141.4475 MAE=104.5674 MAE=130.9299 MAE=86.7310 MAE=114.4538 Epoch: 100/500MAE=91.9220 MAE=101.3123 MAE=87.5270 MAE=81.6159 MAE=76.9644 MAE=75.5276 MAE=80.6344 MAE=72.0984 MAE=74.3296 MAE=72.1323 Epoch: 110/500MAE=78.4685 MAE=74.8255 MAE=73.4785 MAE=69.8288 MAE=68.5291 MAE=73.3744 MAE=73.0774 MAE=67.7444 MAE=68.0732 MAE=74.5996 Epoch: 120/500MAE=66.7143 MAE=66.7219 MAE=70.2707 MAE=71.7996 MAE=69.9759 MAE=66.5988 MAE=66.7238 MAE=68.3527 MAE=66.3585 MAE=68.1908 Epoch: 130/500MAE=66.8518 MAE=67.3899 MAE=65.8838 MAE=67.5058 MAE=66.2661 MAE=67.1729 MAE=66.1413 MAE=66.0218 MAE=67.2825 MAE=66.2527 Epoch: 140/500MAE=66.3023 MAE=66.9690 MAE=66.0112 MAE=65.9312 MAE=65.9924 MAE=65.6331 MAE=65.9195 MAE=65.8355 MAE=65.8222 MAE=65.6073 Epoch: 150/500MAE=65.6898 MAE=65.6084 MAE=66.2035 MAE=65.9218 MAE=65.8758 MAE=65.7255 MAE=65.6657 MAE=65.9262 MAE=65.9051 MAE=65.8852 Epoch: 160/500MAE=65.7414 MAE=65.7625 MAE=65.9447 MAE=66.0137 MAE=65.6799 MAE=65.7577 MAE=65.7435 MAE=65.6998 MAE=65.7165 MAE=65.7096 Epoch: 170/500MAE=65.7279 MAE=65.8533 MAE=65.8595 MAE=65.8593 MAE=65.8841 MAE=65.8506 MAE=65.8654 MAE=65.8552 MAE=65.8707 MAE=65.7391 Epoch: 180/500MAE=65.7422 MAE=65.7717 MAE=65.8792 MAE=65.7806 MAE=65.7417 MAE=65.7443 MAE=65.7259 MAE=65.8819 MAE=65.7400 MAE=65.9798 Epoch: 190/500MAE=65.8820 MAE=66.0946 MAE=65.8736 MAE=65.8752 MAE=65.9453 MAE=66.0927 MAE=66.0814 MAE=66.0931 MAE=65.7196 MAE=65.8679 Epoch: 200/500MAE=65.8696 MAE=65.8629 MAE=65.8682 MAE=65.8676 MAE=65.8870 MAE=65.7169 MAE=65.8567 MAE=65.8580 MAE=65.8520 MAE=65.8695 Epoch: 210/500MAE=65.8827 MAE=65.8674 MAE=65.8890 MAE=65.8768 MAE=65.8745 MAE=65.8785 MAE=65.9193 MAE=66.0508 MAE=66.0759 MAE=66.0887 Epoch: 220/500MAE=66.0626 MAE=66.0611 MAE=66.0512 MAE=66.0354 MAE=66.0449 MAE=66.0747 MAE=66.0739 MAE=66.0498 MAE=66.0480 MAE=66.0640 Epoch: 230/500MAE=66.0840 MAE=66.0589 MAE=65.8804 MAE=65.7571 MAE=66.0585 MAE=66.0581 MAE=66.0838 MAE=66.0657 MAE=65.8387 MAE=65.8345 Epoch: 240/500MAE=65.8663 MAE=66.0461 MAE=65.8778 MAE=66.1319 MAE=66.0628 MAE=66.0791 MAE=66.0500 MAE=66.0784 MAE=66.0879 MAE=66.0743 Epoch: 250/500MAE=66.1272 MAE=66.0397 MAE=66.1320 MAE=65.9961 MAE=65.9842 MAE=65.9586 MAE=65.9776 MAE=65.9691 MAE=65.9460 MAE=65.9695 Epoch: 260/500MAE=65.9519 MAE=65.9590 MAE=65.9626 MAE=66.1337 MAE=66.1155 MAE=65.9896 MAE=66.1064 MAE=66.0163 MAE=65.9548 MAE=65.9776 Epoch: 270/500MAE=65.9599 MAE=65.9397 MAE=65.9528 MAE=65.9463 MAE=65.9615 MAE=65.9690 MAE=65.9629 MAE=65.9431 MAE=65.9406 MAE=66.0795 Epoch: 280/500MAE=66.0961 MAE=66.1007 MAE=65.9623 MAE=66.1060 MAE=66.0795 MAE=65.9606 MAE=66.0694 MAE=66.0894 MAE=66.0592 MAE=66.0897 Epoch: 290/500MAE=66.1364 MAE=66.1112 MAE=66.1175 MAE=66.1011 MAE=66.1026 MAE=66.0963 MAE=66.0524 MAE=66.0851 MAE=66.1055 MAE=66.1213 MAE=69.1655 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 79.698 +/- 6.099\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.8167 MAE=1547.6698 MAE=1544.8757 MAE=1542.3552 MAE=1539.7152 MAE=1535.7440 MAE=1533.7266 MAE=1529.6038 MAE=1526.3030 Epoch: 10/500MAE=1521.9995 MAE=1513.2007 MAE=1510.7661 MAE=1501.0232 MAE=1493.0573 MAE=1485.1869 MAE=1486.2109 MAE=1472.4893 MAE=1463.8213 MAE=1460.0945 Epoch: 20/500MAE=1460.2928 MAE=1435.9376 MAE=1426.4148 MAE=1421.2058 MAE=1402.8413 MAE=1409.8779 MAE=1381.8022 MAE=1377.4518 MAE=1370.0621 MAE=1360.1151 Epoch: 30/500MAE=1351.2330 MAE=1318.6882 MAE=1309.7709 MAE=1298.6611 MAE=1328.9663 MAE=1272.3552 MAE=1261.7437 MAE=1246.9983 MAE=1216.2135 MAE=1201.0095 Epoch: 40/500MAE=1166.5911 MAE=1180.6199 MAE=1120.9885 MAE=1121.3531 MAE=1121.0063 MAE=1059.0444 MAE=1060.7306 MAE=1066.5999 MAE=1073.3068 MAE=1049.8506 Epoch: 50/500MAE=1035.4895 MAE=990.8153 MAE=976.1021 MAE=952.4250 MAE=941.1984 MAE=949.7784 MAE=867.5718 MAE=863.7529 MAE=839.1575 MAE=861.2417 Epoch: 60/500MAE=796.9467 MAE=761.3855 MAE=755.8882 MAE=726.7095 MAE=744.5466 MAE=701.8049 MAE=640.4294 MAE=634.0773 MAE=579.8931 MAE=573.7828 Epoch: 70/500MAE=547.9922 MAE=580.2266 MAE=462.3406 MAE=461.2916 MAE=446.9171 MAE=386.5254 MAE=425.3616 MAE=354.8081 MAE=400.6131 MAE=291.6527 Epoch: 80/500MAE=350.2787 MAE=347.3522 MAE=272.4286 MAE=234.0679 MAE=200.6170 MAE=231.2537 MAE=182.7713 MAE=242.9002 MAE=156.8571 MAE=160.1876 Epoch: 90/500MAE=142.3596 MAE=120.4268 MAE=151.4259 MAE=127.3746 MAE=134.1491 MAE=154.1937 MAE=100.6324 MAE=89.3637 MAE=83.9097 MAE=86.2878 Epoch: 100/500MAE=86.0090 MAE=86.6558 MAE=89.1494 MAE=80.9385 MAE=86.0966 MAE=81.0707 MAE=78.1200 MAE=83.7981 MAE=77.7869 MAE=78.6608 Epoch: 110/500MAE=78.2868 MAE=78.6352 MAE=80.5261 MAE=78.4735 MAE=80.2360 MAE=78.7660 MAE=78.0927 MAE=76.9422 MAE=80.0012 MAE=78.9899 Epoch: 120/500MAE=78.1081 MAE=77.1892 MAE=77.5165 MAE=76.7343 MAE=77.0386 MAE=76.9415 MAE=76.7219 MAE=76.5241 MAE=77.3767 MAE=76.5392 Epoch: 130/500MAE=76.3350 MAE=76.9249 MAE=76.4947 MAE=77.2740 MAE=76.7888 MAE=75.3763 MAE=76.0789 MAE=76.4108 MAE=76.3879 MAE=76.1766 Epoch: 140/500MAE=76.5277 MAE=76.2501 MAE=76.3527 MAE=76.2378 MAE=76.0342 MAE=76.4990 MAE=76.1903 MAE=76.2556 MAE=76.0771 MAE=76.1158 Epoch: 150/500MAE=76.0761 MAE=76.2090 MAE=76.1551 MAE=76.0528 MAE=76.0967 MAE=76.0472 MAE=76.0958 MAE=76.0750 MAE=76.1278 MAE=76.0840 Epoch: 160/500MAE=76.0763 MAE=76.0158 MAE=76.0524 MAE=76.1518 MAE=76.1975 MAE=76.1772 MAE=76.0098 MAE=76.0107 MAE=76.0218 MAE=76.0277 Epoch: 170/500MAE=75.9992 MAE=76.0484 MAE=76.0888 MAE=76.0657 MAE=76.0587 MAE=76.0587 MAE=76.0628 MAE=76.0370 MAE=76.0151 MAE=76.0480 Epoch: 180/500MAE=76.0678 MAE=76.0799 MAE=76.0689 MAE=76.0159 MAE=76.1034 MAE=76.3411 MAE=76.2899 MAE=76.3542 MAE=76.3585 MAE=76.0876 Epoch: 190/500MAE=76.3592 MAE=76.3333 MAE=76.3360 MAE=76.3631 MAE=76.3223 MAE=76.1031 MAE=76.3499 MAE=76.3193 MAE=76.3523 MAE=76.3288 Epoch: 200/500MAE=76.3272 MAE=76.3221 MAE=76.0675 MAE=76.3258 MAE=76.4678 MAE=76.3167 MAE=76.2899 MAE=76.4501 MAE=76.2902 MAE=76.0422 Epoch: 210/500MAE=76.2506 MAE=76.2452 MAE=76.2441 MAE=76.2363 MAE=76.2624 MAE=75.9962 MAE=76.0543 MAE=75.9966 MAE=76.1281 MAE=76.0536 Epoch: 220/500MAE=76.0118 MAE=76.0299 MAE=76.0552 MAE=76.0284 MAE=76.1000 MAE=76.4177 MAE=76.3977 MAE=76.4450 MAE=76.4737 MAE=76.3430 Epoch: 230/500MAE=76.3566 MAE=76.4706 MAE=76.3369 MAE=76.3481 MAE=75.9231 MAE=75.8946 MAE=75.9967 MAE=76.1310 MAE=76.0785 MAE=76.3552 Epoch: 240/500MAE=76.3329 MAE=76.3149 MAE=76.3337 MAE=76.3429 MAE=76.3450 MAE=76.0692 MAE=76.3343 MAE=76.3221 MAE=76.3211 MAE=76.3416 Epoch: 250/500MAE=76.3757 MAE=76.3632 MAE=76.3422 MAE=76.1039 MAE=76.3587 MAE=76.3418 MAE=76.3266 MAE=76.3276 MAE=76.3243 MAE=76.3143 Epoch: 260/500MAE=76.3428 MAE=76.3363 MAE=76.3302 MAE=76.3081 MAE=76.0414 MAE=76.3064 MAE=76.2906 MAE=76.3116 MAE=76.2926 MAE=76.3199 Epoch: 270/500MAE=76.3180 MAE=76.3148 MAE=76.0699 MAE=76.3324 MAE=76.4224 MAE=76.4134 MAE=76.3600 MAE=76.3443 MAE=76.4725 MAE=76.3372 Epoch: 280/500MAE=76.4577 MAE=76.4581 MAE=76.4483 MAE=76.4755 MAE=76.4604 MAE=76.1796 MAE=79.4488 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 79.648 +/- 5.456\n",
                        "\n",
                        "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/500MAE=1549.0596 MAE=1546.3027 MAE=1544.2670 MAE=1541.1693 MAE=1539.7430 MAE=1535.7161 MAE=1531.9773 MAE=1529.7109 MAE=1523.8831 Epoch: 10/500MAE=1520.0958 MAE=1515.0577 MAE=1509.1453 MAE=1503.5421 MAE=1498.0052 MAE=1494.8682 MAE=1485.6184 MAE=1473.9507 MAE=1443.3513 MAE=1454.1621 Epoch: 20/500MAE=1454.6348 MAE=1446.2594 MAE=1436.4958 MAE=1436.2085 MAE=1400.1713 MAE=1426.4475 MAE=1369.6025 MAE=1377.7300 MAE=1339.4387 MAE=1350.3672 Epoch: 30/500MAE=1320.9705 MAE=1327.4521 MAE=1318.5697 MAE=1287.7065 MAE=1243.8843 MAE=1284.0527 MAE=1275.6885 MAE=1227.6997 MAE=1223.4114 MAE=1223.9557 Epoch: 40/500MAE=1223.1217 MAE=1148.0913 MAE=1121.3008 MAE=1194.0684 MAE=1160.8350 MAE=1124.7366 MAE=1057.3409 MAE=1126.4073 MAE=1022.1470 MAE=1077.5614 Epoch: 50/500MAE=1044.2924 MAE=1011.9747 MAE=968.7368 MAE=933.2957 MAE=942.6418 MAE=943.8805 MAE=928.5720 MAE=875.1985 MAE=811.2980 MAE=847.6732 Epoch: 60/500MAE=840.7209 MAE=841.9272 MAE=774.4382 MAE=717.3076 MAE=674.1202 MAE=684.2712 MAE=651.0535 MAE=656.1741 MAE=609.9287 MAE=609.2700 Epoch: 70/500MAE=597.7891 MAE=555.3253 MAE=524.9481 MAE=488.4033 MAE=480.0395 MAE=465.1147 MAE=427.8425 MAE=354.7687 MAE=357.6429 MAE=322.7019 Epoch: 80/500MAE=343.1754 MAE=379.8165 MAE=303.7901 MAE=288.6474 MAE=371.7468 MAE=294.6392 MAE=299.3162 MAE=344.4665 MAE=213.8944 MAE=189.6193 Epoch: 90/500MAE=167.8931 MAE=154.3124 MAE=199.6578 MAE=179.6847 MAE=145.8170 MAE=156.1498 MAE=130.6427 MAE=135.5606 MAE=122.3775 MAE=117.5844 Epoch: 100/500MAE=118.3702 MAE=113.6721 MAE=126.9442 MAE=122.6081 MAE=122.6799 MAE=109.0723 MAE=111.0307 MAE=105.5867 MAE=105.1423 MAE=105.1026 Epoch: 110/500MAE=109.8400 MAE=108.3978 MAE=107.2384 MAE=108.1772 MAE=95.8809 MAE=96.1574 MAE=96.0472 MAE=97.4227 MAE=96.6243 MAE=97.2928 Epoch: 120/500MAE=96.3010 MAE=96.4049 MAE=93.3975 MAE=97.9526 MAE=95.9518 MAE=95.7578 MAE=94.0165 MAE=95.1834 MAE=95.9043 MAE=94.7795 Epoch: 130/500MAE=94.8762 MAE=95.5123 MAE=95.6727 MAE=95.3411 MAE=95.5139 MAE=95.2282 MAE=95.1991 MAE=94.9192 MAE=94.7853 MAE=95.0049 Epoch: 140/500MAE=95.0678 MAE=95.1599 MAE=95.2158 MAE=95.1158 MAE=95.0763 MAE=95.0457 MAE=95.1417 MAE=95.1892 MAE=95.1412 MAE=95.1332 Epoch: 150/500MAE=95.1234 MAE=95.0893 MAE=95.1260 MAE=95.0818 MAE=95.1010 MAE=95.1242 MAE=95.0946 MAE=95.1024 MAE=95.1449 MAE=95.0944 Epoch: 160/500MAE=95.0794 MAE=95.0665 MAE=95.0806 MAE=95.1728 MAE=95.1444 MAE=95.1220 MAE=95.0847 MAE=95.0847 MAE=95.0927 MAE=95.1018 Epoch: 170/500MAE=95.0863 MAE=95.0871 MAE=95.0754 MAE=95.1156 MAE=95.1244 MAE=95.1606 MAE=95.1340 MAE=95.1639 MAE=95.2092 MAE=95.2108 Epoch: 180/500MAE=95.1669 MAE=95.1575 MAE=95.1657 MAE=95.1911 MAE=95.1805 MAE=95.1842 MAE=95.1768 MAE=95.1795 MAE=95.2064 MAE=95.2046 Epoch: 190/500MAE=95.1598 MAE=95.1420 MAE=95.1556 MAE=95.1449 MAE=95.2084 MAE=95.1251 MAE=95.1233 MAE=95.1254 MAE=95.1513 MAE=95.2109 Epoch: 200/500MAE=95.1514 MAE=95.1569 MAE=95.1462 MAE=95.1923 MAE=95.1724 MAE=95.1752 MAE=95.1079 MAE=95.1230 MAE=95.1531 MAE=95.2095 Epoch: 210/500MAE=95.1934 MAE=95.1860 MAE=95.2237 MAE=95.1814 MAE=95.1920 MAE=95.1827 MAE=95.2048 MAE=95.2398 MAE=95.2072 MAE=95.2258 Epoch: 220/500MAE=95.1652 MAE=95.1621 MAE=95.1883 MAE=95.1936 MAE=95.1988 MAE=95.1721 MAE=95.2075 MAE=95.1747 MAE=95.2095 MAE=95.2149 Epoch: 230/500MAE=95.2212 MAE=95.1793 MAE=95.1470 MAE=95.1728 MAE=95.1400 MAE=95.1830 MAE=95.1948 MAE=95.1585 MAE=95.1751 MAE=95.1763 Epoch: 240/500MAE=95.1661 MAE=95.1704 MAE=95.1283 MAE=95.1272 MAE=95.1115 MAE=95.1199 MAE=95.1066 MAE=95.0603 MAE=95.1201 MAE=95.0949 Epoch: 250/500MAE=95.1076 MAE=95.0432 MAE=95.0904 MAE=95.0781 MAE=95.0858 MAE=95.1119 MAE=95.1240 MAE=95.0787 MAE=95.0499 MAE=95.1035 Epoch: 260/500MAE=95.1553 MAE=95.1387 MAE=95.0979 MAE=95.0404 MAE=95.0752 MAE=95.1115 MAE=95.0983 MAE=95.0971 MAE=95.0964 MAE=95.1058 Epoch: 270/500MAE=95.0555 MAE=95.0456 MAE=95.0374 MAE=93.6520 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 93.652 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.0405 MAE=1546.0110 MAE=1542.7905 MAE=1541.8240 MAE=1536.7534 MAE=1532.7533 MAE=1531.2572 MAE=1527.2576 MAE=1522.1870 Epoch: 10/500MAE=1520.0460 MAE=1513.9077 MAE=1508.6392 MAE=1507.3544 MAE=1497.6252 MAE=1482.0815 MAE=1471.2434 MAE=1476.8407 MAE=1470.8754 MAE=1452.0515 Epoch: 20/500MAE=1453.4451 MAE=1443.2496 MAE=1451.9766 MAE=1426.5129 MAE=1411.1360 MAE=1415.2239 MAE=1394.7102 MAE=1366.4972 MAE=1370.3704 MAE=1354.4608 Epoch: 30/500MAE=1367.6158 MAE=1316.9146 MAE=1313.7917 MAE=1287.9626 MAE=1300.6543 MAE=1267.1532 MAE=1277.6655 MAE=1228.2273 MAE=1216.2893 MAE=1210.1772 Epoch: 40/500MAE=1228.2864 MAE=1168.8494 MAE=1160.9678 MAE=1149.1316 MAE=1112.7542 MAE=1086.8940 MAE=1101.4944 MAE=1051.5504 MAE=1038.8545 MAE=1054.1555 Epoch: 50/500MAE=1025.5970 MAE=1001.8996 MAE=970.4804 MAE=939.6204 MAE=930.8173 MAE=882.9551 MAE=890.4019 MAE=872.6730 MAE=850.3547 MAE=801.3362 Epoch: 60/500MAE=813.0762 MAE=802.3208 MAE=731.6295 MAE=689.3294 MAE=693.1281 MAE=685.4373 MAE=628.8177 MAE=593.9256 MAE=634.8925 MAE=589.1628 Epoch: 70/500MAE=548.5508 MAE=525.2736 MAE=473.2906 MAE=551.5655 MAE=480.1902 MAE=517.8335 MAE=416.1796 MAE=324.6516 MAE=346.6007 MAE=397.9954 Epoch: 80/500MAE=287.8829 MAE=271.6995 MAE=251.1430 MAE=241.2144 MAE=217.4273 MAE=219.3437 MAE=232.3123 MAE=220.7787 MAE=198.0683 MAE=149.4281 Epoch: 90/500MAE=174.3351 MAE=176.2777 MAE=149.8621 MAE=136.1470 MAE=129.3339 MAE=151.5461 MAE=120.9832 MAE=125.6000 MAE=122.6448 MAE=125.5569 Epoch: 100/500MAE=124.2859 MAE=114.6528 MAE=110.7781 MAE=113.1803 MAE=109.0991 MAE=109.7645 MAE=110.5477 MAE=107.8513 MAE=103.8778 MAE=103.2600 Epoch: 110/500MAE=102.5293 MAE=101.0452 MAE=106.2098 MAE=98.9687 MAE=100.8200 MAE=97.9251 MAE=99.4774 MAE=100.8593 MAE=97.9394 MAE=96.8870 Epoch: 120/500MAE=98.4135 MAE=103.1514 MAE=98.7783 MAE=97.1527 MAE=95.8048 MAE=99.3574 MAE=93.6718 MAE=99.1017 MAE=92.3701 MAE=96.4369 Epoch: 130/500MAE=93.2791 MAE=95.8788 MAE=92.6084 MAE=93.3976 MAE=93.0279 MAE=93.0531 MAE=94.1725 MAE=93.2189 MAE=94.2911 MAE=94.0221 Epoch: 140/500MAE=93.3689 MAE=92.8344 MAE=93.3305 MAE=92.9734 MAE=93.1364 MAE=92.7471 MAE=92.7086 MAE=93.0655 MAE=93.1409 MAE=93.1139 Epoch: 150/500MAE=93.2643 MAE=93.1377 MAE=93.0106 MAE=93.0053 MAE=93.0800 MAE=93.0838 MAE=93.1007 MAE=93.1166 MAE=93.1064 MAE=93.1036 Epoch: 160/500MAE=93.0738 MAE=93.0718 MAE=93.0648 MAE=93.0705 MAE=93.0643 MAE=93.0409 MAE=93.0465 MAE=93.0668 MAE=93.0397 MAE=93.0427 Epoch: 170/500MAE=93.0212 MAE=93.0648 MAE=93.0545 MAE=93.0379 MAE=93.0256 MAE=93.0494 MAE=93.0384 MAE=93.0470 MAE=93.0494 MAE=93.0608 Epoch: 180/500MAE=93.0493 MAE=93.0474 MAE=93.0358 MAE=93.0218 MAE=93.0612 MAE=93.0574 MAE=93.1083 MAE=93.0288 MAE=93.2188 MAE=93.3229 Epoch: 190/500MAE=93.2463 MAE=93.2393 MAE=93.2482 MAE=93.2605 MAE=93.2532 MAE=93.2622 MAE=93.1830 MAE=93.2764 MAE=93.1774 MAE=93.1738 Epoch: 200/500MAE=93.2082 MAE=93.2203 MAE=93.2779 MAE=93.2595 MAE=93.2402 MAE=93.2393 MAE=93.2510 MAE=93.3217 MAE=93.2287 MAE=93.2229 Epoch: 210/500MAE=93.2344 MAE=93.2286 MAE=93.3043 MAE=93.3002 MAE=93.2245 MAE=93.2840 MAE=93.2170 MAE=93.2129 MAE=93.1961 MAE=93.2012 Epoch: 220/500MAE=93.1347 MAE=93.1502 MAE=93.1374 MAE=93.1808 MAE=93.3146 MAE=93.2204 MAE=93.2466 MAE=93.3049 MAE=93.2354 MAE=93.2362 Epoch: 230/500MAE=93.2295 MAE=93.2963 MAE=93.2227 MAE=93.2274 MAE=93.2701 MAE=93.3167 MAE=93.4044 MAE=93.2173 MAE=93.3003 MAE=93.2524 Epoch: 240/500MAE=93.2351 MAE=93.2297 MAE=93.1507 MAE=93.2118 MAE=93.3358 MAE=93.2188 MAE=93.2973 MAE=93.1918 MAE=93.1169 MAE=93.2209 Epoch: 250/500MAE=93.1971 MAE=93.2326 MAE=93.2012 MAE=93.2234 MAE=93.2202 MAE=93.1255 MAE=93.2173 MAE=93.3062 MAE=93.1869 MAE=93.2603 Epoch: 260/500MAE=93.2922 MAE=93.3109 MAE=93.3210 MAE=93.3005 MAE=93.2548 MAE=93.3313 MAE=93.3958 MAE=93.3535 MAE=93.4912 MAE=93.4655 Epoch: 270/500MAE=93.4150 MAE=93.4186 MAE=93.3983 MAE=93.4199 MAE=93.4660 MAE=93.4453 MAE=93.4296 MAE=93.4250 MAE=93.4230 MAE=96.6520 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 95.152 +/- 1.500\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.3835 MAE=1547.2234 MAE=1544.4775 MAE=1540.7690 MAE=1539.0791 MAE=1534.7107 MAE=1531.8662 MAE=1527.5586 MAE=1522.6299 Epoch: 10/500MAE=1521.6682 MAE=1515.6421 MAE=1507.9490 MAE=1504.2946 MAE=1497.5308 MAE=1484.4661 MAE=1483.8252 MAE=1474.1394 MAE=1467.8389 MAE=1456.8186 Epoch: 20/500MAE=1436.5281 MAE=1433.2361 MAE=1423.1635 MAE=1418.2302 MAE=1397.7124 MAE=1395.7797 MAE=1381.9595 MAE=1381.6803 MAE=1363.2311 MAE=1350.6372 Epoch: 30/500MAE=1331.0237 MAE=1323.4817 MAE=1309.7573 MAE=1298.8035 MAE=1274.7750 MAE=1260.3217 MAE=1245.8048 MAE=1219.8707 MAE=1217.8606 MAE=1193.7828 Epoch: 40/500MAE=1176.3081 MAE=1149.1152 MAE=1139.8159 MAE=1119.8324 MAE=1104.5288 MAE=1100.1254 MAE=1072.5200 MAE=1055.4219 MAE=1021.2192 MAE=1016.0873 Epoch: 50/500MAE=979.0707 MAE=963.1110 MAE=963.4902 MAE=922.8813 MAE=900.7213 MAE=884.6384 MAE=855.9221 MAE=838.3156 MAE=820.5614 MAE=792.5567 Epoch: 60/500MAE=773.6385 MAE=736.0502 MAE=739.4510 MAE=690.0444 MAE=660.1696 MAE=640.9520 MAE=629.9144 MAE=586.3334 MAE=594.6757 MAE=560.9681 Epoch: 70/500MAE=571.5726 MAE=504.7946 MAE=488.7210 MAE=506.7116 MAE=425.1046 MAE=389.0367 MAE=416.5042 MAE=405.5632 MAE=383.7905 MAE=374.0254 Epoch: 80/500MAE=314.6558 MAE=306.2476 MAE=291.2649 MAE=244.2217 MAE=260.2628 MAE=214.3436 MAE=239.2628 MAE=195.2632 MAE=175.0721 MAE=188.6537 Epoch: 90/500MAE=148.2272 MAE=181.4615 MAE=151.0058 MAE=159.6017 MAE=132.9901 MAE=144.3589 MAE=115.3541 MAE=113.9232 MAE=103.3425 MAE=119.4173 Epoch: 100/500MAE=110.6092 MAE=107.1330 MAE=99.1940 MAE=118.6955 MAE=115.0006 MAE=114.8527 MAE=98.8243 MAE=99.8640 MAE=101.4339 MAE=96.1832 Epoch: 110/500MAE=107.6617 MAE=100.9853 MAE=111.7166 MAE=105.5096 MAE=93.4826 MAE=92.0495 MAE=91.3957 MAE=93.7362 MAE=92.3054 MAE=93.8763 Epoch: 120/500MAE=93.4385 MAE=90.8698 MAE=93.7086 MAE=90.1410 MAE=88.2324 MAE=90.5845 MAE=91.5668 MAE=92.8979 MAE=88.4515 MAE=88.5410 Epoch: 130/500MAE=87.9286 MAE=87.8610 MAE=88.2241 MAE=88.0889 MAE=88.3656 MAE=88.1952 MAE=87.5061 MAE=87.5224 MAE=88.1179 MAE=87.5915 Epoch: 140/500MAE=87.3764 MAE=87.4929 MAE=87.6779 MAE=88.3129 MAE=87.7818 MAE=87.5357 MAE=87.4992 MAE=87.4242 MAE=87.5295 MAE=87.5608 Epoch: 150/500MAE=87.3666 MAE=87.4370 MAE=87.4053 MAE=87.6451 MAE=87.5887 MAE=87.5513 MAE=87.5396 MAE=87.4912 MAE=87.4788 MAE=87.5283 Epoch: 160/500MAE=87.5646 MAE=87.6485 MAE=87.5886 MAE=87.5971 MAE=87.4576 MAE=87.4474 MAE=87.4370 MAE=87.4411 MAE=87.5013 MAE=87.4417 Epoch: 170/500MAE=87.4424 MAE=87.4311 MAE=87.4395 MAE=87.4388 MAE=87.4901 MAE=87.4878 MAE=87.4980 MAE=87.5080 MAE=87.5129 MAE=87.5042 Epoch: 180/500MAE=87.4986 MAE=87.4857 MAE=87.4323 MAE=87.5018 MAE=87.5001 MAE=87.5121 MAE=87.5026 MAE=87.4885 MAE=87.4673 MAE=87.4644 Epoch: 190/500MAE=87.4712 MAE=87.4665 MAE=87.4932 MAE=87.3088 MAE=87.4661 MAE=87.4842 MAE=87.4784 MAE=87.4835 MAE=87.4744 MAE=87.4944 Epoch: 200/500MAE=87.4911 MAE=87.5016 MAE=87.4843 MAE=87.5058 MAE=87.4995 MAE=87.4977 MAE=87.4905 MAE=87.4784 MAE=87.4747 MAE=87.4829 Epoch: 210/500MAE=87.5051 MAE=87.4932 MAE=87.4762 MAE=87.4713 MAE=87.4875 MAE=87.4796 MAE=87.5144 MAE=87.4891 MAE=87.4712 MAE=87.4833 Epoch: 220/500MAE=87.4799 MAE=87.4730 MAE=87.4754 MAE=87.4730 MAE=87.4677 MAE=87.4505 MAE=87.4498 MAE=87.4475 MAE=87.4586 MAE=87.4553 Epoch: 230/500MAE=87.4506 MAE=87.4480 MAE=87.4462 MAE=87.4541 MAE=87.4578 MAE=87.4601 MAE=87.4518 MAE=87.4607 MAE=87.4311 MAE=87.4484 Epoch: 240/500MAE=87.4469 MAE=87.4624 MAE=87.4406 MAE=87.4381 MAE=87.4688 MAE=87.4553 MAE=87.4579 MAE=87.4683 MAE=87.4323 MAE=87.4750 Epoch: 250/500MAE=87.4746 MAE=87.4419 MAE=87.4663 MAE=87.4551 MAE=87.4245 MAE=87.4284 MAE=87.4338 MAE=87.4473 MAE=87.4521 MAE=87.4432 Epoch: 260/500MAE=87.2666 MAE=87.4417 MAE=87.4623 MAE=87.4580 MAE=87.4484 MAE=87.4522 MAE=87.4529 MAE=87.4623 MAE=87.4616 MAE=87.4500 Epoch: 270/500MAE=87.4529 MAE=87.4546 MAE=87.4332 MAE=87.4487 MAE=87.4278 MAE=87.4341 MAE=87.4245 MAE=87.4152 MAE=87.4264 MAE=87.4189 Epoch: 280/500MAE=87.4532 MAE=87.4460 MAE=87.4598 MAE=87.4694 MAE=87.4633 MAE=87.4777 MAE=87.4635 MAE=87.4610 MAE=87.4642 MAE=87.4725 Epoch: 290/500MAE=87.4738 MAE=87.4600 MAE=87.4553 MAE=87.2703 MAE=87.2688 MAE=87.2432 MAE=87.4693 MAE=87.4594 MAE=87.2754 MAE=87.4607 Epoch: 300/500MAE=87.4556 MAE=87.4551 MAE=87.4596 MAE=87.4571 MAE=87.4600 MAE=87.4737 MAE=87.4435 MAE=87.4379 MAE=87.4847 MAE=87.4494 Epoch: 310/500MAE=87.4397 MAE=87.4696 MAE=87.4746 MAE=87.4381 MAE=87.4179 MAE=87.4790 MAE=87.4550 MAE=87.4507 MAE=87.4804 MAE=87.4857 Epoch: 320/500MAE=87.4927 MAE=87.4895 MAE=87.4580 MAE=87.4453 MAE=87.4249 MAE=87.4560 MAE=87.4291 MAE=87.4824 MAE=87.4445 MAE=87.4792 Epoch: 330/500MAE=87.4716 MAE=87.4625 MAE=87.4333 MAE=87.4157 MAE=87.4138 MAE=87.4158 MAE=87.4230 MAE=87.4297 MAE=87.4210 MAE=87.4132 Epoch: 340/500MAE=87.4147 MAE=87.4200 MAE=87.4332 MAE=87.4430 MAE=87.4373 MAE=87.4350 MAE=87.4442 MAE=87.4406 MAE=87.4505 MAE=87.4417 Epoch: 350/500MAE=87.4474 MAE=87.4539 MAE=87.4584 MAE=87.4406 MAE=87.4306 MAE=87.4256 MAE=87.4261 MAE=87.4343 MAE=87.4182 MAE=87.4197 Epoch: 360/500MAE=87.4251 MAE=87.4139 MAE=87.4161 MAE=87.4170 MAE=87.4250 MAE=87.4193 MAE=87.4326 MAE=87.4312 MAE=87.4256 MAE=87.4540 Epoch: 370/500MAE=87.4421 MAE=87.4401 MAE=87.4248 MAE=87.4311 MAE=87.4250 MAE=87.4360 MAE=87.4360 MAE=87.4249 MAE=87.4456 MAE=87.4126 Epoch: 380/500MAE=87.4140 MAE=87.4522 MAE=87.4291 MAE=87.4423 MAE=87.4090 MAE=87.4138 MAE=87.4390 MAE=87.4423 MAE=87.4444 MAE=87.4397 Epoch: 390/500MAE=87.4383 MAE=87.3848 MAE=87.3864 MAE=87.4516 MAE=87.4630 MAE=87.4499 MAE=87.4523 MAE=87.3950 MAE=87.3982 MAE=87.4190 Epoch: 400/500MAE=87.3631 MAE=87.3438 MAE=87.4217 MAE=87.4303 MAE=87.4314 MAE=87.4541 MAE=87.3568 MAE=87.3548 MAE=87.3469 MAE=87.3648 Epoch: 410/500MAE=87.3765 MAE=87.3767 MAE=87.4225 MAE=87.3911 MAE=87.3456 MAE=87.3547 MAE=87.3599 MAE=87.4258 MAE=87.3497 MAE=87.3584 Epoch: 420/500MAE=87.3577 MAE=87.3522 MAE=87.3526 MAE=87.3533 MAE=87.3550 MAE=87.3674 MAE=87.3538 MAE=87.3518 MAE=87.3508 MAE=87.3597 Epoch: 430/500MAE=87.3533 MAE=87.3160 MAE=87.3371 MAE=87.3526 MAE=87.3333 MAE=87.3458 MAE=87.3453 MAE=87.3502 MAE=87.3408 MAE=87.3477 Epoch: 440/500MAE=87.3396 MAE=87.3478 MAE=87.3130 MAE=87.3396 MAE=87.3285 MAE=87.3132 MAE=87.5383 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 92.614 +/- 3.792\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.7943 MAE=1547.1890 MAE=1544.2622 MAE=1541.2161 MAE=1538.4722 MAE=1534.0486 MAE=1532.8187 MAE=1528.7688 MAE=1524.8923 Epoch: 10/500MAE=1521.2793 MAE=1516.5737 MAE=1511.1678 MAE=1505.4971 MAE=1498.5087 MAE=1484.3999 MAE=1480.4774 MAE=1473.1298 MAE=1463.7133 MAE=1450.0127 Epoch: 20/500MAE=1453.6610 MAE=1443.3745 MAE=1426.3501 MAE=1420.9346 MAE=1402.3210 MAE=1397.9918 MAE=1387.1162 MAE=1370.6340 MAE=1354.6548 MAE=1369.9774 Epoch: 30/500MAE=1350.2289 MAE=1322.3810 MAE=1306.2622 MAE=1301.8765 MAE=1240.5364 MAE=1280.6875 MAE=1262.5249 MAE=1218.1110 MAE=1224.7891 MAE=1219.3708 Epoch: 40/500MAE=1197.1262 MAE=1146.8359 MAE=1178.2294 MAE=1098.6201 MAE=1133.6965 MAE=1092.3503 MAE=1103.5507 MAE=1044.9463 MAE=1079.4009 MAE=1011.5552 Epoch: 50/500MAE=989.6862 MAE=972.9513 MAE=975.3016 MAE=930.5858 MAE=968.5471 MAE=872.3987 MAE=890.1874 MAE=860.6426 MAE=798.3058 MAE=808.7178 Epoch: 60/500MAE=752.7426 MAE=742.1244 MAE=750.5497 MAE=669.8577 MAE=703.0514 MAE=653.6674 MAE=606.7379 MAE=579.9852 MAE=610.0010 MAE=574.5620 Epoch: 70/500MAE=531.1525 MAE=485.4466 MAE=470.5748 MAE=451.0276 MAE=423.1626 MAE=412.7505 MAE=341.3494 MAE=477.5159 MAE=340.6119 MAE=414.7785 Epoch: 80/500MAE=327.6771 MAE=285.2086 MAE=289.0480 MAE=280.0235 MAE=224.7210 MAE=255.5838 MAE=224.7485 MAE=234.0100 MAE=184.7451 MAE=172.2812 Epoch: 90/500MAE=173.4327 MAE=189.5382 MAE=172.6612 MAE=125.4194 MAE=138.3361 MAE=143.7655 MAE=123.0650 MAE=141.6684 MAE=133.0333 MAE=118.5604 Epoch: 100/500MAE=120.3372 MAE=110.0048 MAE=121.8366 MAE=112.9063 MAE=122.2159 MAE=126.2792 MAE=105.2132 MAE=101.8432 MAE=102.4116 MAE=102.7717 Epoch: 110/500MAE=107.7207 MAE=101.1506 MAE=99.0420 MAE=104.8943 MAE=98.8665 MAE=99.4855 MAE=98.4101 MAE=101.2782 MAE=93.9946 MAE=95.6090 Epoch: 120/500MAE=95.0559 MAE=98.5745 MAE=96.1819 MAE=92.9575 MAE=96.7503 MAE=94.4333 MAE=92.3889 MAE=96.9681 MAE=92.3756 MAE=95.7299 Epoch: 130/500MAE=95.3424 MAE=96.3416 MAE=93.1110 MAE=92.7579 MAE=93.0855 MAE=92.5614 MAE=92.6445 MAE=93.0113 MAE=93.5259 MAE=92.9171 Epoch: 140/500MAE=92.2643 MAE=93.7282 MAE=92.0408 MAE=92.3258 MAE=92.5864 MAE=92.4765 MAE=91.8659 MAE=91.7354 MAE=93.3194 MAE=91.8199 Epoch: 150/500MAE=91.4137 MAE=92.7077 MAE=93.2188 MAE=92.5394 MAE=92.3459 MAE=91.7577 MAE=92.7894 MAE=92.6787 MAE=92.5677 MAE=92.8198 Epoch: 160/500MAE=92.4374 MAE=92.0399 MAE=92.2572 MAE=92.4603 MAE=92.2013 MAE=92.0412 MAE=92.1509 MAE=92.2513 MAE=92.2133 MAE=92.3111 Epoch: 170/500MAE=92.2931 MAE=92.2406 MAE=92.1391 MAE=92.1995 MAE=92.1768 MAE=92.1669 MAE=92.1882 MAE=92.1287 MAE=92.1970 MAE=92.1645 Epoch: 180/500MAE=92.1087 MAE=92.1942 MAE=92.1679 MAE=92.1596 MAE=92.1969 MAE=92.1655 MAE=92.1761 MAE=92.1836 MAE=92.1616 MAE=92.1503 Epoch: 190/500MAE=92.2007 MAE=92.1746 MAE=92.1830 MAE=92.1956 MAE=92.1883 MAE=92.1891 MAE=92.1793 MAE=92.1620 MAE=92.1552 MAE=92.1578 Epoch: 200/500MAE=92.1963 MAE=92.2254 MAE=92.2122 MAE=92.1776 MAE=92.1728 MAE=92.1665 MAE=92.1752 MAE=92.2294 MAE=92.2168 MAE=92.1602 Epoch: 210/500MAE=92.1775 MAE=92.1819 MAE=92.2024 MAE=92.1387 MAE=92.0616 MAE=92.1216 MAE=92.1861 MAE=92.1868 MAE=92.1990 MAE=92.1896 Epoch: 220/500MAE=92.1305 MAE=92.1028 MAE=92.1076 MAE=92.1284 MAE=92.1315 MAE=92.1521 MAE=92.1677 MAE=92.1126 MAE=92.1121 MAE=92.0965 Epoch: 230/500MAE=92.0863 MAE=92.1213 MAE=92.0839 MAE=92.1015 MAE=92.1173 MAE=92.1061 MAE=92.0852 MAE=92.0999 MAE=92.1246 MAE=92.1300 Epoch: 240/500MAE=92.0963 MAE=92.0517 MAE=92.0555 MAE=92.0964 MAE=92.0472 MAE=92.1365 MAE=92.1137 MAE=92.1111 MAE=92.0972 MAE=92.0754 Epoch: 250/500MAE=92.0829 MAE=92.0682 MAE=92.0092 MAE=92.0192 MAE=92.1044 MAE=92.1014 MAE=92.1360 MAE=92.0937 MAE=92.0749 MAE=92.0840 Epoch: 260/500MAE=92.0873 MAE=92.0362 MAE=92.0302 MAE=92.1079 MAE=92.0994 MAE=92.1187 MAE=92.0904 MAE=92.0882 MAE=92.0813 MAE=91.9877 Epoch: 270/500MAE=91.9980 MAE=91.9811 MAE=91.9656 MAE=91.9531 MAE=91.9547 MAE=91.9792 MAE=91.9818 MAE=91.9568 MAE=91.9740 MAE=91.9801 Epoch: 280/500MAE=91.9560 MAE=91.9763 MAE=91.9653 MAE=92.0440 MAE=92.0620 MAE=91.9787 MAE=91.9787 MAE=91.9925 MAE=92.0021 MAE=91.9693 Epoch: 290/500MAE=92.0244 MAE=91.9780 MAE=91.9738 MAE=92.0169 MAE=92.0124 MAE=91.9576 MAE=91.9736 MAE=91.9672 MAE=91.9689 MAE=91.9871 Epoch: 300/500MAE=91.9745 MAE=90.7266 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 92.142 +/- 3.384\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.4288 MAE=1546.7073 MAE=1543.4617 MAE=1541.6780 MAE=1538.2717 MAE=1535.1472 MAE=1530.9463 MAE=1528.6279 MAE=1524.6718 Epoch: 10/500MAE=1519.1599 MAE=1514.1301 MAE=1509.5771 MAE=1504.9421 MAE=1499.8030 MAE=1489.9031 MAE=1478.7668 MAE=1468.1490 MAE=1467.6615 MAE=1459.8920 Epoch: 20/500MAE=1452.0398 MAE=1444.9690 MAE=1430.4307 MAE=1433.1025 MAE=1419.7351 MAE=1396.3599 MAE=1389.7484 MAE=1379.0093 MAE=1367.0836 MAE=1365.0917 Epoch: 30/500MAE=1339.7524 MAE=1332.6764 MAE=1308.6570 MAE=1307.5370 MAE=1278.0066 MAE=1272.4810 MAE=1250.3452 MAE=1234.6877 MAE=1215.0433 MAE=1198.5449 Epoch: 40/500MAE=1190.4082 MAE=1171.8517 MAE=1156.6925 MAE=1142.1824 MAE=1129.2112 MAE=1106.2991 MAE=1080.3386 MAE=1070.5935 MAE=1049.2866 MAE=1033.8538 Epoch: 50/500MAE=1018.6650 MAE=998.7267 MAE=969.8492 MAE=934.9872 MAE=930.4041 MAE=906.9491 MAE=897.4866 MAE=889.7028 MAE=826.8619 MAE=813.2389 Epoch: 60/500MAE=760.8115 MAE=765.6140 MAE=745.3250 MAE=711.4409 MAE=679.6284 MAE=662.6345 MAE=601.4363 MAE=634.8831 MAE=583.4923 MAE=544.0034 Epoch: 70/500MAE=557.7079 MAE=555.2538 MAE=494.6119 MAE=505.1774 MAE=459.1826 MAE=455.9085 MAE=436.1758 MAE=380.2318 MAE=364.6778 MAE=357.2912 Epoch: 80/500MAE=336.1224 MAE=313.9062 MAE=290.5296 MAE=293.4826 MAE=246.1543 MAE=264.8695 MAE=220.2510 MAE=261.1305 MAE=209.2421 MAE=170.8450 Epoch: 90/500MAE=164.2955 MAE=170.2187 MAE=134.2114 MAE=164.2279 MAE=146.8633 MAE=118.4518 MAE=123.6118 MAE=117.8563 MAE=117.7653 MAE=122.7489 Epoch: 100/500MAE=124.3631 MAE=112.4837 MAE=113.2408 MAE=110.7933 MAE=118.0769 MAE=107.3674 MAE=107.6530 MAE=116.1780 MAE=107.2393 MAE=111.5138 Epoch: 110/500MAE=101.1170 MAE=116.7381 MAE=123.5670 MAE=110.4476 MAE=121.4700 MAE=101.5411 MAE=94.6093 MAE=95.0531 MAE=101.1978 MAE=98.6818 Epoch: 120/500MAE=101.7950 MAE=95.2940 MAE=93.3641 MAE=93.8270 MAE=93.3609 MAE=96.1643 MAE=93.0575 MAE=94.0742 MAE=94.3333 MAE=93.6118 Epoch: 130/500MAE=93.5310 MAE=93.1615 MAE=93.8600 MAE=92.9554 MAE=92.3870 MAE=93.5555 MAE=92.6479 MAE=92.4273 MAE=91.9077 MAE=94.7614 Epoch: 140/500MAE=93.3769 MAE=92.3298 MAE=92.7571 MAE=92.1110 MAE=91.6114 MAE=92.5602 MAE=92.5759 MAE=92.2467 MAE=92.3945 MAE=92.4274 Epoch: 150/500MAE=91.7293 MAE=91.9840 MAE=92.0417 MAE=92.5427 MAE=92.4170 MAE=91.9336 MAE=91.9566 MAE=91.9606 MAE=91.9714 MAE=92.0832 Epoch: 160/500MAE=92.0436 MAE=92.1205 MAE=92.1239 MAE=92.1095 MAE=92.0364 MAE=91.9887 MAE=91.9905 MAE=91.9236 MAE=91.8448 MAE=91.8204 Epoch: 170/500MAE=91.8308 MAE=91.9658 MAE=91.8220 MAE=91.7938 MAE=91.8191 MAE=91.8162 MAE=91.9548 MAE=91.8991 MAE=91.7551 MAE=91.8959 Epoch: 180/500MAE=91.8296 MAE=91.9794 MAE=91.9676 MAE=91.9986 MAE=92.0044 MAE=92.1106 MAE=92.1046 MAE=92.0552 MAE=91.9775 MAE=92.0367 Epoch: 190/500MAE=92.0775 MAE=92.0818 MAE=91.9884 MAE=92.1265 MAE=92.0755 MAE=92.1023 MAE=92.0903 MAE=92.0881 MAE=92.0986 MAE=92.1083 Epoch: 200/500MAE=92.0732 MAE=92.0857 MAE=92.0831 MAE=92.0972 MAE=92.0927 MAE=92.0950 MAE=92.0844 MAE=92.1010 MAE=92.0972 MAE=92.0898 Epoch: 210/500MAE=92.0853 MAE=92.0911 MAE=92.0564 MAE=92.0281 MAE=92.0282 MAE=92.0380 MAE=92.0547 MAE=92.0582 MAE=92.0821 MAE=91.9707 Epoch: 220/500MAE=91.9883 MAE=91.9814 MAE=91.8232 MAE=92.0597 MAE=92.0253 MAE=92.0410 MAE=92.0683 MAE=92.0602 MAE=92.0394 MAE=92.0556 Epoch: 230/500MAE=91.8853 MAE=92.0486 MAE=91.9941 MAE=91.9859 MAE=91.8421 MAE=92.0557 MAE=91.9762 MAE=91.8215 MAE=91.9592 MAE=91.9545 Epoch: 240/500MAE=91.9392 MAE=91.8175 MAE=91.9651 MAE=91.9965 MAE=91.9286 MAE=92.0298 MAE=92.0296 MAE=91.9700 MAE=91.9704 MAE=91.9748 Epoch: 250/500MAE=91.9430 MAE=91.9584 MAE=91.9660 MAE=91.9783 MAE=91.9651 MAE=91.8243 MAE=91.9534 MAE=91.9299 MAE=91.9263 MAE=91.9646 Epoch: 260/500MAE=91.9700 MAE=91.9423 MAE=92.0055 MAE=92.0022 MAE=92.0296 MAE=92.0092 MAE=92.0318 MAE=92.0097 MAE=92.0019 MAE=92.0047 Epoch: 270/500MAE=92.0643 MAE=92.0387 MAE=92.0123 MAE=92.0366 MAE=92.0088 MAE=92.0278 MAE=92.0139 MAE=92.0322 MAE=92.0249 MAE=92.0265 Epoch: 280/500MAE=92.0318 MAE=92.0210 MAE=92.0102 MAE=92.0046 MAE=92.0415 MAE=92.0393 MAE=92.0376 MAE=92.0518 MAE=92.0513 MAE=92.0520 Epoch: 290/500MAE=92.0645 MAE=92.0611 MAE=92.0615 MAE=92.0529 MAE=92.0450 MAE=93.8560 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 92.485 +/- 3.104\n",
                        "\n",
                        "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/500MAE=1549.1329 MAE=1546.8604 MAE=1544.8455 MAE=1542.7399 MAE=1539.3395 MAE=1536.5745 MAE=1534.0962 MAE=1528.8440 MAE=1526.2911 Epoch: 10/500MAE=1522.3230 MAE=1517.2156 MAE=1510.4729 MAE=1504.7479 MAE=1500.5947 MAE=1492.9020 MAE=1481.8126 MAE=1471.4016 MAE=1471.6470 MAE=1459.8011 Epoch: 20/500MAE=1448.3685 MAE=1451.9434 MAE=1439.9467 MAE=1424.7008 MAE=1416.3098 MAE=1399.3353 MAE=1398.7335 MAE=1378.0748 MAE=1355.6954 MAE=1354.6648 Epoch: 30/500MAE=1344.2322 MAE=1330.0854 MAE=1318.0535 MAE=1301.8711 MAE=1283.4758 MAE=1281.8524 MAE=1282.1785 MAE=1242.4008 MAE=1232.4631 MAE=1216.0859 Epoch: 40/500MAE=1190.4744 MAE=1196.4346 MAE=1167.1221 MAE=1139.4939 MAE=1126.8528 MAE=1105.0790 MAE=1087.4042 MAE=1109.2853 MAE=1109.6376 MAE=1023.9926 Epoch: 50/500MAE=1012.3053 MAE=999.1392 MAE=982.0607 MAE=950.6505 MAE=925.3173 MAE=903.8209 MAE=901.2642 MAE=873.8210 MAE=844.7847 MAE=837.5490 Epoch: 60/500MAE=826.3911 MAE=785.7993 MAE=777.8060 MAE=721.8029 MAE=780.9373 MAE=676.3915 MAE=690.3571 MAE=601.6285 MAE=629.6509 MAE=581.8754 Epoch: 70/500MAE=529.9821 MAE=554.8516 MAE=501.0123 MAE=502.3892 MAE=500.0796 MAE=467.6864 MAE=377.0386 MAE=362.4429 MAE=409.7573 MAE=391.7822 Epoch: 80/500MAE=326.7847 MAE=312.0156 MAE=300.8035 MAE=267.6072 MAE=229.1045 MAE=195.6348 MAE=238.2523 MAE=176.5412 MAE=175.2728 MAE=160.4129 Epoch: 90/500MAE=131.4179 MAE=124.0336 MAE=134.5335 MAE=110.3424 MAE=102.1662 MAE=90.6961 MAE=88.2184 MAE=69.3350 MAE=71.8692 MAE=62.6957 Epoch: 100/500MAE=61.9837 MAE=86.5066 MAE=61.2439 MAE=63.0111 MAE=58.7332 MAE=60.5465 MAE=61.4194 MAE=58.2768 MAE=58.8921 MAE=62.4234 Epoch: 110/500MAE=58.6400 MAE=64.3613 MAE=58.7791 MAE=57.0945 MAE=53.8401 MAE=54.4764 MAE=51.3811 MAE=57.3875 MAE=51.8719 MAE=54.3184 Epoch: 120/500MAE=57.3477 MAE=52.8006 MAE=53.0278 MAE=51.5471 MAE=50.8396 MAE=51.7303 MAE=51.2790 MAE=52.9374 MAE=51.3732 MAE=50.9749 Epoch: 130/500MAE=51.0308 MAE=51.7164 MAE=52.3386 MAE=50.5472 MAE=50.3832 MAE=50.5724 MAE=50.6113 MAE=50.0740 MAE=50.6944 MAE=51.0966 Epoch: 140/500MAE=51.2215 MAE=50.5080 MAE=50.2788 MAE=50.5164 MAE=50.5477 MAE=50.2616 MAE=50.3070 MAE=50.2346 MAE=50.2525 MAE=50.2570 Epoch: 150/500MAE=50.2611 MAE=50.3502 MAE=50.3534 MAE=50.3427 MAE=50.3597 MAE=50.3472 MAE=50.3643 MAE=50.4137 MAE=50.4171 MAE=50.4141 Epoch: 160/500MAE=50.4067 MAE=50.4631 MAE=50.4511 MAE=50.4496 MAE=50.4403 MAE=50.4443 MAE=50.4416 MAE=50.4436 MAE=50.4381 MAE=50.4410 Epoch: 170/500MAE=50.4424 MAE=50.4301 MAE=50.4403 MAE=50.4203 MAE=50.3997 MAE=50.4033 MAE=50.3961 MAE=50.3834 MAE=50.4021 MAE=50.3965 Epoch: 180/500MAE=50.3940 MAE=50.3980 MAE=50.3811 MAE=50.3710 MAE=50.3731 MAE=50.3733 MAE=50.3674 MAE=50.3671 MAE=50.3733 MAE=50.3882 Epoch: 190/500MAE=50.3817 MAE=50.3814 MAE=50.3897 MAE=50.3861 MAE=50.3820 MAE=50.3853 MAE=50.4004 MAE=50.3952 MAE=50.3936 MAE=50.3945 Epoch: 200/500MAE=50.3823 MAE=50.3886 MAE=50.4059 MAE=50.3994 MAE=50.4199 MAE=50.4090 MAE=50.3883 MAE=50.3949 MAE=50.3898 MAE=50.3783 Epoch: 210/500MAE=50.3852 MAE=50.3984 MAE=50.3862 MAE=50.3746 MAE=50.3858 MAE=50.3860 MAE=50.3982 MAE=50.3840 MAE=50.3962 MAE=50.4056 Epoch: 220/500MAE=50.4092 MAE=50.3977 MAE=50.3774 MAE=50.3977 MAE=50.3978 MAE=50.3947 MAE=50.4061 MAE=50.4064 MAE=50.4041 MAE=50.3998 Epoch: 230/500MAE=50.4140 MAE=50.4114 MAE=50.3932 MAE=50.3912 MAE=50.4096 MAE=50.3983 MAE=50.4031 MAE=50.3771 MAE=50.3836 MAE=50.3870 Epoch: 240/500MAE=50.3992 MAE=50.4296 MAE=50.3938 MAE=50.4110 MAE=50.4158 MAE=50.4469 MAE=50.4528 MAE=50.4571 MAE=50.4431 MAE=50.4276 Epoch: 250/500MAE=50.4326 MAE=50.4496 MAE=50.4264 MAE=50.4324 MAE=50.4448 MAE=50.4627 MAE=50.4554 MAE=50.4457 MAE=50.4326 MAE=50.4169 Epoch: 260/500MAE=50.4389 MAE=50.4500 MAE=50.4586 MAE=50.4511 MAE=50.4622 MAE=50.4474 MAE=50.4744 MAE=50.4829 MAE=50.4807 MAE=50.4843 Epoch: 270/500MAE=50.4824 MAE=50.4795 MAE=50.4658 MAE=50.4440 MAE=50.4606 MAE=50.4656 MAE=50.4698 MAE=50.4637 MAE=50.4452 MAE=50.4254 Epoch: 280/500MAE=50.4083 MAE=50.4003 MAE=50.4105 MAE=50.4076 MAE=50.4165 MAE=50.4119 MAE=50.4134 MAE=50.3953 MAE=50.7779 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 50.778 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.4346 MAE=1546.5841 MAE=1544.9940 MAE=1542.4519 MAE=1540.8750 MAE=1534.8856 MAE=1532.3807 MAE=1527.9996 MAE=1524.3271 Epoch: 10/500MAE=1521.4351 MAE=1517.7705 MAE=1511.3749 MAE=1502.7921 MAE=1502.7031 MAE=1493.2939 MAE=1483.7311 MAE=1481.1980 MAE=1450.5676 MAE=1457.2954 Epoch: 20/500MAE=1457.0610 MAE=1456.3325 MAE=1436.4075 MAE=1422.1785 MAE=1407.7255 MAE=1423.0945 MAE=1381.0107 MAE=1372.9204 MAE=1361.7169 MAE=1370.2769 Epoch: 30/500MAE=1332.3660 MAE=1331.0688 MAE=1310.8051 MAE=1296.0145 MAE=1269.5098 MAE=1238.6802 MAE=1258.2494 MAE=1281.7994 MAE=1240.5546 MAE=1219.0229 Epoch: 40/500MAE=1209.2867 MAE=1179.1023 MAE=1159.8716 MAE=1143.8247 MAE=1134.8472 MAE=1104.3816 MAE=1082.4858 MAE=1066.7571 MAE=1030.0161 MAE=1006.6393 Epoch: 50/500MAE=1032.0586 MAE=1060.6283 MAE=1019.7789 MAE=932.1806 MAE=968.8710 MAE=924.7087 MAE=857.6570 MAE=825.7955 MAE=863.9814 MAE=798.1027 Epoch: 60/500MAE=856.4631 MAE=803.5558 MAE=723.8728 MAE=727.4882 MAE=714.9733 MAE=664.8432 MAE=684.4774 MAE=637.3578 MAE=622.3447 MAE=552.8555 Epoch: 70/500MAE=535.7911 MAE=517.3936 MAE=459.9668 MAE=454.1727 MAE=460.0767 MAE=485.4075 MAE=452.6915 MAE=392.6766 MAE=409.1943 MAE=400.3273 Epoch: 80/500MAE=413.0166 MAE=305.2690 MAE=284.6732 MAE=253.3945 MAE=263.3746 MAE=209.2782 MAE=232.7073 MAE=236.1084 MAE=159.6444 MAE=175.2225 Epoch: 90/500MAE=165.2557 MAE=131.9246 MAE=158.3580 MAE=144.7491 MAE=134.2625 MAE=111.3400 MAE=77.6673 MAE=98.5835 MAE=96.9336 MAE=67.4248 Epoch: 100/500MAE=75.8118 MAE=60.9092 MAE=62.0540 MAE=62.6150 MAE=74.9631 MAE=62.8003 MAE=66.4680 MAE=58.9030 MAE=56.4083 MAE=58.2143 Epoch: 110/500MAE=54.6513 MAE=65.9777 MAE=52.0530 MAE=55.9755 MAE=65.3179 MAE=55.8522 MAE=61.1185 MAE=52.1457 MAE=54.1657 MAE=50.9430 Epoch: 120/500MAE=53.0674 MAE=52.7713 MAE=51.2047 MAE=54.0645 MAE=56.5440 MAE=50.7379 MAE=51.6477 MAE=50.9442 MAE=53.1107 MAE=52.1480 Epoch: 130/500MAE=51.2297 MAE=51.3205 MAE=50.8946 MAE=52.9240 MAE=51.5229 MAE=51.7889 MAE=51.3833 MAE=51.5780 MAE=51.3301 MAE=51.1475 Epoch: 140/500MAE=51.5076 MAE=51.8401 MAE=51.7349 MAE=51.8378 MAE=51.5792 MAE=51.6505 MAE=51.5625 MAE=51.5126 MAE=51.5995 MAE=51.6830 Epoch: 150/500MAE=51.6587 MAE=51.6178 MAE=51.5573 MAE=51.5759 MAE=51.5540 MAE=51.5260 MAE=51.5390 MAE=51.5232 MAE=51.4898 MAE=51.4908 Epoch: 160/500MAE=51.5106 MAE=51.5363 MAE=51.5416 MAE=51.5353 MAE=51.5221 MAE=51.4960 MAE=51.5101 MAE=51.5128 MAE=51.5053 MAE=51.4821 Epoch: 170/500MAE=51.4781 MAE=51.4761 MAE=51.5004 MAE=51.5085 MAE=51.5062 MAE=51.4851 MAE=51.4657 MAE=51.4681 MAE=51.4861 MAE=51.4754 Epoch: 180/500MAE=51.4702 MAE=51.4755 MAE=51.4800 MAE=51.4913 MAE=51.4817 MAE=51.4561 MAE=51.4717 MAE=51.4741 MAE=51.4388 MAE=51.4578 Epoch: 190/500MAE=51.4619 MAE=51.4581 MAE=51.4646 MAE=51.4784 MAE=51.4853 MAE=51.4840 MAE=51.4782 MAE=51.4606 MAE=51.4668 MAE=51.4774 Epoch: 200/500MAE=51.5126 MAE=51.5227 MAE=51.4871 MAE=51.4931 MAE=51.5090 MAE=51.5014 MAE=51.4814 MAE=51.4805 MAE=51.4769 MAE=51.4732 Epoch: 210/500MAE=51.4839 MAE=51.4890 MAE=51.4965 MAE=51.5285 MAE=51.4978 MAE=51.4800 MAE=51.4679 MAE=51.4634 MAE=51.4721 MAE=51.4930 Epoch: 220/500MAE=51.4958 MAE=51.4843 MAE=51.4713 MAE=51.4684 MAE=51.4676 MAE=51.4492 MAE=51.4636 MAE=51.4684 MAE=51.4919 MAE=51.4817 Epoch: 230/500MAE=51.4743 MAE=51.4872 MAE=51.4457 MAE=51.4570 MAE=51.4484 MAE=51.4184 MAE=51.4281 MAE=51.4243 MAE=51.4454 MAE=51.4417 Epoch: 240/500MAE=51.4561 MAE=51.4605 MAE=51.4662 MAE=51.4555 MAE=51.4757 MAE=51.4372 MAE=51.4358 MAE=51.4361 MAE=51.4479 MAE=51.4122 Epoch: 250/500MAE=51.4232 MAE=51.4439 MAE=51.4424 MAE=51.4329 MAE=51.4380 MAE=51.4358 MAE=51.4592 MAE=51.4158 MAE=51.4259 MAE=51.4106 Epoch: 260/500MAE=51.4498 MAE=51.4442 MAE=51.4735 MAE=51.4616 MAE=51.4683 MAE=51.4628 MAE=51.4745 MAE=51.4779 MAE=51.4866 MAE=51.4792 Epoch: 270/500MAE=51.4501 MAE=51.4595 MAE=51.4454 MAE=51.4544 MAE=51.4290 MAE=51.4217 MAE=50.9376 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 50.858 +/- 0.080\n",
                        "\n",
                        "Epoch: 1/500MAE=1548.7467 MAE=1546.7314 MAE=1544.2358 MAE=1541.9012 MAE=1539.6328 MAE=1537.7417 MAE=1534.0579 MAE=1530.6624 MAE=1525.6876 Epoch: 10/500MAE=1522.6572 MAE=1516.7783 MAE=1511.9780 MAE=1503.5059 MAE=1499.9165 MAE=1490.5330 MAE=1481.6426 MAE=1480.7286 MAE=1469.9819 MAE=1460.7782 Epoch: 20/500MAE=1447.7014 MAE=1445.6910 MAE=1438.7771 MAE=1413.4781 MAE=1411.2404 MAE=1400.6804 MAE=1389.3203 MAE=1379.6755 MAE=1363.5378 MAE=1351.4326 Epoch: 30/500MAE=1335.6223 MAE=1327.8359 MAE=1296.2778 MAE=1287.7426 MAE=1283.8988 MAE=1265.0798 MAE=1249.2944 MAE=1230.6379 MAE=1219.3640 MAE=1209.2590 Epoch: 40/500MAE=1176.4962 MAE=1169.0292 MAE=1140.8184 MAE=1128.6306 MAE=1118.6033 MAE=1087.3098 MAE=1079.3356 MAE=1053.7446 MAE=1037.4482 MAE=1012.5555 Epoch: 50/500MAE=999.4652 MAE=988.2373 MAE=971.2632 MAE=957.8141 MAE=919.2989 MAE=891.2688 MAE=851.1450 MAE=864.1179 MAE=815.9971 MAE=776.0531 Epoch: 60/500MAE=803.3566 MAE=744.5322 MAE=707.6349 MAE=680.8020 MAE=633.7528 MAE=592.8047 MAE=663.1700 MAE=581.5717 MAE=516.0852 MAE=528.9108 Epoch: 70/500MAE=497.3587 MAE=581.5458 MAE=547.1483 MAE=447.6308 MAE=421.6208 MAE=420.1945 MAE=413.5458 MAE=411.7799 MAE=363.2161 MAE=384.5199 Epoch: 80/500MAE=290.5872 MAE=269.7093 MAE=299.3380 MAE=226.7490 MAE=319.3259 MAE=287.6529 MAE=255.8193 MAE=263.5297 MAE=167.5054 MAE=140.4216 Epoch: 90/500MAE=120.9532 MAE=112.8311 MAE=127.6573 MAE=146.0784 MAE=126.0120 MAE=87.3825 MAE=110.9883 MAE=100.7513 MAE=91.3802 MAE=76.9036 Epoch: 100/500MAE=96.4077 MAE=78.8098 MAE=108.4671 MAE=120.5111 MAE=89.0438 MAE=80.3604 MAE=89.6264 MAE=85.1823 MAE=67.1525 MAE=68.5957 Epoch: 110/500MAE=70.0899 MAE=65.8471 MAE=65.6275 MAE=64.1996 MAE=67.2368 MAE=63.8552 MAE=63.7947 MAE=62.9104 MAE=63.8165 MAE=62.2532 Epoch: 120/500MAE=67.6399 MAE=64.4666 MAE=65.3830 MAE=60.9758 MAE=60.1179 MAE=60.0143 MAE=61.3041 MAE=63.1174 MAE=61.9271 MAE=62.3601 Epoch: 130/500MAE=60.8234 MAE=59.7349 MAE=60.3169 MAE=59.1807 MAE=60.8925 MAE=59.8043 MAE=60.6658 MAE=60.1898 MAE=59.3489 MAE=59.2717 Epoch: 140/500MAE=58.6774 MAE=58.9128 MAE=59.5211 MAE=58.4620 MAE=58.5564 MAE=58.6466 MAE=58.5498 MAE=58.7043 MAE=58.7110 MAE=59.0470 Epoch: 150/500MAE=58.5941 MAE=58.2731 MAE=58.7165 MAE=58.6843 MAE=58.5016 MAE=58.3958 MAE=58.8661 MAE=58.6336 MAE=58.5573 MAE=58.5450 Epoch: 160/500MAE=58.6461 MAE=58.7242 MAE=58.7746 MAE=58.6857 MAE=58.5180 MAE=58.4514 MAE=58.4631 MAE=58.5320 MAE=58.5193 MAE=58.4960 Epoch: 170/500MAE=58.4747 MAE=58.5027 MAE=58.4821 MAE=58.4976 MAE=58.5114 MAE=58.5090 MAE=58.5356 MAE=58.5008 MAE=58.5414 MAE=58.4899 Epoch: 180/500MAE=58.5034 MAE=58.4728 MAE=58.5013 MAE=58.4999 MAE=58.5557 MAE=58.5771 MAE=58.5527 MAE=58.5032 MAE=58.5496 MAE=58.5647 Epoch: 190/500MAE=58.5503 MAE=58.5564 MAE=58.5591 MAE=58.5527 MAE=58.5322 MAE=58.5637 MAE=58.5436 MAE=58.5367 MAE=58.5245 MAE=58.5275 Epoch: 200/500MAE=58.5467 MAE=58.5441 MAE=58.5230 MAE=58.5201 MAE=58.5233 MAE=58.5165 MAE=58.4939 MAE=58.5465 MAE=58.5410 MAE=58.5133 Epoch: 210/500MAE=58.5013 MAE=58.4472 MAE=58.4644 MAE=58.5046 MAE=58.5135 MAE=58.4910 MAE=58.5039 MAE=58.4906 MAE=58.5223 MAE=58.4648 Epoch: 220/500MAE=58.4375 MAE=58.4529 MAE=58.4604 MAE=58.5532 MAE=58.5177 MAE=58.5306 MAE=58.4840 MAE=58.4771 MAE=58.4786 MAE=58.4484 Epoch: 230/500MAE=58.4474 MAE=58.4700 MAE=58.4744 MAE=58.4464 MAE=58.4809 MAE=58.4389 MAE=58.4021 MAE=58.3904 MAE=58.4729 MAE=58.4919 Epoch: 240/500MAE=58.4876 MAE=58.4518 MAE=58.5186 MAE=58.4664 MAE=58.5031 MAE=58.5159 MAE=58.5134 MAE=58.5259 MAE=58.4985 MAE=58.5153 Epoch: 250/500MAE=58.4872 MAE=58.4785 MAE=58.4599 MAE=58.5297 MAE=58.5770 MAE=58.6007 MAE=58.5778 MAE=58.5101 MAE=58.4563 MAE=58.4562 Epoch: 260/500MAE=58.4545 MAE=58.3766 MAE=58.3740 MAE=58.4029 MAE=58.3830 MAE=58.3527 MAE=58.3416 MAE=58.3766 MAE=58.3972 MAE=58.4283 Epoch: 270/500MAE=58.4182 MAE=58.4301 MAE=58.4334 MAE=58.4666 MAE=58.4738 MAE=58.4570 MAE=58.4569 MAE=58.4661 MAE=58.4643 MAE=58.4599 Epoch: 280/500MAE=58.4425 MAE=58.4437 MAE=58.3991 MAE=58.4051 MAE=58.3961 MAE=58.4136 MAE=58.3979 MAE=58.3678 MAE=58.4048 MAE=58.4051 Epoch: 290/500MAE=58.3995 MAE=58.3976 MAE=58.3993 MAE=58.3671 MAE=58.3869 MAE=58.4272 MAE=58.4038 MAE=58.4094 MAE=58.3769 MAE=58.3693 Epoch: 300/500MAE=58.3964 MAE=58.2937 MAE=60.8438 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 54.186 +/- 4.708\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.4664 MAE=1547.1282 MAE=1545.6720 MAE=1543.1204 MAE=1540.7107 MAE=1536.8245 MAE=1533.1749 MAE=1529.0967 MAE=1525.5901 Epoch: 10/500MAE=1520.0542 MAE=1513.4969 MAE=1511.6848 MAE=1507.5057 MAE=1496.1807 MAE=1492.5974 MAE=1485.4707 MAE=1476.4548 MAE=1469.1315 MAE=1460.3490 Epoch: 20/500MAE=1461.3843 MAE=1445.4592 MAE=1435.7439 MAE=1423.0781 MAE=1414.1865 MAE=1392.5613 MAE=1385.9397 MAE=1398.1758 MAE=1366.1719 MAE=1368.1433 Epoch: 30/500MAE=1341.4114 MAE=1331.0916 MAE=1319.7761 MAE=1301.0510 MAE=1297.1455 MAE=1275.7341 MAE=1268.8530 MAE=1229.2474 MAE=1219.9946 MAE=1210.2408 Epoch: 40/500MAE=1202.9497 MAE=1178.9011 MAE=1153.3945 MAE=1136.0989 MAE=1128.5493 MAE=1102.9128 MAE=1076.2385 MAE=1054.1786 MAE=1023.2785 MAE=1011.1844 Epoch: 50/500MAE=979.4602 MAE=987.7856 MAE=975.4572 MAE=951.1723 MAE=917.9075 MAE=881.5735 MAE=854.1517 MAE=837.0352 MAE=800.0833 MAE=788.2944 Epoch: 60/500MAE=770.5837 MAE=745.3805 MAE=723.7265 MAE=673.1243 MAE=664.4193 MAE=660.8764 MAE=621.9899 MAE=640.5424 MAE=548.9379 MAE=530.6755 Epoch: 70/500MAE=575.0546 MAE=604.4411 MAE=498.7829 MAE=505.6695 MAE=477.3693 MAE=456.2394 MAE=446.7430 MAE=367.4031 MAE=466.4066 MAE=432.6697 Epoch: 80/500MAE=280.9956 MAE=363.1475 MAE=397.3884 MAE=281.4906 MAE=287.9075 MAE=197.3683 MAE=199.1146 MAE=163.7233 MAE=140.4022 MAE=160.9267 Epoch: 90/500MAE=198.2764 MAE=160.0652 MAE=111.6395 MAE=135.6142 MAE=194.1032 MAE=102.5787 MAE=138.4558 MAE=115.6548 MAE=120.8731 MAE=87.8577 Epoch: 100/500MAE=106.8950 MAE=109.2611 MAE=86.4751 MAE=93.2665 MAE=72.8102 MAE=99.1022 MAE=76.1301 MAE=78.6339 MAE=73.6350 MAE=67.8808 Epoch: 110/500MAE=87.5983 MAE=69.9794 MAE=78.9378 MAE=76.1946 MAE=69.1128 MAE=72.9719 MAE=69.0813 MAE=75.7613 MAE=67.7638 MAE=67.8888 Epoch: 120/500MAE=65.6360 MAE=66.9823 MAE=66.6739 MAE=66.5039 MAE=66.8517 MAE=66.6466 MAE=66.4065 MAE=67.7160 MAE=67.3874 MAE=66.7367 Epoch: 130/500MAE=66.5675 MAE=66.4426 MAE=66.3761 MAE=66.5799 MAE=66.6702 MAE=66.5654 MAE=66.4823 MAE=66.5426 MAE=66.5640 MAE=66.4871 Epoch: 140/500MAE=66.4467 MAE=66.5110 MAE=66.4325 MAE=66.4121 MAE=66.4549 MAE=66.4622 MAE=66.4257 MAE=66.4476 MAE=66.4448 MAE=66.4193 Epoch: 150/500MAE=66.4282 MAE=66.4418 MAE=66.4512 MAE=66.4503 MAE=66.4412 MAE=66.4156 MAE=66.4234 MAE=66.4328 MAE=66.4290 MAE=66.4070 Epoch: 160/500MAE=66.4298 MAE=66.4429 MAE=66.4290 MAE=66.3951 MAE=66.3869 MAE=66.3880 MAE=66.3938 MAE=66.3884 MAE=66.3892 MAE=66.3987 Epoch: 170/500MAE=66.3955 MAE=66.3909 MAE=66.3442 MAE=66.3768 MAE=66.3532 MAE=66.3348 MAE=66.3567 MAE=66.3455 MAE=66.3485 MAE=66.3350 Epoch: 180/500MAE=66.3451 MAE=66.3478 MAE=66.3374 MAE=66.3234 MAE=66.3077 MAE=66.3250 MAE=66.3220 MAE=66.3167 MAE=66.2909 MAE=66.3088 Epoch: 190/500MAE=66.3100 MAE=66.3145 MAE=66.2846 MAE=66.2861 MAE=66.2495 MAE=66.2024 MAE=66.2264 MAE=66.2371 MAE=66.2511 MAE=66.2520 Epoch: 200/500MAE=66.2399 MAE=66.2491 MAE=66.2255 MAE=66.2306 MAE=66.2256 MAE=66.2211 MAE=66.2369 MAE=66.2482 MAE=66.2437 MAE=66.2165 Epoch: 210/500MAE=66.2154 MAE=66.2105 MAE=66.2189 MAE=66.2133 MAE=66.1909 MAE=66.2335 MAE=66.2101 MAE=66.2304 MAE=66.2386 MAE=66.1851 Epoch: 220/500MAE=66.1813 MAE=66.1742 MAE=66.1636 MAE=66.1460 MAE=66.1332 MAE=66.1319 MAE=66.1210 MAE=66.1388 MAE=66.0964 MAE=66.1116 Epoch: 230/500MAE=66.1019 MAE=66.0874 MAE=66.0948 MAE=66.1227 MAE=66.1243 MAE=66.0777 MAE=66.1085 MAE=66.0710 MAE=66.0692 MAE=66.1091 Epoch: 240/500MAE=66.1197 MAE=66.1331 MAE=66.1086 MAE=66.1463 MAE=66.1335 MAE=66.0862 MAE=66.1369 MAE=66.1007 MAE=66.0897 MAE=66.0660 Epoch: 250/500MAE=66.0464 MAE=66.0457 MAE=66.0488 MAE=66.0407 MAE=66.0376 MAE=66.0542 MAE=66.0070 MAE=66.0163 MAE=65.9990 MAE=65.9899 Epoch: 260/500MAE=65.9597 MAE=65.9743 MAE=65.9824 MAE=66.0019 MAE=65.9861 MAE=65.9892 MAE=65.9722 MAE=65.9613 MAE=65.9731 MAE=65.9660 Epoch: 270/500MAE=65.9509 MAE=66.7466 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 57.327 +/- 6.797\n",
                        "\n",
                        "Epoch: 1/500MAE=1549.1237 MAE=1546.5221 MAE=1544.2458 MAE=1541.4720 MAE=1538.4083 MAE=1535.8110 MAE=1533.0383 MAE=1529.6879 MAE=1524.7866 Epoch: 10/500MAE=1520.4126 MAE=1516.2150 MAE=1511.9749 MAE=1501.2871 MAE=1498.8687 MAE=1492.6287 MAE=1481.8792 MAE=1471.1440 MAE=1465.4717 MAE=1462.9143 Epoch: 20/500MAE=1458.8674 MAE=1438.8450 MAE=1434.4873 MAE=1424.3441 MAE=1403.8196 MAE=1402.2260 MAE=1398.1680 MAE=1357.1509 MAE=1377.9866 MAE=1352.0173 Epoch: 30/500MAE=1371.6003 MAE=1329.3569 MAE=1309.9954 MAE=1312.8064 MAE=1292.2816 MAE=1293.7449 MAE=1257.8672 MAE=1268.2771 MAE=1229.8734 MAE=1187.0088 Epoch: 40/500MAE=1163.2507 MAE=1209.1664 MAE=1146.0544 MAE=1113.3870 MAE=1159.9618 MAE=1152.0492 MAE=1099.7244 MAE=1045.6331 MAE=1047.4868 MAE=1023.6722 Epoch: 50/500MAE=1069.7905 MAE=1010.5845 MAE=944.8834 MAE=958.7350 MAE=893.6506 MAE=908.2540 MAE=786.9393 MAE=957.4474 MAE=816.5538 MAE=870.9340 Epoch: 60/500MAE=764.9142 MAE=763.3964 MAE=732.8749 MAE=693.4490 MAE=611.4401 MAE=625.5757 MAE=774.0704 MAE=621.9456 MAE=550.7728 MAE=566.6996 Epoch: 70/500MAE=531.7539 MAE=582.1080 MAE=478.9881 MAE=552.4058 MAE=589.9980 MAE=449.5339 MAE=367.0993 MAE=316.6066 MAE=337.5100 MAE=318.6013 Epoch: 80/500MAE=351.9831 MAE=270.7135 MAE=248.6729 MAE=229.4417 MAE=234.6355 MAE=219.9831 MAE=155.5882 MAE=168.1405 MAE=164.3206 MAE=224.4690 Epoch: 90/500MAE=211.3743 MAE=127.2043 MAE=79.5914 MAE=83.1062 MAE=77.3359 MAE=71.4002 MAE=77.9277 MAE=67.7331 MAE=75.6109 MAE=76.2115 Epoch: 100/500MAE=64.4761 MAE=61.2742 MAE=64.4478 MAE=64.7754 MAE=65.4388 MAE=65.7739 MAE=62.7974 MAE=68.9105 MAE=58.6225 MAE=58.7557 Epoch: 110/500MAE=57.0782 MAE=58.2598 MAE=56.2420 MAE=57.8104 MAE=56.4549 MAE=55.4089 MAE=61.7345 MAE=59.4015 MAE=71.6991 MAE=54.8946 Epoch: 120/500MAE=61.6262 MAE=54.4708 MAE=56.0248 MAE=56.5287 MAE=57.0036 MAE=57.9665 MAE=55.0025 MAE=56.6871 MAE=55.2487 MAE=56.1204 Epoch: 130/500MAE=53.9348 MAE=54.4401 MAE=54.3055 MAE=56.0205 MAE=53.9769 MAE=54.0870 MAE=54.3448 MAE=54.1422 MAE=54.0648 MAE=54.1548 Epoch: 140/500MAE=54.3806 MAE=54.6516 MAE=54.7002 MAE=54.2155 MAE=54.1933 MAE=54.2247 MAE=54.1656 MAE=54.1847 MAE=54.2180 MAE=54.1723 Epoch: 150/500MAE=54.2120 MAE=54.1654 MAE=54.1684 MAE=54.1313 MAE=54.1515 MAE=54.1580 MAE=54.1400 MAE=54.1442 MAE=54.1435 MAE=54.1230 Epoch: 160/500MAE=54.1087 MAE=54.1290 MAE=54.1398 MAE=54.1218 MAE=54.1115 MAE=54.1356 MAE=54.1341 MAE=54.1466 MAE=54.1452 MAE=54.1569 Epoch: 170/500MAE=54.1115 MAE=54.1452 MAE=54.1671 MAE=54.1309 MAE=54.1404 MAE=54.1501 MAE=54.1678 MAE=54.1907 MAE=54.1993 MAE=54.2103 Epoch: 180/500MAE=54.2167 MAE=54.2127 MAE=54.2050 MAE=54.1818 MAE=54.1882 MAE=54.2277 MAE=54.1908 MAE=54.1809 MAE=54.1737 MAE=54.1517 Epoch: 190/500MAE=54.1623 MAE=54.1404 MAE=54.1442 MAE=54.1540 MAE=54.1771 MAE=54.1702 MAE=54.1665 MAE=54.1689 MAE=54.1676 MAE=54.1677 Epoch: 200/500MAE=54.1543 MAE=54.1598 MAE=54.1837 MAE=54.1586 MAE=54.1475 MAE=54.1340 MAE=54.1268 MAE=54.1455 MAE=54.1487 MAE=54.1211 Epoch: 210/500MAE=54.1151 MAE=54.1263 MAE=54.1405 MAE=54.1519 MAE=54.1287 MAE=54.1209 MAE=54.1410 MAE=54.1328 MAE=54.1261 MAE=54.1528 Epoch: 220/500MAE=54.1660 MAE=54.1812 MAE=54.1604 MAE=54.1313 MAE=54.1056 MAE=54.1281 MAE=54.1435 MAE=54.1482 MAE=54.1569 MAE=54.1611 Epoch: 230/500MAE=54.1477 MAE=54.1476 MAE=54.1472 MAE=54.1428 MAE=54.1412 MAE=54.1298 MAE=54.1225 MAE=54.1291 MAE=54.1250 MAE=54.1177 Epoch: 240/500MAE=54.1384 MAE=54.1342 MAE=54.1211 MAE=54.0951 MAE=54.1279 MAE=54.1380 MAE=54.1190 MAE=54.0901 MAE=54.0834 MAE=54.0758 Epoch: 250/500MAE=54.0782 MAE=54.0566 MAE=54.0981 MAE=54.0686 MAE=54.0922 MAE=54.0677 MAE=54.0508 MAE=54.0346 MAE=54.0678 MAE=54.0556 Epoch: 260/500MAE=54.0389 MAE=54.0406 MAE=54.0643 MAE=54.0593 MAE=54.0384 MAE=54.0471 MAE=54.0711 MAE=54.0634 MAE=54.0523 MAE=54.0543 Epoch: 270/500MAE=54.0699 MAE=54.0662 MAE=54.0606 MAE=54.0750 MAE=54.0757 MAE=54.0759 MAE=54.0635 MAE=54.0644 MAE=54.0864 MAE=54.1106 Epoch: 280/500MAE=54.1378 MAE=54.6392 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 56.789 +/- 6.174\n",
                        "\n"
                    ]
                }
            ],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --asa_ratio=0.1 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --asa_ratio=0.3 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --asa_ratio=0.5 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --asa_ratio=0.7 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --run_times=5 --patience=150 --epochs=500 --asa_ratio=0.9 --pooling='ASAP'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### QM8"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150MAE=0.1424 MAE=0.1349 MAE=0.1242 MAE=0.1136 MAE=0.1028 MAE=0.0883 MAE=0.0732 MAE=0.0555 MAE=0.0437 Epoch: 10/150MAE=0.0439 MAE=0.0347 MAE=0.0385 MAE=0.0390 MAE=0.0330 MAE=0.0350 MAE=0.0394 MAE=0.0441 MAE=0.0536 MAE=0.0434 Epoch: 20/150MAE=0.0387 MAE=0.0432 MAE=0.0350 MAE=0.0319 MAE=0.0313 MAE=0.0307 MAE=0.0308 MAE=0.0297 MAE=0.0302 MAE=0.0293 Epoch: 30/150MAE=0.0289 MAE=0.0315 MAE=0.0318 MAE=0.0329 MAE=0.0379 MAE=0.0358 MAE=0.0322 MAE=0.0291 MAE=0.0282 MAE=0.0267 Epoch: 40/150MAE=0.0266 MAE=0.0269 MAE=0.0277 MAE=0.0306 MAE=0.0284 MAE=0.0272 MAE=0.0271 MAE=0.0270 MAE=0.0270 MAE=0.0265 Epoch: 50/150MAE=0.0267 MAE=0.0270 MAE=0.0266 MAE=0.0261 MAE=0.0260 MAE=0.0260 MAE=0.0259 MAE=0.0259 MAE=0.0259 MAE=0.0259 Epoch: 60/150MAE=0.0260 MAE=0.0264 MAE=0.0261 MAE=0.0260 MAE=0.0260 MAE=0.0261 MAE=0.0261 MAE=0.0260 MAE=0.0259 MAE=0.0269 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 0.027 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1319 MAE=0.1231 MAE=0.1161 MAE=0.1060 MAE=0.0962 MAE=0.0881 MAE=0.0758 MAE=0.0469 MAE=0.1066 Epoch: 10/150MAE=0.0990 MAE=0.0793 MAE=0.0975 MAE=0.0686 MAE=0.1039 MAE=0.0796 MAE=0.1264 MAE=0.0688 MAE=0.1055 MAE=0.0481 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 0.038 +/- 0.011\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1488 MAE=0.1434 MAE=0.1350 MAE=0.1275 MAE=0.1232 MAE=0.1147 MAE=0.1106 MAE=0.1074 MAE=0.0883 Epoch: 10/150MAE=0.1088 MAE=0.1262 MAE=0.0767 MAE=0.1694 MAE=1.7430 MAE=3.3676 MAE=1.5513 MAE=1.2929 MAE=2.9416 MAE=1.6031 Epoch: 20/150MAE=3.0886 MAE=2.5559 MAE=1.8888 MAE=0.0781 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 0.051 +/- 0.021\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1417 MAE=0.1309 MAE=0.1208 MAE=0.1116 MAE=0.1043 MAE=0.0900 MAE=0.0754 MAE=0.0685 MAE=0.0790 Epoch: 10/150MAE=0.0598 MAE=0.0599 MAE=0.0597 MAE=0.0637 MAE=0.1045 MAE=0.1140 MAE=0.1125 MAE=0.1589 MAE=0.1456 MAE=0.0591 Epoch: 20/150MAE=0.0411 MAE=0.0470 MAE=0.0947 MAE=0.1329 MAE=0.0966 MAE=0.0713 MAE=0.0901 MAE=0.0569 MAE=0.0393 MAE=0.0381 Epoch: 30/150MAE=0.0426 MAE=0.0652 MAE=0.0533 MAE=0.0369 MAE=0.0346 MAE=0.0336 MAE=0.0414 MAE=0.0389 MAE=0.0385 MAE=0.0344 Epoch: 40/150MAE=0.0747 MAE=0.0732 MAE=0.0755 MAE=0.0398 MAE=0.0418 MAE=0.0496 MAE=0.0313 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 0.046 +/- 0.020\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1452 MAE=0.1310 MAE=0.1210 MAE=0.1134 MAE=0.1061 MAE=0.0974 MAE=0.0852 MAE=0.0716 MAE=0.0528 Epoch: 10/150MAE=0.0451 MAE=0.0441 MAE=0.0414 MAE=0.0396 MAE=0.0417 MAE=0.0487 MAE=0.0410 MAE=0.0483 MAE=0.0425 MAE=0.0315 Epoch: 20/150MAE=0.0320 MAE=0.0310 MAE=0.0327 MAE=0.0480 MAE=0.0386 MAE=0.0373 MAE=0.0342 MAE=0.0292 MAE=0.0359 MAE=0.0294 Epoch: 30/150MAE=0.0281 MAE=0.0280 MAE=0.0301 MAE=0.0278 MAE=0.0278 MAE=0.0294 MAE=0.0297 MAE=0.0344 MAE=0.0303 MAE=0.0289 Epoch: 40/150MAE=0.0283 MAE=0.0296 MAE=0.0293 MAE=0.0293 MAE=0.0290 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 0.043 +/- 0.019\n",
                        "\n",
                        "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150MAE=0.1394 MAE=0.1295 MAE=0.1221 MAE=0.1133 MAE=0.1048 MAE=0.0958 MAE=0.0878 MAE=0.0657 MAE=0.0428 Epoch: 10/150MAE=0.0446 MAE=0.0582 MAE=0.1258 MAE=0.3291 MAE=0.2292 MAE=0.2534 MAE=0.0840 MAE=0.1445 MAE=0.1373 MAE=0.1875 MAE=0.0430 \n",
                        "********************1's fold 1's run over********************\n",
                        "MAE: 0.043 +/- 0.000\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1164 MAE=0.1065 MAE=0.0965 MAE=0.0880 MAE=0.0790 MAE=0.0705 MAE=0.0600 MAE=0.0523 MAE=0.0417 Epoch: 10/150MAE=0.0295 MAE=0.0328 MAE=0.0322 MAE=0.0303 MAE=0.0331 MAE=0.0301 MAE=0.0286 MAE=0.0291 MAE=0.0320 MAE=0.0292 Epoch: 20/150MAE=0.0275 MAE=0.0287 MAE=0.0296 MAE=0.0274 MAE=0.0294 MAE=0.0312 MAE=0.0283 MAE=0.0310 MAE=0.0288 MAE=0.0288 Epoch: 30/150MAE=0.0265 MAE=0.0265 MAE=0.0256 MAE=0.0263 MAE=0.0288 MAE=0.0320 MAE=0.0299 MAE=0.0255 MAE=0.0249 MAE=0.0247 Epoch: 40/150MAE=0.0247 MAE=0.0249 MAE=0.0247 MAE=0.0246 MAE=0.0247 MAE=0.0251 MAE=0.0246 MAE=0.0243 MAE=0.0245 MAE=0.0245 Epoch: 50/150MAE=0.0246 MAE=0.0243 MAE=0.0243 MAE=0.0243 MAE=0.0241 MAE=0.0240 MAE=0.0239 MAE=0.0239 MAE=0.0238 MAE=0.0238 Epoch: 60/150MAE=0.0241 MAE=0.0239 MAE=0.0238 MAE=0.0239 MAE=0.0240 MAE=0.0241 MAE=0.0241 MAE=0.0240 MAE=0.0240 MAE=0.0241 \n",
                        "********************1's fold 2's run over********************\n",
                        "MAE: 0.034 +/- 0.009\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1353 MAE=0.1252 MAE=0.1192 MAE=0.1108 MAE=0.1004 MAE=0.0899 MAE=0.0875 MAE=0.0603 MAE=0.0384 Epoch: 10/150MAE=0.0404 MAE=0.0761 MAE=0.0949 MAE=0.1170 MAE=0.0734 MAE=0.0550 MAE=0.1144 MAE=0.0568 MAE=0.0359 MAE=0.0378 Epoch: 20/150MAE=0.0389 MAE=0.0342 MAE=0.0371 MAE=0.0370 MAE=0.0353 MAE=0.0384 MAE=0.0376 MAE=0.0399 MAE=0.0343 MAE=0.0328 Epoch: 30/150MAE=0.0335 MAE=0.0333 MAE=0.0323 MAE=0.0360 MAE=0.0341 MAE=0.0353 MAE=0.0339 MAE=0.0347 MAE=0.0385 MAE=0.0389 Epoch: 40/150MAE=0.0400 MAE=0.0398 MAE=0.0400 MAE=0.0319 \n",
                        "********************1's fold 3's run over********************\n",
                        "MAE: 0.033 +/- 0.008\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1308 MAE=0.1288 MAE=0.1151 MAE=0.1068 MAE=0.1002 MAE=0.0893 MAE=0.0831 MAE=0.0694 MAE=0.0641 Epoch: 10/150MAE=0.0509 MAE=0.0501 MAE=0.0502 MAE=0.1293 MAE=0.1724 MAE=0.2171 MAE=0.2197 MAE=0.1743 MAE=0.1884 MAE=0.1942 Epoch: 20/150MAE=0.1082 MAE=0.0372 MAE=0.0390 MAE=0.0426 MAE=0.0415 MAE=0.0435 MAE=0.0457 MAE=0.0449 MAE=0.0390 MAE=0.0484 Epoch: 30/150MAE=0.0470 MAE=0.0367 MAE=0.0339 MAE=0.0350 MAE=0.0413 MAE=0.0390 MAE=0.0497 MAE=0.0560 MAE=0.0476 MAE=0.0394 Epoch: 40/150MAE=0.0343 MAE=0.0340 MAE=0.0367 MAE=0.0344 \n",
                        "********************1's fold 4's run over********************\n",
                        "MAE: 0.033 +/- 0.007\n",
                        "\n",
                        "Epoch: 1/150MAE=0.1338 MAE=0.1200 MAE=0.1123 MAE=0.1025 MAE=0.0932 MAE=0.0809 MAE=0.0712 MAE=0.0596 MAE=0.0387 Epoch: 10/150MAE=0.0362 MAE=0.0416 MAE=0.0518 MAE=0.0539 MAE=0.0461 MAE=0.0425 MAE=0.0575 MAE=0.0366 MAE=0.0397 MAE=0.0372 Epoch: 20/150MAE=0.0349 MAE=0.0333 MAE=0.0331 MAE=0.0328 MAE=0.0357 MAE=0.0341 MAE=0.0371 MAE=0.0340 MAE=0.0396 MAE=0.0351 Epoch: 30/150MAE=0.0319 MAE=0.0321 MAE=0.0324 MAE=0.0337 MAE=0.0356 MAE=0.0350 MAE=0.0348 MAE=0.0363 MAE=0.0360 MAE=0.0361 Epoch: 40/150MAE=0.0354 MAE=0.0326 \n",
                        "********************1's fold 5's run over********************\n",
                        "MAE: 0.033 +/- 0.006\n",
                        "\n",
                        "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
                        "Generating dataset...\n",
                        "Packaging molecules, finish 100.0%\n",
                        "\n",
                        "Current dataset: qm8, include 21783 molecules and 12 regression tasks\n",
                        "\n",
                        "Splitting, finish 1/1  \n",
                        "Epoch: 1/150MAE=0.1508 MAE=0.1439 MAE=0.1157 MAE=0.1065 MAE=0.0986 MAE=0.0937 MAE=0.0813 MAE=0.0790 MAE=0.0652 Epoch: 10/150MAE=0.0548 MAE=0.0614 MAE=0.0600 MAE=0.0679 MAE=0.0581 MAE=0.0513 MAE=0.0464 MAE=0.0552 MAE=0.0530 MAE=0.0854 Epoch: 20/150MAE=0.1733 MAE=0.0956 MAE=0.0669 MAE=0.0410 MAE=0.0605 MAE=0.0730 MAE=0.0583 MAE=0.0507 MAE=0.0361 MAE=0.0370 Epoch: 30/150MAE=0.0424 MAE=0.0486 MAE=0.0527 MAE=0.0453 MAE=0.0390 MAE=0.0295 MAE=0.0315 MAE=0.0316 MAE=0.0283 MAE=0.0327 Epoch: 40/150MAE=0.0323 MAE=0.0320 MAE=0.0314 MAE=0.0296 MAE=0.0310 "
                    ]
                }
            ],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --asa_ratio=0.1 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --asa_ratio=0.3 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --asa_ratio=0.5 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --asa_ratio=0.7 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --asa_ratio=0.9 --pooling='ASAP'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### BACE"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.1 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.3 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.5 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.7 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.9 --pooling='ASAP'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ESOL"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.1 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.3 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.5 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.7 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.9 --pooling='ASAP'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Freesolv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.1 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.3 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.5 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.7 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.9 --pooling='ASAP'"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Lipophilicity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.1 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.3 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.5 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.7 --pooling='ASAP'\nprint(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n!python /data1/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --asa_ratio=0.9 --pooling='ASAP'"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "CG-ODE",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.16"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}