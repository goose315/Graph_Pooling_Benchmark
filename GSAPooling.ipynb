{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import sys\n",
    "import torch\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "from typing import Optional\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import WebKB\n",
    "from torch_geometric.datasets import Actor\n",
    "from torch_geometric.datasets import GNNBenchmarkDataset\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from sklearn.metrics import r2_score\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_sparse import spspmm\n",
    "from torch_sparse import coalesce\n",
    "from torch_sparse import eye\n",
    "from torch.nn import Parameter\n",
    "from torch_scatter import scatter_add\n",
    "from torch_scatter import scatter_max\n",
    "\n",
    "from torch_scatter import scatter_add, scatter\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch_geometric.nn import GCNConv, GATConv, LEConv, SAGEConv, GraphConv\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.utils import add_remaining_self_loops, to_dense_adj, add_self_loops\n",
    "from typing import Callable, Optional, Union\n",
    "from torch_sparse import coalesce, transpose\n",
    "from torch_scatter import scatter\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.nn import Parameter\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from typing import Union, Optional, Callable\n",
    "from torch_scatter import scatter_add, scatter_max\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GCNConv, GATConv, ChebConv, GraphConv\n",
    "\n",
    "\n",
    "def uniform(size, tensor):\n",
    "    if tensor is not None:\n",
    "        bound = 1.0 / math.sqrt(size)\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "\n",
    "def maybe_num_nodes(edge_index, num_nodes=None):\n",
    "    if num_nodes is not None:\n",
    "        return num_nodes\n",
    "    elif isinstance(edge_index, Tensor):\n",
    "        return int(edge_index.max()) + 1 if edge_index.numel() > 0 else 0\n",
    "    else:\n",
    "        return max(edge_index.size(0), edge_index.size(1))\n",
    "\n",
    "\n",
    "def topk(x, ratio, batch, min_score=None, tol=1e-7):\n",
    "    if min_score is not None:\n",
    "        # Make sure that we do not drop all nodes in a graph.\n",
    "        scores_max = scatter_max(x, batch)[0][batch] - tol\n",
    "        scores_min = scores_max.clamp(max=min_score)\n",
    "\n",
    "        perm = (x > scores_min).nonzero(as_tuple=False).view(-1)\n",
    "    else:\n",
    "        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)\n",
    "        batch_size, max_num_nodes = num_nodes.size(0), num_nodes.max().item()\n",
    "\n",
    "        cum_num_nodes = torch.cat(\n",
    "            [num_nodes.new_zeros(1),\n",
    "             num_nodes.cumsum(dim=0)[:-1]], dim=0)\n",
    "\n",
    "        index = torch.arange(batch.size(0), dtype=torch.long, device=x.device)\n",
    "        index = (index - cum_num_nodes[batch]) + (batch * max_num_nodes)\n",
    "\n",
    "        dense_x = x.new_full((batch_size * max_num_nodes, ),\n",
    "                             torch.finfo(x.dtype).min)\n",
    "        dense_x[index] = x\n",
    "        dense_x = dense_x.view(batch_size, max_num_nodes)\n",
    "\n",
    "        _, perm = dense_x.sort(dim=-1, descending=True)\n",
    "\n",
    "        perm = perm + cum_num_nodes.view(-1, 1)\n",
    "        perm = perm.view(-1)\n",
    "\n",
    "        if isinstance(ratio, int):\n",
    "            k = num_nodes.new_full((num_nodes.size(0), ), ratio)\n",
    "            k = torch.min(k, num_nodes)\n",
    "        else:\n",
    "            k = (ratio * num_nodes.to(torch.float)).ceil().to(torch.long)\n",
    "\n",
    "        mask = [\n",
    "            torch.arange(k[i], dtype=torch.long, device=x.device) +\n",
    "            i * max_num_nodes for i in range(batch_size)\n",
    "        ]\n",
    "        mask = torch.cat(mask, dim=0)\n",
    "\n",
    "        perm = perm[mask]\n",
    "\n",
    "    return perm\n",
    "\n",
    "\n",
    "def filter_adj(edge_index, edge_attr, perm, num_nodes=None):\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    mask = perm.new_full((num_nodes, ), -1)\n",
    "    i = torch.arange(perm.size(0), dtype=torch.long, device=perm.device)\n",
    "    mask[perm] = i\n",
    "\n",
    "    row, col = edge_index\n",
    "    row, col = mask[row], mask[col]\n",
    "    mask = (row >= 0) & (col >= 0)\n",
    "    row, col = row[mask], col[mask]\n",
    "\n",
    "    if edge_attr is not None:\n",
    "        edge_attr = edge_attr[mask]\n",
    "\n",
    "    return torch.stack([row, col], dim=0), edge_attr\n",
    "\n",
    "\n",
    "class GSAPool(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, pooling_ratio=0.5, alpha=0.6,\n",
    "                        min_score=None, multiplier=1,\n",
    "                        non_linearity=torch.tanh,\n",
    "                        cus_drop_ratio =0):\n",
    "        super(GSAPool,self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.ratio = pooling_ratio\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.sbtl_layer = GCNConv(in_channels,1)\n",
    "        self.fbtl_layer = nn.Linear(in_channels, 1)\n",
    "        self.fusion = GCNConv(in_channels,in_channels)\n",
    "\n",
    "        self.min_score = min_score\n",
    "        self.multiplier = multiplier\n",
    "        self.fusion_flag = 0\n",
    "        self.non_linearity = non_linearity\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(cus_drop_ratio)\n",
    "\n",
    "    def conv_selection(self, conv, in_channels, conv_type=0):\n",
    "        if(conv_type == 0):\n",
    "            out_channels = 1\n",
    "        elif(conv_type == 1):\n",
    "            out_channels = in_channels\n",
    "        if(conv == \"GCNConv\"):\n",
    "            return GCNConv(in_channels,out_channels)\n",
    "        elif(conv == \"ChebConv\"):\n",
    "            return ChebConv(in_channels,out_channels,1)\n",
    "        elif(conv == \"GCNConv\"):\n",
    "            return GCNConv(in_channels,out_channels)\n",
    "        elif(conv == \"GATConv\"):\n",
    "            return GATConv(in_channels,out_channels, heads=1, concat=True)\n",
    "        elif(conv == \"GraphConv\"):\n",
    "            return GraphConv(in_channels,out_channels)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, batch=None):\n",
    "        if batch is None:\n",
    "            batch = edge_index.new_zeros(x.size(0))\n",
    "        x = x.unsqueeze(-1) if x.dim() == 1 else x\n",
    "\n",
    "        #SBTL\n",
    "        score_s = self.sbtl_layer(x,edge_index).squeeze()\n",
    "        #FBTL\n",
    "        score_f = self.fbtl_layer(x).squeeze()\n",
    "        #hyperparametr alpha\n",
    "        score = score_s*self.alpha + score_f*(1-self.alpha)\n",
    "\n",
    "        score = score.unsqueeze(-1) if score.dim()==0 else score\n",
    "\n",
    "        if self.min_score is None:\n",
    "            score = self.non_linearity(score)\n",
    "        else:\n",
    "            score = softmax(score, batch)\n",
    "\n",
    "        sc = self.dropout(score)\n",
    "        perm = topk(sc, self.ratio, batch)\n",
    "\n",
    "        #fusion\n",
    "        if(self.fusion_flag == 1):\n",
    "            x = self.fusion(x, edge_index)\n",
    "        x_ae = x[perm]\n",
    "        x = x[perm] * score[perm].view(-1, 1)\n",
    "        x = self.multiplier * x if self.multiplier != 1 else x\n",
    "\n",
    "        batch = batch[perm]\n",
    "        edge_index, edge_attr = filter_adj(\n",
    "            edge_index, edge_attr, perm, num_nodes=score.size(0))\n",
    "\n",
    "        return x, edge_index, edge_attr, batch, perm, x_ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MUTAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 165 for seed 42\n",
      "Early stopping at epoch 161 for seed 43\n",
      "Early stopping at epoch 176 for seed 44\n",
      "Average Time: 9.19 seconds\n",
      "Var Time: 0.27 seconds\n",
      "Average Memory: 34.67 MB\n",
      "Average Best Val Acc: 0.7738\n",
      "Std Best Test Acc: 0.0163\n",
      "Average Test Acc: 0.8736\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 150\n",
    "data_path = \"/data/Zeyu/Pooling/1\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"MUTAG\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = GSAPool(64, pooling_ratio=0.7, alpha = 0.6, cus_drop_ratio = 0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = GSAPool(64, pooling_ratio=0.7, alpha = 0.6, cus_drop_ratio = 0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool1(x, edge_index, None, batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool2(x, edge_index, None, batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 172 for seed 44\n",
      "Average Time: 35.35 seconds\n",
      "Var Time: 3.83 seconds\n",
      "Average Memory: 726.00 MB\n",
      "Average Best Val Acc: 0.7688\n",
      "Std Best Test Acc: 0.0127\n",
      "Average Test Acc: 0.7297\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"DD\", pre_filter=lambda data: data.num_nodes <= max_nodes, use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = GSAPool(64, pooling_ratio=0.9, alpha = 0.5, cus_drop_ratio = 0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = GSAPool(64, pooling_ratio=0.9, alpha = 0.5, cus_drop_ratio = 0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool1(x, edge_index, None, batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool2(x, edge_index, None, batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Time: 46.34 seconds\n",
      "Var Time: 0.25 seconds\n",
      "Average Memory: 197.33 MB\n",
      "Average Best Val Acc: 0.7711\n",
      "Std Best Test Acc: 0.0393\n",
      "Average Test Acc: 0.7467\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-BINARY\", transform=T.Compose([T.OneHotDegree(136)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = GSAPool(64, pooling_ratio=0.9, alpha = 0.7, cus_drop_ratio = 0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = GSAPool(64, pooling_ratio=0.9, alpha = 0.7, cus_drop_ratio = 0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool1(x, edge_index, None, batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool2(x, edge_index, None, batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-MULTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 192 for seed 43\n",
      "Average Time: 68.92 seconds\n",
      "Var Time: 3.73 seconds\n",
      "Average Memory: 168.67 MB\n",
      "Average Best Val Acc: 0.5185\n",
      "Std Best Test Acc: 0.0413\n",
      "Average Test Acc: 0.4637\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "max_nodes = 500\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"IMDB-MULTI\", transform=T.Compose([T.OneHotDegree(88)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = GSAPool(64, pooling_ratio=0.7, alpha = 0.7, cus_drop_ratio = 0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = GSAPool(64, pooling_ratio=0.7, alpha = 0.7, cus_drop_ratio = 0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool1(x, edge_index, None, batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool2(x, edge_index, None, batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        #print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COLLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42, Epoch: 001, Loss: 1.0888, Val Acc: 0.3213, Test Acc: 0.3320\n",
      "Seed: 42, Epoch: 002, Loss: 1.0804, Val Acc: 0.3213, Test Acc: 0.3320\n",
      "Seed: 42, Epoch: 003, Loss: 1.0723, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 004, Loss: 1.0645, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 005, Loss: 1.0565, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 006, Loss: 1.0479, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 007, Loss: 1.0359, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 008, Loss: 1.0168, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 009, Loss: 0.9862, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 010, Loss: 0.9493, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 011, Loss: 0.9234, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 012, Loss: 0.8993, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 013, Loss: 0.8811, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 014, Loss: 0.8637, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 015, Loss: 0.8514, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 016, Loss: 0.8412, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 017, Loss: 0.8303, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 018, Loss: 0.8220, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 019, Loss: 0.8110, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 020, Loss: 0.7985, Val Acc: 0.5053, Test Acc: 0.5147\n",
      "Seed: 42, Epoch: 021, Loss: 0.7861, Val Acc: 0.5107, Test Acc: 0.5160\n",
      "Seed: 42, Epoch: 022, Loss: 0.7698, Val Acc: 0.5627, Test Acc: 0.5467\n",
      "Seed: 42, Epoch: 023, Loss: 0.7520, Val Acc: 0.5760, Test Acc: 0.5373\n",
      "Seed: 42, Epoch: 024, Loss: 0.7300, Val Acc: 0.5787, Test Acc: 0.5373\n",
      "Seed: 42, Epoch: 025, Loss: 0.7146, Val Acc: 0.5973, Test Acc: 0.5587\n",
      "Seed: 42, Epoch: 026, Loss: 0.7038, Val Acc: 0.6587, Test Acc: 0.6160\n",
      "Seed: 42, Epoch: 027, Loss: 0.6917, Val Acc: 0.6627, Test Acc: 0.6173\n",
      "Seed: 42, Epoch: 028, Loss: 0.6815, Val Acc: 0.6800, Test Acc: 0.6427\n",
      "Seed: 42, Epoch: 029, Loss: 0.6704, Val Acc: 0.6920, Test Acc: 0.6613\n",
      "Seed: 42, Epoch: 030, Loss: 0.6587, Val Acc: 0.7027, Test Acc: 0.6867\n",
      "Seed: 42, Epoch: 031, Loss: 0.6461, Val Acc: 0.7067, Test Acc: 0.6893\n",
      "Seed: 42, Epoch: 032, Loss: 0.6313, Val Acc: 0.7027, Test Acc: 0.6920\n",
      "Seed: 42, Epoch: 033, Loss: 0.6176, Val Acc: 0.7040, Test Acc: 0.7040\n",
      "Seed: 42, Epoch: 034, Loss: 0.6033, Val Acc: 0.6933, Test Acc: 0.7080\n",
      "Seed: 42, Epoch: 035, Loss: 0.5903, Val Acc: 0.6987, Test Acc: 0.7067\n",
      "Seed: 42, Epoch: 036, Loss: 0.5771, Val Acc: 0.7053, Test Acc: 0.7107\n",
      "Seed: 42, Epoch: 037, Loss: 0.5670, Val Acc: 0.7013, Test Acc: 0.7013\n",
      "Seed: 42, Epoch: 038, Loss: 0.5605, Val Acc: 0.7120, Test Acc: 0.7013\n",
      "Seed: 42, Epoch: 039, Loss: 0.5521, Val Acc: 0.7133, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 040, Loss: 0.5445, Val Acc: 0.7080, Test Acc: 0.7013\n",
      "Seed: 42, Epoch: 041, Loss: 0.5445, Val Acc: 0.7067, Test Acc: 0.6867\n",
      "Seed: 42, Epoch: 042, Loss: 0.5393, Val Acc: 0.7000, Test Acc: 0.7000\n",
      "Seed: 42, Epoch: 043, Loss: 0.5330, Val Acc: 0.7120, Test Acc: 0.7093\n",
      "Seed: 42, Epoch: 044, Loss: 0.5274, Val Acc: 0.7120, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 045, Loss: 0.5238, Val Acc: 0.7187, Test Acc: 0.7187\n",
      "Seed: 42, Epoch: 046, Loss: 0.5165, Val Acc: 0.7107, Test Acc: 0.7027\n",
      "Seed: 42, Epoch: 047, Loss: 0.5172, Val Acc: 0.7240, Test Acc: 0.7053\n",
      "Seed: 42, Epoch: 048, Loss: 0.5112, Val Acc: 0.7240, Test Acc: 0.7160\n",
      "Seed: 42, Epoch: 049, Loss: 0.5071, Val Acc: 0.7227, Test Acc: 0.7173\n",
      "Seed: 42, Epoch: 050, Loss: 0.5046, Val Acc: 0.7280, Test Acc: 0.7120\n",
      "Seed: 42, Epoch: 051, Loss: 0.5048, Val Acc: 0.7280, Test Acc: 0.7160\n",
      "Seed: 42, Epoch: 052, Loss: 0.5004, Val Acc: 0.7293, Test Acc: 0.7160\n",
      "Seed: 42, Epoch: 053, Loss: 0.4963, Val Acc: 0.7320, Test Acc: 0.7133\n",
      "Seed: 42, Epoch: 054, Loss: 0.4959, Val Acc: 0.7187, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 055, Loss: 0.4925, Val Acc: 0.7307, Test Acc: 0.7187\n",
      "Seed: 42, Epoch: 056, Loss: 0.4894, Val Acc: 0.7347, Test Acc: 0.7227\n",
      "Seed: 42, Epoch: 057, Loss: 0.4890, Val Acc: 0.7333, Test Acc: 0.7213\n",
      "Seed: 42, Epoch: 058, Loss: 0.4860, Val Acc: 0.7387, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 059, Loss: 0.4871, Val Acc: 0.7320, Test Acc: 0.7213\n",
      "Seed: 42, Epoch: 060, Loss: 0.4837, Val Acc: 0.7347, Test Acc: 0.7227\n",
      "Seed: 42, Epoch: 061, Loss: 0.4809, Val Acc: 0.7413, Test Acc: 0.7227\n",
      "Seed: 42, Epoch: 062, Loss: 0.4815, Val Acc: 0.7387, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 063, Loss: 0.4823, Val Acc: 0.7387, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 064, Loss: 0.4776, Val Acc: 0.7427, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 065, Loss: 0.4735, Val Acc: 0.7400, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 066, Loss: 0.4749, Val Acc: 0.7267, Test Acc: 0.7200\n",
      "Seed: 42, Epoch: 067, Loss: 0.4735, Val Acc: 0.7347, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 068, Loss: 0.4762, Val Acc: 0.7440, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 069, Loss: 0.4703, Val Acc: 0.7493, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 070, Loss: 0.4680, Val Acc: 0.7453, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 071, Loss: 0.4674, Val Acc: 0.7493, Test Acc: 0.7307\n",
      "Seed: 42, Epoch: 072, Loss: 0.4661, Val Acc: 0.7467, Test Acc: 0.7307\n",
      "Seed: 42, Epoch: 073, Loss: 0.4675, Val Acc: 0.7467, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 074, Loss: 0.4658, Val Acc: 0.7493, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 075, Loss: 0.4631, Val Acc: 0.7493, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 076, Loss: 0.4630, Val Acc: 0.7493, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 077, Loss: 0.4626, Val Acc: 0.7440, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 078, Loss: 0.4598, Val Acc: 0.7533, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 079, Loss: 0.4605, Val Acc: 0.7440, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 080, Loss: 0.4579, Val Acc: 0.7480, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 081, Loss: 0.4561, Val Acc: 0.7467, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 082, Loss: 0.4573, Val Acc: 0.7453, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 083, Loss: 0.4536, Val Acc: 0.7440, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 084, Loss: 0.4542, Val Acc: 0.7467, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 085, Loss: 0.4531, Val Acc: 0.7480, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 086, Loss: 0.4516, Val Acc: 0.7467, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 087, Loss: 0.4504, Val Acc: 0.7493, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 088, Loss: 0.4516, Val Acc: 0.7520, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 089, Loss: 0.4485, Val Acc: 0.7520, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 090, Loss: 0.4470, Val Acc: 0.7507, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 091, Loss: 0.4457, Val Acc: 0.7493, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 092, Loss: 0.4445, Val Acc: 0.7547, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 093, Loss: 0.4439, Val Acc: 0.7573, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 094, Loss: 0.4431, Val Acc: 0.7587, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 095, Loss: 0.4413, Val Acc: 0.7587, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 096, Loss: 0.4398, Val Acc: 0.7600, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 097, Loss: 0.4383, Val Acc: 0.7640, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 098, Loss: 0.4378, Val Acc: 0.7560, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 099, Loss: 0.4361, Val Acc: 0.7560, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 100, Loss: 0.4367, Val Acc: 0.7507, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 101, Loss: 0.4352, Val Acc: 0.7560, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 102, Loss: 0.4341, Val Acc: 0.7600, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 103, Loss: 0.4311, Val Acc: 0.7613, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 104, Loss: 0.4307, Val Acc: 0.7600, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 105, Loss: 0.4342, Val Acc: 0.7560, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 106, Loss: 0.4315, Val Acc: 0.7560, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 107, Loss: 0.4298, Val Acc: 0.7520, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 108, Loss: 0.4288, Val Acc: 0.7600, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 109, Loss: 0.4289, Val Acc: 0.7560, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 110, Loss: 0.4253, Val Acc: 0.7667, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 111, Loss: 0.4251, Val Acc: 0.7613, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 112, Loss: 0.4222, Val Acc: 0.7627, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 113, Loss: 0.4200, Val Acc: 0.7627, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 114, Loss: 0.4210, Val Acc: 0.7613, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 115, Loss: 0.4211, Val Acc: 0.7653, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 116, Loss: 0.4204, Val Acc: 0.7680, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 117, Loss: 0.4199, Val Acc: 0.7600, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 118, Loss: 0.4170, Val Acc: 0.7653, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 119, Loss: 0.4178, Val Acc: 0.7560, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 120, Loss: 0.4193, Val Acc: 0.7627, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 121, Loss: 0.4206, Val Acc: 0.7640, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 122, Loss: 0.4188, Val Acc: 0.7600, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 123, Loss: 0.4192, Val Acc: 0.7680, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 124, Loss: 0.4155, Val Acc: 0.7653, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 125, Loss: 0.4132, Val Acc: 0.7573, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 126, Loss: 0.4123, Val Acc: 0.7640, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 127, Loss: 0.4117, Val Acc: 0.7707, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 128, Loss: 0.4100, Val Acc: 0.7640, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 129, Loss: 0.4087, Val Acc: 0.7733, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 130, Loss: 0.4084, Val Acc: 0.7600, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 131, Loss: 0.4081, Val Acc: 0.7773, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 132, Loss: 0.4071, Val Acc: 0.7707, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 133, Loss: 0.4083, Val Acc: 0.7733, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 134, Loss: 0.4065, Val Acc: 0.7667, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 135, Loss: 0.4065, Val Acc: 0.7627, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 136, Loss: 0.4048, Val Acc: 0.7720, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 137, Loss: 0.4039, Val Acc: 0.7733, Test Acc: 0.7493\n",
      "Seed: 42, Epoch: 138, Loss: 0.4034, Val Acc: 0.7773, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 139, Loss: 0.4049, Val Acc: 0.7720, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 140, Loss: 0.4017, Val Acc: 0.7667, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 141, Loss: 0.4022, Val Acc: 0.7827, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 142, Loss: 0.3992, Val Acc: 0.7813, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 143, Loss: 0.3984, Val Acc: 0.7693, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 144, Loss: 0.4024, Val Acc: 0.7787, Test Acc: 0.7453\n",
      "Seed: 42, Epoch: 145, Loss: 0.3981, Val Acc: 0.7760, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 146, Loss: 0.3963, Val Acc: 0.7747, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 147, Loss: 0.3966, Val Acc: 0.7867, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 148, Loss: 0.3969, Val Acc: 0.7760, Test Acc: 0.7293\n",
      "Seed: 42, Epoch: 149, Loss: 0.3962, Val Acc: 0.7813, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 150, Loss: 0.3944, Val Acc: 0.7827, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 151, Loss: 0.3927, Val Acc: 0.7827, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 152, Loss: 0.3919, Val Acc: 0.7840, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 153, Loss: 0.3924, Val Acc: 0.7773, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 154, Loss: 0.3907, Val Acc: 0.7827, Test Acc: 0.7307\n",
      "Seed: 42, Epoch: 155, Loss: 0.3906, Val Acc: 0.7787, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 156, Loss: 0.3895, Val Acc: 0.7880, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 157, Loss: 0.3903, Val Acc: 0.7733, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 158, Loss: 0.3915, Val Acc: 0.7813, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 159, Loss: 0.3909, Val Acc: 0.7813, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 160, Loss: 0.3939, Val Acc: 0.7733, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 161, Loss: 0.3867, Val Acc: 0.7867, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 162, Loss: 0.3885, Val Acc: 0.7800, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 163, Loss: 0.3874, Val Acc: 0.7787, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 164, Loss: 0.3864, Val Acc: 0.7760, Test Acc: 0.7280\n",
      "Seed: 42, Epoch: 165, Loss: 0.3853, Val Acc: 0.7840, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 166, Loss: 0.3832, Val Acc: 0.7813, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 167, Loss: 0.3830, Val Acc: 0.7800, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 168, Loss: 0.3820, Val Acc: 0.7747, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 169, Loss: 0.3812, Val Acc: 0.7827, Test Acc: 0.7307\n",
      "Seed: 42, Epoch: 170, Loss: 0.3818, Val Acc: 0.7853, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 171, Loss: 0.3829, Val Acc: 0.7880, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 172, Loss: 0.3807, Val Acc: 0.7667, Test Acc: 0.7253\n",
      "Seed: 42, Epoch: 173, Loss: 0.3845, Val Acc: 0.7773, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 174, Loss: 0.3799, Val Acc: 0.7853, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 175, Loss: 0.3795, Val Acc: 0.7747, Test Acc: 0.7267\n",
      "Seed: 42, Epoch: 176, Loss: 0.3776, Val Acc: 0.7787, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 177, Loss: 0.3744, Val Acc: 0.7720, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 178, Loss: 0.3758, Val Acc: 0.7720, Test Acc: 0.7280\n",
      "Seed: 42, Epoch: 179, Loss: 0.3757, Val Acc: 0.7880, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 180, Loss: 0.3748, Val Acc: 0.7813, Test Acc: 0.7400\n",
      "Seed: 42, Epoch: 181, Loss: 0.3734, Val Acc: 0.7720, Test Acc: 0.7240\n",
      "Seed: 42, Epoch: 182, Loss: 0.3733, Val Acc: 0.7853, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 183, Loss: 0.3715, Val Acc: 0.7787, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 184, Loss: 0.3713, Val Acc: 0.7840, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 185, Loss: 0.3684, Val Acc: 0.7880, Test Acc: 0.7427\n",
      "Seed: 42, Epoch: 186, Loss: 0.3712, Val Acc: 0.7733, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 187, Loss: 0.3682, Val Acc: 0.7853, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 188, Loss: 0.3675, Val Acc: 0.7960, Test Acc: 0.7333\n",
      "Seed: 42, Epoch: 189, Loss: 0.3675, Val Acc: 0.7787, Test Acc: 0.7467\n",
      "Seed: 42, Epoch: 190, Loss: 0.3663, Val Acc: 0.7840, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 191, Loss: 0.3657, Val Acc: 0.7893, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 192, Loss: 0.3661, Val Acc: 0.7827, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 193, Loss: 0.3638, Val Acc: 0.7880, Test Acc: 0.7360\n",
      "Seed: 42, Epoch: 194, Loss: 0.3626, Val Acc: 0.7720, Test Acc: 0.7320\n",
      "Seed: 42, Epoch: 195, Loss: 0.3636, Val Acc: 0.7933, Test Acc: 0.7387\n",
      "Seed: 42, Epoch: 196, Loss: 0.3613, Val Acc: 0.7813, Test Acc: 0.7373\n",
      "Seed: 42, Epoch: 197, Loss: 0.3602, Val Acc: 0.7933, Test Acc: 0.7413\n",
      "Seed: 42, Epoch: 198, Loss: 0.3622, Val Acc: 0.7920, Test Acc: 0.7440\n",
      "Seed: 42, Epoch: 199, Loss: 0.3604, Val Acc: 0.7773, Test Acc: 0.7347\n",
      "Seed: 42, Epoch: 200, Loss: 0.3601, Val Acc: 0.7987, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 001, Loss: 1.0865, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 002, Loss: 1.0798, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 003, Loss: 1.0733, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 004, Loss: 1.0670, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 005, Loss: 1.0605, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 006, Loss: 1.0526, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 007, Loss: 1.0396, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 008, Loss: 1.0097, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 009, Loss: 0.9681, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 010, Loss: 0.9398, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 011, Loss: 0.9147, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 012, Loss: 0.8938, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 013, Loss: 0.8718, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 014, Loss: 0.8537, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 015, Loss: 0.8387, Val Acc: 0.5053, Test Acc: 0.5160\n",
      "Seed: 43, Epoch: 016, Loss: 0.8260, Val Acc: 0.5587, Test Acc: 0.5547\n",
      "Seed: 43, Epoch: 017, Loss: 0.8184, Val Acc: 0.6267, Test Acc: 0.5987\n",
      "Seed: 43, Epoch: 018, Loss: 0.8093, Val Acc: 0.6280, Test Acc: 0.6000\n",
      "Seed: 43, Epoch: 019, Loss: 0.8023, Val Acc: 0.6280, Test Acc: 0.6053\n",
      "Seed: 43, Epoch: 020, Loss: 0.7922, Val Acc: 0.6280, Test Acc: 0.6053\n",
      "Seed: 43, Epoch: 021, Loss: 0.7814, Val Acc: 0.6320, Test Acc: 0.6013\n",
      "Seed: 43, Epoch: 022, Loss: 0.7655, Val Acc: 0.6347, Test Acc: 0.6000\n",
      "Seed: 43, Epoch: 023, Loss: 0.7461, Val Acc: 0.6347, Test Acc: 0.6013\n",
      "Seed: 43, Epoch: 024, Loss: 0.7253, Val Acc: 0.6520, Test Acc: 0.6253\n",
      "Seed: 43, Epoch: 025, Loss: 0.7070, Val Acc: 0.6533, Test Acc: 0.6293\n",
      "Seed: 43, Epoch: 026, Loss: 0.6901, Val Acc: 0.6453, Test Acc: 0.6200\n",
      "Seed: 43, Epoch: 027, Loss: 0.6750, Val Acc: 0.6613, Test Acc: 0.6413\n",
      "Seed: 43, Epoch: 028, Loss: 0.6617, Val Acc: 0.6627, Test Acc: 0.6253\n",
      "Seed: 43, Epoch: 029, Loss: 0.6487, Val Acc: 0.6747, Test Acc: 0.6680\n",
      "Seed: 43, Epoch: 030, Loss: 0.6359, Val Acc: 0.7040, Test Acc: 0.7013\n",
      "Seed: 43, Epoch: 031, Loss: 0.6233, Val Acc: 0.7493, Test Acc: 0.7333\n",
      "Seed: 43, Epoch: 032, Loss: 0.6105, Val Acc: 0.7613, Test Acc: 0.7387\n",
      "Seed: 43, Epoch: 033, Loss: 0.5929, Val Acc: 0.7653, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 034, Loss: 0.5801, Val Acc: 0.7667, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 035, Loss: 0.5691, Val Acc: 0.7613, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 036, Loss: 0.5557, Val Acc: 0.7667, Test Acc: 0.7493\n",
      "Seed: 43, Epoch: 037, Loss: 0.5433, Val Acc: 0.7627, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 038, Loss: 0.5336, Val Acc: 0.7653, Test Acc: 0.7427\n",
      "Seed: 43, Epoch: 039, Loss: 0.5293, Val Acc: 0.7720, Test Acc: 0.7493\n",
      "Seed: 43, Epoch: 040, Loss: 0.5260, Val Acc: 0.7653, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 041, Loss: 0.5234, Val Acc: 0.7587, Test Acc: 0.7480\n",
      "Seed: 43, Epoch: 042, Loss: 0.5183, Val Acc: 0.7613, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 043, Loss: 0.5166, Val Acc: 0.7680, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 044, Loss: 0.5123, Val Acc: 0.7627, Test Acc: 0.7400\n",
      "Seed: 43, Epoch: 045, Loss: 0.5090, Val Acc: 0.7667, Test Acc: 0.7480\n",
      "Seed: 43, Epoch: 046, Loss: 0.5110, Val Acc: 0.7627, Test Acc: 0.7493\n",
      "Seed: 43, Epoch: 047, Loss: 0.5063, Val Acc: 0.7640, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 048, Loss: 0.5006, Val Acc: 0.7667, Test Acc: 0.7440\n",
      "Seed: 43, Epoch: 049, Loss: 0.4993, Val Acc: 0.7613, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 050, Loss: 0.5028, Val Acc: 0.7653, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 051, Loss: 0.4975, Val Acc: 0.7720, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 052, Loss: 0.4934, Val Acc: 0.7587, Test Acc: 0.7440\n",
      "Seed: 43, Epoch: 053, Loss: 0.4907, Val Acc: 0.7667, Test Acc: 0.7467\n",
      "Seed: 43, Epoch: 054, Loss: 0.4881, Val Acc: 0.7680, Test Acc: 0.7560\n",
      "Seed: 43, Epoch: 055, Loss: 0.4868, Val Acc: 0.7707, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 056, Loss: 0.4833, Val Acc: 0.7627, Test Acc: 0.7453\n",
      "Seed: 43, Epoch: 057, Loss: 0.4816, Val Acc: 0.7720, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 058, Loss: 0.4805, Val Acc: 0.7680, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 059, Loss: 0.4795, Val Acc: 0.7693, Test Acc: 0.7480\n",
      "Seed: 43, Epoch: 060, Loss: 0.4789, Val Acc: 0.7627, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 061, Loss: 0.4761, Val Acc: 0.7680, Test Acc: 0.7520\n",
      "Seed: 43, Epoch: 062, Loss: 0.4737, Val Acc: 0.7720, Test Acc: 0.7533\n",
      "Seed: 43, Epoch: 063, Loss: 0.4717, Val Acc: 0.7733, Test Acc: 0.7507\n",
      "Seed: 43, Epoch: 064, Loss: 0.4737, Val Acc: 0.7693, Test Acc: 0.7613\n",
      "Seed: 43, Epoch: 065, Loss: 0.4681, Val Acc: 0.7680, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 066, Loss: 0.4676, Val Acc: 0.7693, Test Acc: 0.7587\n",
      "Seed: 43, Epoch: 067, Loss: 0.4710, Val Acc: 0.7733, Test Acc: 0.7573\n",
      "Seed: 43, Epoch: 068, Loss: 0.4659, Val Acc: 0.7733, Test Acc: 0.7653\n",
      "Seed: 43, Epoch: 069, Loss: 0.4662, Val Acc: 0.7707, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 070, Loss: 0.4611, Val Acc: 0.7747, Test Acc: 0.7560\n",
      "Seed: 43, Epoch: 071, Loss: 0.4592, Val Acc: 0.7680, Test Acc: 0.7640\n",
      "Seed: 43, Epoch: 072, Loss: 0.4564, Val Acc: 0.7640, Test Acc: 0.7640\n",
      "Seed: 43, Epoch: 073, Loss: 0.4565, Val Acc: 0.7720, Test Acc: 0.7707\n",
      "Seed: 43, Epoch: 074, Loss: 0.4535, Val Acc: 0.7800, Test Acc: 0.7587\n",
      "Seed: 43, Epoch: 075, Loss: 0.4519, Val Acc: 0.7720, Test Acc: 0.7627\n",
      "Seed: 43, Epoch: 076, Loss: 0.4492, Val Acc: 0.7693, Test Acc: 0.7747\n",
      "Seed: 43, Epoch: 077, Loss: 0.4475, Val Acc: 0.7747, Test Acc: 0.7627\n",
      "Seed: 43, Epoch: 078, Loss: 0.4469, Val Acc: 0.7733, Test Acc: 0.7653\n",
      "Seed: 43, Epoch: 079, Loss: 0.4444, Val Acc: 0.7760, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 080, Loss: 0.4420, Val Acc: 0.7720, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 081, Loss: 0.4428, Val Acc: 0.7760, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 082, Loss: 0.4439, Val Acc: 0.7720, Test Acc: 0.7600\n",
      "Seed: 43, Epoch: 083, Loss: 0.4435, Val Acc: 0.7773, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 084, Loss: 0.4371, Val Acc: 0.7827, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 085, Loss: 0.4366, Val Acc: 0.7787, Test Acc: 0.7667\n",
      "Seed: 43, Epoch: 086, Loss: 0.4356, Val Acc: 0.7813, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 087, Loss: 0.4351, Val Acc: 0.7760, Test Acc: 0.7653\n",
      "Seed: 43, Epoch: 088, Loss: 0.4315, Val Acc: 0.7827, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 089, Loss: 0.4306, Val Acc: 0.7760, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 090, Loss: 0.4285, Val Acc: 0.7827, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 091, Loss: 0.4280, Val Acc: 0.7813, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 092, Loss: 0.4267, Val Acc: 0.7773, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 093, Loss: 0.4225, Val Acc: 0.7787, Test Acc: 0.7707\n",
      "Seed: 43, Epoch: 094, Loss: 0.4227, Val Acc: 0.7827, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 095, Loss: 0.4192, Val Acc: 0.7827, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 096, Loss: 0.4189, Val Acc: 0.7800, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 097, Loss: 0.4184, Val Acc: 0.7893, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 098, Loss: 0.4157, Val Acc: 0.7867, Test Acc: 0.7680\n",
      "Seed: 43, Epoch: 099, Loss: 0.4116, Val Acc: 0.7840, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 100, Loss: 0.4120, Val Acc: 0.7907, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 101, Loss: 0.4102, Val Acc: 0.7853, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 102, Loss: 0.4081, Val Acc: 0.7907, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 103, Loss: 0.4064, Val Acc: 0.7853, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 104, Loss: 0.4046, Val Acc: 0.7880, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 105, Loss: 0.4019, Val Acc: 0.7947, Test Acc: 0.7720\n",
      "Seed: 43, Epoch: 106, Loss: 0.4037, Val Acc: 0.7960, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 107, Loss: 0.4021, Val Acc: 0.7947, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 108, Loss: 0.3964, Val Acc: 0.7907, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 109, Loss: 0.3938, Val Acc: 0.7893, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 110, Loss: 0.3939, Val Acc: 0.7960, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 111, Loss: 0.3960, Val Acc: 0.7920, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 112, Loss: 0.3951, Val Acc: 0.7867, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 113, Loss: 0.3958, Val Acc: 0.7960, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 114, Loss: 0.3923, Val Acc: 0.7933, Test Acc: 0.7733\n",
      "Seed: 43, Epoch: 115, Loss: 0.3916, Val Acc: 0.7960, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 116, Loss: 0.3909, Val Acc: 0.7960, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 117, Loss: 0.3901, Val Acc: 0.7933, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 118, Loss: 0.3867, Val Acc: 0.7907, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 119, Loss: 0.3903, Val Acc: 0.7907, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 120, Loss: 0.3863, Val Acc: 0.7893, Test Acc: 0.7653\n",
      "Seed: 43, Epoch: 121, Loss: 0.3893, Val Acc: 0.8013, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 122, Loss: 0.3833, Val Acc: 0.7960, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 123, Loss: 0.3835, Val Acc: 0.7933, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 124, Loss: 0.3809, Val Acc: 0.8120, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 125, Loss: 0.3904, Val Acc: 0.7947, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 126, Loss: 0.3864, Val Acc: 0.8040, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 127, Loss: 0.3779, Val Acc: 0.7987, Test Acc: 0.7747\n",
      "Seed: 43, Epoch: 128, Loss: 0.3747, Val Acc: 0.8013, Test Acc: 0.7760\n",
      "Seed: 43, Epoch: 129, Loss: 0.3731, Val Acc: 0.8040, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 130, Loss: 0.3728, Val Acc: 0.7960, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 131, Loss: 0.3736, Val Acc: 0.8000, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 132, Loss: 0.3713, Val Acc: 0.7947, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 133, Loss: 0.3719, Val Acc: 0.8000, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 134, Loss: 0.3681, Val Acc: 0.7960, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 135, Loss: 0.3655, Val Acc: 0.7987, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 136, Loss: 0.3628, Val Acc: 0.8013, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 137, Loss: 0.3632, Val Acc: 0.7987, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 138, Loss: 0.3629, Val Acc: 0.8053, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 139, Loss: 0.3631, Val Acc: 0.7947, Test Acc: 0.7693\n",
      "Seed: 43, Epoch: 140, Loss: 0.3707, Val Acc: 0.8093, Test Acc: 0.7800\n",
      "Seed: 43, Epoch: 141, Loss: 0.3659, Val Acc: 0.8067, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 142, Loss: 0.3616, Val Acc: 0.7987, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 143, Loss: 0.3602, Val Acc: 0.7947, Test Acc: 0.7773\n",
      "Seed: 43, Epoch: 144, Loss: 0.3628, Val Acc: 0.8040, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 145, Loss: 0.3561, Val Acc: 0.8000, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 146, Loss: 0.3554, Val Acc: 0.8067, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 147, Loss: 0.3524, Val Acc: 0.8000, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 148, Loss: 0.3520, Val Acc: 0.8000, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 149, Loss: 0.3510, Val Acc: 0.8160, Test Acc: 0.7853\n",
      "Seed: 43, Epoch: 150, Loss: 0.3563, Val Acc: 0.8027, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 151, Loss: 0.3506, Val Acc: 0.7960, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 152, Loss: 0.3517, Val Acc: 0.8053, Test Acc: 0.7853\n",
      "Seed: 43, Epoch: 153, Loss: 0.3486, Val Acc: 0.8053, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 154, Loss: 0.3507, Val Acc: 0.7907, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 155, Loss: 0.3484, Val Acc: 0.8067, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 156, Loss: 0.3488, Val Acc: 0.8133, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 157, Loss: 0.3457, Val Acc: 0.7987, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 158, Loss: 0.3494, Val Acc: 0.8093, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 159, Loss: 0.3442, Val Acc: 0.8093, Test Acc: 0.7853\n",
      "Seed: 43, Epoch: 160, Loss: 0.3455, Val Acc: 0.8040, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 161, Loss: 0.3428, Val Acc: 0.7933, Test Acc: 0.7840\n",
      "Seed: 43, Epoch: 162, Loss: 0.3426, Val Acc: 0.8080, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 163, Loss: 0.3373, Val Acc: 0.8107, Test Acc: 0.7947\n",
      "Seed: 43, Epoch: 164, Loss: 0.3353, Val Acc: 0.8107, Test Acc: 0.7920\n",
      "Seed: 43, Epoch: 165, Loss: 0.3388, Val Acc: 0.8067, Test Acc: 0.7960\n",
      "Seed: 43, Epoch: 166, Loss: 0.3357, Val Acc: 0.8093, Test Acc: 0.7987\n",
      "Seed: 43, Epoch: 167, Loss: 0.3340, Val Acc: 0.8080, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 168, Loss: 0.3331, Val Acc: 0.8080, Test Acc: 0.7920\n",
      "Seed: 43, Epoch: 169, Loss: 0.3327, Val Acc: 0.8120, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 170, Loss: 0.3298, Val Acc: 0.8160, Test Acc: 0.8000\n",
      "Seed: 43, Epoch: 171, Loss: 0.3305, Val Acc: 0.8133, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 172, Loss: 0.3322, Val Acc: 0.8107, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 173, Loss: 0.3312, Val Acc: 0.8120, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 174, Loss: 0.3279, Val Acc: 0.8120, Test Acc: 0.7987\n",
      "Seed: 43, Epoch: 175, Loss: 0.3279, Val Acc: 0.7987, Test Acc: 0.7933\n",
      "Seed: 43, Epoch: 176, Loss: 0.3337, Val Acc: 0.8080, Test Acc: 0.7960\n",
      "Seed: 43, Epoch: 177, Loss: 0.3350, Val Acc: 0.8053, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 178, Loss: 0.3325, Val Acc: 0.8080, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 179, Loss: 0.3251, Val Acc: 0.8107, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 180, Loss: 0.3229, Val Acc: 0.8147, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 181, Loss: 0.3228, Val Acc: 0.8107, Test Acc: 0.7960\n",
      "Seed: 43, Epoch: 182, Loss: 0.3218, Val Acc: 0.8107, Test Acc: 0.7880\n",
      "Seed: 43, Epoch: 183, Loss: 0.3194, Val Acc: 0.8107, Test Acc: 0.7853\n",
      "Seed: 43, Epoch: 184, Loss: 0.3190, Val Acc: 0.8093, Test Acc: 0.7987\n",
      "Seed: 43, Epoch: 185, Loss: 0.3204, Val Acc: 0.8067, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 186, Loss: 0.3203, Val Acc: 0.8133, Test Acc: 0.7787\n",
      "Seed: 43, Epoch: 187, Loss: 0.3172, Val Acc: 0.8147, Test Acc: 0.7827\n",
      "Seed: 43, Epoch: 188, Loss: 0.3217, Val Acc: 0.8040, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 189, Loss: 0.3288, Val Acc: 0.8173, Test Acc: 0.7813\n",
      "Seed: 43, Epoch: 190, Loss: 0.3203, Val Acc: 0.8133, Test Acc: 0.7920\n",
      "Seed: 43, Epoch: 191, Loss: 0.3220, Val Acc: 0.8107, Test Acc: 0.7933\n",
      "Seed: 43, Epoch: 192, Loss: 0.3142, Val Acc: 0.8027, Test Acc: 0.7907\n",
      "Seed: 43, Epoch: 193, Loss: 0.3213, Val Acc: 0.8120, Test Acc: 0.8067\n",
      "Seed: 43, Epoch: 194, Loss: 0.3144, Val Acc: 0.8133, Test Acc: 0.8093\n",
      "Seed: 43, Epoch: 195, Loss: 0.3126, Val Acc: 0.8133, Test Acc: 0.7853\n",
      "Seed: 43, Epoch: 196, Loss: 0.3085, Val Acc: 0.8173, Test Acc: 0.7867\n",
      "Seed: 43, Epoch: 197, Loss: 0.3085, Val Acc: 0.8093, Test Acc: 0.7973\n",
      "Seed: 43, Epoch: 198, Loss: 0.3103, Val Acc: 0.8107, Test Acc: 0.7893\n",
      "Seed: 43, Epoch: 199, Loss: 0.3074, Val Acc: 0.8053, Test Acc: 0.7987\n",
      "Seed: 43, Epoch: 200, Loss: 0.3155, Val Acc: 0.8160, Test Acc: 0.7960\n",
      "Seed: 44, Epoch: 001, Loss: 1.1059, Val Acc: 0.1493, Test Acc: 0.1600\n",
      "Seed: 44, Epoch: 002, Loss: 1.0956, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 003, Loss: 1.0850, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 004, Loss: 1.0745, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 005, Loss: 1.0636, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 006, Loss: 1.0499, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 007, Loss: 1.0288, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 008, Loss: 0.9888, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 009, Loss: 0.9393, Val Acc: 0.4947, Test Acc: 0.5080\n",
      "Seed: 44, Epoch: 010, Loss: 0.9033, Val Acc: 0.5587, Test Acc: 0.5867\n",
      "Seed: 44, Epoch: 011, Loss: 0.8757, Val Acc: 0.5933, Test Acc: 0.6013\n",
      "Seed: 44, Epoch: 012, Loss: 0.8461, Val Acc: 0.6040, Test Acc: 0.6133\n",
      "Seed: 44, Epoch: 013, Loss: 0.8203, Val Acc: 0.6347, Test Acc: 0.6160\n",
      "Seed: 44, Epoch: 014, Loss: 0.7972, Val Acc: 0.6467, Test Acc: 0.6320\n",
      "Seed: 44, Epoch: 015, Loss: 0.7730, Val Acc: 0.6600, Test Acc: 0.6427\n",
      "Seed: 44, Epoch: 016, Loss: 0.7519, Val Acc: 0.6893, Test Acc: 0.6640\n",
      "Seed: 44, Epoch: 017, Loss: 0.7312, Val Acc: 0.6933, Test Acc: 0.6733\n",
      "Seed: 44, Epoch: 018, Loss: 0.7099, Val Acc: 0.7040, Test Acc: 0.6827\n",
      "Seed: 44, Epoch: 019, Loss: 0.6929, Val Acc: 0.7013, Test Acc: 0.6947\n",
      "Seed: 44, Epoch: 020, Loss: 0.6760, Val Acc: 0.6933, Test Acc: 0.6853\n",
      "Seed: 44, Epoch: 021, Loss: 0.6611, Val Acc: 0.6907, Test Acc: 0.6893\n",
      "Seed: 44, Epoch: 022, Loss: 0.6466, Val Acc: 0.7000, Test Acc: 0.6987\n",
      "Seed: 44, Epoch: 023, Loss: 0.6321, Val Acc: 0.6907, Test Acc: 0.6893\n",
      "Seed: 44, Epoch: 024, Loss: 0.6187, Val Acc: 0.7080, Test Acc: 0.6987\n",
      "Seed: 44, Epoch: 025, Loss: 0.6058, Val Acc: 0.6973, Test Acc: 0.7040\n",
      "Seed: 44, Epoch: 026, Loss: 0.5921, Val Acc: 0.7000, Test Acc: 0.7040\n",
      "Seed: 44, Epoch: 027, Loss: 0.5831, Val Acc: 0.7133, Test Acc: 0.7000\n",
      "Seed: 44, Epoch: 028, Loss: 0.5732, Val Acc: 0.7053, Test Acc: 0.7053\n",
      "Seed: 44, Epoch: 029, Loss: 0.5688, Val Acc: 0.7080, Test Acc: 0.6973\n",
      "Seed: 44, Epoch: 030, Loss: 0.5641, Val Acc: 0.7080, Test Acc: 0.6987\n",
      "Seed: 44, Epoch: 031, Loss: 0.5551, Val Acc: 0.7080, Test Acc: 0.6987\n",
      "Seed: 44, Epoch: 032, Loss: 0.5503, Val Acc: 0.7107, Test Acc: 0.6973\n",
      "Seed: 44, Epoch: 033, Loss: 0.5465, Val Acc: 0.7133, Test Acc: 0.6987\n",
      "Seed: 44, Epoch: 034, Loss: 0.5407, Val Acc: 0.7173, Test Acc: 0.7013\n",
      "Seed: 44, Epoch: 035, Loss: 0.5380, Val Acc: 0.7320, Test Acc: 0.7067\n",
      "Seed: 44, Epoch: 036, Loss: 0.5369, Val Acc: 0.7493, Test Acc: 0.7240\n",
      "Seed: 44, Epoch: 037, Loss: 0.5291, Val Acc: 0.7480, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 038, Loss: 0.5270, Val Acc: 0.7467, Test Acc: 0.7493\n",
      "Seed: 44, Epoch: 039, Loss: 0.5240, Val Acc: 0.7413, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 040, Loss: 0.5220, Val Acc: 0.7400, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 041, Loss: 0.5217, Val Acc: 0.7480, Test Acc: 0.7520\n",
      "Seed: 44, Epoch: 042, Loss: 0.5206, Val Acc: 0.7427, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 043, Loss: 0.5210, Val Acc: 0.7507, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 044, Loss: 0.5180, Val Acc: 0.7467, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 045, Loss: 0.5172, Val Acc: 0.7467, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 046, Loss: 0.5157, Val Acc: 0.7467, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 047, Loss: 0.5110, Val Acc: 0.7533, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 048, Loss: 0.5101, Val Acc: 0.7467, Test Acc: 0.7493\n",
      "Seed: 44, Epoch: 049, Loss: 0.5109, Val Acc: 0.7507, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 050, Loss: 0.5083, Val Acc: 0.7480, Test Acc: 0.7480\n",
      "Seed: 44, Epoch: 051, Loss: 0.5069, Val Acc: 0.7467, Test Acc: 0.7480\n",
      "Seed: 44, Epoch: 052, Loss: 0.5067, Val Acc: 0.7467, Test Acc: 0.7427\n",
      "Seed: 44, Epoch: 053, Loss: 0.5057, Val Acc: 0.7453, Test Acc: 0.7507\n",
      "Seed: 44, Epoch: 054, Loss: 0.5049, Val Acc: 0.7480, Test Acc: 0.7493\n",
      "Seed: 44, Epoch: 055, Loss: 0.5019, Val Acc: 0.7360, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 056, Loss: 0.5019, Val Acc: 0.7400, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 057, Loss: 0.5009, Val Acc: 0.7440, Test Acc: 0.7480\n",
      "Seed: 44, Epoch: 058, Loss: 0.5001, Val Acc: 0.7413, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 059, Loss: 0.4985, Val Acc: 0.7373, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 060, Loss: 0.4988, Val Acc: 0.7373, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 061, Loss: 0.4968, Val Acc: 0.7440, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 062, Loss: 0.4959, Val Acc: 0.7400, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 063, Loss: 0.4927, Val Acc: 0.7387, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 064, Loss: 0.4970, Val Acc: 0.7467, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 065, Loss: 0.4951, Val Acc: 0.7387, Test Acc: 0.7400\n",
      "Seed: 44, Epoch: 066, Loss: 0.4903, Val Acc: 0.7453, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 067, Loss: 0.4903, Val Acc: 0.7360, Test Acc: 0.7440\n",
      "Seed: 44, Epoch: 068, Loss: 0.4896, Val Acc: 0.7400, Test Acc: 0.7520\n",
      "Seed: 44, Epoch: 069, Loss: 0.4866, Val Acc: 0.7373, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 070, Loss: 0.4865, Val Acc: 0.7467, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 071, Loss: 0.4845, Val Acc: 0.7387, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 072, Loss: 0.4849, Val Acc: 0.7400, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 073, Loss: 0.4823, Val Acc: 0.7440, Test Acc: 0.7507\n",
      "Seed: 44, Epoch: 074, Loss: 0.4818, Val Acc: 0.7413, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 075, Loss: 0.4810, Val Acc: 0.7440, Test Acc: 0.7493\n",
      "Seed: 44, Epoch: 076, Loss: 0.4792, Val Acc: 0.7387, Test Acc: 0.7493\n",
      "Seed: 44, Epoch: 077, Loss: 0.4808, Val Acc: 0.7440, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 078, Loss: 0.4797, Val Acc: 0.7427, Test Acc: 0.7520\n",
      "Seed: 44, Epoch: 079, Loss: 0.4772, Val Acc: 0.7440, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 080, Loss: 0.4762, Val Acc: 0.7467, Test Acc: 0.7613\n",
      "Seed: 44, Epoch: 081, Loss: 0.4757, Val Acc: 0.7453, Test Acc: 0.7560\n",
      "Seed: 44, Epoch: 082, Loss: 0.4732, Val Acc: 0.7413, Test Acc: 0.7520\n",
      "Seed: 44, Epoch: 083, Loss: 0.4744, Val Acc: 0.7467, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 084, Loss: 0.4726, Val Acc: 0.7507, Test Acc: 0.7640\n",
      "Seed: 44, Epoch: 085, Loss: 0.4717, Val Acc: 0.7480, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 086, Loss: 0.4716, Val Acc: 0.7507, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 087, Loss: 0.4688, Val Acc: 0.7493, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 088, Loss: 0.4692, Val Acc: 0.7507, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 089, Loss: 0.4685, Val Acc: 0.7520, Test Acc: 0.7680\n",
      "Seed: 44, Epoch: 090, Loss: 0.4663, Val Acc: 0.7520, Test Acc: 0.7680\n",
      "Seed: 44, Epoch: 091, Loss: 0.4653, Val Acc: 0.7507, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 092, Loss: 0.4647, Val Acc: 0.7507, Test Acc: 0.7640\n",
      "Seed: 44, Epoch: 093, Loss: 0.4642, Val Acc: 0.7520, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 094, Loss: 0.4639, Val Acc: 0.7533, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 095, Loss: 0.4620, Val Acc: 0.7480, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 096, Loss: 0.4623, Val Acc: 0.7507, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 097, Loss: 0.4605, Val Acc: 0.7533, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 098, Loss: 0.4576, Val Acc: 0.7533, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 099, Loss: 0.4595, Val Acc: 0.7587, Test Acc: 0.7560\n",
      "Seed: 44, Epoch: 100, Loss: 0.4598, Val Acc: 0.7533, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 101, Loss: 0.4601, Val Acc: 0.7560, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 102, Loss: 0.4579, Val Acc: 0.7600, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 103, Loss: 0.4600, Val Acc: 0.7560, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 104, Loss: 0.4579, Val Acc: 0.7587, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 105, Loss: 0.4562, Val Acc: 0.7627, Test Acc: 0.7667\n",
      "Seed: 44, Epoch: 106, Loss: 0.4583, Val Acc: 0.7547, Test Acc: 0.7613\n",
      "Seed: 44, Epoch: 107, Loss: 0.4538, Val Acc: 0.7653, Test Acc: 0.7680\n",
      "Seed: 44, Epoch: 108, Loss: 0.4512, Val Acc: 0.7587, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 109, Loss: 0.4495, Val Acc: 0.7613, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 110, Loss: 0.4466, Val Acc: 0.7547, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 111, Loss: 0.4462, Val Acc: 0.7613, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 112, Loss: 0.4451, Val Acc: 0.7653, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 113, Loss: 0.4436, Val Acc: 0.7520, Test Acc: 0.7547\n",
      "Seed: 44, Epoch: 114, Loss: 0.4449, Val Acc: 0.7680, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 115, Loss: 0.4405, Val Acc: 0.7533, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 116, Loss: 0.4411, Val Acc: 0.7560, Test Acc: 0.7653\n",
      "Seed: 44, Epoch: 117, Loss: 0.4383, Val Acc: 0.7627, Test Acc: 0.7560\n",
      "Seed: 44, Epoch: 118, Loss: 0.4344, Val Acc: 0.7547, Test Acc: 0.7600\n",
      "Seed: 44, Epoch: 119, Loss: 0.4373, Val Acc: 0.7600, Test Acc: 0.7533\n",
      "Seed: 44, Epoch: 120, Loss: 0.4335, Val Acc: 0.7627, Test Acc: 0.7613\n",
      "Seed: 44, Epoch: 121, Loss: 0.4319, Val Acc: 0.7440, Test Acc: 0.7453\n",
      "Seed: 44, Epoch: 122, Loss: 0.4285, Val Acc: 0.7653, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 123, Loss: 0.4277, Val Acc: 0.7667, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 124, Loss: 0.4262, Val Acc: 0.7440, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 125, Loss: 0.4235, Val Acc: 0.7680, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 126, Loss: 0.4216, Val Acc: 0.7653, Test Acc: 0.7640\n",
      "Seed: 44, Epoch: 127, Loss: 0.4194, Val Acc: 0.7493, Test Acc: 0.7467\n",
      "Seed: 44, Epoch: 128, Loss: 0.4216, Val Acc: 0.7707, Test Acc: 0.7653\n",
      "Seed: 44, Epoch: 129, Loss: 0.4199, Val Acc: 0.7667, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 130, Loss: 0.4233, Val Acc: 0.7493, Test Acc: 0.7520\n",
      "Seed: 44, Epoch: 131, Loss: 0.4182, Val Acc: 0.7760, Test Acc: 0.7653\n",
      "Seed: 44, Epoch: 132, Loss: 0.4169, Val Acc: 0.7747, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 133, Loss: 0.4128, Val Acc: 0.7533, Test Acc: 0.7627\n",
      "Seed: 44, Epoch: 134, Loss: 0.4135, Val Acc: 0.7773, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 135, Loss: 0.4148, Val Acc: 0.7600, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 136, Loss: 0.4148, Val Acc: 0.7627, Test Acc: 0.7680\n",
      "Seed: 44, Epoch: 137, Loss: 0.4137, Val Acc: 0.7667, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 138, Loss: 0.4107, Val Acc: 0.7693, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 139, Loss: 0.4052, Val Acc: 0.7680, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 140, Loss: 0.4031, Val Acc: 0.7733, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 141, Loss: 0.4033, Val Acc: 0.7667, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 142, Loss: 0.3988, Val Acc: 0.7707, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 143, Loss: 0.3986, Val Acc: 0.7720, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 144, Loss: 0.3976, Val Acc: 0.7640, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 145, Loss: 0.3953, Val Acc: 0.7747, Test Acc: 0.7907\n",
      "Seed: 44, Epoch: 146, Loss: 0.3956, Val Acc: 0.7520, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 147, Loss: 0.3935, Val Acc: 0.7720, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 148, Loss: 0.3945, Val Acc: 0.7667, Test Acc: 0.7773\n",
      "Seed: 44, Epoch: 149, Loss: 0.3917, Val Acc: 0.7693, Test Acc: 0.7667\n",
      "Seed: 44, Epoch: 150, Loss: 0.3933, Val Acc: 0.7693, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 151, Loss: 0.3913, Val Acc: 0.7773, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 152, Loss: 0.3910, Val Acc: 0.7587, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 153, Loss: 0.3891, Val Acc: 0.7587, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 154, Loss: 0.3879, Val Acc: 0.7787, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 155, Loss: 0.3858, Val Acc: 0.7733, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 156, Loss: 0.3856, Val Acc: 0.7560, Test Acc: 0.7693\n",
      "Seed: 44, Epoch: 157, Loss: 0.3828, Val Acc: 0.7587, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 158, Loss: 0.3857, Val Acc: 0.7720, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 159, Loss: 0.3825, Val Acc: 0.7680, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 160, Loss: 0.3804, Val Acc: 0.7613, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 161, Loss: 0.3801, Val Acc: 0.7627, Test Acc: 0.7587\n",
      "Seed: 44, Epoch: 162, Loss: 0.3850, Val Acc: 0.7693, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 163, Loss: 0.3791, Val Acc: 0.7707, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 164, Loss: 0.3788, Val Acc: 0.7587, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 165, Loss: 0.3740, Val Acc: 0.7640, Test Acc: 0.7800\n",
      "Seed: 44, Epoch: 166, Loss: 0.3699, Val Acc: 0.7627, Test Acc: 0.7773\n",
      "Seed: 44, Epoch: 167, Loss: 0.3716, Val Acc: 0.7587, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 168, Loss: 0.3704, Val Acc: 0.7733, Test Acc: 0.7840\n",
      "Seed: 44, Epoch: 169, Loss: 0.3710, Val Acc: 0.7600, Test Acc: 0.7573\n",
      "Seed: 44, Epoch: 170, Loss: 0.3817, Val Acc: 0.7693, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 171, Loss: 0.3694, Val Acc: 0.7653, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 172, Loss: 0.3654, Val Acc: 0.7773, Test Acc: 0.7773\n",
      "Seed: 44, Epoch: 173, Loss: 0.3702, Val Acc: 0.7760, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 174, Loss: 0.3717, Val Acc: 0.7680, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 175, Loss: 0.3698, Val Acc: 0.7760, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 176, Loss: 0.3637, Val Acc: 0.7600, Test Acc: 0.7720\n",
      "Seed: 44, Epoch: 177, Loss: 0.3623, Val Acc: 0.7653, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 178, Loss: 0.3579, Val Acc: 0.7747, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 179, Loss: 0.3601, Val Acc: 0.7693, Test Acc: 0.7760\n",
      "Seed: 44, Epoch: 180, Loss: 0.3572, Val Acc: 0.7667, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 181, Loss: 0.3552, Val Acc: 0.7693, Test Acc: 0.7760\n",
      "Seed: 44, Epoch: 182, Loss: 0.3619, Val Acc: 0.7640, Test Acc: 0.7773\n",
      "Seed: 44, Epoch: 183, Loss: 0.3654, Val Acc: 0.7800, Test Acc: 0.7773\n",
      "Seed: 44, Epoch: 184, Loss: 0.3558, Val Acc: 0.7867, Test Acc: 0.7853\n",
      "Seed: 44, Epoch: 185, Loss: 0.3533, Val Acc: 0.7693, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 186, Loss: 0.3535, Val Acc: 0.7733, Test Acc: 0.7773\n",
      "Seed: 44, Epoch: 187, Loss: 0.3541, Val Acc: 0.7760, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 188, Loss: 0.3514, Val Acc: 0.7800, Test Acc: 0.7760\n",
      "Seed: 44, Epoch: 189, Loss: 0.3484, Val Acc: 0.7760, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 190, Loss: 0.3470, Val Acc: 0.7760, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 191, Loss: 0.3483, Val Acc: 0.7680, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 192, Loss: 0.3544, Val Acc: 0.7840, Test Acc: 0.7707\n",
      "Seed: 44, Epoch: 193, Loss: 0.3532, Val Acc: 0.7787, Test Acc: 0.7653\n",
      "Seed: 44, Epoch: 194, Loss: 0.3501, Val Acc: 0.7813, Test Acc: 0.7773\n",
      "Seed: 44, Epoch: 195, Loss: 0.3441, Val Acc: 0.7760, Test Acc: 0.7813\n",
      "Seed: 44, Epoch: 196, Loss: 0.3443, Val Acc: 0.7800, Test Acc: 0.7787\n",
      "Seed: 44, Epoch: 197, Loss: 0.3418, Val Acc: 0.7827, Test Acc: 0.7827\n",
      "Seed: 44, Epoch: 198, Loss: 0.3392, Val Acc: 0.7827, Test Acc: 0.7733\n",
      "Seed: 44, Epoch: 199, Loss: 0.3385, Val Acc: 0.7867, Test Acc: 0.7747\n",
      "Seed: 44, Epoch: 200, Loss: 0.3386, Val Acc: 0.7813, Test Acc: 0.7773\n",
      "Average Time: 547.72 seconds\n",
      "Var Time: 13027.72 seconds\n",
      "Average Memory: 10768.67 MB\n",
      "Average Best Val Acc: 0.8009\n",
      "Std Best Test Acc: 0.0211\n",
      "Average Test Acc: 0.7684\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import TUDataset\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "data_path = \"/data/Zeyu/Pooling\"\n",
    "\n",
    "dataset_sparse = TUDataset(root=data_path, name=\"COLLAB\", transform=T.Compose([T.OneHotDegree(491)]), use_node_attr=True)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, ASAPooling\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.transforms import ToUndirected\n",
    "from torch.nn import Linear\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "from torch_geometric.nn import BatchNorm\n",
    "\n",
    "class HierarchicalGCN_GSA(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_classes):\n",
    "        super(HierarchicalGCN_GSA, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool1 = GSAPool(64, pooling_ratio=0.9, alpha = 0.7, cus_drop_ratio = 0)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.pool2 = GSAPool(64, pooling_ratio=0.9, alpha = 0.7, cus_drop_ratio = 0)\n",
    "        self.conv3 = GCNConv(hidden_channels, out_channels)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(out_channels, 32)\n",
    "        self.lin2 = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GCN and pooling layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn1(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool1(x, edge_index, None, batch)\n",
    "\n",
    "        # Second GCN and pooling layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn2(x)\n",
    "        x, edge_index, _, batch, perm_1, x_ae1 = self.pool2(x, edge_index, None, batch)\n",
    "\n",
    "        # Third GCN layer\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = self.bn3(x)\n",
    "\n",
    "        # Mean pooling over the nodes\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "num_classes = dataset_sparse.num_classes\n",
    "in_channels = dataset_sparse.num_features\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += (pred == data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seeds = [42, 43, 44]\n",
    "times = []\n",
    "memories = []\n",
    "best_val_accs = []\n",
    "best_test_accs = []\n",
    "\n",
    "early_stop_patience = 150\n",
    "tolerance = 0.0001\n",
    "\n",
    "for seed in seeds:\n",
    "    set_seed(seed)\n",
    "\n",
    "    dataset_sparse = dataset_sparse.shuffle()\n",
    "\n",
    "    train_ratio = 0.7\n",
    "    val_ratio = 0.15\n",
    "    val_ratio = 0.15\n",
    "    # Calculate the sizes of each subset\n",
    "    num_total = len(dataset_sparse)\n",
    "    num_train = int(num_total * train_ratio)\n",
    "    num_val = int(num_total * val_ratio)\n",
    "    num_test = num_total - num_train - num_val\n",
    "    train_dataset = dataset_sparse[:num_train]\n",
    "    val_dataset = dataset_sparse[num_train:num_train + num_val]\n",
    "    test_dataset = dataset_sparse[num_train + num_val:]\n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    model = HierarchicalGCN_GSA(in_channels=dataset_sparse.num_features, hidden_channels=64,out_channels=64, num_classes=dataset_sparse.num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_val_acc = 0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        val_acc = test(valid_loader)\n",
    "        test_acc = test(test_loader)\n",
    "        if val_acc > best_val_acc + tolerance:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f'Seed: {seed}, Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(f'Early stopping at epoch {epoch} for seed {seed}')\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    memory_allocated = torch.cuda.memory_reserved(device) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    times.append(total_time)\n",
    "    memories.append(memory_allocated)\n",
    "    best_val_accs.append(best_val_acc)\n",
    "    best_test_accs.append(best_test_acc)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'Average Time: {np.mean(times):.2f} seconds')\n",
    "print(f'Var Time: {np.var(times):.2f} seconds')\n",
    "print(f'Average Memory: {np.mean(memories):.2f} MB')\n",
    "print(f'Average Best Val Acc: {np.mean(best_val_accs):.4f}')\n",
    "print(f'Std Best Test Acc: {np.std(best_test_accs):.4f}')\n",
    "print(f'Average Test Acc: {np.mean(best_test_accs):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 139, in <module>\n",
      "    trainer = Trainer(args, model, device)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 24, in __init__\n",
      "    self.model = model.to(device)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 139, in <module>\n",
      "    trainer = Trainer(args, model, device)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 24, in __init__\n",
      "    self.model = model.to(device)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 139, in <module>\n",
      "    trainer = Trainer(args, model, device)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 24, in __init__\n",
      "    self.model = model.to(device)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 139, in <module>\n",
      "    trainer = Trainer(args, model, device)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 24, in __init__\n",
      "    self.model = model.to(device)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/qm7.pt\n",
      "\n",
      "Current dataset: qm7, include 6832 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 139, in <module>\n",
      "    trainer = Trainer(args, model, device)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 24, in __init__\n",
      "    self.model = model.to(device)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1160, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 810, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 833, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1158, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "RuntimeError: CUDA error: invalid device ordinal\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 2 --run_times=5 --patience=150 --epochs=500 --gsa_ratio=0.1 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 2 --run_times=5 --patience=150 --epochs=500 --gsa_ratio=0.3 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 2 --run_times=5 --patience=150 --epochs=500 --gsa_ratio=0.5 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 2 --run_times=5 --patience=150 --epochs=500 --gsa_ratio=0.7 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm7 --cuda_num 2 --run_times=5 --patience=150 --epochs=500 --gsa_ratio=0.9 --pooling='GSA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QM8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=qm8 --run_times=5 --patience=10 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=bace --cuda_num -1 --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existed dataset loaded: datasets/processed/esol.pt\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.8914 RMSE=1.6965 RMSE=1.6109 RMSE=1.5297 RMSE=1.5380 RMSE=1.5551 RMSE=1.6278 RMSE=1.4268 ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 140, in <module>\n",
      "    _, metric_te = trainer.fit_and_test(loader_tr[i], loader_va[i], loader_te[i], log_train_results=args.log_train_results,\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 137, in _train_test_reg\n",
      "    pred = self.model(data)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/models/baseline.py\", line 154, in forward\n",
      "    embds = self.GNN(data)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/models/baseline.py\", line 136, in GNN\n",
      "    embd = F.relu(self.bns_embd[i](embd))\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/functional.py\", line 1471, in relu\n",
      "    result = torch.relu(input)\n",
      "KeyboardInterrupt\n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/esol.pt\n",
      "\n",
      "Current dataset: esol, include 1127 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=1.6204 "
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.1 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.3 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.5 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.7 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python -W ignore /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=esol --cuda_num 2 --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.9 --pooling='GSA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freesolv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++0.1++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.0411 RMSE=4.5047 RMSE=3.6504 RMSE=2.7286 RMSE=2.5968 RMSE=2.2729 RMSE=2.0802 RMSE=1.9278 RMSE=1.7044 Epoch: 10/150RMSE=1.8732 RMSE=1.8231 RMSE=1.7190 RMSE=2.0828 RMSE=1.8115 RMSE=1.7359 RMSE=1.8679 RMSE=1.8224 RMSE=1.8846 RMSE=1.8286 Epoch: 20/150RMSE=1.8557 RMSE=1.8984 RMSE=1.8940 RMSE=1.9062 RMSE=1.9206 RMSE=1.9904 RMSE=1.9730 RMSE=1.9646 RMSE=1.9732 RMSE=1.9860 RMSE=2.9039 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 2.904 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.1722 RMSE=4.7161 RMSE=3.4686 RMSE=2.5562 RMSE=2.0097 RMSE=1.8680 RMSE=1.9896 RMSE=2.0792 RMSE=1.8554 Epoch: 10/150RMSE=1.9647 RMSE=2.0001 RMSE=2.0051 RMSE=1.9176 RMSE=1.8042 RMSE=1.7982 RMSE=1.8105 RMSE=1.8067 RMSE=1.8765 RMSE=1.7830 Epoch: 20/150RMSE=1.8575 RMSE=1.7024 RMSE=1.6727 RMSE=1.7833 RMSE=1.6711 RMSE=1.7957 RMSE=1.7705 RMSE=1.7269 RMSE=1.6765 RMSE=1.6502 Epoch: 30/150RMSE=1.6399 RMSE=1.6339 RMSE=1.6662 RMSE=1.6920 RMSE=1.7315 RMSE=1.7464 RMSE=1.7304 RMSE=1.6850 RMSE=1.6563 RMSE=1.6673 Epoch: 40/150RMSE=1.6773 RMSE=1.6585 RMSE=1.6785 RMSE=1.6538 RMSE=1.6515 RMSE=1.6475 RMSE=1.6966 RMSE=1.7031 RMSE=1.6813 RMSE=1.6915 Epoch: 50/150RMSE=1.6975 RMSE=1.7095 RMSE=2.6745 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 2.789 +/- 0.115\n",
      "\n",
      "Epoch: 1/150RMSE=5.2499 RMSE=4.9866 RMSE=4.1762 RMSE=3.5469 RMSE=3.0303 RMSE=2.7147 RMSE=2.7921 RMSE=2.3867 RMSE=2.9289 Epoch: 10/150RMSE=2.8345 RMSE=2.6112 RMSE=2.6845 RMSE=2.6749 RMSE=2.6633 RMSE=2.5733 RMSE=2.6734 RMSE=2.6842 RMSE=2.6903 RMSE=2.7267 Epoch: 20/150RMSE=2.7245 RMSE=2.7343 RMSE=2.7287 RMSE=2.7028 RMSE=2.6892 RMSE=2.7353 RMSE=2.7575 RMSE=2.7368 RMSE=2.7203 RMSE=3.5606 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 3.046 +/- 0.376\n",
      "\n",
      "Epoch: 1/150RMSE=5.2664 RMSE=4.9021 RMSE=4.0065 RMSE=3.5656 RMSE=2.8602 RMSE=2.3433 RMSE=2.4571 RMSE=2.2449 RMSE=2.4796 Epoch: 10/150RMSE=2.3853 RMSE=2.1152 RMSE=2.3509 RMSE=1.8709 RMSE=1.9783 RMSE=2.0662 RMSE=2.0133 RMSE=2.2504 RMSE=1.9863 RMSE=1.8012 Epoch: 20/150RMSE=2.0167 RMSE=1.9849 RMSE=2.0937 RMSE=2.0534 RMSE=2.0750 RMSE=2.0282 RMSE=1.9563 RMSE=2.0403 RMSE=2.0733 RMSE=2.0797 Epoch: 30/150RMSE=2.0838 RMSE=2.1063 RMSE=2.1070 RMSE=2.0910 RMSE=2.0734 RMSE=2.0820 RMSE=2.0900 RMSE=2.0906 RMSE=2.0889 RMSE=2.0905 Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 140, in <module>\n",
      "    _, metric_te = trainer.fit_and_test(loader_tr[i], loader_va[i], loader_te[i], log_train_results=args.log_train_results,\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 181, in _train_test_reg\n",
      "    self.model.load_state_dict(torch.load(save_path))\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for GSA:\n",
      "\tMissing key(s) in state_dict: \"pools.0.sbtl_layer.bias\", \"pools.0.sbtl_layer.lin.weight\", \"pools.0.fbtl_layer.weight\", \"pools.0.fbtl_layer.bias\", \"pools.0.fusion.bias\", \"pools.0.fusion.lin.weight\", \"pools.1.sbtl_layer.bias\", \"pools.1.sbtl_layer.lin.weight\", \"pools.1.fbtl_layer.weight\", \"pools.1.fbtl_layer.bias\", \"pools.1.fusion.bias\", \"pools.1.fusion.lin.weight\", \"pools.2.sbtl_layer.bias\", \"pools.2.sbtl_layer.lin.weight\", \"pools.2.fbtl_layer.weight\", \"pools.2.fbtl_layer.bias\", \"pools.2.fusion.bias\", \"pools.2.fusion.lin.weight\". \n",
      "\tUnexpected key(s) in state_dict: \"pools.0.transform.lin_rel.weight\", \"pools.0.transform.lin_rel.bias\", \"pools.0.transform.lin_root.weight\", \"pools.0.pp_conv.lin_rel.weight\", \"pools.0.pp_conv.lin_rel.bias\", \"pools.0.pp_conv.lin_root.weight\", \"pools.0.np_conv.lin_rel.weight\", \"pools.0.np_conv.lin_rel.bias\", \"pools.0.np_conv.lin_root.weight\", \"pools.0.positive_pooling.lin_rel.weight\", \"pools.0.positive_pooling.lin_rel.bias\", \"pools.0.positive_pooling.lin_root.weight\", \"pools.0.negative_pooling.lin_rel.weight\", \"pools.0.negative_pooling.lin_rel.bias\", \"pools.0.negative_pooling.lin_root.weight\", \"pools.0.discriminator.fc1.weight\", \"pools.0.discriminator.fc1.bias\", \"pools.0.discriminator.fc2.weight\", \"pools.0.discriminator.fc2.bias\", \"pools.1.transform.lin_rel.weight\", \"pools.1.transform.lin_rel.bias\", \"pools.1.transform.lin_root.weight\", \"pools.1.pp_conv.lin_rel.weight\", \"pools.1.pp_conv.lin_rel.bias\", \"pools.1.pp_conv.lin_root.weight\", \"pools.1.np_conv.lin_rel.weight\", \"pools.1.np_conv.lin_rel.bias\", \"pools.1.np_conv.lin_root.weight\", \"pools.1.positive_pooling.lin_rel.weight\", \"pools.1.positive_pooling.lin_rel.bias\", \"pools.1.positive_pooling.lin_root.weight\", \"pools.1.negative_pooling.lin_rel.weight\", \"pools.1.negative_pooling.lin_rel.bias\", \"pools.1.negative_pooling.lin_root.weight\", \"pools.1.discriminator.fc1.weight\", \"pools.1.discriminator.fc1.bias\", \"pools.1.discriminator.fc2.weight\", \"pools.1.discriminator.fc2.bias\", \"pools.2.transform.lin_rel.weight\", \"pools.2.transform.lin_rel.bias\", \"pools.2.transform.lin_root.weight\", \"pools.2.pp_conv.lin_rel.weight\", \"pools.2.pp_conv.lin_rel.bias\", \"pools.2.pp_conv.lin_root.weight\", \"pools.2.np_conv.lin_rel.weight\", \"pools.2.np_conv.lin_rel.bias\", \"pools.2.np_conv.lin_root.weight\", \"pools.2.positive_pooling.lin_rel.weight\", \"pools.2.positive_pooling.lin_rel.bias\", \"pools.2.positive_pooling.lin_root.weight\", \"pools.2.negative_pooling.lin_rel.weight\", \"pools.2.negative_pooling.lin_rel.bias\", \"pools.2.negative_pooling.lin_root.weight\", \"pools.2.discriminator.fc1.weight\", \"pools.2.discriminator.fc1.bias\", \"pools.2.discriminator.fc2.weight\", \"pools.2.discriminator.fc2.bias\". \n",
      "++++++++++++++++++++++0.3++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.2649 RMSE=4.9608 RMSE=3.9493 RMSE=2.5879 RMSE=2.1645 RMSE=1.8539 RMSE=1.9364 RMSE=1.7893 RMSE=1.8249 Epoch: 10/150RMSE=1.8541 RMSE=1.6428 RMSE=1.5745 RMSE=1.5641 RMSE=1.6149 RMSE=1.6793 RMSE=1.9059 RMSE=2.0972 RMSE=1.6991 RMSE=1.6804 Epoch: 20/150RMSE=1.7022 RMSE=1.7171 RMSE=1.7114 RMSE=1.7251 RMSE=1.6759 RMSE=1.6556 RMSE=1.6485 RMSE=1.6676 RMSE=1.6804 RMSE=1.7011 Epoch: 30/150RMSE=1.7430 RMSE=1.7293 RMSE=1.7310 RMSE=1.7354 Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 140, in <module>\n",
      "    _, metric_te = trainer.fit_and_test(loader_tr[i], loader_va[i], loader_te[i], log_train_results=args.log_train_results,\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 181, in _train_test_reg\n",
      "    self.model.load_state_dict(torch.load(save_path))\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for GSA:\n",
      "\tMissing key(s) in state_dict: \"pools.0.sbtl_layer.bias\", \"pools.0.sbtl_layer.lin.weight\", \"pools.0.fbtl_layer.weight\", \"pools.0.fbtl_layer.bias\", \"pools.0.fusion.bias\", \"pools.0.fusion.lin.weight\", \"pools.1.sbtl_layer.bias\", \"pools.1.sbtl_layer.lin.weight\", \"pools.1.fbtl_layer.weight\", \"pools.1.fbtl_layer.bias\", \"pools.1.fusion.bias\", \"pools.1.fusion.lin.weight\", \"pools.2.sbtl_layer.bias\", \"pools.2.sbtl_layer.lin.weight\", \"pools.2.fbtl_layer.weight\", \"pools.2.fbtl_layer.bias\", \"pools.2.fusion.bias\", \"pools.2.fusion.lin.weight\". \n",
      "\tUnexpected key(s) in state_dict: \"pools.0.transform.lin_rel.weight\", \"pools.0.transform.lin_rel.bias\", \"pools.0.transform.lin_root.weight\", \"pools.0.pp_conv.lin_rel.weight\", \"pools.0.pp_conv.lin_rel.bias\", \"pools.0.pp_conv.lin_root.weight\", \"pools.0.np_conv.lin_rel.weight\", \"pools.0.np_conv.lin_rel.bias\", \"pools.0.np_conv.lin_root.weight\", \"pools.0.positive_pooling.lin_rel.weight\", \"pools.0.positive_pooling.lin_rel.bias\", \"pools.0.positive_pooling.lin_root.weight\", \"pools.0.negative_pooling.lin_rel.weight\", \"pools.0.negative_pooling.lin_rel.bias\", \"pools.0.negative_pooling.lin_root.weight\", \"pools.0.discriminator.fc1.weight\", \"pools.0.discriminator.fc1.bias\", \"pools.0.discriminator.fc2.weight\", \"pools.0.discriminator.fc2.bias\", \"pools.1.transform.lin_rel.weight\", \"pools.1.transform.lin_rel.bias\", \"pools.1.transform.lin_root.weight\", \"pools.1.pp_conv.lin_rel.weight\", \"pools.1.pp_conv.lin_rel.bias\", \"pools.1.pp_conv.lin_root.weight\", \"pools.1.np_conv.lin_rel.weight\", \"pools.1.np_conv.lin_rel.bias\", \"pools.1.np_conv.lin_root.weight\", \"pools.1.positive_pooling.lin_rel.weight\", \"pools.1.positive_pooling.lin_rel.bias\", \"pools.1.positive_pooling.lin_root.weight\", \"pools.1.negative_pooling.lin_rel.weight\", \"pools.1.negative_pooling.lin_rel.bias\", \"pools.1.negative_pooling.lin_root.weight\", \"pools.1.discriminator.fc1.weight\", \"pools.1.discriminator.fc1.bias\", \"pools.1.discriminator.fc2.weight\", \"pools.1.discriminator.fc2.bias\", \"pools.2.transform.lin_rel.weight\", \"pools.2.transform.lin_rel.bias\", \"pools.2.transform.lin_root.weight\", \"pools.2.pp_conv.lin_rel.weight\", \"pools.2.pp_conv.lin_rel.bias\", \"pools.2.pp_conv.lin_root.weight\", \"pools.2.np_conv.lin_rel.weight\", \"pools.2.np_conv.lin_rel.bias\", \"pools.2.np_conv.lin_root.weight\", \"pools.2.positive_pooling.lin_rel.weight\", \"pools.2.positive_pooling.lin_rel.bias\", \"pools.2.positive_pooling.lin_root.weight\", \"pools.2.negative_pooling.lin_rel.weight\", \"pools.2.negative_pooling.lin_rel.bias\", \"pools.2.negative_pooling.lin_root.weight\", \"pools.2.discriminator.fc1.weight\", \"pools.2.discriminator.fc1.bias\", \"pools.2.discriminator.fc2.weight\", \"pools.2.discriminator.fc2.bias\". \n",
      "++++++++++++++++++++++0.5++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.3032 RMSE=4.8550 RMSE=3.7826 RMSE=3.1037 RMSE=2.3869 RMSE=1.8651 RMSE=2.0297 RMSE=1.8929 RMSE=1.8095 Epoch: 10/150RMSE=2.1784 RMSE=1.6528 RMSE=1.5305 RMSE=1.6747 RMSE=1.9379 RMSE=1.8405 RMSE=1.6424 RMSE=1.6915 RMSE=1.5938 RMSE=1.5273 Epoch: 20/150RMSE=1.6800 RMSE=1.6781 RMSE=1.7558 RMSE=1.6498 RMSE=1.6202 RMSE=1.5947 RMSE=1.5810 RMSE=1.5533 RMSE=1.5946 RMSE=1.6046 Epoch: 30/150RMSE=1.6046 RMSE=1.5832 RMSE=1.5749 RMSE=1.5881 RMSE=1.6038 RMSE=1.5800 RMSE=1.5757 RMSE=1.5741 RMSE=1.5732 RMSE=1.5758 Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 140, in <module>\n",
      "    _, metric_te = trainer.fit_and_test(loader_tr[i], loader_va[i], loader_te[i], log_train_results=args.log_train_results,\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 181, in _train_test_reg\n",
      "    self.model.load_state_dict(torch.load(save_path))\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for GSA:\n",
      "\tMissing key(s) in state_dict: \"pools.0.sbtl_layer.bias\", \"pools.0.sbtl_layer.lin.weight\", \"pools.0.fbtl_layer.weight\", \"pools.0.fbtl_layer.bias\", \"pools.0.fusion.bias\", \"pools.0.fusion.lin.weight\", \"pools.1.sbtl_layer.bias\", \"pools.1.sbtl_layer.lin.weight\", \"pools.1.fbtl_layer.weight\", \"pools.1.fbtl_layer.bias\", \"pools.1.fusion.bias\", \"pools.1.fusion.lin.weight\", \"pools.2.sbtl_layer.bias\", \"pools.2.sbtl_layer.lin.weight\", \"pools.2.fbtl_layer.weight\", \"pools.2.fbtl_layer.bias\", \"pools.2.fusion.bias\", \"pools.2.fusion.lin.weight\". \n",
      "\tUnexpected key(s) in state_dict: \"pools.0.transform.lin_rel.weight\", \"pools.0.transform.lin_rel.bias\", \"pools.0.transform.lin_root.weight\", \"pools.0.pp_conv.lin_rel.weight\", \"pools.0.pp_conv.lin_rel.bias\", \"pools.0.pp_conv.lin_root.weight\", \"pools.0.np_conv.lin_rel.weight\", \"pools.0.np_conv.lin_rel.bias\", \"pools.0.np_conv.lin_root.weight\", \"pools.0.positive_pooling.lin_rel.weight\", \"pools.0.positive_pooling.lin_rel.bias\", \"pools.0.positive_pooling.lin_root.weight\", \"pools.0.negative_pooling.lin_rel.weight\", \"pools.0.negative_pooling.lin_rel.bias\", \"pools.0.negative_pooling.lin_root.weight\", \"pools.0.discriminator.fc1.weight\", \"pools.0.discriminator.fc1.bias\", \"pools.0.discriminator.fc2.weight\", \"pools.0.discriminator.fc2.bias\", \"pools.1.transform.lin_rel.weight\", \"pools.1.transform.lin_rel.bias\", \"pools.1.transform.lin_root.weight\", \"pools.1.pp_conv.lin_rel.weight\", \"pools.1.pp_conv.lin_rel.bias\", \"pools.1.pp_conv.lin_root.weight\", \"pools.1.np_conv.lin_rel.weight\", \"pools.1.np_conv.lin_rel.bias\", \"pools.1.np_conv.lin_root.weight\", \"pools.1.positive_pooling.lin_rel.weight\", \"pools.1.positive_pooling.lin_rel.bias\", \"pools.1.positive_pooling.lin_root.weight\", \"pools.1.negative_pooling.lin_rel.weight\", \"pools.1.negative_pooling.lin_rel.bias\", \"pools.1.negative_pooling.lin_root.weight\", \"pools.1.discriminator.fc1.weight\", \"pools.1.discriminator.fc1.bias\", \"pools.1.discriminator.fc2.weight\", \"pools.1.discriminator.fc2.bias\", \"pools.2.transform.lin_rel.weight\", \"pools.2.transform.lin_rel.bias\", \"pools.2.transform.lin_root.weight\", \"pools.2.pp_conv.lin_rel.weight\", \"pools.2.pp_conv.lin_rel.bias\", \"pools.2.pp_conv.lin_root.weight\", \"pools.2.np_conv.lin_rel.weight\", \"pools.2.np_conv.lin_rel.bias\", \"pools.2.np_conv.lin_root.weight\", \"pools.2.positive_pooling.lin_rel.weight\", \"pools.2.positive_pooling.lin_rel.bias\", \"pools.2.positive_pooling.lin_root.weight\", \"pools.2.negative_pooling.lin_rel.weight\", \"pools.2.negative_pooling.lin_rel.bias\", \"pools.2.negative_pooling.lin_root.weight\", \"pools.2.discriminator.fc1.weight\", \"pools.2.discriminator.fc1.bias\", \"pools.2.discriminator.fc2.weight\", \"pools.2.discriminator.fc2.bias\". \n",
      "++++++++++++++++++++++0.7++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=4.9507 RMSE=4.7178 RMSE=3.8400 RMSE=2.9608 RMSE=2.9652 RMSE=1.6098 RMSE=2.2844 RMSE=1.4750 RMSE=1.7592 Epoch: 10/150RMSE=1.4142 RMSE=1.5238 RMSE=1.6696 RMSE=1.4566 RMSE=1.4165 RMSE=1.5373 RMSE=1.4724 RMSE=1.3623 RMSE=1.3867 RMSE=1.3874 Epoch: 20/150RMSE=1.4264 RMSE=1.3977 RMSE=1.3669 RMSE=1.4022 RMSE=1.3235 RMSE=1.3901 RMSE=1.4190 RMSE=1.4074 RMSE=1.3909 RMSE=1.4030 Epoch: 30/150RMSE=1.4406 RMSE=1.4160 RMSE=1.4470 RMSE=1.4214 RMSE=1.4312 RMSE=1.4351 RMSE=1.4297 RMSE=1.4260 RMSE=1.4272 RMSE=1.4283 Epoch: 40/150RMSE=1.4244 RMSE=1.4230 RMSE=1.4221 RMSE=1.4195 RMSE=1.4193 RMSE=1.5851 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.585 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=4.9765 RMSE=4.4839 RMSE=3.4665 RMSE=3.1574 RMSE=2.5955 RMSE=2.5610 RMSE=2.0744 RMSE=2.2568 RMSE=2.1993 Epoch: 10/150RMSE=2.0768 RMSE=2.2972 RMSE=2.1648 RMSE=2.1106 RMSE=1.9286 RMSE=1.8492 RMSE=1.9977 RMSE=1.9431 RMSE=1.8176 RMSE=1.9261 Epoch: 20/150RMSE=2.0812 RMSE=1.9078 RMSE=2.0652 RMSE=1.9085 RMSE=2.0058 RMSE=2.0204 RMSE=1.9004 RMSE=1.9637 RMSE=1.9569 RMSE=1.9707 Epoch: 30/150RMSE=1.9855 RMSE=1.9668 RMSE=1.9750 RMSE=1.9783 RMSE=1.9689 RMSE=1.9639 RMSE=1.9538 RMSE=1.9539 RMSE=1.9593 RMSE=1.9009 \n",
      "********************1's fold 2's run over********************\n",
      "RMSE: 1.743 +/- 0.158\n",
      "\n",
      "Epoch: 1/150RMSE=4.6355 RMSE=4.0278 RMSE=3.5009 RMSE=2.7596 RMSE=2.3353 RMSE=2.4496 RMSE=2.3344 RMSE=2.3253 RMSE=2.1849 Epoch: 10/150RMSE=2.0869 RMSE=2.2041 RMSE=2.5138 RMSE=2.1322 RMSE=2.0141 RMSE=2.8111 RMSE=2.4332 RMSE=2.0545 RMSE=1.8909 RMSE=2.1438 Epoch: 20/150RMSE=2.0668 RMSE=1.8917 RMSE=1.9974 RMSE=1.9355 RMSE=2.1860 RMSE=1.9971 RMSE=2.1867 RMSE=2.1596 RMSE=2.1363 RMSE=2.1536 Epoch: 30/150RMSE=2.2125 RMSE=2.2228 RMSE=2.1993 RMSE=2.2000 RMSE=2.1835 RMSE=2.2121 RMSE=2.2259 RMSE=2.2378 RMSE=2.2283 RMSE=2.2327 \n",
      "********************1's fold 3's run over********************\n",
      "RMSE: 1.906 +/- 0.264\n",
      "\n",
      "Epoch: 1/150RMSE=5.1824 RMSE=4.6519 RMSE=3.5329 RMSE=3.1371 RMSE=2.2915 RMSE=2.1807 RMSE=1.9409 RMSE=2.3640 RMSE=1.7706 Epoch: 10/150RMSE=1.7268 RMSE=1.7275 RMSE=1.8001 RMSE=1.6910 RMSE=1.6849 RMSE=1.6679 RMSE=1.7324 RMSE=1.7900 RMSE=1.7317 RMSE=1.7196 Epoch: 20/150RMSE=1.5909 RMSE=1.6479 RMSE=1.5979 RMSE=1.5179 RMSE=1.5338 RMSE=1.5638 RMSE=1.5576 RMSE=1.6603 RMSE=1.5743 RMSE=1.5665 Epoch: 30/150RMSE=1.5613 RMSE=1.5625 RMSE=1.5649 RMSE=1.5587 RMSE=1.5578 RMSE=1.5819 RMSE=1.5765 RMSE=1.5801 RMSE=1.5892 RMSE=1.5921 Epoch: 40/150RMSE=1.6038 RMSE=1.6138 RMSE=1.6126 RMSE=1.6097 Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 140, in <module>\n",
      "    _, metric_te = trainer.fit_and_test(loader_tr[i], loader_va[i], loader_te[i], log_train_results=args.log_train_results,\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 181, in _train_test_reg\n",
      "    self.model.load_state_dict(torch.load(save_path))\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2152, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for GSA:\n",
      "\tMissing key(s) in state_dict: \"pools.0.sbtl_layer.bias\", \"pools.0.sbtl_layer.lin.weight\", \"pools.0.fbtl_layer.weight\", \"pools.0.fbtl_layer.bias\", \"pools.0.fusion.bias\", \"pools.0.fusion.lin.weight\", \"pools.1.sbtl_layer.bias\", \"pools.1.sbtl_layer.lin.weight\", \"pools.1.fbtl_layer.weight\", \"pools.1.fbtl_layer.bias\", \"pools.1.fusion.bias\", \"pools.1.fusion.lin.weight\", \"pools.2.sbtl_layer.bias\", \"pools.2.sbtl_layer.lin.weight\", \"pools.2.fbtl_layer.weight\", \"pools.2.fbtl_layer.bias\", \"pools.2.fusion.bias\", \"pools.2.fusion.lin.weight\". \n",
      "\tUnexpected key(s) in state_dict: \"pools.0.transform.lin_rel.weight\", \"pools.0.transform.lin_rel.bias\", \"pools.0.transform.lin_root.weight\", \"pools.0.pp_conv.lin_rel.weight\", \"pools.0.pp_conv.lin_rel.bias\", \"pools.0.pp_conv.lin_root.weight\", \"pools.0.np_conv.lin_rel.weight\", \"pools.0.np_conv.lin_rel.bias\", \"pools.0.np_conv.lin_root.weight\", \"pools.0.positive_pooling.lin_rel.weight\", \"pools.0.positive_pooling.lin_rel.bias\", \"pools.0.positive_pooling.lin_root.weight\", \"pools.0.negative_pooling.lin_rel.weight\", \"pools.0.negative_pooling.lin_rel.bias\", \"pools.0.negative_pooling.lin_root.weight\", \"pools.0.discriminator.fc1.weight\", \"pools.0.discriminator.fc1.bias\", \"pools.0.discriminator.fc2.weight\", \"pools.0.discriminator.fc2.bias\", \"pools.1.transform.lin_rel.weight\", \"pools.1.transform.lin_rel.bias\", \"pools.1.transform.lin_root.weight\", \"pools.1.pp_conv.lin_rel.weight\", \"pools.1.pp_conv.lin_rel.bias\", \"pools.1.pp_conv.lin_root.weight\", \"pools.1.np_conv.lin_rel.weight\", \"pools.1.np_conv.lin_rel.bias\", \"pools.1.np_conv.lin_root.weight\", \"pools.1.positive_pooling.lin_rel.weight\", \"pools.1.positive_pooling.lin_rel.bias\", \"pools.1.positive_pooling.lin_root.weight\", \"pools.1.negative_pooling.lin_rel.weight\", \"pools.1.negative_pooling.lin_rel.bias\", \"pools.1.negative_pooling.lin_root.weight\", \"pools.1.discriminator.fc1.weight\", \"pools.1.discriminator.fc1.bias\", \"pools.1.discriminator.fc2.weight\", \"pools.1.discriminator.fc2.bias\", \"pools.2.transform.lin_rel.weight\", \"pools.2.transform.lin_rel.bias\", \"pools.2.transform.lin_root.weight\", \"pools.2.pp_conv.lin_rel.weight\", \"pools.2.pp_conv.lin_rel.bias\", \"pools.2.pp_conv.lin_root.weight\", \"pools.2.np_conv.lin_rel.weight\", \"pools.2.np_conv.lin_rel.bias\", \"pools.2.np_conv.lin_root.weight\", \"pools.2.positive_pooling.lin_rel.weight\", \"pools.2.positive_pooling.lin_rel.bias\", \"pools.2.positive_pooling.lin_root.weight\", \"pools.2.negative_pooling.lin_rel.weight\", \"pools.2.negative_pooling.lin_rel.bias\", \"pools.2.negative_pooling.lin_root.weight\", \"pools.2.discriminator.fc1.weight\", \"pools.2.discriminator.fc1.bias\", \"pools.2.discriminator.fc2.weight\", \"pools.2.discriminator.fc2.bias\". \n",
      "++++++++++++++++++++++0.9++++++++++++++++++++++++\n",
      "Existed dataset loaded: datasets/processed/freesolv.pt\n",
      "\n",
      "Current dataset: freesolv, include 639 molecules and 1 regression tasks\n",
      "\n",
      "Splitting, finish 1/1  \n",
      "Epoch: 1/150RMSE=5.1921 RMSE=4.8095 RMSE=3.8138 RMSE=2.5820 RMSE=1.9198 RMSE=1.7940 RMSE=1.6379 RMSE=1.8236 RMSE=1.6723 Epoch: 10/150RMSE=1.6411 RMSE=1.5329 RMSE=1.5814 RMSE=1.6042 RMSE=1.6865 RMSE=1.5198 RMSE=1.6211 RMSE=1.4581 RMSE=1.5504 RMSE=1.7020 Epoch: 20/150RMSE=1.8472 RMSE=1.4813 RMSE=1.3005 RMSE=1.4346 RMSE=1.4131 RMSE=1.3534 RMSE=1.3944 RMSE=1.4001 RMSE=1.3946 RMSE=1.4032 Epoch: 30/150RMSE=1.3502 RMSE=1.3861 RMSE=1.3913 RMSE=1.3861 RMSE=1.4186 RMSE=1.3822 RMSE=1.3708 RMSE=1.3899 RMSE=1.3934 RMSE=1.3861 Epoch: 40/150RMSE=1.3838 RMSE=1.3853 RMSE=1.3881 RMSE=1.9149 \n",
      "********************1's fold 1's run over********************\n",
      "RMSE: 1.915 +/- 0.000\n",
      "\n",
      "Epoch: 1/150RMSE=5.1564 RMSE=4.6556 RMSE=3.5244 RMSE=2.5529 RMSE=2.4308 RMSE=2.2445 RMSE=2.1186 RMSE=1.8085 RMSE=1.9304 Epoch: 10/150RMSE=1.9221 RMSE=1.8123 RMSE=1.8120 RMSE=1.4584 RMSE=1.4799 RMSE=2.0814 RMSE=1.2547 RMSE=1.3181 RMSE=1.3345 RMSE=1.3102 Epoch: 20/150RMSE=1.3394 RMSE=1.3237 RMSE=1.2793 RMSE=1.2269 RMSE=1.3255 RMSE=1.4811 RMSE=1.5428 RMSE=1.6751 RMSE=1.2756 RMSE=1.3851 Epoch: 30/150RMSE=1.3894 RMSE=1.4419 RMSE=1.3444 RMSE=1.4235 RMSE=1.5464 RMSE=1.5833 RMSE=1.6362 RMSE=1.5229 RMSE=1.4055 RMSE=1.4115 Epoch: 40/150RMSE=1.4046 RMSE=1.4711 ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py\", line 140, in <module>\n",
      "    _, metric_te = trainer.fit_and_test(loader_tr[i], loader_va[i], loader_te[i], log_train_results=args.log_train_results,\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/process/trainer.py\", line 137, in _train_test_reg\n",
      "    pred = self.model(data)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/models/baseline.py\", line 154, in forward\n",
      "    embds = self.GNN(data)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/models/baseline.py\", line 134, in GNN\n",
      "    x, edge_index, batch = self.step_pool(self.pools[i], x, edge_index, batch)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/models/baseline.py\", line 1462, in step_pool\n",
      "    x, edge_index, _, batch, perm_1, x_ae1 = pool(x, edge_index, None, batch)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/models/baseline.py\", line 1413, in forward\n",
      "    score_s = self.sbtl_layer(x,edge_index).squeeze()\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py\", line 241, in forward\n",
      "    edge_index, edge_weight = gcn_norm(  # yapf: disable\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py\", line 99, in gcn_norm\n",
      "    edge_index, edge_weight = add_remaining_self_loops(\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch_geometric/utils/loop.py\", line 651, in add_remaining_self_loops\n",
      "    if not torch.jit.is_scripting() and isinstance(edge_index, EdgeIndex):\n",
      "  File \"/home/boot/anaconda3/envs/zeyu1/lib/python3.10/site-packages/torch/_jit_internal.py\", line 1109, in is_scripting\n",
      "    def is_scripting() -> bool:\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python -W ignore  /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.1 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python -W ignore  /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.3 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python -W ignore  /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.5 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python -W ignore  /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.7 --pooling='GSA'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python -W ignore  /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=freesolv --run_times=5 --patience=20 --epochs=150 --gsa_ratio=0.9 --pooling='GSA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipophilicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"++++++++++++++++++++++0.1++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.1 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.3++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.3 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.5++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.5 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.7++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.7 --pooling='CGI'\n",
    "print(\"++++++++++++++++++++++0.9++++++++++++++++++++++++\")\n",
    "!python /data/Zeyu/Pooling/Graph_Pooling_Benchmark/Regression/run_regression.py --dataset=lipo --run_times=5 --patience=20 --epochs=150 --cgi_ratio=0.9 --pooling='CGI'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CG-ODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
